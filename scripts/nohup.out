Run this job and save the output to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/25] time 0.068 (0.292) data 0.000 (0.180) loss 2.5117 (2.4188) acc 78.1250 (77.5000) lr 1.0000e-05 eta 0:06:03
epoch [1/50] batch [10/25] time 0.072 (0.181) data 0.000 (0.090) loss 2.0527 (2.4033) acc 84.3750 (77.8125) lr 1.0000e-05 eta 0:03:44
epoch [1/50] batch [15/25] time 0.070 (0.144) data 0.000 (0.060) loss 2.3906 (2.4307) acc 75.0000 (77.0833) lr 1.0000e-05 eta 0:02:58
epoch [1/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.045) loss 2.1133 (2.3775) acc 75.0000 (76.8750) lr 1.0000e-05 eta 0:02:34
epoch [1/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.036) loss 2.5312 (2.3879) acc 78.1250 (77.0000) lr 2.0000e-03 eta 0:02:20
epoch [2/50] batch [5/25] time 0.069 (0.224) data 0.000 (0.155) loss 1.3232 (1.3891) acc 81.2500 (83.7500) lr 2.0000e-03 eta 0:04:33
epoch [2/50] batch [10/25] time 0.069 (0.146) data 0.000 (0.077) loss 1.4287 (1.2224) acc 78.1250 (84.6875) lr 2.0000e-03 eta 0:02:57
epoch [2/50] batch [15/25] time 0.068 (0.120) data 0.000 (0.052) loss 0.8623 (1.0999) acc 84.3750 (84.1667) lr 2.0000e-03 eta 0:02:25
epoch [2/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.039) loss 0.6924 (1.0236) acc 81.2500 (84.2188) lr 2.0000e-03 eta 0:02:09
epoch [2/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.031) loss 1.0918 (1.0061) acc 75.0000 (82.8750) lr 1.9980e-03 eta 0:01:59
epoch [3/50] batch [5/25] time 0.069 (0.217) data 0.000 (0.146) loss 1.0635 (0.7016) acc 81.2500 (88.1250) lr 1.9980e-03 eta 0:04:18
epoch [3/50] batch [10/25] time 0.069 (0.143) data 0.000 (0.073) loss 1.0645 (0.6950) acc 81.2500 (87.1875) lr 1.9980e-03 eta 0:02:49
epoch [3/50] batch [15/25] time 0.068 (0.118) data 0.000 (0.049) loss 1.3125 (0.7339) acc 68.7500 (85.6250) lr 1.9980e-03 eta 0:02:19
epoch [3/50] batch [20/25] time 0.069 (0.106) data 0.000 (0.037) loss 0.4417 (0.7266) acc 90.6250 (85.0000) lr 1.9980e-03 eta 0:02:04
epoch [3/50] batch [25/25] time 0.069 (0.098) data 0.000 (0.029) loss 0.5771 (0.6901) acc 90.6250 (85.8750) lr 1.9921e-03 eta 0:01:55
epoch [4/50] batch [5/25] time 0.071 (0.218) data 0.000 (0.146) loss 0.7676 (0.6768) acc 84.3750 (82.5000) lr 1.9921e-03 eta 0:04:14
epoch [4/50] batch [10/25] time 0.070 (0.144) data 0.000 (0.073) loss 0.5752 (0.6318) acc 84.3750 (84.6875) lr 1.9921e-03 eta 0:02:47
epoch [4/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.7080 (0.6184) acc 84.3750 (84.5833) lr 1.9921e-03 eta 0:02:18
epoch [4/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.037) loss 0.4050 (0.6190) acc 93.7500 (84.5312) lr 1.9921e-03 eta 0:02:03
epoch [4/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.4504 (0.5955) acc 87.5000 (84.8750) lr 1.9823e-03 eta 0:01:53
epoch [5/50] batch [5/25] time 0.069 (0.220) data 0.000 (0.146) loss 0.2209 (0.5676) acc 93.7500 (84.3750) lr 1.9823e-03 eta 0:04:11
epoch [5/50] batch [10/25] time 0.069 (0.144) data 0.000 (0.073) loss 0.5576 (0.4857) acc 84.3750 (86.2500) lr 1.9823e-03 eta 0:02:44
epoch [5/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.8516 (0.5339) acc 78.1250 (85.2083) lr 1.9823e-03 eta 0:02:15
epoch [5/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.037) loss 0.5459 (0.5180) acc 90.6250 (86.4062) lr 1.9823e-03 eta 0:02:00
epoch [5/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.6729 (0.5306) acc 81.2500 (86.5000) lr 1.9686e-03 eta 0:01:51
epoch [6/50] batch [5/25] time 0.069 (0.219) data 0.000 (0.146) loss 0.4104 (0.5706) acc 93.7500 (86.8750) lr 1.9686e-03 eta 0:04:04
epoch [6/50] batch [10/25] time 0.069 (0.144) data 0.000 (0.073) loss 0.3997 (0.5727) acc 87.5000 (85.9375) lr 1.9686e-03 eta 0:02:40
epoch [6/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.6445 (0.5878) acc 81.2500 (85.0000) lr 1.9686e-03 eta 0:02:12
epoch [6/50] batch [20/25] time 0.069 (0.106) data 0.000 (0.037) loss 0.2974 (0.5768) acc 96.8750 (85.7812) lr 1.9686e-03 eta 0:01:57
epoch [6/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.3877 (0.5613) acc 93.7500 (86.5000) lr 1.9511e-03 eta 0:01:48
epoch [7/50] batch [5/25] time 0.069 (0.220) data 0.000 (0.151) loss 0.3977 (0.4250) acc 93.7500 (91.2500) lr 1.9511e-03 eta 0:04:01
epoch [7/50] batch [10/25] time 0.069 (0.145) data 0.000 (0.076) loss 0.2102 (0.4531) acc 100.0000 (90.0000) lr 1.9511e-03 eta 0:02:37
epoch [7/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.050) loss 0.3086 (0.4392) acc 96.8750 (90.2083) lr 1.9511e-03 eta 0:02:09
epoch [7/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.038) loss 0.5986 (0.4797) acc 90.6250 (88.9062) lr 1.9511e-03 eta 0:01:55
epoch [7/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.030) loss 0.8408 (0.5125) acc 78.1250 (88.1250) lr 1.9298e-03 eta 0:01:46
epoch [8/50] batch [5/25] time 0.069 (0.224) data 0.000 (0.152) loss 0.5439 (0.4875) acc 84.3750 (86.8750) lr 1.9298e-03 eta 0:03:59
epoch [8/50] batch [10/25] time 0.071 (0.147) data 0.000 (0.076) loss 0.6396 (0.5317) acc 81.2500 (85.6250) lr 1.9298e-03 eta 0:02:36
epoch [8/50] batch [15/25] time 0.069 (0.121) data 0.000 (0.051) loss 0.6230 (0.5016) acc 81.2500 (86.4583) lr 1.9298e-03 eta 0:02:08
epoch [8/50] batch [20/25] time 0.069 (0.108) data 0.000 (0.038) loss 0.4097 (0.4969) acc 93.7500 (86.8750) lr 1.9298e-03 eta 0:01:54
epoch [8/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.031) loss 0.7744 (0.5039) acc 71.8750 (86.3750) lr 1.9048e-03 eta 0:01:45
epoch [9/50] batch [5/25] time 0.069 (0.218) data 0.000 (0.146) loss 0.3523 (0.5130) acc 87.5000 (86.2500) lr 1.9048e-03 eta 0:03:47
epoch [9/50] batch [10/25] time 0.072 (0.144) data 0.000 (0.073) loss 0.6230 (0.5430) acc 84.3750 (85.6250) lr 1.9048e-03 eta 0:02:29
epoch [9/50] batch [15/25] time 0.070 (0.119) data 0.000 (0.049) loss 0.4434 (0.5272) acc 90.6250 (86.4583) lr 1.9048e-03 eta 0:02:03
epoch [9/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.037) loss 0.4160 (0.5141) acc 93.7500 (86.5625) lr 1.9048e-03 eta 0:01:50
epoch [9/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.5244 (0.5259) acc 90.6250 (86.3750) lr 1.8763e-03 eta 0:01:41
epoch [10/50] batch [5/25] time 0.069 (0.236) data 0.000 (0.161) loss 0.6724 (0.5324) acc 84.3750 (87.5000) lr 1.8763e-03 eta 0:04:00
epoch [10/50] batch [10/25] time 0.069 (0.153) data 0.000 (0.080) loss 0.8599 (0.5625) acc 81.2500 (87.1875) lr 1.8763e-03 eta 0:02:35
epoch [10/50] batch [15/25] time 0.069 (0.125) data 0.000 (0.054) loss 0.2107 (0.5268) acc 100.0000 (88.1250) lr 1.8763e-03 eta 0:02:06
epoch [10/50] batch [20/25] time 0.069 (0.111) data 0.000 (0.040) loss 0.5820 (0.5180) acc 84.3750 (87.9688) lr 1.8763e-03 eta 0:01:51
epoch [10/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.6895 (0.5408) acc 81.2500 (87.0000) lr 1.8443e-03 eta 0:01:42
epoch [11/50] batch [5/25] time 0.070 (0.241) data 0.000 (0.170) loss 0.6621 (0.6213) acc 84.3750 (83.1250) lr 1.8443e-03 eta 0:04:00
epoch [11/50] batch [10/25] time 0.069 (0.155) data 0.000 (0.085) loss 0.8271 (0.5974) acc 71.8750 (83.1250) lr 1.8443e-03 eta 0:02:33
epoch [11/50] batch [15/25] time 0.070 (0.127) data 0.000 (0.057) loss 0.5508 (0.5705) acc 87.5000 (84.3750) lr 1.8443e-03 eta 0:02:04
epoch [11/50] batch [20/25] time 0.069 (0.112) data 0.000 (0.043) loss 0.2666 (0.5488) acc 93.7500 (85.3125) lr 1.8443e-03 eta 0:01:50
epoch [11/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.2854 (0.5151) acc 93.7500 (86.0000) lr 1.8090e-03 eta 0:01:41
epoch [12/50] batch [5/25] time 0.069 (0.221) data 0.000 (0.146) loss 0.7153 (0.6224) acc 78.1250 (83.1250) lr 1.8090e-03 eta 0:03:33
epoch [12/50] batch [10/25] time 0.069 (0.145) data 0.000 (0.073) loss 0.3496 (0.6422) acc 90.6250 (84.0625) lr 1.8090e-03 eta 0:02:20
epoch [12/50] batch [15/25] time 0.069 (0.120) data 0.000 (0.049) loss 0.3943 (0.5819) acc 93.7500 (84.5833) lr 1.8090e-03 eta 0:01:54
epoch [12/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.037) loss 0.8276 (0.5787) acc 78.1250 (84.8438) lr 1.8090e-03 eta 0:01:42
epoch [12/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.029) loss 0.5347 (0.5664) acc 90.6250 (85.3750) lr 1.7705e-03 eta 0:01:34
epoch [13/50] batch [5/25] time 0.069 (0.220) data 0.000 (0.150) loss 0.3811 (0.5740) acc 93.7500 (86.2500) lr 1.7705e-03 eta 0:03:28
epoch [13/50] batch [10/25] time 0.069 (0.145) data 0.000 (0.075) loss 0.4424 (0.5389) acc 93.7500 (87.1875) lr 1.7705e-03 eta 0:02:16
epoch [13/50] batch [15/25] time 0.069 (0.120) data 0.000 (0.050) loss 0.4539 (0.4858) acc 90.6250 (88.7500) lr 1.7705e-03 eta 0:01:51
epoch [13/50] batch [20/25] time 0.069 (0.107) data 0.000 (0.038) loss 0.6562 (0.4847) acc 78.1250 (87.9688) lr 1.7705e-03 eta 0:01:39
epoch [13/50] batch [25/25] time 0.070 (0.099) data 0.000 (0.030) loss 0.4934 (0.5021) acc 90.6250 (87.7500) lr 1.7290e-03 eta 0:01:32
epoch [14/50] batch [5/25] time 0.069 (0.213) data 0.000 (0.141) loss 0.5508 (0.4804) acc 84.3750 (88.1250) lr 1.7290e-03 eta 0:03:15
epoch [14/50] batch [10/25] time 0.069 (0.141) data 0.000 (0.071) loss 0.4238 (0.4582) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:02:09
epoch [14/50] batch [15/25] time 0.070 (0.117) data 0.000 (0.047) loss 0.6914 (0.5312) acc 75.0000 (85.8333) lr 1.7290e-03 eta 0:01:46
epoch [14/50] batch [20/25] time 0.069 (0.105) data 0.000 (0.036) loss 0.4275 (0.5471) acc 93.7500 (85.4688) lr 1.7290e-03 eta 0:01:35
epoch [14/50] batch [25/25] time 0.069 (0.098) data 0.000 (0.028) loss 0.2471 (0.5409) acc 93.7500 (85.8750) lr 1.6845e-03 eta 0:01:28
epoch [15/50] batch [5/25] time 0.071 (0.240) data 0.000 (0.168) loss 0.5410 (0.6170) acc 84.3750 (82.5000) lr 1.6845e-03 eta 0:03:34
epoch [15/50] batch [10/25] time 0.070 (0.155) data 0.000 (0.084) loss 0.4688 (0.5840) acc 87.5000 (84.3750) lr 1.6845e-03 eta 0:02:18
epoch [15/50] batch [15/25] time 0.069 (0.127) data 0.000 (0.056) loss 0.3027 (0.5578) acc 96.8750 (85.8333) lr 1.6845e-03 eta 0:01:52
epoch [15/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.042) loss 0.6206 (0.5197) acc 81.2500 (86.8750) lr 1.6845e-03 eta 0:01:38
epoch [15/50] batch [25/25] time 0.069 (0.104) data 0.000 (0.034) loss 0.7490 (0.5383) acc 90.6250 (87.0000) lr 1.6374e-03 eta 0:01:30
epoch [16/50] batch [5/25] time 0.070 (0.229) data 0.000 (0.159) loss 0.2423 (0.5149) acc 96.8750 (85.6250) lr 1.6374e-03 eta 0:03:19
epoch [16/50] batch [10/25] time 0.069 (0.149) data 0.000 (0.080) loss 0.6743 (0.5479) acc 84.3750 (86.2500) lr 1.6374e-03 eta 0:02:09
epoch [16/50] batch [15/25] time 0.069 (0.123) data 0.000 (0.053) loss 0.3413 (0.5076) acc 93.7500 (86.8750) lr 1.6374e-03 eta 0:01:45
epoch [16/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.040) loss 0.2737 (0.4986) acc 93.7500 (87.1875) lr 1.6374e-03 eta 0:01:33
epoch [16/50] batch [25/25] time 0.069 (0.101) data 0.000 (0.032) loss 0.3904 (0.4776) acc 90.6250 (88.0000) lr 1.5878e-03 eta 0:01:26
epoch [17/50] batch [5/25] time 0.069 (0.214) data 0.000 (0.142) loss 0.3918 (0.5427) acc 93.7500 (87.5000) lr 1.5878e-03 eta 0:03:01
epoch [17/50] batch [10/25] time 0.070 (0.142) data 0.000 (0.071) loss 0.8813 (0.5508) acc 81.2500 (87.5000) lr 1.5878e-03 eta 0:01:59
epoch [17/50] batch [15/25] time 0.070 (0.118) data 0.000 (0.047) loss 0.4128 (0.5006) acc 90.6250 (88.5417) lr 1.5878e-03 eta 0:01:38
epoch [17/50] batch [20/25] time 0.070 (0.106) data 0.000 (0.036) loss 0.4478 (0.4984) acc 93.7500 (88.4375) lr 1.5878e-03 eta 0:01:27
epoch [17/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.029) loss 0.5757 (0.5090) acc 78.1250 (87.8750) lr 1.5358e-03 eta 0:01:21
epoch [18/50] batch [5/25] time 0.069 (0.213) data 0.000 (0.143) loss 0.4727 (0.5255) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:02:54
epoch [18/50] batch [10/25] time 0.069 (0.141) data 0.000 (0.071) loss 0.5059 (0.5274) acc 90.6250 (85.9375) lr 1.5358e-03 eta 0:01:54
epoch [18/50] batch [15/25] time 0.069 (0.117) data 0.000 (0.048) loss 0.6060 (0.5647) acc 81.2500 (84.5833) lr 1.5358e-03 eta 0:01:34
epoch [18/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.036) loss 0.6548 (0.5488) acc 87.5000 (85.3125) lr 1.5358e-03 eta 0:01:24
epoch [18/50] batch [25/25] time 0.071 (0.098) data 0.000 (0.029) loss 0.3975 (0.5281) acc 84.3750 (85.8750) lr 1.4818e-03 eta 0:01:18
epoch [19/50] batch [5/25] time 0.071 (0.223) data 0.000 (0.151) loss 0.2983 (0.4238) acc 93.7500 (90.6250) lr 1.4818e-03 eta 0:02:57
epoch [19/50] batch [10/25] time 0.069 (0.147) data 0.000 (0.075) loss 0.3323 (0.4424) acc 93.7500 (89.0625) lr 1.4818e-03 eta 0:01:55
epoch [19/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.050) loss 0.9004 (0.4754) acc 81.2500 (89.1667) lr 1.4818e-03 eta 0:01:34
epoch [19/50] batch [20/25] time 0.069 (0.108) data 0.000 (0.038) loss 0.4734 (0.4813) acc 90.6250 (89.3750) lr 1.4818e-03 eta 0:01:24
epoch [19/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.030) loss 0.6152 (0.4718) acc 81.2500 (89.1250) lr 1.4258e-03 eta 0:01:17
epoch [20/50] batch [5/25] time 0.069 (0.224) data 0.000 (0.151) loss 0.8296 (0.4306) acc 78.1250 (90.6250) lr 1.4258e-03 eta 0:02:52
epoch [20/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.076) loss 0.3550 (0.4397) acc 93.7500 (90.0000) lr 1.4258e-03 eta 0:01:52
epoch [20/50] batch [15/25] time 0.069 (0.121) data 0.000 (0.051) loss 0.3203 (0.4667) acc 90.6250 (88.3333) lr 1.4258e-03 eta 0:01:31
epoch [20/50] batch [20/25] time 0.073 (0.108) data 0.000 (0.038) loss 0.8740 (0.4822) acc 78.1250 (88.1250) lr 1.4258e-03 eta 0:01:21
epoch [20/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.030) loss 0.6621 (0.4787) acc 81.2500 (88.3750) lr 1.3681e-03 eta 0:01:15
epoch [21/50] batch [5/25] time 0.070 (0.272) data 0.000 (0.200) loss 0.1981 (0.3677) acc 93.7500 (92.5000) lr 1.3681e-03 eta 0:03:22
epoch [21/50] batch [10/25] time 0.069 (0.171) data 0.000 (0.100) loss 0.3928 (0.4273) acc 93.7500 (90.9375) lr 1.3681e-03 eta 0:02:06
epoch [21/50] batch [15/25] time 0.070 (0.137) data 0.000 (0.067) loss 0.6543 (0.4202) acc 81.2500 (91.0417) lr 1.3681e-03 eta 0:01:40
epoch [21/50] batch [20/25] time 0.069 (0.120) data 0.000 (0.050) loss 0.4641 (0.4380) acc 93.7500 (90.7812) lr 1.3681e-03 eta 0:01:27
epoch [21/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.6074 (0.4439) acc 81.2500 (89.7500) lr 1.3090e-03 eta 0:01:19
epoch [22/50] batch [5/25] time 0.069 (0.220) data 0.000 (0.149) loss 0.3425 (0.6266) acc 90.6250 (82.5000) lr 1.3090e-03 eta 0:02:38
epoch [22/50] batch [10/25] time 0.069 (0.145) data 0.000 (0.074) loss 0.3850 (0.5036) acc 84.3750 (86.2500) lr 1.3090e-03 eta 0:01:43
epoch [22/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.050) loss 0.4766 (0.4950) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:24
epoch [22/50] batch [20/25] time 0.071 (0.107) data 0.000 (0.037) loss 0.4397 (0.4754) acc 90.6250 (87.8125) lr 1.3090e-03 eta 0:01:15
epoch [22/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.030) loss 0.8184 (0.4702) acc 81.2500 (87.8750) lr 1.2487e-03 eta 0:01:09
epoch [23/50] batch [5/25] time 0.070 (0.247) data 0.000 (0.175) loss 0.5146 (0.3819) acc 90.6250 (93.1250) lr 1.2487e-03 eta 0:02:51
epoch [23/50] batch [10/25] time 0.070 (0.158) data 0.000 (0.088) loss 0.7627 (0.4884) acc 75.0000 (88.4375) lr 1.2487e-03 eta 0:01:49
epoch [23/50] batch [15/25] time 0.069 (0.129) data 0.000 (0.059) loss 0.7681 (0.4896) acc 81.2500 (88.3333) lr 1.2487e-03 eta 0:01:28
epoch [23/50] batch [20/25] time 0.069 (0.114) data 0.000 (0.044) loss 0.3433 (0.4896) acc 93.7500 (88.5938) lr 1.2487e-03 eta 0:01:17
epoch [23/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.4392 (0.5075) acc 90.6250 (88.1250) lr 1.1874e-03 eta 0:01:10
epoch [24/50] batch [5/25] time 0.070 (0.214) data 0.000 (0.142) loss 0.7354 (0.7055) acc 81.2500 (83.1250) lr 1.1874e-03 eta 0:02:23
epoch [24/50] batch [10/25] time 0.070 (0.142) data 0.000 (0.071) loss 0.4863 (0.6161) acc 93.7500 (85.0000) lr 1.1874e-03 eta 0:01:34
epoch [24/50] batch [15/25] time 0.070 (0.118) data 0.000 (0.048) loss 0.5327 (0.6088) acc 81.2500 (85.0000) lr 1.1874e-03 eta 0:01:17
epoch [24/50] batch [20/25] time 0.070 (0.106) data 0.000 (0.036) loss 0.3040 (0.5506) acc 96.8750 (87.0312) lr 1.1874e-03 eta 0:01:09
epoch [24/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.029) loss 0.8184 (0.5355) acc 78.1250 (87.5000) lr 1.1253e-03 eta 0:01:03
epoch [25/50] batch [5/25] time 0.069 (0.229) data 0.000 (0.158) loss 0.4602 (0.3697) acc 87.5000 (91.2500) lr 1.1253e-03 eta 0:02:27
epoch [25/50] batch [10/25] time 0.070 (0.149) data 0.000 (0.079) loss 0.6021 (0.4533) acc 84.3750 (89.0625) lr 1.1253e-03 eta 0:01:35
epoch [25/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.053) loss 0.4409 (0.4707) acc 87.5000 (89.1667) lr 1.1253e-03 eta 0:01:18
epoch [25/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.7285 (0.5024) acc 81.2500 (88.1250) lr 1.1253e-03 eta 0:01:09
epoch [25/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.4912 (0.4950) acc 87.5000 (88.0000) lr 1.0628e-03 eta 0:01:03
epoch [26/50] batch [5/25] time 0.069 (0.224) data 0.000 (0.154) loss 0.5405 (0.4571) acc 87.5000 (89.3750) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.077) loss 0.3730 (0.4737) acc 90.6250 (87.8125) lr 1.0628e-03 eta 0:01:30
epoch [26/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.052) loss 0.1272 (0.4268) acc 100.0000 (88.7500) lr 1.0628e-03 eta 0:01:13
epoch [26/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.039) loss 0.3809 (0.4450) acc 93.7500 (88.7500) lr 1.0628e-03 eta 0:01:05
epoch [26/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.031) loss 0.3762 (0.4329) acc 93.7500 (89.6250) lr 1.0000e-03 eta 0:01:00
epoch [27/50] batch [5/25] time 0.071 (0.220) data 0.000 (0.149) loss 0.3411 (0.3671) acc 90.6250 (92.5000) lr 1.0000e-03 eta 0:02:11
epoch [27/50] batch [10/25] time 0.070 (0.145) data 0.000 (0.075) loss 0.6309 (0.4086) acc 90.6250 (91.8750) lr 1.0000e-03 eta 0:01:25
epoch [27/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.050) loss 0.4290 (0.4407) acc 90.6250 (90.8333) lr 1.0000e-03 eta 0:01:10
epoch [27/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.3616 (0.4479) acc 93.7500 (90.6250) lr 1.0000e-03 eta 0:01:02
epoch [27/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.030) loss 0.5034 (0.4620) acc 90.6250 (90.2500) lr 9.3721e-04 eta 0:00:57
epoch [28/50] batch [5/25] time 0.070 (0.215) data 0.000 (0.142) loss 0.6162 (0.5090) acc 71.8750 (86.2500) lr 9.3721e-04 eta 0:02:02
epoch [28/50] batch [10/25] time 0.069 (0.142) data 0.000 (0.071) loss 0.2515 (0.4895) acc 96.8750 (88.1250) lr 9.3721e-04 eta 0:01:20
epoch [28/50] batch [15/25] time 0.070 (0.118) data 0.000 (0.048) loss 0.7881 (0.4702) acc 84.3750 (88.5417) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [20/25] time 0.072 (0.106) data 0.000 (0.036) loss 0.3872 (0.4397) acc 87.5000 (89.0625) lr 9.3721e-04 eta 0:00:58
epoch [28/50] batch [25/25] time 0.071 (0.099) data 0.000 (0.029) loss 0.3220 (0.4483) acc 93.7500 (88.6250) lr 8.7467e-04 eta 0:00:54
epoch [29/50] batch [5/25] time 0.069 (0.206) data 0.000 (0.136) loss 0.3774 (0.4759) acc 93.7500 (88.7500) lr 8.7467e-04 eta 0:01:52
epoch [29/50] batch [10/25] time 0.070 (0.138) data 0.000 (0.068) loss 0.7974 (0.5594) acc 75.0000 (87.1875) lr 8.7467e-04 eta 0:01:14
epoch [29/50] batch [15/25] time 0.069 (0.115) data 0.000 (0.045) loss 0.2451 (0.5115) acc 90.6250 (88.1250) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.6855 (0.5165) acc 81.2500 (87.0312) lr 8.7467e-04 eta 0:00:54
epoch [29/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.5869 (0.4888) acc 84.3750 (87.5000) lr 8.1262e-04 eta 0:00:50
epoch [30/50] batch [5/25] time 0.070 (0.217) data 0.000 (0.146) loss 0.4028 (0.5096) acc 90.6250 (87.5000) lr 8.1262e-04 eta 0:01:52
epoch [30/50] batch [10/25] time 0.070 (0.143) data 0.000 (0.073) loss 0.3381 (0.4856) acc 93.7500 (88.7500) lr 8.1262e-04 eta 0:01:13
epoch [30/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.4707 (0.4520) acc 90.6250 (89.5833) lr 8.1262e-04 eta 0:01:00
epoch [30/50] batch [20/25] time 0.069 (0.106) data 0.000 (0.037) loss 0.5239 (0.4330) acc 87.5000 (90.1562) lr 8.1262e-04 eta 0:00:53
epoch [30/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.5410 (0.4679) acc 87.5000 (88.8750) lr 7.5131e-04 eta 0:00:49
epoch [31/50] batch [5/25] time 0.070 (0.210) data 0.000 (0.137) loss 0.4724 (0.4313) acc 90.6250 (86.8750) lr 7.5131e-04 eta 0:01:44
epoch [31/50] batch [10/25] time 0.069 (0.140) data 0.000 (0.069) loss 0.6753 (0.4178) acc 84.3750 (88.1250) lr 7.5131e-04 eta 0:01:08
epoch [31/50] batch [15/25] time 0.070 (0.117) data 0.000 (0.046) loss 0.4944 (0.4600) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:00:56
epoch [31/50] batch [20/25] time 0.069 (0.105) data 0.000 (0.034) loss 0.1854 (0.4627) acc 93.7500 (87.0312) lr 7.5131e-04 eta 0:00:50
epoch [31/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.5781 (0.4749) acc 84.3750 (87.2500) lr 6.9098e-04 eta 0:00:46
epoch [32/50] batch [5/25] time 0.069 (0.212) data 0.000 (0.141) loss 0.4893 (0.5045) acc 90.6250 (90.0000) lr 6.9098e-04 eta 0:01:39
epoch [32/50] batch [10/25] time 0.070 (0.142) data 0.000 (0.071) loss 0.4946 (0.5077) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:01:05
epoch [32/50] batch [15/25] time 0.070 (0.118) data 0.000 (0.047) loss 0.3269 (0.4772) acc 90.6250 (87.9167) lr 6.9098e-04 eta 0:00:54
epoch [32/50] batch [20/25] time 0.070 (0.106) data 0.000 (0.035) loss 0.2593 (0.4453) acc 93.7500 (88.9062) lr 6.9098e-04 eta 0:00:48
epoch [32/50] batch [25/25] time 0.071 (0.099) data 0.000 (0.028) loss 0.6206 (0.4495) acc 81.2500 (89.2500) lr 6.3188e-04 eta 0:00:44
epoch [33/50] batch [5/25] time 0.071 (0.242) data 0.000 (0.166) loss 0.3569 (0.4930) acc 90.6250 (84.3750) lr 6.3188e-04 eta 0:01:47
epoch [33/50] batch [10/25] time 0.070 (0.157) data 0.000 (0.083) loss 0.9795 (0.4672) acc 81.2500 (88.1250) lr 6.3188e-04 eta 0:01:08
epoch [33/50] batch [15/25] time 0.070 (0.128) data 0.000 (0.056) loss 0.3774 (0.4591) acc 90.6250 (88.3333) lr 6.3188e-04 eta 0:00:55
epoch [33/50] batch [20/25] time 0.070 (0.114) data 0.000 (0.042) loss 0.5376 (0.4501) acc 90.6250 (89.0625) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/25] time 0.071 (0.105) data 0.000 (0.034) loss 0.2361 (0.4388) acc 93.7500 (89.6250) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [5/25] time 0.070 (0.225) data 0.000 (0.151) loss 0.4055 (0.3700) acc 93.7500 (94.3750) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.076) loss 0.5469 (0.4158) acc 93.7500 (92.5000) lr 5.7422e-04 eta 0:01:01
epoch [34/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.051) loss 0.2732 (0.4015) acc 96.8750 (92.0833) lr 5.7422e-04 eta 0:00:49
epoch [34/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.038) loss 0.4319 (0.3904) acc 93.7500 (92.3438) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.030) loss 0.3027 (0.4231) acc 93.7500 (91.3750) lr 5.1825e-04 eta 0:00:40
epoch [35/50] batch [5/25] time 0.070 (0.202) data 0.000 (0.129) loss 0.3655 (0.4130) acc 93.7500 (90.6250) lr 5.1825e-04 eta 0:01:19
epoch [35/50] batch [10/25] time 0.070 (0.136) data 0.000 (0.065) loss 0.2515 (0.4377) acc 93.7500 (90.0000) lr 5.1825e-04 eta 0:00:53
epoch [35/50] batch [15/25] time 0.070 (0.114) data 0.000 (0.043) loss 0.4182 (0.4465) acc 90.6250 (89.7917) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [20/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.4072 (0.4554) acc 93.7500 (89.0625) lr 5.1825e-04 eta 0:00:39
epoch [35/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.026) loss 0.5869 (0.4320) acc 84.3750 (89.8750) lr 4.6417e-04 eta 0:00:36
epoch [36/50] batch [5/25] time 0.070 (0.200) data 0.000 (0.129) loss 0.3721 (0.3634) acc 93.7500 (90.6250) lr 4.6417e-04 eta 0:01:13
epoch [36/50] batch [10/25] time 0.070 (0.135) data 0.000 (0.065) loss 0.5010 (0.4220) acc 87.5000 (90.0000) lr 4.6417e-04 eta 0:00:49
epoch [36/50] batch [15/25] time 0.070 (0.113) data 0.000 (0.043) loss 0.8071 (0.4604) acc 84.3750 (89.1667) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [20/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.5234 (0.4879) acc 87.5000 (88.2812) lr 4.6417e-04 eta 0:00:36
epoch [36/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.026) loss 0.5127 (0.4873) acc 90.6250 (88.0000) lr 4.1221e-04 eta 0:00:33
epoch [37/50] batch [5/25] time 0.070 (0.205) data 0.000 (0.133) loss 0.3076 (0.4513) acc 96.8750 (88.7500) lr 4.1221e-04 eta 0:01:10
epoch [37/50] batch [10/25] time 0.070 (0.138) data 0.000 (0.067) loss 0.5913 (0.4834) acc 81.2500 (87.5000) lr 4.1221e-04 eta 0:00:46
epoch [37/50] batch [15/25] time 0.070 (0.115) data 0.000 (0.045) loss 0.4526 (0.4964) acc 87.5000 (87.2917) lr 4.1221e-04 eta 0:00:38
epoch [37/50] batch [20/25] time 0.069 (0.104) data 0.000 (0.033) loss 0.3274 (0.4886) acc 93.7500 (88.2812) lr 4.1221e-04 eta 0:00:34
epoch [37/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.1801 (0.4469) acc 93.7500 (89.1250) lr 3.6258e-04 eta 0:00:31
epoch [38/50] batch [5/25] time 0.070 (0.201) data 0.000 (0.131) loss 0.7261 (0.5415) acc 81.2500 (86.8750) lr 3.6258e-04 eta 0:01:04
epoch [38/50] batch [10/25] time 0.070 (0.136) data 0.000 (0.066) loss 0.2827 (0.5161) acc 93.7500 (88.1250) lr 3.6258e-04 eta 0:00:42
epoch [38/50] batch [15/25] time 0.070 (0.114) data 0.000 (0.044) loss 0.5283 (0.4860) acc 87.5000 (89.3750) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [20/25] time 0.070 (0.103) data 0.000 (0.033) loss 0.7090 (0.4947) acc 81.2500 (88.9062) lr 3.6258e-04 eta 0:00:31
epoch [38/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.026) loss 0.2079 (0.4481) acc 93.7500 (90.1250) lr 3.1545e-04 eta 0:00:28
epoch [39/50] batch [5/25] time 0.070 (0.203) data 0.000 (0.128) loss 0.3567 (0.3442) acc 90.6250 (92.5000) lr 3.1545e-04 eta 0:00:59
epoch [39/50] batch [10/25] time 0.070 (0.136) data 0.000 (0.064) loss 0.5479 (0.3878) acc 90.6250 (91.5625) lr 3.1545e-04 eta 0:00:39
epoch [39/50] batch [15/25] time 0.069 (0.114) data 0.000 (0.043) loss 0.4478 (0.4284) acc 90.6250 (90.2083) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [20/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.4614 (0.4301) acc 84.3750 (89.8438) lr 3.1545e-04 eta 0:00:28
epoch [39/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.026) loss 0.5596 (0.4375) acc 90.6250 (90.0000) lr 2.7103e-04 eta 0:00:26
epoch [40/50] batch [5/25] time 0.070 (0.221) data 0.000 (0.147) loss 0.3350 (0.3869) acc 93.7500 (92.5000) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [10/25] time 0.070 (0.146) data 0.000 (0.074) loss 0.4141 (0.3956) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:38
epoch [40/50] batch [15/25] time 0.072 (0.121) data 0.000 (0.049) loss 0.3987 (0.4057) acc 93.7500 (90.6250) lr 2.7103e-04 eta 0:00:31
epoch [40/50] batch [20/25] time 0.073 (0.108) data 0.000 (0.037) loss 0.3770 (0.4114) acc 93.7500 (90.4688) lr 2.7103e-04 eta 0:00:27
epoch [40/50] batch [25/25] time 0.071 (0.101) data 0.000 (0.030) loss 0.3987 (0.4217) acc 90.6250 (89.8750) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [5/25] time 0.070 (0.204) data 0.000 (0.131) loss 0.7017 (0.4130) acc 87.5000 (91.2500) lr 2.2949e-04 eta 0:00:49
epoch [41/50] batch [10/25] time 0.070 (0.137) data 0.000 (0.066) loss 0.2607 (0.3717) acc 93.7500 (92.1875) lr 2.2949e-04 eta 0:00:32
epoch [41/50] batch [15/25] time 0.070 (0.115) data 0.000 (0.044) loss 0.4397 (0.4045) acc 93.7500 (91.4583) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.2367 (0.3841) acc 96.8750 (92.1875) lr 2.2949e-04 eta 0:00:23
epoch [41/50] batch [25/25] time 0.071 (0.097) data 0.000 (0.026) loss 0.3062 (0.3699) acc 93.7500 (92.2500) lr 1.9098e-04 eta 0:00:21
epoch [42/50] batch [5/25] time 0.070 (0.211) data 0.000 (0.140) loss 0.4834 (0.5272) acc 87.5000 (88.1250) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [10/25] time 0.071 (0.141) data 0.000 (0.070) loss 0.4226 (0.4763) acc 87.5000 (88.4375) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [15/25] time 0.070 (0.117) data 0.000 (0.047) loss 0.2791 (0.4724) acc 93.7500 (88.7500) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [20/25] time 0.069 (0.105) data 0.000 (0.035) loss 0.5361 (0.4744) acc 84.3750 (88.7500) lr 1.9098e-04 eta 0:00:21
epoch [42/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.5479 (0.4642) acc 90.6250 (89.1250) lr 1.5567e-04 eta 0:00:19
epoch [43/50] batch [5/25] time 0.070 (0.205) data 0.000 (0.134) loss 0.2698 (0.3993) acc 93.7500 (91.2500) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [10/25] time 0.070 (0.138) data 0.000 (0.067) loss 0.4094 (0.4155) acc 87.5000 (90.0000) lr 1.5567e-04 eta 0:00:26
epoch [43/50] batch [15/25] time 0.070 (0.115) data 0.000 (0.045) loss 0.8418 (0.4392) acc 81.2500 (89.7917) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.5991 (0.4304) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:18
epoch [43/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.1292 (0.4202) acc 100.0000 (91.0000) lr 1.2369e-04 eta 0:00:16
epoch [44/50] batch [5/25] time 0.070 (0.208) data 0.000 (0.138) loss 0.2842 (0.4583) acc 93.7500 (88.7500) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [10/25] time 0.070 (0.139) data 0.000 (0.069) loss 0.4988 (0.4282) acc 90.6250 (90.9375) lr 1.2369e-04 eta 0:00:22
epoch [44/50] batch [15/25] time 0.070 (0.116) data 0.000 (0.046) loss 0.6777 (0.4361) acc 81.2500 (90.2083) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.5391 (0.4692) acc 87.5000 (89.5312) lr 1.2369e-04 eta 0:00:16
epoch [44/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.4446 (0.4556) acc 90.6250 (89.7500) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [5/25] time 0.070 (0.205) data 0.000 (0.135) loss 0.4324 (0.4587) acc 84.3750 (87.5000) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [10/25] time 0.070 (0.138) data 0.000 (0.067) loss 0.4343 (0.4643) acc 84.3750 (88.4375) lr 9.5173e-05 eta 0:00:19
epoch [45/50] batch [15/25] time 0.069 (0.115) data 0.000 (0.045) loss 0.5459 (0.5158) acc 84.3750 (86.4583) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.5537 (0.4814) acc 84.3750 (88.1250) lr 9.5173e-05 eta 0:00:13
epoch [45/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.2793 (0.5036) acc 96.8750 (87.7500) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [5/25] time 0.070 (0.212) data 0.000 (0.141) loss 0.3293 (0.3839) acc 96.8750 (91.2500) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [10/25] time 0.070 (0.141) data 0.000 (0.070) loss 0.5557 (0.4237) acc 81.2500 (89.6875) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [15/25] time 0.070 (0.117) data 0.000 (0.047) loss 0.4666 (0.4228) acc 90.6250 (89.5833) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.5918 (0.4212) acc 84.3750 (89.3750) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.5156 (0.4144) acc 87.5000 (89.7500) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [5/25] time 0.070 (0.213) data 0.000 (0.141) loss 0.7021 (0.5165) acc 87.5000 (85.6250) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [10/25] time 0.070 (0.141) data 0.000 (0.071) loss 0.2308 (0.4542) acc 93.7500 (87.8125) lr 4.8943e-05 eta 0:00:12
epoch [47/50] batch [15/25] time 0.071 (0.118) data 0.000 (0.047) loss 0.2739 (0.4318) acc 96.8750 (89.5833) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [20/25] time 0.070 (0.106) data 0.000 (0.035) loss 0.4824 (0.4406) acc 90.6250 (89.0625) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [25/25] time 0.070 (0.099) data 0.000 (0.028) loss 0.6089 (0.4442) acc 84.3750 (89.0000) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/25] time 0.069 (0.208) data 0.000 (0.138) loss 0.4531 (0.4208) acc 87.5000 (90.6250) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [10/25] time 0.070 (0.139) data 0.000 (0.069) loss 0.5312 (0.4026) acc 87.5000 (90.9375) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/25] time 0.070 (0.116) data 0.000 (0.046) loss 0.2185 (0.3993) acc 96.8750 (90.4167) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.4910 (0.4012) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:05
epoch [48/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.1592 (0.3949) acc 96.8750 (91.1250) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/25] time 0.070 (0.207) data 0.000 (0.135) loss 0.2693 (0.4130) acc 96.8750 (89.3750) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [10/25] time 0.070 (0.139) data 0.000 (0.068) loss 0.4126 (0.4198) acc 90.6250 (89.3750) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [15/25] time 0.070 (0.116) data 0.000 (0.045) loss 0.8696 (0.4797) acc 75.0000 (87.5000) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.5771 (0.4478) acc 84.3750 (88.1250) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.1289 (0.4305) acc 100.0000 (88.7500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.070 (0.202) data 0.000 (0.128) loss 0.3523 (0.3725) acc 93.7500 (91.2500) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/25] time 0.070 (0.136) data 0.000 (0.064) loss 0.1597 (0.3626) acc 96.8750 (91.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.070 (0.114) data 0.000 (0.043) loss 0.4224 (0.3622) acc 81.2500 (90.8333) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.4883 (0.3844) acc 84.3750 (90.0000) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.026) loss 0.5117 (0.3794) acc 93.7500 (90.2500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:04<00:13,  4.44s/it] 50%|█████     | 2/4 [00:05<00:04,  2.17s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.44s/it]100%|██████████| 4/4 [00:05<00:00,  1.09it/s]100%|██████████| 4/4 [00:05<00:00,  1.46s/it]
=> result
* total: 1,549
* correct: 1,520
* accuracy: 98.1%
* error: 1.9%
* macro_f1: 96.3%
Elapsed: 0:02:16
Run this job and save the output to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/25] time 0.074 (0.291) data 0.005 (0.186) loss 2.2188 (2.2223) acc 75.0000 (74.3750) lr 1.0000e-05 eta 0:06:02
epoch [1/50] batch [10/25] time 0.070 (0.181) data 0.000 (0.093) loss 1.6748 (2.1359) acc 90.6250 (76.2500) lr 1.0000e-05 eta 0:03:44
epoch [1/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.062) loss 1.7480 (2.1049) acc 87.5000 (76.4583) lr 1.0000e-05 eta 0:02:58
epoch [1/50] batch [20/25] time 0.069 (0.126) data 0.000 (0.047) loss 2.0234 (2.1034) acc 78.1250 (76.5625) lr 1.0000e-05 eta 0:02:35
epoch [1/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.037) loss 2.2227 (2.0766) acc 75.0000 (77.6250) lr 2.0000e-03 eta 0:02:20
epoch [2/50] batch [5/25] time 0.074 (0.234) data 0.000 (0.160) loss 1.2393 (1.4301) acc 84.3750 (85.6250) lr 2.0000e-03 eta 0:04:45
epoch [2/50] batch [10/25] time 0.074 (0.154) data 0.000 (0.080) loss 1.0664 (1.3130) acc 81.2500 (82.5000) lr 2.0000e-03 eta 0:03:06
epoch [2/50] batch [15/25] time 0.072 (0.127) data 0.000 (0.054) loss 0.7871 (1.1642) acc 90.6250 (83.1250) lr 2.0000e-03 eta 0:02:33
epoch [2/50] batch [20/25] time 0.070 (0.113) data 0.000 (0.040) loss 1.1123 (1.0792) acc 78.1250 (83.2812) lr 2.0000e-03 eta 0:02:15
epoch [2/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.032) loss 0.3308 (0.9865) acc 90.6250 (83.6250) lr 1.9980e-03 eta 0:02:04
epoch [3/50] batch [5/25] time 0.069 (0.230) data 0.000 (0.159) loss 0.6553 (0.6128) acc 81.2500 (86.8750) lr 1.9980e-03 eta 0:04:35
epoch [3/50] batch [10/25] time 0.069 (0.150) data 0.000 (0.080) loss 0.5303 (0.6023) acc 84.3750 (85.9375) lr 1.9980e-03 eta 0:02:58
epoch [3/50] batch [15/25] time 0.069 (0.123) data 0.000 (0.053) loss 0.5547 (0.5828) acc 81.2500 (86.4583) lr 1.9980e-03 eta 0:02:25
epoch [3/50] batch [20/25] time 0.069 (0.110) data 0.000 (0.040) loss 0.5000 (0.6091) acc 90.6250 (85.7812) lr 1.9980e-03 eta 0:02:09
epoch [3/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.4812 (0.6016) acc 87.5000 (85.3750) lr 1.9921e-03 eta 0:01:59
epoch [4/50] batch [5/25] time 0.070 (0.221) data 0.000 (0.150) loss 0.4302 (0.7243) acc 90.6250 (81.2500) lr 1.9921e-03 eta 0:04:19
epoch [4/50] batch [10/25] time 0.070 (0.146) data 0.000 (0.075) loss 0.7041 (0.6624) acc 84.3750 (82.5000) lr 1.9921e-03 eta 0:02:49
epoch [4/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.050) loss 0.9619 (0.6726) acc 78.1250 (83.9583) lr 1.9921e-03 eta 0:02:19
epoch [4/50] batch [20/25] time 0.069 (0.108) data 0.000 (0.038) loss 0.2876 (0.6451) acc 96.8750 (84.0625) lr 1.9921e-03 eta 0:02:04
epoch [4/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.030) loss 0.5298 (0.6030) acc 81.2500 (84.7500) lr 1.9823e-03 eta 0:01:55
epoch [5/50] batch [5/25] time 0.069 (0.221) data 0.000 (0.149) loss 0.4607 (0.5140) acc 87.5000 (88.7500) lr 1.9823e-03 eta 0:04:13
epoch [5/50] batch [10/25] time 0.070 (0.145) data 0.000 (0.075) loss 0.5127 (0.4935) acc 84.3750 (88.7500) lr 1.9823e-03 eta 0:02:45
epoch [5/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.050) loss 0.4268 (0.5921) acc 90.6250 (85.8333) lr 1.9823e-03 eta 0:02:16
epoch [5/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.4585 (0.5945) acc 87.5000 (85.9375) lr 1.9823e-03 eta 0:02:01
epoch [5/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.030) loss 0.6182 (0.5957) acc 78.1250 (85.3750) lr 1.9686e-03 eta 0:01:52
epoch [6/50] batch [5/25] time 0.069 (0.219) data 0.000 (0.148) loss 0.6357 (0.5816) acc 87.5000 (86.8750) lr 1.9686e-03 eta 0:04:04
epoch [6/50] batch [10/25] time 0.070 (0.144) data 0.000 (0.074) loss 0.8633 (0.5659) acc 81.2500 (86.5625) lr 1.9686e-03 eta 0:02:40
epoch [6/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.4814 (0.5329) acc 90.6250 (87.2917) lr 1.9686e-03 eta 0:02:12
epoch [6/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.6064 (0.5613) acc 84.3750 (86.2500) lr 1.9686e-03 eta 0:01:58
epoch [6/50] batch [25/25] time 0.070 (0.099) data 0.000 (0.030) loss 0.7842 (0.5725) acc 68.7500 (85.5000) lr 1.9511e-03 eta 0:01:49
epoch [7/50] batch [5/25] time 0.070 (0.232) data 0.000 (0.161) loss 0.6445 (0.5737) acc 78.1250 (83.7500) lr 1.9511e-03 eta 0:04:13
epoch [7/50] batch [10/25] time 0.069 (0.151) data 0.000 (0.081) loss 0.5996 (0.6023) acc 84.3750 (84.0625) lr 1.9511e-03 eta 0:02:44
epoch [7/50] batch [15/25] time 0.069 (0.123) data 0.000 (0.054) loss 0.8823 (0.6176) acc 75.0000 (84.1667) lr 1.9511e-03 eta 0:02:13
epoch [7/50] batch [20/25] time 0.069 (0.110) data 0.000 (0.040) loss 0.6904 (0.6081) acc 81.2500 (84.3750) lr 1.9511e-03 eta 0:01:58
epoch [7/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.4744 (0.5752) acc 87.5000 (85.5000) lr 1.9298e-03 eta 0:01:49
epoch [8/50] batch [5/25] time 0.069 (0.226) data 0.000 (0.155) loss 0.8521 (0.5819) acc 78.1250 (83.7500) lr 1.9298e-03 eta 0:04:01
epoch [8/50] batch [10/25] time 0.069 (0.148) data 0.000 (0.078) loss 0.4844 (0.5277) acc 90.6250 (85.0000) lr 1.9298e-03 eta 0:02:37
epoch [8/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.052) loss 0.5410 (0.5488) acc 90.6250 (85.6250) lr 1.9298e-03 eta 0:02:09
epoch [8/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.039) loss 0.6138 (0.5309) acc 90.6250 (86.4062) lr 1.9298e-03 eta 0:01:54
epoch [8/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.5273 (0.5461) acc 90.6250 (86.1250) lr 1.9048e-03 eta 0:01:46
epoch [9/50] batch [5/25] time 0.070 (0.227) data 0.000 (0.155) loss 0.3616 (0.6669) acc 93.7500 (81.8750) lr 1.9048e-03 eta 0:03:57
epoch [9/50] batch [10/25] time 0.071 (0.149) data 0.000 (0.077) loss 0.7192 (0.6163) acc 78.1250 (84.0625) lr 1.9048e-03 eta 0:02:34
epoch [9/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.052) loss 0.3887 (0.5532) acc 90.6250 (86.4583) lr 1.9048e-03 eta 0:02:06
epoch [9/50] batch [20/25] time 0.069 (0.109) data 0.000 (0.039) loss 0.4243 (0.5787) acc 84.3750 (85.7812) lr 1.9048e-03 eta 0:01:52
epoch [9/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.4851 (0.5710) acc 87.5000 (86.5000) lr 1.8763e-03 eta 0:01:43
epoch [10/50] batch [5/25] time 0.070 (0.235) data 0.000 (0.165) loss 0.5522 (0.5619) acc 87.5000 (88.7500) lr 1.8763e-03 eta 0:03:59
epoch [10/50] batch [10/25] time 0.070 (0.152) data 0.000 (0.083) loss 0.3562 (0.5473) acc 87.5000 (85.9375) lr 1.8763e-03 eta 0:02:34
epoch [10/50] batch [15/25] time 0.069 (0.125) data 0.000 (0.055) loss 0.4087 (0.5367) acc 87.5000 (85.8333) lr 1.8763e-03 eta 0:02:06
epoch [10/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.041) loss 0.5254 (0.5281) acc 84.3750 (86.4062) lr 1.8763e-03 eta 0:01:51
epoch [10/50] batch [25/25] time 0.069 (0.103) data 0.000 (0.033) loss 0.4575 (0.5441) acc 93.7500 (86.6250) lr 1.8443e-03 eta 0:01:42
epoch [11/50] batch [5/25] time 0.069 (0.227) data 0.000 (0.157) loss 0.5317 (0.4684) acc 84.3750 (85.0000) lr 1.8443e-03 eta 0:03:46
epoch [11/50] batch [10/25] time 0.070 (0.148) data 0.000 (0.079) loss 0.4019 (0.5014) acc 87.5000 (85.6250) lr 1.8443e-03 eta 0:02:26
epoch [11/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.053) loss 0.4387 (0.5136) acc 87.5000 (85.0000) lr 1.8443e-03 eta 0:02:00
epoch [11/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.039) loss 0.3689 (0.5433) acc 93.7500 (84.5312) lr 1.8443e-03 eta 0:01:46
epoch [11/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.032) loss 0.4033 (0.5267) acc 90.6250 (85.8750) lr 1.8090e-03 eta 0:01:38
epoch [12/50] batch [5/25] time 0.070 (0.217) data 0.000 (0.146) loss 0.3906 (0.4748) acc 93.7500 (88.1250) lr 1.8090e-03 eta 0:03:30
epoch [12/50] batch [10/25] time 0.069 (0.143) data 0.000 (0.073) loss 0.5488 (0.5384) acc 87.5000 (85.9375) lr 1.8090e-03 eta 0:02:18
epoch [12/50] batch [15/25] time 0.069 (0.119) data 0.000 (0.049) loss 0.4548 (0.5316) acc 90.6250 (85.4167) lr 1.8090e-03 eta 0:01:53
epoch [12/50] batch [20/25] time 0.069 (0.106) data 0.000 (0.037) loss 0.3765 (0.5264) acc 93.7500 (85.4688) lr 1.8090e-03 eta 0:01:41
epoch [12/50] batch [25/25] time 0.070 (0.099) data 0.000 (0.029) loss 0.5742 (0.5193) acc 87.5000 (86.1250) lr 1.7705e-03 eta 0:01:33
epoch [13/50] batch [5/25] time 0.070 (0.272) data 0.000 (0.202) loss 0.3728 (0.4499) acc 93.7500 (90.0000) lr 1.7705e-03 eta 0:04:16
epoch [13/50] batch [10/25] time 0.070 (0.171) data 0.000 (0.101) loss 0.3174 (0.4557) acc 90.6250 (89.0625) lr 1.7705e-03 eta 0:02:40
epoch [13/50] batch [15/25] time 0.070 (0.137) data 0.000 (0.068) loss 0.4856 (0.4633) acc 81.2500 (87.5000) lr 1.7705e-03 eta 0:02:08
epoch [13/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.051) loss 0.2974 (0.4505) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:01:52
epoch [13/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.041) loss 0.2642 (0.4491) acc 90.6250 (87.6250) lr 1.7290e-03 eta 0:01:42
epoch [14/50] batch [5/25] time 0.071 (0.246) data 0.000 (0.174) loss 0.4695 (0.5234) acc 87.5000 (86.2500) lr 1.7290e-03 eta 0:03:46
epoch [14/50] batch [10/25] time 0.070 (0.158) data 0.000 (0.087) loss 0.5371 (0.5631) acc 90.6250 (85.9375) lr 1.7290e-03 eta 0:02:24
epoch [14/50] batch [15/25] time 0.070 (0.129) data 0.000 (0.058) loss 0.5420 (0.5604) acc 81.2500 (86.0417) lr 1.7290e-03 eta 0:01:57
epoch [14/50] batch [20/25] time 0.070 (0.114) data 0.000 (0.044) loss 0.5107 (0.5273) acc 84.3750 (87.0312) lr 1.7290e-03 eta 0:01:43
epoch [14/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.3916 (0.5076) acc 90.6250 (87.8750) lr 1.6845e-03 eta 0:01:34
epoch [15/50] batch [5/25] time 0.076 (0.262) data 0.000 (0.189) loss 0.3447 (0.4533) acc 90.6250 (86.8750) lr 1.6845e-03 eta 0:03:54
epoch [15/50] batch [10/25] time 0.070 (0.166) data 0.000 (0.095) loss 0.3745 (0.4774) acc 90.6250 (87.8125) lr 1.6845e-03 eta 0:02:28
epoch [15/50] batch [15/25] time 0.070 (0.134) data 0.000 (0.063) loss 0.3867 (0.4922) acc 87.5000 (86.8750) lr 1.6845e-03 eta 0:01:58
epoch [15/50] batch [20/25] time 0.070 (0.118) data 0.000 (0.047) loss 0.7510 (0.5054) acc 78.1250 (86.4062) lr 1.6845e-03 eta 0:01:44
epoch [15/50] batch [25/25] time 0.070 (0.109) data 0.000 (0.038) loss 0.6318 (0.5046) acc 84.3750 (86.8750) lr 1.6374e-03 eta 0:01:35
epoch [16/50] batch [5/25] time 0.070 (0.238) data 0.000 (0.165) loss 0.4092 (0.3412) acc 90.6250 (91.8750) lr 1.6374e-03 eta 0:03:27
epoch [16/50] batch [10/25] time 0.070 (0.154) data 0.000 (0.083) loss 0.7075 (0.4822) acc 78.1250 (88.1250) lr 1.6374e-03 eta 0:02:13
epoch [16/50] batch [15/25] time 0.071 (0.126) data 0.000 (0.055) loss 1.0146 (0.5335) acc 71.8750 (86.6667) lr 1.6374e-03 eta 0:01:48
epoch [16/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.042) loss 0.4922 (0.5417) acc 90.6250 (85.6250) lr 1.6374e-03 eta 0:01:35
epoch [16/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.5649 (0.5492) acc 87.5000 (85.3750) lr 1.5878e-03 eta 0:01:28
epoch [17/50] batch [5/25] time 0.071 (0.252) data 0.000 (0.181) loss 0.4243 (0.4730) acc 87.5000 (88.1250) lr 1.5878e-03 eta 0:03:32
epoch [17/50] batch [10/25] time 0.070 (0.161) data 0.000 (0.091) loss 0.7710 (0.4935) acc 78.1250 (87.1875) lr 1.5878e-03 eta 0:02:15
epoch [17/50] batch [15/25] time 0.070 (0.131) data 0.000 (0.060) loss 0.5195 (0.5220) acc 81.2500 (86.6667) lr 1.5878e-03 eta 0:01:49
epoch [17/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.045) loss 0.4512 (0.5152) acc 84.3750 (86.7188) lr 1.5878e-03 eta 0:01:36
epoch [17/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.036) loss 0.6440 (0.5354) acc 84.3750 (86.2500) lr 1.5358e-03 eta 0:01:28
epoch [18/50] batch [5/25] time 0.070 (0.220) data 0.000 (0.148) loss 0.6880 (0.4941) acc 78.1250 (83.1250) lr 1.5358e-03 eta 0:03:00
epoch [18/50] batch [10/25] time 0.070 (0.145) data 0.000 (0.074) loss 0.8232 (0.5157) acc 78.1250 (84.3750) lr 1.5358e-03 eta 0:01:58
epoch [18/50] batch [15/25] time 0.072 (0.120) data 0.000 (0.050) loss 0.4741 (0.5150) acc 84.3750 (84.3750) lr 1.5358e-03 eta 0:01:37
epoch [18/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.037) loss 0.4119 (0.5289) acc 90.6250 (85.1562) lr 1.5358e-03 eta 0:01:26
epoch [18/50] batch [25/25] time 0.069 (0.100) data 0.000 (0.030) loss 0.5396 (0.5189) acc 84.3750 (85.5000) lr 1.4818e-03 eta 0:01:19
epoch [19/50] batch [5/25] time 0.070 (0.227) data 0.000 (0.157) loss 0.4744 (0.5604) acc 87.5000 (83.1250) lr 1.4818e-03 eta 0:03:00
epoch [19/50] batch [10/25] time 0.070 (0.148) data 0.000 (0.078) loss 0.3040 (0.5170) acc 90.6250 (84.0625) lr 1.4818e-03 eta 0:01:57
epoch [19/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.052) loss 0.3835 (0.5037) acc 90.6250 (85.4167) lr 1.4818e-03 eta 0:01:35
epoch [19/50] batch [20/25] time 0.071 (0.109) data 0.000 (0.039) loss 0.6831 (0.5389) acc 78.1250 (84.5312) lr 1.4818e-03 eta 0:01:25
epoch [19/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.031) loss 0.4902 (0.5371) acc 87.5000 (84.8750) lr 1.4258e-03 eta 0:01:18
epoch [20/50] batch [5/25] time 0.070 (0.257) data 0.000 (0.185) loss 0.4612 (0.5880) acc 93.7500 (85.6250) lr 1.4258e-03 eta 0:03:17
epoch [20/50] batch [10/25] time 0.070 (0.163) data 0.000 (0.093) loss 0.3601 (0.5194) acc 93.7500 (87.1875) lr 1.4258e-03 eta 0:02:04
epoch [20/50] batch [15/25] time 0.070 (0.133) data 0.000 (0.062) loss 0.2720 (0.4865) acc 90.6250 (87.7083) lr 1.4258e-03 eta 0:01:40
epoch [20/50] batch [20/25] time 0.072 (0.117) data 0.000 (0.046) loss 0.5308 (0.4986) acc 84.3750 (87.5000) lr 1.4258e-03 eta 0:01:28
epoch [20/50] batch [25/25] time 0.070 (0.108) data 0.000 (0.037) loss 0.2593 (0.4688) acc 90.6250 (88.1250) lr 1.3681e-03 eta 0:01:20
epoch [21/50] batch [5/25] time 0.071 (0.277) data 0.000 (0.204) loss 0.7402 (0.4440) acc 78.1250 (86.8750) lr 1.3681e-03 eta 0:03:26
epoch [21/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.102) loss 0.4260 (0.4218) acc 87.5000 (89.0625) lr 1.3681e-03 eta 0:02:08
epoch [21/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 0.5610 (0.4366) acc 84.3750 (89.1667) lr 1.3681e-03 eta 0:01:42
epoch [21/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 0.2729 (0.4397) acc 96.8750 (88.5938) lr 1.3681e-03 eta 0:01:28
epoch [21/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 0.6250 (0.4507) acc 78.1250 (88.0000) lr 1.3090e-03 eta 0:01:20
epoch [22/50] batch [5/25] time 0.070 (0.223) data 0.000 (0.151) loss 0.3577 (0.6349) acc 90.6250 (84.3750) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.076) loss 0.2915 (0.5670) acc 90.6250 (84.6875) lr 1.3090e-03 eta 0:01:44
epoch [22/50] batch [15/25] time 0.071 (0.121) data 0.000 (0.051) loss 0.2883 (0.5632) acc 87.5000 (83.9583) lr 1.3090e-03 eta 0:01:26
epoch [22/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.038) loss 0.6372 (0.5594) acc 84.3750 (84.6875) lr 1.3090e-03 eta 0:01:16
epoch [22/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.5366 (0.5493) acc 84.3750 (85.3750) lr 1.2487e-03 eta 0:01:10
epoch [23/50] batch [5/25] time 0.070 (0.221) data 0.000 (0.149) loss 0.5317 (0.5110) acc 87.5000 (88.1250) lr 1.2487e-03 eta 0:02:33
epoch [23/50] batch [10/25] time 0.070 (0.145) data 0.000 (0.075) loss 0.3140 (0.4960) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:01:40
epoch [23/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.050) loss 0.2617 (0.4753) acc 96.8750 (88.1250) lr 1.2487e-03 eta 0:01:22
epoch [23/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.037) loss 0.3623 (0.4707) acc 90.6250 (88.4375) lr 1.2487e-03 eta 0:01:13
epoch [23/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.030) loss 0.6729 (0.4665) acc 78.1250 (88.5000) lr 1.1874e-03 eta 0:01:07
epoch [24/50] batch [5/25] time 0.070 (0.220) data 0.000 (0.147) loss 0.3972 (0.3526) acc 90.6250 (92.5000) lr 1.1874e-03 eta 0:02:27
epoch [24/50] batch [10/25] time 0.070 (0.145) data 0.000 (0.074) loss 1.0020 (0.5303) acc 71.8750 (87.5000) lr 1.1874e-03 eta 0:01:36
epoch [24/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.049) loss 0.6338 (0.4889) acc 84.3750 (88.7500) lr 1.1874e-03 eta 0:01:19
epoch [24/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.4556 (0.4961) acc 87.5000 (88.4375) lr 1.1874e-03 eta 0:01:10
epoch [24/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.030) loss 0.5894 (0.5080) acc 81.2500 (88.1250) lr 1.1253e-03 eta 0:01:04
epoch [25/50] batch [5/25] time 0.069 (0.224) data 0.000 (0.154) loss 0.4802 (0.4218) acc 90.6250 (91.8750) lr 1.1253e-03 eta 0:02:24
epoch [25/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.077) loss 0.6089 (0.4287) acc 81.2500 (89.6875) lr 1.1253e-03 eta 0:01:34
epoch [25/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.052) loss 0.3655 (0.4369) acc 93.7500 (89.5833) lr 1.1253e-03 eta 0:01:17
epoch [25/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.039) loss 0.7261 (0.4737) acc 81.2500 (88.2812) lr 1.1253e-03 eta 0:01:08
epoch [25/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.2832 (0.4700) acc 93.7500 (88.8750) lr 1.0628e-03 eta 0:01:02
epoch [26/50] batch [5/25] time 0.070 (0.230) data 0.000 (0.160) loss 0.7349 (0.4925) acc 78.1250 (89.3750) lr 1.0628e-03 eta 0:02:22
epoch [26/50] batch [10/25] time 0.070 (0.150) data 0.000 (0.080) loss 0.5645 (0.4934) acc 84.3750 (87.1875) lr 1.0628e-03 eta 0:01:32
epoch [26/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.053) loss 0.5410 (0.5436) acc 84.3750 (85.8333) lr 1.0628e-03 eta 0:01:15
epoch [26/50] batch [20/25] time 0.072 (0.110) data 0.000 (0.040) loss 0.1119 (0.5086) acc 100.0000 (86.8750) lr 1.0628e-03 eta 0:01:06
epoch [26/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.5439 (0.5029) acc 90.6250 (87.1250) lr 1.0000e-03 eta 0:01:01
epoch [27/50] batch [5/25] time 0.070 (0.223) data 0.000 (0.149) loss 0.4663 (0.4704) acc 90.6250 (90.0000) lr 1.0000e-03 eta 0:02:12
epoch [27/50] batch [10/25] time 0.070 (0.146) data 0.000 (0.075) loss 0.5801 (0.4631) acc 87.5000 (90.6250) lr 1.0000e-03 eta 0:01:26
epoch [27/50] batch [15/25] time 0.069 (0.121) data 0.000 (0.050) loss 0.5107 (0.4818) acc 84.3750 (88.9583) lr 1.0000e-03 eta 0:01:10
epoch [27/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.037) loss 0.4670 (0.4707) acc 87.5000 (88.9062) lr 1.0000e-03 eta 0:01:02
epoch [27/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.030) loss 0.4800 (0.4701) acc 90.6250 (88.3750) lr 9.3721e-04 eta 0:00:57
epoch [28/50] batch [5/25] time 0.070 (0.216) data 0.000 (0.143) loss 0.2766 (0.4066) acc 96.8750 (90.6250) lr 9.3721e-04 eta 0:02:03
epoch [28/50] batch [10/25] time 0.070 (0.143) data 0.000 (0.072) loss 0.6030 (0.4783) acc 87.5000 (87.8125) lr 9.3721e-04 eta 0:01:20
epoch [28/50] batch [15/25] time 0.070 (0.119) data 0.000 (0.048) loss 0.3235 (0.4665) acc 93.7500 (88.7500) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [20/25] time 0.069 (0.106) data 0.000 (0.036) loss 0.3464 (0.4630) acc 90.6250 (88.5938) lr 9.3721e-04 eta 0:00:59
epoch [28/50] batch [25/25] time 0.069 (0.099) data 0.000 (0.029) loss 0.2764 (0.4549) acc 100.0000 (89.5000) lr 8.7467e-04 eta 0:00:54
epoch [29/50] batch [5/25] time 0.070 (0.205) data 0.000 (0.133) loss 0.4265 (0.5007) acc 90.6250 (86.2500) lr 8.7467e-04 eta 0:01:51
epoch [29/50] batch [10/25] time 0.070 (0.137) data 0.000 (0.067) loss 0.3445 (0.5422) acc 93.7500 (85.3125) lr 8.7467e-04 eta 0:01:14
epoch [29/50] batch [15/25] time 0.070 (0.115) data 0.000 (0.045) loss 0.5195 (0.5302) acc 87.5000 (86.0417) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.4163 (0.5124) acc 81.2500 (85.9375) lr 8.7467e-04 eta 0:00:54
epoch [29/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.8296 (0.5195) acc 81.2500 (86.3750) lr 8.1262e-04 eta 0:00:50
epoch [30/50] batch [5/25] time 0.071 (0.210) data 0.000 (0.135) loss 0.3623 (0.4274) acc 90.6250 (90.0000) lr 8.1262e-04 eta 0:01:49
epoch [30/50] batch [10/25] time 0.070 (0.140) data 0.000 (0.068) loss 0.5176 (0.4876) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:01:12
epoch [30/50] batch [15/25] time 0.069 (0.117) data 0.000 (0.045) loss 0.6948 (0.5497) acc 81.2500 (85.4167) lr 8.1262e-04 eta 0:00:59
epoch [30/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.034) loss 0.7148 (0.5326) acc 78.1250 (86.5625) lr 8.1262e-04 eta 0:00:52
epoch [30/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.027) loss 0.2460 (0.5108) acc 93.7500 (86.8750) lr 7.5131e-04 eta 0:00:48
epoch [31/50] batch [5/25] time 0.069 (0.205) data 0.000 (0.131) loss 0.5420 (0.5405) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:01:41
epoch [31/50] batch [10/25] time 0.070 (0.138) data 0.000 (0.066) loss 0.3936 (0.4799) acc 87.5000 (88.7500) lr 7.5131e-04 eta 0:01:07
epoch [31/50] batch [15/25] time 0.070 (0.115) data 0.000 (0.044) loss 0.2798 (0.4672) acc 96.8750 (89.1667) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.5244 (0.4838) acc 84.3750 (88.2812) lr 7.5131e-04 eta 0:00:49
epoch [31/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.026) loss 0.5132 (0.5031) acc 87.5000 (87.8750) lr 6.9098e-04 eta 0:00:45
epoch [32/50] batch [5/25] time 0.070 (0.203) data 0.000 (0.132) loss 0.4934 (0.5618) acc 84.3750 (84.3750) lr 6.9098e-04 eta 0:01:35
epoch [32/50] batch [10/25] time 0.070 (0.137) data 0.000 (0.066) loss 0.4644 (0.4933) acc 84.3750 (87.1875) lr 6.9098e-04 eta 0:01:03
epoch [32/50] batch [15/25] time 0.070 (0.114) data 0.000 (0.044) loss 0.4280 (0.5043) acc 93.7500 (87.2917) lr 6.9098e-04 eta 0:00:52
epoch [32/50] batch [20/25] time 0.069 (0.103) data 0.000 (0.033) loss 0.4292 (0.4640) acc 90.6250 (88.9062) lr 6.9098e-04 eta 0:00:46
epoch [32/50] batch [25/25] time 0.070 (0.096) data 0.000 (0.027) loss 0.2754 (0.4671) acc 93.7500 (88.7500) lr 6.3188e-04 eta 0:00:43
epoch [33/50] batch [5/25] time 0.070 (0.236) data 0.000 (0.164) loss 0.5791 (0.4386) acc 84.3750 (90.0000) lr 6.3188e-04 eta 0:01:45
epoch [33/50] batch [10/25] time 0.072 (0.153) data 0.000 (0.082) loss 0.6948 (0.4624) acc 84.3750 (89.3750) lr 6.3188e-04 eta 0:01:07
epoch [33/50] batch [15/25] time 0.071 (0.126) data 0.000 (0.055) loss 0.4885 (0.4977) acc 87.5000 (88.3333) lr 6.3188e-04 eta 0:00:54
epoch [33/50] batch [20/25] time 0.071 (0.112) data 0.000 (0.041) loss 0.6270 (0.4970) acc 84.3750 (87.9688) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.4617 (0.4945) acc 87.5000 (88.1250) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [5/25] time 0.070 (0.206) data 0.000 (0.132) loss 0.9385 (0.5663) acc 78.1250 (88.1250) lr 5.7422e-04 eta 0:01:26
epoch [34/50] batch [10/25] time 0.071 (0.138) data 0.000 (0.066) loss 0.6846 (0.5384) acc 84.3750 (87.1875) lr 5.7422e-04 eta 0:00:57
epoch [34/50] batch [15/25] time 0.071 (0.115) data 0.000 (0.044) loss 0.3643 (0.4860) acc 90.6250 (88.5417) lr 5.7422e-04 eta 0:00:47
epoch [34/50] batch [20/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.6978 (0.4864) acc 75.0000 (87.8125) lr 5.7422e-04 eta 0:00:42
epoch [34/50] batch [25/25] time 0.070 (0.097) data 0.000 (0.027) loss 0.4741 (0.4739) acc 87.5000 (88.1250) lr 5.1825e-04 eta 0:00:38
epoch [35/50] batch [5/25] time 0.070 (0.218) data 0.000 (0.145) loss 0.7012 (0.4225) acc 87.5000 (90.6250) lr 5.1825e-04 eta 0:01:26
epoch [35/50] batch [10/25] time 0.070 (0.144) data 0.000 (0.072) loss 0.5376 (0.4177) acc 87.5000 (90.9375) lr 5.1825e-04 eta 0:00:56
epoch [35/50] batch [15/25] time 0.070 (0.120) data 0.000 (0.048) loss 0.1616 (0.4225) acc 96.8750 (90.4167) lr 5.1825e-04 eta 0:00:46
epoch [35/50] batch [20/25] time 0.071 (0.107) data 0.000 (0.036) loss 0.5562 (0.4236) acc 87.5000 (90.3125) lr 5.1825e-04 eta 0:00:40
epoch [35/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.029) loss 0.2424 (0.4111) acc 96.8750 (91.0000) lr 4.6417e-04 eta 0:00:37
epoch [36/50] batch [5/25] time 0.071 (0.217) data 0.000 (0.146) loss 0.3174 (0.3943) acc 93.7500 (90.6250) lr 4.6417e-04 eta 0:01:20
epoch [36/50] batch [10/25] time 0.071 (0.144) data 0.000 (0.073) loss 0.6357 (0.4740) acc 87.5000 (89.3750) lr 4.6417e-04 eta 0:00:52
epoch [36/50] batch [15/25] time 0.070 (0.119) data 0.000 (0.049) loss 0.3950 (0.5199) acc 93.7500 (88.1250) lr 4.6417e-04 eta 0:00:42
epoch [36/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.6465 (0.5268) acc 84.3750 (87.5000) lr 4.6417e-04 eta 0:00:38
epoch [36/50] batch [25/25] time 0.074 (0.100) data 0.000 (0.029) loss 0.7905 (0.5198) acc 78.1250 (88.1250) lr 4.1221e-04 eta 0:00:35
epoch [37/50] batch [5/25] time 0.071 (0.238) data 0.000 (0.166) loss 0.2886 (0.3764) acc 93.7500 (91.2500) lr 4.1221e-04 eta 0:01:21
epoch [37/50] batch [10/25] time 0.071 (0.154) data 0.000 (0.083) loss 0.4299 (0.4471) acc 87.5000 (87.8125) lr 4.1221e-04 eta 0:00:52
epoch [37/50] batch [15/25] time 0.070 (0.126) data 0.000 (0.055) loss 0.5356 (0.4467) acc 84.3750 (86.6667) lr 4.1221e-04 eta 0:00:42
epoch [37/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.042) loss 0.8462 (0.4685) acc 75.0000 (86.7188) lr 4.1221e-04 eta 0:00:36
epoch [37/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.1317 (0.5007) acc 100.0000 (86.7500) lr 3.6258e-04 eta 0:00:33
epoch [38/50] batch [5/25] time 0.073 (0.239) data 0.000 (0.165) loss 0.2979 (0.4121) acc 96.8750 (91.2500) lr 3.6258e-04 eta 0:01:16
epoch [38/50] batch [10/25] time 0.071 (0.155) data 0.000 (0.083) loss 0.4211 (0.3827) acc 87.5000 (91.5625) lr 3.6258e-04 eta 0:00:48
epoch [38/50] batch [15/25] time 0.070 (0.127) data 0.000 (0.055) loss 0.4900 (0.4018) acc 81.2500 (90.6250) lr 3.6258e-04 eta 0:00:39
epoch [38/50] batch [20/25] time 0.071 (0.113) data 0.000 (0.041) loss 0.4409 (0.4084) acc 84.3750 (90.0000) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.4309 (0.4414) acc 87.5000 (89.3750) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [5/25] time 0.070 (0.246) data 0.000 (0.175) loss 0.7148 (0.5092) acc 84.3750 (88.1250) lr 3.1545e-04 eta 0:01:12
epoch [39/50] batch [10/25] time 0.070 (0.158) data 0.000 (0.087) loss 0.2505 (0.4897) acc 93.7500 (87.8125) lr 3.1545e-04 eta 0:00:45
epoch [39/50] batch [15/25] time 0.070 (0.129) data 0.000 (0.058) loss 0.7231 (0.4708) acc 84.3750 (88.7500) lr 3.1545e-04 eta 0:00:36
epoch [39/50] batch [20/25] time 0.073 (0.114) data 0.000 (0.044) loss 0.2085 (0.4515) acc 93.7500 (89.0625) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [25/25] time 0.071 (0.106) data 0.000 (0.035) loss 0.3245 (0.4382) acc 93.7500 (89.6250) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [5/25] time 0.071 (0.242) data 0.001 (0.170) loss 0.3801 (0.4818) acc 96.8750 (90.0000) lr 2.7103e-04 eta 0:01:05
epoch [40/50] batch [10/25] time 0.070 (0.156) data 0.000 (0.085) loss 0.4724 (0.4722) acc 87.5000 (90.0000) lr 2.7103e-04 eta 0:00:41
epoch [40/50] batch [15/25] time 0.070 (0.127) data 0.000 (0.057) loss 0.8159 (0.4649) acc 81.2500 (89.5833) lr 2.7103e-04 eta 0:00:33
epoch [40/50] batch [20/25] time 0.070 (0.113) data 0.000 (0.043) loss 0.3804 (0.4645) acc 90.6250 (89.3750) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.034) loss 0.3843 (0.4599) acc 87.5000 (89.3750) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [5/25] time 0.070 (0.236) data 0.000 (0.165) loss 0.7583 (0.4652) acc 84.3750 (91.2500) lr 2.2949e-04 eta 0:00:57
epoch [41/50] batch [10/25] time 0.070 (0.153) data 0.000 (0.082) loss 0.5229 (0.4620) acc 84.3750 (88.7500) lr 2.2949e-04 eta 0:00:36
epoch [41/50] batch [15/25] time 0.070 (0.126) data 0.000 (0.055) loss 0.3879 (0.4509) acc 90.6250 (88.7500) lr 2.2949e-04 eta 0:00:29
epoch [41/50] batch [20/25] time 0.071 (0.112) data 0.000 (0.041) loss 0.6084 (0.4710) acc 84.3750 (87.9688) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [25/25] time 0.074 (0.104) data 0.000 (0.033) loss 0.8286 (0.4747) acc 84.3750 (88.5000) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [5/25] time 0.071 (0.244) data 0.000 (0.173) loss 0.4604 (0.4372) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:00:53
epoch [42/50] batch [10/25] time 0.070 (0.158) data 0.000 (0.087) loss 0.3970 (0.4204) acc 90.6250 (88.7500) lr 1.9098e-04 eta 0:00:33
epoch [42/50] batch [15/25] time 0.070 (0.129) data 0.000 (0.058) loss 0.5181 (0.4347) acc 87.5000 (88.3333) lr 1.9098e-04 eta 0:00:27
epoch [42/50] batch [20/25] time 0.071 (0.114) data 0.000 (0.044) loss 0.4194 (0.4312) acc 87.5000 (89.0625) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.4343 (0.4496) acc 90.6250 (88.3750) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [5/25] time 0.070 (0.238) data 0.000 (0.165) loss 0.7690 (0.5309) acc 75.0000 (85.0000) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [10/25] time 0.070 (0.154) data 0.000 (0.083) loss 0.4114 (0.4930) acc 90.6250 (86.8750) lr 1.5567e-04 eta 0:00:29
epoch [43/50] batch [15/25] time 0.070 (0.126) data 0.000 (0.055) loss 0.4333 (0.4855) acc 84.3750 (86.6667) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.041) loss 0.4060 (0.4718) acc 87.5000 (87.3438) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/25] time 0.071 (0.104) data 0.000 (0.033) loss 0.4587 (0.4628) acc 87.5000 (87.6250) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [5/25] time 0.070 (0.210) data 0.000 (0.139) loss 0.6240 (0.5569) acc 84.3750 (83.1250) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [10/25] time 0.070 (0.140) data 0.000 (0.069) loss 0.1758 (0.4659) acc 96.8750 (87.1875) lr 1.2369e-04 eta 0:00:23
epoch [44/50] batch [15/25] time 0.070 (0.117) data 0.000 (0.046) loss 0.4270 (0.4787) acc 90.6250 (87.0833) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [20/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.6831 (0.4674) acc 81.2500 (87.9688) lr 1.2369e-04 eta 0:00:16
epoch [44/50] batch [25/25] time 0.070 (0.098) data 0.000 (0.028) loss 0.3623 (0.4618) acc 93.7500 (88.5000) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [5/25] time 0.071 (0.206) data 0.000 (0.132) loss 0.4248 (0.4656) acc 90.6250 (88.1250) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [10/25] time 0.071 (0.138) data 0.000 (0.066) loss 0.6689 (0.4667) acc 84.3750 (87.8125) lr 9.5173e-05 eta 0:00:19
epoch [45/50] batch [15/25] time 0.071 (0.116) data 0.000 (0.044) loss 0.2456 (0.4520) acc 96.8750 (88.3333) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [20/25] time 0.071 (0.104) data 0.000 (0.033) loss 0.2849 (0.4528) acc 93.7500 (87.9688) lr 9.5173e-05 eta 0:00:13
epoch [45/50] batch [25/25] time 0.071 (0.098) data 0.000 (0.027) loss 0.2355 (0.4541) acc 93.7500 (88.3750) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [5/25] time 0.070 (0.216) data 0.000 (0.145) loss 0.4409 (0.4063) acc 93.7500 (88.7500) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [10/25] time 0.070 (0.143) data 0.000 (0.073) loss 0.3486 (0.3891) acc 90.6250 (89.6875) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [15/25] time 0.070 (0.119) data 0.000 (0.049) loss 0.2681 (0.3812) acc 93.7500 (90.6250) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.4495 (0.3664) acc 87.5000 (91.2500) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/25] time 0.074 (0.100) data 0.000 (0.029) loss 0.2230 (0.3815) acc 96.8750 (90.8750) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [5/25] time 0.073 (0.234) data 0.000 (0.158) loss 0.2827 (0.3673) acc 96.8750 (92.5000) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [10/25] time 0.071 (0.154) data 0.000 (0.079) loss 0.8193 (0.4062) acc 78.1250 (91.2500) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/25] time 0.070 (0.126) data 0.000 (0.053) loss 0.3948 (0.4063) acc 90.6250 (91.2500) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.040) loss 0.2869 (0.4090) acc 96.8750 (91.2500) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [25/25] time 0.071 (0.104) data 0.000 (0.032) loss 0.9736 (0.4495) acc 75.0000 (90.1250) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/25] time 0.071 (0.233) data 0.000 (0.161) loss 0.5093 (0.4656) acc 90.6250 (88.7500) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/25] time 0.071 (0.152) data 0.000 (0.081) loss 0.7676 (0.4607) acc 78.1250 (88.7500) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/25] time 0.070 (0.125) data 0.000 (0.054) loss 0.4287 (0.4124) acc 93.7500 (90.6250) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/25] time 0.071 (0.111) data 0.000 (0.041) loss 0.2031 (0.4230) acc 93.7500 (90.3125) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/25] time 0.071 (0.103) data 0.000 (0.032) loss 0.4568 (0.4232) acc 84.3750 (90.1250) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [5/25] time 0.070 (0.213) data 0.000 (0.140) loss 0.4478 (0.5038) acc 90.6250 (88.1250) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [10/25] time 0.070 (0.142) data 0.000 (0.070) loss 0.3447 (0.4542) acc 96.8750 (89.6875) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [15/25] time 0.070 (0.118) data 0.000 (0.047) loss 0.2534 (0.4467) acc 93.7500 (89.5833) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.070 (0.106) data 0.000 (0.035) loss 0.4717 (0.4604) acc 87.5000 (88.9062) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.073 (0.099) data 0.000 (0.028) loss 0.8833 (0.4631) acc 78.1250 (89.1250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.070 (0.241) data 0.000 (0.167) loss 0.2186 (0.4033) acc 93.7500 (90.0000) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/25] time 0.070 (0.156) data 0.000 (0.083) loss 0.4021 (0.3830) acc 93.7500 (91.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.070 (0.127) data 0.000 (0.056) loss 0.3003 (0.3757) acc 90.6250 (91.6667) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.070 (0.113) data 0.000 (0.042) loss 0.4880 (0.4063) acc 90.6250 (90.4688) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.071 (0.104) data 0.000 (0.034) loss 0.2588 (0.4010) acc 93.7500 (90.6250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:10,  3.58s/it] 50%|█████     | 2/4 [00:04<00:03,  1.81s/it] 75%|███████▌  | 3/4 [00:04<00:01,  1.25s/it]100%|██████████| 4/4 [00:04<00:00,  1.26it/s]100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
=> result
* total: 1,549
* correct: 1,524
* accuracy: 98.4%
* error: 1.6%
* macro_f1: 96.8%
Elapsed: 0:02:19
Run this job and save the output to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/25] time 0.073 (0.302) data 0.000 (0.189) loss 2.7051 (2.5949) acc 75.0000 (76.2500) lr 1.0000e-05 eta 0:06:16
epoch [1/50] batch [10/25] time 0.070 (0.187) data 0.000 (0.095) loss 2.2031 (2.5436) acc 87.5000 (78.1250) lr 1.0000e-05 eta 0:03:51
epoch [1/50] batch [15/25] time 0.069 (0.148) data 0.000 (0.063) loss 2.5586 (2.5329) acc 78.1250 (78.1250) lr 1.0000e-05 eta 0:03:02
epoch [1/50] batch [20/25] time 0.071 (0.129) data 0.000 (0.047) loss 2.6016 (2.5387) acc 75.0000 (77.0312) lr 1.0000e-05 eta 0:02:38
epoch [1/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.038) loss 2.3555 (2.5147) acc 81.2500 (76.8750) lr 2.0000e-03 eta 0:02:23
epoch [2/50] batch [5/25] time 0.070 (0.276) data 0.000 (0.204) loss 1.3672 (1.5400) acc 87.5000 (84.3750) lr 2.0000e-03 eta 0:05:36
epoch [2/50] batch [10/25] time 0.070 (0.173) data 0.000 (0.102) loss 1.0078 (1.4206) acc 87.5000 (82.5000) lr 2.0000e-03 eta 0:03:30
epoch [2/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 0.8193 (1.2693) acc 90.6250 (84.5833) lr 2.0000e-03 eta 0:02:47
epoch [2/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.051) loss 0.7793 (1.1952) acc 90.6250 (84.2188) lr 2.0000e-03 eta 0:02:26
epoch [2/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 1.1514 (1.1149) acc 78.1250 (84.8750) lr 1.9980e-03 eta 0:02:13
epoch [3/50] batch [5/25] time 0.070 (0.249) data 0.000 (0.174) loss 0.5596 (0.8426) acc 96.8750 (87.5000) lr 1.9980e-03 eta 0:04:57
epoch [3/50] batch [10/25] time 0.070 (0.160) data 0.000 (0.087) loss 0.6338 (0.8100) acc 90.6250 (85.9375) lr 1.9980e-03 eta 0:03:09
epoch [3/50] batch [15/25] time 0.070 (0.130) data 0.000 (0.058) loss 0.8096 (0.8114) acc 84.3750 (84.3750) lr 1.9980e-03 eta 0:02:33
epoch [3/50] batch [20/25] time 0.070 (0.115) data 0.000 (0.044) loss 0.5693 (0.7571) acc 84.3750 (84.5312) lr 1.9980e-03 eta 0:02:15
epoch [3/50] batch [25/25] time 0.071 (0.106) data 0.000 (0.035) loss 1.1738 (0.7543) acc 68.7500 (84.5000) lr 1.9921e-03 eta 0:02:04
epoch [4/50] batch [5/25] time 0.069 (0.253) data 0.000 (0.180) loss 0.6772 (0.7267) acc 84.3750 (83.1250) lr 1.9921e-03 eta 0:04:56
epoch [4/50] batch [10/25] time 0.069 (0.161) data 0.000 (0.090) loss 0.5742 (0.6372) acc 87.5000 (85.9375) lr 1.9921e-03 eta 0:03:08
epoch [4/50] batch [15/25] time 0.069 (0.131) data 0.000 (0.060) loss 0.4241 (0.6420) acc 93.7500 (85.8333) lr 1.9921e-03 eta 0:02:31
epoch [4/50] batch [20/25] time 0.069 (0.116) data 0.000 (0.045) loss 0.3574 (0.6036) acc 90.6250 (86.4062) lr 1.9921e-03 eta 0:02:13
epoch [4/50] batch [25/25] time 0.071 (0.106) data 0.000 (0.036) loss 0.7705 (0.5834) acc 78.1250 (86.7500) lr 1.9823e-03 eta 0:02:02
epoch [5/50] batch [5/25] time 0.074 (0.244) data 0.000 (0.169) loss 0.3149 (0.4911) acc 96.8750 (90.0000) lr 1.9823e-03 eta 0:04:39
epoch [5/50] batch [10/25] time 0.074 (0.159) data 0.000 (0.085) loss 0.5015 (0.5422) acc 90.6250 (89.0625) lr 1.9823e-03 eta 0:03:01
epoch [5/50] batch [15/25] time 0.074 (0.131) data 0.000 (0.057) loss 0.4299 (0.5132) acc 87.5000 (89.3750) lr 1.9823e-03 eta 0:02:28
epoch [5/50] batch [20/25] time 0.075 (0.117) data 0.000 (0.043) loss 0.3628 (0.5231) acc 93.7500 (88.7500) lr 1.9823e-03 eta 0:02:11
epoch [5/50] batch [25/25] time 0.075 (0.108) data 0.001 (0.034) loss 0.7163 (0.5374) acc 81.2500 (87.8750) lr 1.9686e-03 eta 0:02:01
epoch [6/50] batch [5/25] time 0.074 (0.236) data 0.000 (0.158) loss 0.6421 (0.5927) acc 84.3750 (85.6250) lr 1.9686e-03 eta 0:04:24
epoch [6/50] batch [10/25] time 0.073 (0.155) data 0.000 (0.079) loss 0.3081 (0.5729) acc 93.7500 (86.2500) lr 1.9686e-03 eta 0:02:52
epoch [6/50] batch [15/25] time 0.073 (0.128) data 0.000 (0.053) loss 0.3398 (0.6015) acc 96.8750 (85.4167) lr 1.9686e-03 eta 0:02:21
epoch [6/50] batch [20/25] time 0.075 (0.114) data 0.000 (0.040) loss 0.3823 (0.5710) acc 87.5000 (86.2500) lr 1.9686e-03 eta 0:02:06
epoch [6/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.032) loss 0.3723 (0.5565) acc 96.8750 (87.0000) lr 1.9511e-03 eta 0:01:55
epoch [7/50] batch [5/25] time 0.071 (0.230) data 0.000 (0.154) loss 0.5454 (0.6746) acc 90.6250 (83.1250) lr 1.9511e-03 eta 0:04:11
epoch [7/50] batch [10/25] time 0.070 (0.150) data 0.000 (0.077) loss 0.4387 (0.6015) acc 87.5000 (85.9375) lr 1.9511e-03 eta 0:02:43
epoch [7/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.052) loss 0.6099 (0.5641) acc 84.3750 (87.0833) lr 1.9511e-03 eta 0:02:14
epoch [7/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.039) loss 0.4421 (0.5428) acc 90.6250 (87.1875) lr 1.9511e-03 eta 0:01:59
epoch [7/50] batch [25/25] time 0.074 (0.103) data 0.000 (0.031) loss 0.7300 (0.5435) acc 75.0000 (86.7500) lr 1.9298e-03 eta 0:01:50
epoch [8/50] batch [5/25] time 0.070 (0.251) data 0.000 (0.179) loss 0.5005 (0.5481) acc 90.6250 (85.6250) lr 1.9298e-03 eta 0:04:28
epoch [8/50] batch [10/25] time 0.071 (0.161) data 0.000 (0.090) loss 0.4297 (0.6138) acc 87.5000 (83.1250) lr 1.9298e-03 eta 0:02:51
epoch [8/50] batch [15/25] time 0.071 (0.131) data 0.000 (0.060) loss 0.6724 (0.6374) acc 78.1250 (82.0833) lr 1.9298e-03 eta 0:02:18
epoch [8/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.045) loss 0.2102 (0.5907) acc 96.8750 (83.4375) lr 1.9298e-03 eta 0:02:02
epoch [8/50] batch [25/25] time 0.071 (0.107) data 0.000 (0.036) loss 0.5576 (0.5771) acc 84.3750 (84.1250) lr 1.9048e-03 eta 0:01:52
epoch [9/50] batch [5/25] time 0.070 (0.232) data 0.000 (0.160) loss 0.3369 (0.4985) acc 90.6250 (86.8750) lr 1.9048e-03 eta 0:04:02
epoch [9/50] batch [10/25] time 0.071 (0.152) data 0.000 (0.080) loss 0.4966 (0.5212) acc 93.7500 (88.4375) lr 1.9048e-03 eta 0:02:37
epoch [9/50] batch [15/25] time 0.071 (0.125) data 0.000 (0.053) loss 0.4595 (0.5181) acc 87.5000 (87.7083) lr 1.9048e-03 eta 0:02:09
epoch [9/50] batch [20/25] time 0.071 (0.111) data 0.000 (0.040) loss 0.3669 (0.4995) acc 87.5000 (88.1250) lr 1.9048e-03 eta 0:01:54
epoch [9/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.7764 (0.5070) acc 84.3750 (87.8750) lr 1.8763e-03 eta 0:01:45
epoch [10/50] batch [5/25] time 0.070 (0.259) data 0.000 (0.187) loss 0.9136 (0.6716) acc 78.1250 (83.7500) lr 1.8763e-03 eta 0:04:23
epoch [10/50] batch [10/25] time 0.070 (0.164) data 0.000 (0.094) loss 0.6870 (0.5878) acc 78.1250 (85.6250) lr 1.8763e-03 eta 0:02:46
epoch [10/50] batch [15/25] time 0.071 (0.133) data 0.000 (0.063) loss 0.4763 (0.5376) acc 90.6250 (87.0833) lr 1.8763e-03 eta 0:02:14
epoch [10/50] batch [20/25] time 0.071 (0.118) data 0.000 (0.047) loss 0.6230 (0.5404) acc 81.2500 (86.5625) lr 1.8763e-03 eta 0:01:58
epoch [10/50] batch [25/25] time 0.070 (0.108) data 0.000 (0.038) loss 0.5732 (0.5473) acc 84.3750 (86.2500) lr 1.8443e-03 eta 0:01:48
epoch [11/50] batch [5/25] time 0.074 (0.271) data 0.000 (0.192) loss 0.2035 (0.4017) acc 100.0000 (91.8750) lr 1.8443e-03 eta 0:04:29
epoch [11/50] batch [10/25] time 0.074 (0.172) data 0.000 (0.096) loss 0.3633 (0.4347) acc 90.6250 (89.6875) lr 1.8443e-03 eta 0:02:50
epoch [11/50] batch [15/25] time 0.073 (0.139) data 0.000 (0.064) loss 0.3894 (0.4960) acc 87.5000 (87.2917) lr 1.8443e-03 eta 0:02:17
epoch [11/50] batch [20/25] time 0.074 (0.123) data 0.000 (0.048) loss 0.2656 (0.4502) acc 96.8750 (88.5938) lr 1.8443e-03 eta 0:02:00
epoch [11/50] batch [25/25] time 0.073 (0.113) data 0.000 (0.039) loss 0.6548 (0.4770) acc 81.2500 (87.5000) lr 1.8090e-03 eta 0:01:50
epoch [12/50] batch [5/25] time 0.073 (0.260) data 0.000 (0.183) loss 0.4189 (0.6756) acc 96.8750 (83.1250) lr 1.8090e-03 eta 0:04:11
epoch [12/50] batch [10/25] time 0.074 (0.167) data 0.000 (0.091) loss 0.2632 (0.5582) acc 96.8750 (86.8750) lr 1.8090e-03 eta 0:02:40
epoch [12/50] batch [15/25] time 0.071 (0.135) data 0.000 (0.061) loss 0.5371 (0.5540) acc 84.3750 (86.2500) lr 1.8090e-03 eta 0:02:09
epoch [12/50] batch [20/25] time 0.071 (0.119) data 0.000 (0.046) loss 0.5864 (0.5252) acc 81.2500 (87.1875) lr 1.8090e-03 eta 0:01:53
epoch [12/50] batch [25/25] time 0.072 (0.109) data 0.000 (0.037) loss 0.5762 (0.5228) acc 84.3750 (87.3750) lr 1.7705e-03 eta 0:01:43
epoch [13/50] batch [5/25] time 0.071 (0.234) data 0.000 (0.160) loss 0.4905 (0.4373) acc 93.7500 (90.6250) lr 1.7705e-03 eta 0:03:41
epoch [13/50] batch [10/25] time 0.070 (0.153) data 0.000 (0.080) loss 0.6538 (0.4973) acc 78.1250 (87.5000) lr 1.7705e-03 eta 0:02:24
epoch [13/50] batch [15/25] time 0.070 (0.126) data 0.000 (0.053) loss 0.6558 (0.4942) acc 81.2500 (87.5000) lr 1.7705e-03 eta 0:01:57
epoch [13/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.040) loss 0.3096 (0.4752) acc 90.6250 (88.4375) lr 1.7705e-03 eta 0:01:43
epoch [13/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.5059 (0.4964) acc 84.3750 (87.6250) lr 1.7290e-03 eta 0:01:35
epoch [14/50] batch [5/25] time 0.071 (0.250) data 0.000 (0.176) loss 0.5376 (0.5288) acc 87.5000 (86.8750) lr 1.7290e-03 eta 0:03:50
epoch [14/50] batch [10/25] time 0.071 (0.160) data 0.000 (0.088) loss 0.4309 (0.5009) acc 84.3750 (86.8750) lr 1.7290e-03 eta 0:02:26
epoch [14/50] batch [15/25] time 0.070 (0.130) data 0.000 (0.059) loss 0.4700 (0.4941) acc 84.3750 (87.2917) lr 1.7290e-03 eta 0:01:58
epoch [14/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.044) loss 0.4958 (0.5087) acc 90.6250 (86.8750) lr 1.7290e-03 eta 0:01:44
epoch [14/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.035) loss 0.5444 (0.4959) acc 87.5000 (87.6250) lr 1.6845e-03 eta 0:01:36
epoch [15/50] batch [5/25] time 0.071 (0.248) data 0.000 (0.175) loss 0.2135 (0.4180) acc 90.6250 (89.3750) lr 1.6845e-03 eta 0:03:42
epoch [15/50] batch [10/25] time 0.070 (0.160) data 0.000 (0.088) loss 0.4128 (0.4744) acc 90.6250 (87.8125) lr 1.6845e-03 eta 0:02:21
epoch [15/50] batch [15/25] time 0.071 (0.130) data 0.000 (0.059) loss 0.9912 (0.5046) acc 78.1250 (87.9167) lr 1.6845e-03 eta 0:01:54
epoch [15/50] batch [20/25] time 0.070 (0.115) data 0.000 (0.044) loss 0.5591 (0.5215) acc 84.3750 (87.6562) lr 1.6845e-03 eta 0:01:41
epoch [15/50] batch [25/25] time 0.070 (0.106) data 0.000 (0.035) loss 0.4146 (0.5441) acc 87.5000 (86.7500) lr 1.6374e-03 eta 0:01:32
epoch [16/50] batch [5/25] time 0.070 (0.245) data 0.000 (0.171) loss 0.3350 (0.4940) acc 93.7500 (88.7500) lr 1.6374e-03 eta 0:03:32
epoch [16/50] batch [10/25] time 0.070 (0.158) data 0.000 (0.086) loss 0.3032 (0.4716) acc 93.7500 (90.0000) lr 1.6374e-03 eta 0:02:16
epoch [16/50] batch [15/25] time 0.069 (0.129) data 0.000 (0.057) loss 0.2944 (0.4902) acc 96.8750 (89.3750) lr 1.6374e-03 eta 0:01:50
epoch [16/50] batch [20/25] time 0.070 (0.114) data 0.000 (0.043) loss 0.5049 (0.5170) acc 84.3750 (88.1250) lr 1.6374e-03 eta 0:01:37
epoch [16/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.034) loss 0.5469 (0.4874) acc 84.3750 (89.0000) lr 1.5878e-03 eta 0:01:29
epoch [17/50] batch [5/25] time 0.070 (0.253) data 0.000 (0.181) loss 0.7305 (0.5261) acc 78.1250 (87.5000) lr 1.5878e-03 eta 0:03:33
epoch [17/50] batch [10/25] time 0.070 (0.161) data 0.000 (0.091) loss 1.0117 (0.5282) acc 68.7500 (85.6250) lr 1.5878e-03 eta 0:02:15
epoch [17/50] batch [15/25] time 0.069 (0.131) data 0.000 (0.061) loss 0.4167 (0.4874) acc 87.5000 (86.2500) lr 1.5878e-03 eta 0:01:49
epoch [17/50] batch [20/25] time 0.069 (0.116) data 0.000 (0.045) loss 0.5479 (0.4931) acc 87.5000 (87.0312) lr 1.5878e-03 eta 0:01:36
epoch [17/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.036) loss 0.9580 (0.4883) acc 78.1250 (87.3750) lr 1.5358e-03 eta 0:01:27
epoch [18/50] batch [5/25] time 0.071 (0.248) data 0.000 (0.177) loss 0.2278 (0.4468) acc 93.7500 (89.3750) lr 1.5358e-03 eta 0:03:23
epoch [18/50] batch [10/25] time 0.071 (0.160) data 0.000 (0.089) loss 0.5498 (0.4574) acc 90.6250 (89.6875) lr 1.5358e-03 eta 0:02:10
epoch [18/50] batch [15/25] time 0.070 (0.130) data 0.000 (0.059) loss 0.6455 (0.4748) acc 87.5000 (89.3750) lr 1.5358e-03 eta 0:01:44
epoch [18/50] batch [20/25] time 0.070 (0.115) data 0.000 (0.045) loss 0.5654 (0.5093) acc 90.6250 (88.2812) lr 1.5358e-03 eta 0:01:32
epoch [18/50] batch [25/25] time 0.070 (0.106) data 0.000 (0.036) loss 0.6909 (0.5247) acc 81.2500 (87.8750) lr 1.4818e-03 eta 0:01:24
epoch [19/50] batch [5/25] time 0.070 (0.255) data 0.000 (0.183) loss 0.4399 (0.4804) acc 90.6250 (89.3750) lr 1.4818e-03 eta 0:03:22
epoch [19/50] batch [10/25] time 0.070 (0.163) data 0.000 (0.092) loss 0.2705 (0.4603) acc 93.7500 (89.3750) lr 1.4818e-03 eta 0:02:08
epoch [19/50] batch [15/25] time 0.070 (0.132) data 0.000 (0.061) loss 0.4092 (0.4436) acc 90.6250 (89.3750) lr 1.4818e-03 eta 0:01:43
epoch [19/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.046) loss 0.4492 (0.4559) acc 84.3750 (88.5938) lr 1.4818e-03 eta 0:01:30
epoch [19/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.3403 (0.4556) acc 96.8750 (88.8750) lr 1.4258e-03 eta 0:01:22
epoch [20/50] batch [5/25] time 0.070 (0.247) data 0.000 (0.173) loss 0.5923 (0.4901) acc 84.3750 (88.7500) lr 1.4258e-03 eta 0:03:10
epoch [20/50] batch [10/25] time 0.070 (0.159) data 0.000 (0.087) loss 0.2607 (0.5127) acc 93.7500 (86.5625) lr 1.4258e-03 eta 0:02:01
epoch [20/50] batch [15/25] time 0.070 (0.129) data 0.000 (0.058) loss 0.3989 (0.4830) acc 87.5000 (87.0833) lr 1.4258e-03 eta 0:01:38
epoch [20/50] batch [20/25] time 0.070 (0.114) data 0.000 (0.043) loss 0.5449 (0.4846) acc 84.3750 (87.1875) lr 1.4258e-03 eta 0:01:26
epoch [20/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.2812 (0.5023) acc 90.6250 (87.2500) lr 1.3681e-03 eta 0:01:19
epoch [21/50] batch [5/25] time 0.070 (0.249) data 0.000 (0.178) loss 0.4861 (0.4577) acc 90.6250 (86.8750) lr 1.3681e-03 eta 0:03:05
epoch [21/50] batch [10/25] time 0.070 (0.160) data 0.000 (0.089) loss 0.4214 (0.4897) acc 87.5000 (86.8750) lr 1.3681e-03 eta 0:01:58
epoch [21/50] batch [15/25] time 0.073 (0.131) data 0.000 (0.059) loss 0.3027 (0.5231) acc 90.6250 (85.8333) lr 1.3681e-03 eta 0:01:36
epoch [21/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.045) loss 0.6538 (0.5431) acc 87.5000 (86.0938) lr 1.3681e-03 eta 0:01:24
epoch [21/50] batch [25/25] time 0.070 (0.106) data 0.000 (0.036) loss 0.5869 (0.5383) acc 84.3750 (85.7500) lr 1.3090e-03 eta 0:01:17
epoch [22/50] batch [5/25] time 0.070 (0.244) data 0.000 (0.173) loss 0.6646 (0.5233) acc 84.3750 (88.7500) lr 1.3090e-03 eta 0:02:55
epoch [22/50] batch [10/25] time 0.070 (0.157) data 0.000 (0.087) loss 0.4692 (0.5164) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:52
epoch [22/50] batch [15/25] time 0.070 (0.128) data 0.000 (0.058) loss 0.3938 (0.4943) acc 90.6250 (88.3333) lr 1.3090e-03 eta 0:01:30
epoch [22/50] batch [20/25] time 0.070 (0.114) data 0.000 (0.044) loss 0.1908 (0.4605) acc 96.8750 (89.8438) lr 1.3090e-03 eta 0:01:20
epoch [22/50] batch [25/25] time 0.070 (0.105) data 0.000 (0.035) loss 0.5952 (0.4587) acc 78.1250 (89.3750) lr 1.2487e-03 eta 0:01:13
epoch [23/50] batch [5/25] time 0.070 (0.250) data 0.000 (0.179) loss 0.9004 (0.5909) acc 75.0000 (86.2500) lr 1.2487e-03 eta 0:02:53
epoch [23/50] batch [10/25] time 0.071 (0.160) data 0.000 (0.090) loss 0.5527 (0.5518) acc 81.2500 (85.6250) lr 1.2487e-03 eta 0:01:50
epoch [23/50] batch [15/25] time 0.070 (0.130) data 0.000 (0.060) loss 0.6304 (0.5409) acc 75.0000 (85.2083) lr 1.2487e-03 eta 0:01:29
epoch [23/50] batch [20/25] time 0.070 (0.115) data 0.000 (0.045) loss 0.2671 (0.4966) acc 96.8750 (86.4062) lr 1.2487e-03 eta 0:01:18
epoch [23/50] batch [25/25] time 0.070 (0.106) data 0.000 (0.036) loss 0.4780 (0.4884) acc 90.6250 (87.0000) lr 1.1874e-03 eta 0:01:11
epoch [24/50] batch [5/25] time 0.070 (0.264) data 0.000 (0.191) loss 0.4856 (0.4674) acc 87.5000 (90.6250) lr 1.1874e-03 eta 0:02:57
epoch [24/50] batch [10/25] time 0.070 (0.167) data 0.000 (0.096) loss 0.2993 (0.4001) acc 93.7500 (92.1875) lr 1.1874e-03 eta 0:01:51
epoch [24/50] batch [15/25] time 0.070 (0.135) data 0.000 (0.064) loss 0.4614 (0.4262) acc 87.5000 (91.0417) lr 1.1874e-03 eta 0:01:28
epoch [24/50] batch [20/25] time 0.070 (0.118) data 0.000 (0.048) loss 0.5781 (0.4296) acc 78.1250 (90.6250) lr 1.1874e-03 eta 0:01:17
epoch [24/50] batch [25/25] time 0.070 (0.109) data 0.000 (0.038) loss 0.3108 (0.4306) acc 93.7500 (90.6250) lr 1.1253e-03 eta 0:01:10
epoch [25/50] batch [5/25] time 0.070 (0.256) data 0.000 (0.185) loss 0.5894 (0.5097) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:02:45
epoch [25/50] batch [10/25] time 0.070 (0.163) data 0.000 (0.093) loss 0.3706 (0.4777) acc 90.6250 (87.1875) lr 1.1253e-03 eta 0:01:44
epoch [25/50] batch [15/25] time 0.073 (0.133) data 0.000 (0.062) loss 0.4216 (0.4580) acc 87.5000 (87.5000) lr 1.1253e-03 eta 0:01:24
epoch [25/50] batch [20/25] time 0.070 (0.117) data 0.000 (0.046) loss 0.5283 (0.4590) acc 84.3750 (87.8125) lr 1.1253e-03 eta 0:01:13
epoch [25/50] batch [25/25] time 0.071 (0.108) data 0.000 (0.037) loss 0.2220 (0.4397) acc 100.0000 (89.0000) lr 1.0628e-03 eta 0:01:07
epoch [26/50] batch [5/25] time 0.070 (0.255) data 0.000 (0.184) loss 0.4094 (0.5727) acc 87.5000 (85.0000) lr 1.0628e-03 eta 0:02:38
epoch [26/50] batch [10/25] time 0.071 (0.163) data 0.000 (0.092) loss 0.5103 (0.5008) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:01:40
epoch [26/50] batch [15/25] time 0.070 (0.132) data 0.000 (0.062) loss 0.4150 (0.5028) acc 90.6250 (87.5000) lr 1.0628e-03 eta 0:01:20
epoch [26/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.046) loss 0.4536 (0.4937) acc 93.7500 (87.5000) lr 1.0628e-03 eta 0:01:10
epoch [26/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.037) loss 0.4473 (0.4607) acc 87.5000 (88.3750) lr 1.0000e-03 eta 0:01:04
epoch [27/50] batch [5/25] time 0.071 (0.253) data 0.000 (0.182) loss 0.3550 (0.4927) acc 90.6250 (85.6250) lr 1.0000e-03 eta 0:02:30
epoch [27/50] batch [10/25] time 0.070 (0.162) data 0.000 (0.091) loss 0.6143 (0.4606) acc 78.1250 (86.5625) lr 1.0000e-03 eta 0:01:35
epoch [27/50] batch [15/25] time 0.070 (0.131) data 0.000 (0.061) loss 0.4653 (0.4643) acc 90.6250 (87.2917) lr 1.0000e-03 eta 0:01:16
epoch [27/50] batch [20/25] time 0.070 (0.116) data 0.000 (0.046) loss 0.3550 (0.4415) acc 93.7500 (88.9062) lr 1.0000e-03 eta 0:01:07
epoch [27/50] batch [25/25] time 0.070 (0.107) data 0.000 (0.036) loss 0.3359 (0.4271) acc 90.6250 (89.3750) lr 9.3721e-04 eta 0:01:01
epoch [28/50] batch [5/25] time 0.071 (0.235) data 0.000 (0.157) loss 0.3362 (0.4478) acc 90.6250 (90.0000) lr 9.3721e-04 eta 0:02:14
epoch [28/50] batch [10/25] time 0.070 (0.153) data 0.000 (0.079) loss 0.6016 (0.4304) acc 84.3750 (90.6250) lr 9.3721e-04 eta 0:01:26
epoch [28/50] batch [15/25] time 0.070 (0.125) data 0.000 (0.053) loss 0.3652 (0.4251) acc 90.6250 (90.2083) lr 9.3721e-04 eta 0:01:10
epoch [28/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.040) loss 0.1591 (0.4232) acc 96.8750 (90.1562) lr 9.3721e-04 eta 0:01:02
epoch [28/50] batch [25/25] time 0.071 (0.104) data 0.000 (0.032) loss 0.6895 (0.4185) acc 87.5000 (90.7500) lr 8.7467e-04 eta 0:00:57
epoch [29/50] batch [5/25] time 0.071 (0.234) data 0.000 (0.157) loss 0.7573 (0.5191) acc 84.3750 (88.7500) lr 8.7467e-04 eta 0:02:07
epoch [29/50] batch [10/25] time 0.070 (0.152) data 0.000 (0.079) loss 0.4290 (0.4519) acc 93.7500 (90.3125) lr 8.7467e-04 eta 0:01:22
epoch [29/50] batch [15/25] time 0.071 (0.125) data 0.000 (0.053) loss 0.4827 (0.4664) acc 90.6250 (89.5833) lr 8.7467e-04 eta 0:01:06
epoch [29/50] batch [20/25] time 0.071 (0.111) data 0.000 (0.039) loss 0.3660 (0.4557) acc 90.6250 (89.8438) lr 8.7467e-04 eta 0:00:59
epoch [29/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.4636 (0.4486) acc 90.6250 (90.0000) lr 8.1262e-04 eta 0:00:54
epoch [30/50] batch [5/25] time 0.070 (0.227) data 0.000 (0.156) loss 0.4739 (0.5538) acc 84.3750 (87.5000) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [10/25] time 0.070 (0.149) data 0.000 (0.078) loss 0.3862 (0.4947) acc 90.6250 (89.0625) lr 8.1262e-04 eta 0:01:16
epoch [30/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.052) loss 0.3022 (0.4214) acc 93.7500 (91.0417) lr 8.1262e-04 eta 0:01:02
epoch [30/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.039) loss 0.3828 (0.4206) acc 96.8750 (90.7812) lr 8.1262e-04 eta 0:00:55
epoch [30/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.031) loss 0.8281 (0.4602) acc 75.0000 (89.6250) lr 7.5131e-04 eta 0:00:50
epoch [31/50] batch [5/25] time 0.070 (0.231) data 0.000 (0.159) loss 0.3755 (0.4876) acc 93.7500 (89.3750) lr 7.5131e-04 eta 0:01:54
epoch [31/50] batch [10/25] time 0.070 (0.151) data 0.000 (0.080) loss 0.2800 (0.4471) acc 87.5000 (88.7500) lr 7.5131e-04 eta 0:01:13
epoch [31/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.053) loss 0.6523 (0.4262) acc 87.5000 (90.0000) lr 7.5131e-04 eta 0:01:00
epoch [31/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.040) loss 0.4023 (0.4118) acc 93.7500 (90.3125) lr 7.5131e-04 eta 0:00:53
epoch [31/50] batch [25/25] time 0.075 (0.103) data 0.000 (0.032) loss 0.3250 (0.4084) acc 90.6250 (90.3750) lr 6.9098e-04 eta 0:00:48
epoch [32/50] batch [5/25] time 0.071 (0.234) data 0.000 (0.160) loss 0.3350 (0.4812) acc 93.7500 (88.1250) lr 6.9098e-04 eta 0:01:50
epoch [32/50] batch [10/25] time 0.074 (0.153) data 0.000 (0.080) loss 0.7764 (0.4819) acc 75.0000 (87.8125) lr 6.9098e-04 eta 0:01:11
epoch [32/50] batch [15/25] time 0.071 (0.126) data 0.000 (0.053) loss 0.3726 (0.4865) acc 90.6250 (87.2917) lr 6.9098e-04 eta 0:00:57
epoch [32/50] batch [20/25] time 0.072 (0.112) data 0.000 (0.040) loss 0.3457 (0.5189) acc 93.7500 (86.7188) lr 6.9098e-04 eta 0:00:50
epoch [32/50] batch [25/25] time 0.071 (0.104) data 0.000 (0.032) loss 0.2849 (0.4807) acc 93.7500 (88.1250) lr 6.3188e-04 eta 0:00:46
epoch [33/50] batch [5/25] time 0.072 (0.230) data 0.000 (0.156) loss 0.5708 (0.5666) acc 90.6250 (86.8750) lr 6.3188e-04 eta 0:01:42
epoch [33/50] batch [10/25] time 0.073 (0.151) data 0.000 (0.078) loss 0.2079 (0.5378) acc 96.8750 (86.5625) lr 6.3188e-04 eta 0:01:06
epoch [33/50] batch [15/25] time 0.074 (0.125) data 0.000 (0.052) loss 0.2910 (0.4832) acc 93.7500 (88.1250) lr 6.3188e-04 eta 0:00:54
epoch [33/50] batch [20/25] time 0.073 (0.112) data 0.000 (0.039) loss 0.2233 (0.4695) acc 93.7500 (88.4375) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/25] time 0.074 (0.104) data 0.000 (0.031) loss 0.3735 (0.4679) acc 87.5000 (88.8750) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [5/25] time 0.070 (0.232) data 0.000 (0.161) loss 0.5029 (0.4004) acc 90.6250 (92.5000) lr 5.7422e-04 eta 0:01:37
epoch [34/50] batch [10/25] time 0.070 (0.152) data 0.000 (0.080) loss 0.5723 (0.3990) acc 84.3750 (91.5625) lr 5.7422e-04 eta 0:01:02
epoch [34/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.054) loss 0.4463 (0.4008) acc 90.6250 (91.6667) lr 5.7422e-04 eta 0:00:50
epoch [34/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.040) loss 0.6074 (0.4294) acc 87.5000 (90.9375) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.032) loss 0.1931 (0.4299) acc 96.8750 (90.6250) lr 5.1825e-04 eta 0:00:41
epoch [35/50] batch [5/25] time 0.070 (0.230) data 0.000 (0.158) loss 0.6333 (0.4665) acc 84.3750 (88.7500) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [10/25] time 0.070 (0.150) data 0.000 (0.079) loss 0.4775 (0.4694) acc 87.5000 (88.4375) lr 5.1825e-04 eta 0:00:58
epoch [35/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.053) loss 0.6216 (0.4525) acc 87.5000 (89.1667) lr 5.1825e-04 eta 0:00:47
epoch [35/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.5576 (0.4410) acc 87.5000 (89.2188) lr 5.1825e-04 eta 0:00:41
epoch [35/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.3660 (0.4246) acc 93.7500 (89.8750) lr 4.6417e-04 eta 0:00:38
epoch [36/50] batch [5/25] time 0.070 (0.232) data 0.000 (0.158) loss 0.5010 (0.3935) acc 93.7500 (91.8750) lr 4.6417e-04 eta 0:01:25
epoch [36/50] batch [10/25] time 0.070 (0.151) data 0.000 (0.079) loss 0.2054 (0.4141) acc 96.8750 (91.2500) lr 4.6417e-04 eta 0:00:55
epoch [36/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.053) loss 0.2915 (0.4192) acc 93.7500 (91.0417) lr 4.6417e-04 eta 0:00:44
epoch [36/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.2859 (0.4477) acc 90.6250 (89.8438) lr 4.6417e-04 eta 0:00:39
epoch [36/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.6689 (0.4680) acc 81.2500 (89.0000) lr 4.1221e-04 eta 0:00:35
epoch [37/50] batch [5/25] time 0.070 (0.226) data 0.000 (0.154) loss 0.3611 (0.4108) acc 90.6250 (88.7500) lr 4.1221e-04 eta 0:01:18
epoch [37/50] batch [10/25] time 0.070 (0.148) data 0.000 (0.077) loss 0.6665 (0.5153) acc 81.2500 (86.5625) lr 4.1221e-04 eta 0:00:50
epoch [37/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.051) loss 0.3032 (0.4542) acc 93.7500 (88.7500) lr 4.1221e-04 eta 0:00:40
epoch [37/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.039) loss 0.5532 (0.4536) acc 87.5000 (88.5938) lr 4.1221e-04 eta 0:00:35
epoch [37/50] batch [25/25] time 0.071 (0.101) data 0.000 (0.031) loss 0.2178 (0.4329) acc 93.7500 (89.2500) lr 3.6258e-04 eta 0:00:32
epoch [38/50] batch [5/25] time 0.071 (0.238) data 0.000 (0.164) loss 0.5420 (0.5715) acc 84.3750 (85.6250) lr 3.6258e-04 eta 0:01:16
epoch [38/50] batch [10/25] time 0.070 (0.154) data 0.000 (0.082) loss 0.4167 (0.5013) acc 90.6250 (87.8125) lr 3.6258e-04 eta 0:00:48
epoch [38/50] batch [15/25] time 0.074 (0.126) data 0.000 (0.055) loss 0.1650 (0.4774) acc 96.8750 (88.3333) lr 3.6258e-04 eta 0:00:39
epoch [38/50] batch [20/25] time 0.070 (0.112) data 0.000 (0.041) loss 0.2421 (0.4388) acc 93.7500 (89.2188) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.033) loss 0.3367 (0.4406) acc 90.6250 (88.8750) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [5/25] time 0.070 (0.241) data 0.000 (0.170) loss 0.3376 (0.3644) acc 93.7500 (92.5000) lr 3.1545e-04 eta 0:01:11
epoch [39/50] batch [10/25] time 0.070 (0.155) data 0.000 (0.085) loss 0.4517 (0.3753) acc 84.3750 (91.2500) lr 3.1545e-04 eta 0:00:45
epoch [39/50] batch [15/25] time 0.070 (0.127) data 0.000 (0.057) loss 0.4314 (0.4233) acc 93.7500 (90.2083) lr 3.1545e-04 eta 0:00:36
epoch [39/50] batch [20/25] time 0.070 (0.113) data 0.000 (0.043) loss 0.2188 (0.3995) acc 96.8750 (91.0938) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/25] time 0.070 (0.104) data 0.000 (0.034) loss 0.2023 (0.3960) acc 93.7500 (90.7500) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [5/25] time 0.070 (0.223) data 0.000 (0.152) loss 0.6123 (0.4117) acc 84.3750 (90.6250) lr 2.7103e-04 eta 0:01:00
epoch [40/50] batch [10/25] time 0.070 (0.146) data 0.000 (0.076) loss 0.6904 (0.4234) acc 87.5000 (90.3125) lr 2.7103e-04 eta 0:00:38
epoch [40/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.051) loss 0.5400 (0.4592) acc 90.6250 (89.1667) lr 2.7103e-04 eta 0:00:31
epoch [40/50] batch [20/25] time 0.070 (0.108) data 0.000 (0.038) loss 0.3430 (0.4509) acc 93.7500 (89.5312) lr 2.7103e-04 eta 0:00:27
epoch [40/50] batch [25/25] time 0.070 (0.100) data 0.000 (0.031) loss 0.4187 (0.4618) acc 90.6250 (89.3750) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [5/25] time 0.070 (0.224) data 0.000 (0.153) loss 0.3506 (0.3795) acc 93.7500 (89.3750) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [10/25] time 0.070 (0.147) data 0.000 (0.077) loss 0.5298 (0.4046) acc 87.5000 (90.3125) lr 2.2949e-04 eta 0:00:35
epoch [41/50] batch [15/25] time 0.070 (0.121) data 0.000 (0.051) loss 0.3203 (0.4046) acc 90.6250 (90.8333) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.038) loss 0.2886 (0.4086) acc 96.8750 (90.9375) lr 2.2949e-04 eta 0:00:24
epoch [41/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.4622 (0.3962) acc 84.3750 (91.0000) lr 1.9098e-04 eta 0:00:22
epoch [42/50] batch [5/25] time 0.069 (0.233) data 0.000 (0.162) loss 0.2710 (0.2381) acc 90.6250 (95.0000) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [10/25] time 0.070 (0.151) data 0.000 (0.081) loss 0.5190 (0.3377) acc 87.5000 (92.8125) lr 1.9098e-04 eta 0:00:32
epoch [42/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.054) loss 0.6240 (0.3816) acc 87.5000 (91.6667) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.041) loss 0.3115 (0.3959) acc 93.7500 (91.0938) lr 1.9098e-04 eta 0:00:22
epoch [42/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.033) loss 0.6855 (0.3917) acc 84.3750 (91.0000) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [5/25] time 0.070 (0.232) data 0.000 (0.161) loss 0.5835 (0.4554) acc 87.5000 (88.1250) lr 1.5567e-04 eta 0:00:45
epoch [43/50] batch [10/25] time 0.070 (0.151) data 0.000 (0.081) loss 0.6177 (0.5101) acc 84.3750 (87.5000) lr 1.5567e-04 eta 0:00:28
epoch [43/50] batch [15/25] time 0.070 (0.124) data 0.000 (0.054) loss 0.4404 (0.4624) acc 90.6250 (89.3750) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.3215 (0.4398) acc 90.6250 (90.0000) lr 1.5567e-04 eta 0:00:19
epoch [43/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.1575 (0.4108) acc 100.0000 (91.0000) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [5/25] time 0.070 (0.229) data 0.000 (0.158) loss 0.2986 (0.3816) acc 93.7500 (92.5000) lr 1.2369e-04 eta 0:00:38
epoch [44/50] batch [10/25] time 0.071 (0.150) data 0.000 (0.079) loss 0.4065 (0.4075) acc 90.6250 (91.5625) lr 1.2369e-04 eta 0:00:24
epoch [44/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.053) loss 0.4736 (0.4053) acc 90.6250 (91.8750) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.040) loss 0.4243 (0.4116) acc 84.3750 (90.7812) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/25] time 0.071 (0.102) data 0.000 (0.032) loss 0.3684 (0.4062) acc 93.7500 (90.7500) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [5/25] time 0.070 (0.230) data 0.000 (0.157) loss 0.5166 (0.4689) acc 90.6250 (89.3750) lr 9.5173e-05 eta 0:00:33
epoch [45/50] batch [10/25] time 0.070 (0.150) data 0.000 (0.079) loss 0.4639 (0.4315) acc 90.6250 (90.0000) lr 9.5173e-05 eta 0:00:20
epoch [45/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.053) loss 0.3733 (0.4028) acc 93.7500 (90.2083) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.039) loss 0.3174 (0.3889) acc 87.5000 (90.0000) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.032) loss 0.3896 (0.4042) acc 93.7500 (89.8750) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [5/25] time 0.070 (0.225) data 0.000 (0.153) loss 0.2644 (0.3726) acc 96.8750 (93.1250) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [10/25] time 0.070 (0.148) data 0.000 (0.077) loss 0.7363 (0.3956) acc 78.1250 (91.5625) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [15/25] time 0.070 (0.122) data 0.000 (0.051) loss 0.2490 (0.4116) acc 96.8750 (90.6250) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/25] time 0.070 (0.109) data 0.000 (0.038) loss 0.4734 (0.3961) acc 81.2500 (90.6250) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/25] time 0.070 (0.101) data 0.000 (0.031) loss 0.4431 (0.4076) acc 90.6250 (90.3750) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [5/25] time 0.070 (0.228) data 0.000 (0.155) loss 0.3645 (0.4233) acc 93.7500 (90.6250) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [10/25] time 0.070 (0.149) data 0.000 (0.078) loss 0.4719 (0.4278) acc 90.6250 (90.3125) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/25] time 0.070 (0.123) data 0.000 (0.052) loss 0.3606 (0.3777) acc 90.6250 (91.6667) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/25] time 0.070 (0.110) data 0.000 (0.039) loss 0.5312 (0.3606) acc 90.6250 (92.3438) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [25/25] time 0.070 (0.102) data 0.000 (0.031) loss 0.2585 (0.3633) acc 96.8750 (92.6250) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/25] time 0.071 (0.235) data 0.000 (0.162) loss 0.5405 (0.4728) acc 84.3750 (87.5000) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/25] time 0.070 (0.152) data 0.000 (0.081) loss 0.4573 (0.4423) acc 87.5000 (88.4375) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/25] time 0.070 (0.125) data 0.000 (0.054) loss 0.7822 (0.4637) acc 78.1250 (87.9167) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/25] time 0.071 (0.111) data 0.000 (0.041) loss 0.4368 (0.4451) acc 84.3750 (88.1250) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.033) loss 0.5488 (0.4529) acc 87.5000 (88.0000) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [5/25] time 0.071 (0.232) data 0.000 (0.156) loss 0.6182 (0.4625) acc 87.5000 (88.7500) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/25] time 0.070 (0.151) data 0.000 (0.078) loss 0.5771 (0.4956) acc 87.5000 (88.7500) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/25] time 0.071 (0.125) data 0.000 (0.052) loss 0.4646 (0.4965) acc 87.5000 (88.1250) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.039) loss 0.4185 (0.4938) acc 90.6250 (88.4375) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.072 (0.103) data 0.000 (0.032) loss 0.7690 (0.4714) acc 75.0000 (89.1250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.070 (0.235) data 0.000 (0.165) loss 0.5083 (0.4262) acc 93.7500 (91.8750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/25] time 0.070 (0.153) data 0.000 (0.083) loss 0.4329 (0.3774) acc 96.8750 (93.7500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.070 (0.125) data 0.000 (0.055) loss 0.3042 (0.3924) acc 90.6250 (92.0833) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.070 (0.111) data 0.000 (0.041) loss 0.5371 (0.3931) acc 90.6250 (92.0312) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.070 (0.103) data 0.000 (0.033) loss 0.2491 (0.4091) acc 93.7500 (91.3750) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:10,  3.40s/it] 50%|█████     | 2/4 [00:03<00:03,  1.74s/it] 75%|███████▌  | 3/4 [00:04<00:01,  1.21s/it]100%|██████████| 4/4 [00:04<00:00,  1.30it/s]100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
=> result
* total: 1,549
* correct: 1,521
* accuracy: 98.2%
* error: 1.8%
* macro_f1: 96.4%
Elapsed: 0:02:22
Run this job and save the output to output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     916
---------  ----------
['ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X ibis.', 'X X X X inline skate.', 'X X X X joshua tree.', 'X X X X kangaroo.', 'X X X X ketch.', 'X X X X lamp.', 'X X X X laptop.', 'X X X X llama.', 'X X X X lobster.', 'X X X X lotus.', 'X X X X mandolin.', 'X X X X mayfly.', 'X X X X menorah.', 'X X X X metronome.', 'X X X X minaret.', 'X X X X nautilus.', 'X X X X octopus.', 'X X X X okapi.', 'X X X X pagoda.', 'X X X X panda.', 'X X X X pigeon.', 'X X X X pizza.', 'X X X X platypus.', 'X X X X pyramid.', 'X X X X revolver.', 'X X X X rhino.', 'X X X X rooster.', 'X X X X saxophone.', 'X X X X schooner.', 'X X X X scissors.', 'X X X X scorpion.', 'X X X X sea horse.', 'X X X X snoopy.', 'X X X X soccer ball.', 'X X X X stapler.', 'X X X X starfish.', 'X X X X stegosaurus.', 'X X X X stop sign.', 'X X X X strawberry.', 'X X X X sunflower.', 'X X X X tick.', 'X X X X trilobite.', 'X X X X umbrella.', 'X X X X watch.', 'X X X X water lilly.', 'X X X X wheelchair.', 'X X X X wild cat.', 'X X X X windsor chair.', 'X X X X wrench.', 'X X X X yin yang.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:04<00:04,  4.58s/it]100%|██████████| 2/2 [00:05<00:00,  2.20s/it]100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
=> result
* total: 916
* correct: 866
* accuracy: 94.5%
* error: 5.5%
* macro_f1: 94.2%
Run this job and save the output to output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     916
---------  ----------
['ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X ibis.', 'X X X X inline skate.', 'X X X X joshua tree.', 'X X X X kangaroo.', 'X X X X ketch.', 'X X X X lamp.', 'X X X X laptop.', 'X X X X llama.', 'X X X X lobster.', 'X X X X lotus.', 'X X X X mandolin.', 'X X X X mayfly.', 'X X X X menorah.', 'X X X X metronome.', 'X X X X minaret.', 'X X X X nautilus.', 'X X X X octopus.', 'X X X X okapi.', 'X X X X pagoda.', 'X X X X panda.', 'X X X X pigeon.', 'X X X X pizza.', 'X X X X platypus.', 'X X X X pyramid.', 'X X X X revolver.', 'X X X X rhino.', 'X X X X rooster.', 'X X X X saxophone.', 'X X X X schooner.', 'X X X X scissors.', 'X X X X scorpion.', 'X X X X sea horse.', 'X X X X snoopy.', 'X X X X soccer ball.', 'X X X X stapler.', 'X X X X starfish.', 'X X X X stegosaurus.', 'X X X X stop sign.', 'X X X X strawberry.', 'X X X X sunflower.', 'X X X X tick.', 'X X X X trilobite.', 'X X X X umbrella.', 'X X X X watch.', 'X X X X water lilly.', 'X X X X wheelchair.', 'X X X X wild cat.', 'X X X X windsor chair.', 'X X X X wrench.', 'X X X X yin yang.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:03<00:03,  3.30s/it]100%|██████████| 2/2 [00:03<00:00,  1.68s/it]100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
=> result
* total: 916
* correct: 867
* accuracy: 94.7%
* error: 5.3%
* macro_f1: 94.6%
Run this job and save the output to output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     916
---------  ----------
['ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X ibis.', 'X X X X inline skate.', 'X X X X joshua tree.', 'X X X X kangaroo.', 'X X X X ketch.', 'X X X X lamp.', 'X X X X laptop.', 'X X X X llama.', 'X X X X lobster.', 'X X X X lotus.', 'X X X X mandolin.', 'X X X X mayfly.', 'X X X X menorah.', 'X X X X metronome.', 'X X X X minaret.', 'X X X X nautilus.', 'X X X X octopus.', 'X X X X okapi.', 'X X X X pagoda.', 'X X X X panda.', 'X X X X pigeon.', 'X X X X pizza.', 'X X X X platypus.', 'X X X X pyramid.', 'X X X X revolver.', 'X X X X rhino.', 'X X X X rooster.', 'X X X X saxophone.', 'X X X X schooner.', 'X X X X scissors.', 'X X X X scorpion.', 'X X X X sea horse.', 'X X X X snoopy.', 'X X X X soccer ball.', 'X X X X stapler.', 'X X X X starfish.', 'X X X X stegosaurus.', 'X X X X stop sign.', 'X X X X strawberry.', 'X X X X sunflower.', 'X X X X tick.', 'X X X X trilobite.', 'X X X X umbrella.', 'X X X X watch.', 'X X X X water lilly.', 'X X X X wheelchair.', 'X X X X wild cat.', 'X X X X windsor chair.', 'X X X X wrench.', 'X X X X yin yang.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:03<00:03,  3.34s/it]100%|██████████| 2/2 [00:03<00:00,  1.70s/it]100%|██████████| 2/2 [00:03<00:00,  1.99s/it]
=> result
* total: 916
* correct: 868
* accuracy: 94.8%
* error: 5.2%
* macro_f1: 94.4%
Run this job and save the output to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/12] time 0.061 (0.363) data 0.000 (0.287) loss 3.6211 (3.7199) acc 46.8750 (38.7500) lr 1.0000e-05 eta 0:03:36
epoch [1/50] batch [10/12] time 0.061 (0.213) data 0.000 (0.143) loss 3.4375 (3.6123) acc 50.0000 (41.5625) lr 1.0000e-05 eta 0:02:05
epoch [2/50] batch [5/12] time 0.063 (0.258) data 0.000 (0.192) loss 2.3145 (2.4572) acc 46.8750 (50.6250) lr 2.0000e-03 eta 0:02:30
epoch [2/50] batch [10/12] time 0.065 (0.160) data 0.000 (0.096) loss 1.6836 (2.1806) acc 62.5000 (52.8125) lr 2.0000e-03 eta 0:01:32
epoch [3/50] batch [5/12] time 0.063 (0.226) data 0.000 (0.161) loss 1.5918 (1.6002) acc 53.1250 (63.7500) lr 1.9980e-03 eta 0:02:09
epoch [3/50] batch [10/12] time 0.061 (0.144) data 0.000 (0.081) loss 1.3320 (1.5284) acc 65.6250 (63.1250) lr 1.9980e-03 eta 0:01:21
epoch [4/50] batch [5/12] time 0.062 (0.237) data 0.000 (0.172) loss 1.1826 (1.2711) acc 71.8750 (70.6250) lr 1.9921e-03 eta 0:02:12
epoch [4/50] batch [10/12] time 0.063 (0.150) data 0.000 (0.086) loss 1.2637 (1.2786) acc 68.7500 (70.3125) lr 1.9921e-03 eta 0:01:22
epoch [5/50] batch [5/12] time 0.063 (0.226) data 0.000 (0.162) loss 1.3936 (1.2771) acc 59.3750 (71.2500) lr 1.9823e-03 eta 0:02:03
epoch [5/50] batch [10/12] time 0.063 (0.144) data 0.000 (0.081) loss 1.3691 (1.1824) acc 68.7500 (74.3750) lr 1.9823e-03 eta 0:01:18
epoch [6/50] batch [5/12] time 0.062 (0.225) data 0.000 (0.162) loss 1.0273 (1.1078) acc 75.0000 (73.1250) lr 1.9686e-03 eta 0:02:00
epoch [6/50] batch [10/12] time 0.062 (0.144) data 0.000 (0.081) loss 0.9346 (1.0297) acc 78.1250 (75.9375) lr 1.9686e-03 eta 0:01:16
epoch [7/50] batch [5/12] time 0.060 (0.228) data 0.000 (0.167) loss 1.3828 (0.9702) acc 68.7500 (83.1250) lr 1.9511e-03 eta 0:01:59
epoch [7/50] batch [10/12] time 0.062 (0.144) data 0.000 (0.083) loss 0.8579 (0.9691) acc 81.2500 (80.9375) lr 1.9511e-03 eta 0:01:14
epoch [8/50] batch [5/12] time 0.061 (0.271) data 0.000 (0.206) loss 1.1152 (1.0150) acc 78.1250 (79.3750) lr 1.9298e-03 eta 0:02:18
epoch [8/50] batch [10/12] time 0.061 (0.167) data 0.000 (0.103) loss 0.7773 (0.9439) acc 90.6250 (80.6250) lr 1.9298e-03 eta 0:01:24
epoch [9/50] batch [5/12] time 0.061 (0.230) data 0.000 (0.167) loss 0.8525 (0.8458) acc 78.1250 (83.1250) lr 1.9048e-03 eta 0:01:54
epoch [9/50] batch [10/12] time 0.062 (0.146) data 0.000 (0.083) loss 1.0430 (0.8786) acc 75.0000 (82.5000) lr 1.9048e-03 eta 0:01:12
epoch [10/50] batch [5/12] time 0.062 (0.230) data 0.000 (0.166) loss 0.7480 (0.7099) acc 93.7500 (93.7500) lr 1.8763e-03 eta 0:01:52
epoch [10/50] batch [10/12] time 0.062 (0.146) data 0.000 (0.083) loss 0.8818 (0.8063) acc 75.0000 (87.5000) lr 1.8763e-03 eta 0:01:10
epoch [11/50] batch [5/12] time 0.061 (0.223) data 0.000 (0.160) loss 0.7725 (0.7714) acc 84.3750 (86.2500) lr 1.8443e-03 eta 0:01:46
epoch [11/50] batch [10/12] time 0.062 (0.143) data 0.000 (0.080) loss 0.6719 (0.7476) acc 93.7500 (87.8125) lr 1.8443e-03 eta 0:01:07
epoch [12/50] batch [5/12] time 0.061 (0.230) data 0.000 (0.167) loss 0.5420 (0.7007) acc 96.8750 (89.3750) lr 1.8090e-03 eta 0:01:46
epoch [12/50] batch [10/12] time 0.063 (0.146) data 0.000 (0.084) loss 1.0605 (0.7576) acc 75.0000 (87.5000) lr 1.8090e-03 eta 0:01:06
epoch [13/50] batch [5/12] time 0.062 (0.222) data 0.000 (0.158) loss 0.6289 (0.6831) acc 93.7500 (90.6250) lr 1.7705e-03 eta 0:01:40
epoch [13/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.079) loss 0.7246 (0.6970) acc 93.7500 (90.6250) lr 1.7705e-03 eta 0:01:03
epoch [14/50] batch [5/12] time 0.065 (0.222) data 0.000 (0.156) loss 0.7432 (0.6590) acc 84.3750 (91.8750) lr 1.7290e-03 eta 0:01:37
epoch [14/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.078) loss 0.5742 (0.6491) acc 90.6250 (91.8750) lr 1.7290e-03 eta 0:01:01
epoch [15/50] batch [5/12] time 0.063 (0.222) data 0.000 (0.158) loss 0.8062 (0.6797) acc 84.3750 (86.8750) lr 1.6845e-03 eta 0:01:34
epoch [15/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.079) loss 0.6514 (0.6738) acc 93.7500 (90.0000) lr 1.6845e-03 eta 0:00:59
epoch [16/50] batch [5/12] time 0.063 (0.228) data 0.000 (0.164) loss 0.5918 (0.6027) acc 96.8750 (93.7500) lr 1.6374e-03 eta 0:01:34
epoch [16/50] batch [10/12] time 0.062 (0.145) data 0.000 (0.082) loss 0.5703 (0.6264) acc 87.5000 (91.8750) lr 1.6374e-03 eta 0:00:59
epoch [17/50] batch [5/12] time 0.062 (0.231) data 0.000 (0.167) loss 0.6758 (0.6301) acc 87.5000 (92.5000) lr 1.5878e-03 eta 0:01:32
epoch [17/50] batch [10/12] time 0.062 (0.146) data 0.000 (0.084) loss 0.6875 (0.6136) acc 90.6250 (92.1875) lr 1.5878e-03 eta 0:00:58
epoch [18/50] batch [5/12] time 0.061 (0.232) data 0.000 (0.169) loss 0.5039 (0.5979) acc 93.7500 (90.6250) lr 1.5358e-03 eta 0:01:30
epoch [18/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.085) loss 0.6367 (0.6110) acc 90.6250 (90.3125) lr 1.5358e-03 eta 0:00:56
epoch [19/50] batch [5/12] time 0.062 (0.229) data 0.000 (0.164) loss 0.5444 (0.5282) acc 100.0000 (97.5000) lr 1.4818e-03 eta 0:01:26
epoch [19/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.082) loss 0.6143 (0.5904) acc 90.6250 (93.1250) lr 1.4818e-03 eta 0:00:54
epoch [20/50] batch [5/12] time 0.061 (0.232) data 0.000 (0.168) loss 0.6494 (0.6460) acc 93.7500 (93.1250) lr 1.4258e-03 eta 0:01:25
epoch [20/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 0.6289 (0.6196) acc 93.7500 (93.1250) lr 1.4258e-03 eta 0:00:53
epoch [21/50] batch [5/12] time 0.062 (0.227) data 0.000 (0.163) loss 0.5195 (0.5777) acc 93.7500 (93.7500) lr 1.3681e-03 eta 0:01:20
epoch [21/50] batch [10/12] time 0.062 (0.145) data 0.000 (0.081) loss 0.4646 (0.5552) acc 100.0000 (95.3125) lr 1.3681e-03 eta 0:00:50
epoch [22/50] batch [5/12] time 0.062 (0.225) data 0.000 (0.162) loss 0.5498 (0.5544) acc 96.8750 (93.7500) lr 1.3090e-03 eta 0:01:17
epoch [22/50] batch [10/12] time 0.062 (0.144) data 0.000 (0.081) loss 0.5244 (0.5397) acc 93.7500 (95.0000) lr 1.3090e-03 eta 0:00:48
epoch [23/50] batch [5/12] time 0.061 (0.226) data 0.000 (0.163) loss 0.5615 (0.5301) acc 93.7500 (95.6250) lr 1.2487e-03 eta 0:01:14
epoch [23/50] batch [10/12] time 0.063 (0.144) data 0.000 (0.081) loss 0.5073 (0.5106) acc 100.0000 (95.3125) lr 1.2487e-03 eta 0:00:46
epoch [24/50] batch [5/12] time 0.063 (0.233) data 0.000 (0.170) loss 0.4014 (0.5173) acc 96.8750 (96.2500) lr 1.1874e-03 eta 0:01:14
epoch [24/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.085) loss 0.3784 (0.5427) acc 100.0000 (95.6250) lr 1.1874e-03 eta 0:00:46
epoch [25/50] batch [5/12] time 0.061 (0.231) data 0.000 (0.169) loss 0.5591 (0.5163) acc 96.8750 (95.6250) lr 1.1253e-03 eta 0:01:10
epoch [25/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.085) loss 0.6040 (0.5223) acc 96.8750 (95.9375) lr 1.1253e-03 eta 0:00:44
epoch [26/50] batch [5/12] time 0.062 (0.221) data 0.000 (0.155) loss 0.5879 (0.5613) acc 87.5000 (91.2500) lr 1.0628e-03 eta 0:01:05
epoch [26/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.078) loss 0.5859 (0.5633) acc 90.6250 (92.1875) lr 1.0628e-03 eta 0:00:41
epoch [27/50] batch [5/12] time 0.063 (0.226) data 0.000 (0.160) loss 0.5371 (0.5349) acc 84.3750 (93.1250) lr 1.0000e-03 eta 0:01:03
epoch [27/50] batch [10/12] time 0.063 (0.144) data 0.000 (0.080) loss 0.6543 (0.5502) acc 93.7500 (93.1250) lr 1.0000e-03 eta 0:00:40
epoch [28/50] batch [5/12] time 0.062 (0.227) data 0.000 (0.165) loss 0.5693 (0.5415) acc 90.6250 (93.1250) lr 9.3721e-04 eta 0:01:01
epoch [28/50] batch [10/12] time 0.061 (0.144) data 0.000 (0.083) loss 0.4541 (0.5171) acc 93.7500 (93.4375) lr 9.3721e-04 eta 0:00:38
epoch [29/50] batch [5/12] time 0.061 (0.229) data 0.000 (0.167) loss 0.4214 (0.5185) acc 100.0000 (95.0000) lr 8.7467e-04 eta 0:00:59
epoch [29/50] batch [10/12] time 0.062 (0.146) data 0.000 (0.084) loss 0.5547 (0.5022) acc 96.8750 (95.0000) lr 8.7467e-04 eta 0:00:37
epoch [30/50] batch [5/12] time 0.062 (0.228) data 0.000 (0.164) loss 0.4729 (0.4459) acc 96.8750 (96.2500) lr 8.1262e-04 eta 0:00:56
epoch [30/50] batch [10/12] time 0.062 (0.145) data 0.000 (0.082) loss 0.6025 (0.5043) acc 93.7500 (94.3750) lr 8.1262e-04 eta 0:00:35
epoch [31/50] batch [5/12] time 0.063 (0.232) data 0.000 (0.168) loss 0.5352 (0.5168) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:54
epoch [31/50] batch [10/12] time 0.064 (0.147) data 0.000 (0.084) loss 0.4714 (0.5101) acc 96.8750 (95.0000) lr 7.5131e-04 eta 0:00:33
epoch [32/50] batch [5/12] time 0.062 (0.261) data 0.000 (0.197) loss 0.4775 (0.4623) acc 96.8750 (96.2500) lr 6.9098e-04 eta 0:00:58
epoch [32/50] batch [10/12] time 0.063 (0.162) data 0.000 (0.099) loss 0.4551 (0.4764) acc 96.8750 (96.5625) lr 6.9098e-04 eta 0:00:35
epoch [33/50] batch [5/12] time 0.064 (0.275) data 0.000 (0.211) loss 0.4751 (0.5087) acc 100.0000 (95.6250) lr 6.3188e-04 eta 0:00:57
epoch [33/50] batch [10/12] time 0.062 (0.169) data 0.000 (0.106) loss 0.4812 (0.4990) acc 93.7500 (95.6250) lr 6.3188e-04 eta 0:00:34
epoch [34/50] batch [5/12] time 0.063 (0.231) data 0.000 (0.167) loss 0.4082 (0.4501) acc 96.8750 (97.5000) lr 5.7422e-04 eta 0:00:45
epoch [34/50] batch [10/12] time 0.063 (0.147) data 0.000 (0.083) loss 0.4607 (0.4785) acc 93.7500 (96.2500) lr 5.7422e-04 eta 0:00:28
epoch [35/50] batch [5/12] time 0.062 (0.257) data 0.000 (0.193) loss 0.4978 (0.4765) acc 100.0000 (97.5000) lr 5.1825e-04 eta 0:00:48
epoch [35/50] batch [10/12] time 0.062 (0.160) data 0.000 (0.097) loss 0.5889 (0.4873) acc 87.5000 (95.9375) lr 5.1825e-04 eta 0:00:29
epoch [36/50] batch [5/12] time 0.064 (0.255) data 0.000 (0.190) loss 0.5200 (0.4787) acc 96.8750 (95.6250) lr 4.6417e-04 eta 0:00:44
epoch [36/50] batch [10/12] time 0.064 (0.159) data 0.000 (0.095) loss 0.4719 (0.4748) acc 96.8750 (96.2500) lr 4.6417e-04 eta 0:00:27
epoch [37/50] batch [5/12] time 0.063 (0.263) data 0.000 (0.198) loss 0.4756 (0.5144) acc 100.0000 (96.8750) lr 4.1221e-04 eta 0:00:42
epoch [37/50] batch [10/12] time 0.065 (0.163) data 0.000 (0.099) loss 0.4492 (0.4962) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:00:25
epoch [38/50] batch [5/12] time 0.062 (0.233) data 0.000 (0.171) loss 0.4336 (0.4226) acc 100.0000 (97.5000) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [10/12] time 0.064 (0.148) data 0.000 (0.086) loss 0.4514 (0.4309) acc 93.7500 (97.1875) lr 3.6258e-04 eta 0:00:21
epoch [39/50] batch [5/12] time 0.063 (0.231) data 0.000 (0.165) loss 0.5029 (0.4562) acc 90.6250 (96.8750) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [10/12] time 0.063 (0.147) data 0.000 (0.083) loss 0.4453 (0.4610) acc 100.0000 (97.5000) lr 3.1545e-04 eta 0:00:19
epoch [40/50] batch [5/12] time 0.063 (0.255) data 0.000 (0.187) loss 0.4321 (0.4370) acc 100.0000 (98.1250) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [10/12] time 0.065 (0.159) data 0.000 (0.094) loss 0.4333 (0.4600) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:00:19
epoch [41/50] batch [5/12] time 0.062 (0.269) data 0.000 (0.202) loss 0.4556 (0.4801) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [10/12] time 0.064 (0.166) data 0.000 (0.101) loss 0.3650 (0.4723) acc 96.8750 (94.6875) lr 2.2949e-04 eta 0:00:18
epoch [42/50] batch [5/12] time 0.064 (0.250) data 0.000 (0.183) loss 0.3633 (0.3946) acc 100.0000 (98.1250) lr 1.9098e-04 eta 0:00:25
epoch [42/50] batch [10/12] time 0.063 (0.157) data 0.000 (0.091) loss 0.3618 (0.4455) acc 100.0000 (96.8750) lr 1.9098e-04 eta 0:00:15
epoch [43/50] batch [5/12] time 0.062 (0.270) data 0.000 (0.206) loss 0.4067 (0.4747) acc 96.8750 (96.2500) lr 1.5567e-04 eta 0:00:24
epoch [43/50] batch [10/12] time 0.063 (0.167) data 0.000 (0.103) loss 0.3784 (0.4448) acc 100.0000 (98.1250) lr 1.5567e-04 eta 0:00:14
epoch [44/50] batch [5/12] time 0.063 (0.227) data 0.000 (0.163) loss 0.4978 (0.4620) acc 93.7500 (96.2500) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [10/12] time 0.064 (0.145) data 0.000 (0.082) loss 0.4856 (0.4459) acc 90.6250 (96.8750) lr 1.2369e-04 eta 0:00:10
epoch [45/50] batch [5/12] time 0.063 (0.240) data 0.000 (0.177) loss 0.3911 (0.4705) acc 96.8750 (97.5000) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [10/12] time 0.062 (0.151) data 0.000 (0.089) loss 0.4106 (0.4493) acc 100.0000 (97.1875) lr 9.5173e-05 eta 0:00:09
epoch [46/50] batch [5/12] time 0.063 (0.236) data 0.000 (0.174) loss 0.3586 (0.4292) acc 96.8750 (98.1250) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [10/12] time 0.062 (0.149) data 0.000 (0.087) loss 0.4785 (0.4394) acc 96.8750 (97.8125) lr 7.0224e-05 eta 0:00:07
epoch [47/50] batch [5/12] time 0.065 (0.273) data 0.000 (0.208) loss 0.5752 (0.5182) acc 87.5000 (93.7500) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [10/12] time 0.062 (0.168) data 0.000 (0.104) loss 0.4170 (0.4678) acc 100.0000 (95.9375) lr 4.8943e-05 eta 0:00:06
epoch [48/50] batch [5/12] time 0.062 (0.236) data 0.000 (0.170) loss 0.6406 (0.4417) acc 87.5000 (96.2500) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [10/12] time 0.063 (0.150) data 0.000 (0.085) loss 0.4358 (0.4283) acc 96.8750 (97.1875) lr 3.1417e-05 eta 0:00:03
epoch [49/50] batch [5/12] time 0.062 (0.251) data 0.000 (0.186) loss 0.4639 (0.4152) acc 100.0000 (99.3750) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/12] time 0.062 (0.157) data 0.000 (0.093) loss 0.4731 (0.4253) acc 96.8750 (98.1250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/12] time 0.062 (0.232) data 0.000 (0.169) loss 0.4902 (0.5153) acc 100.0000 (93.7500) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 0.3662 (0.4867) acc 100.0000 (94.6875) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:06<00:06,  6.50s/it]100%|██████████| 2/2 [00:06<00:00,  2.96s/it]100%|██████████| 2/2 [00:07<00:00,  3.53s/it]
=> result
* total: 864
* correct: 704
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 81.3%
Elapsed: 0:01:35
Run this job and save the output to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/12] time 0.061 (0.304) data 0.000 (0.213) loss 3.1562 (3.3023) acc 43.7500 (40.6250) lr 1.0000e-05 eta 0:03:00
epoch [1/50] batch [10/12] time 0.062 (0.183) data 0.000 (0.107) loss 2.9727 (3.1586) acc 53.1250 (45.9375) lr 1.0000e-05 eta 0:01:47
epoch [2/50] batch [5/12] time 0.061 (0.246) data 0.000 (0.183) loss 2.1562 (2.5119) acc 46.8750 (50.6250) lr 2.0000e-03 eta 0:02:23
epoch [2/50] batch [10/12] time 0.062 (0.154) data 0.000 (0.091) loss 1.5068 (2.2279) acc 68.7500 (53.4375) lr 2.0000e-03 eta 0:01:29
epoch [3/50] batch [5/12] time 0.061 (0.234) data 0.000 (0.172) loss 1.1914 (1.5020) acc 75.0000 (65.6250) lr 1.9980e-03 eta 0:02:13
epoch [3/50] batch [10/12] time 0.061 (0.148) data 0.000 (0.086) loss 1.1133 (1.4513) acc 75.0000 (66.8750) lr 1.9980e-03 eta 0:01:23
epoch [4/50] batch [5/12] time 0.062 (0.232) data 0.000 (0.169) loss 1.0938 (1.0960) acc 78.1250 (78.7500) lr 1.9921e-03 eta 0:02:09
epoch [4/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.085) loss 1.1514 (1.1868) acc 71.8750 (75.3125) lr 1.9921e-03 eta 0:01:21
epoch [5/50] batch [5/12] time 0.063 (0.227) data 0.000 (0.162) loss 0.9575 (1.1521) acc 78.1250 (73.7500) lr 1.9823e-03 eta 0:02:03
epoch [5/50] batch [10/12] time 0.061 (0.145) data 0.000 (0.081) loss 0.7412 (1.0504) acc 93.7500 (79.6875) lr 1.9823e-03 eta 0:01:18
epoch [6/50] batch [5/12] time 0.062 (0.231) data 0.000 (0.168) loss 0.7900 (0.8429) acc 90.6250 (87.5000) lr 1.9686e-03 eta 0:02:03
epoch [6/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 1.0605 (0.9038) acc 78.1250 (85.0000) lr 1.9686e-03 eta 0:01:17
epoch [7/50] batch [5/12] time 0.063 (0.238) data 0.000 (0.175) loss 0.9238 (0.8885) acc 78.1250 (81.2500) lr 1.9511e-03 eta 0:02:04
epoch [7/50] batch [10/12] time 0.063 (0.150) data 0.000 (0.087) loss 0.8062 (0.9024) acc 93.7500 (81.8750) lr 1.9511e-03 eta 0:01:17
epoch [8/50] batch [5/12] time 0.061 (0.239) data 0.000 (0.177) loss 0.6533 (0.7533) acc 90.6250 (88.7500) lr 1.9298e-03 eta 0:02:02
epoch [8/50] batch [10/12] time 0.062 (0.151) data 0.000 (0.088) loss 0.6777 (0.8256) acc 90.6250 (85.3125) lr 1.9298e-03 eta 0:01:16
epoch [9/50] batch [5/12] time 0.062 (0.244) data 0.000 (0.179) loss 0.6143 (0.7891) acc 96.8750 (88.7500) lr 1.9048e-03 eta 0:02:01
epoch [9/50] batch [10/12] time 0.061 (0.153) data 0.000 (0.090) loss 0.8320 (0.7851) acc 84.3750 (89.0625) lr 1.9048e-03 eta 0:01:15
epoch [10/50] batch [5/12] time 0.063 (0.231) data 0.000 (0.164) loss 0.7441 (0.7529) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:01:52
epoch [10/50] batch [10/12] time 0.064 (0.147) data 0.000 (0.082) loss 0.7158 (0.7340) acc 87.5000 (88.4375) lr 1.8763e-03 eta 0:01:10
epoch [11/50] batch [5/12] time 0.062 (0.238) data 0.000 (0.174) loss 0.7139 (0.7092) acc 84.3750 (86.2500) lr 1.8443e-03 eta 0:01:52
epoch [11/50] batch [10/12] time 0.064 (0.150) data 0.000 (0.087) loss 0.6904 (0.7347) acc 84.3750 (84.6875) lr 1.8443e-03 eta 0:01:10
epoch [12/50] batch [5/12] time 0.063 (0.229) data 0.000 (0.163) loss 0.8872 (0.7536) acc 81.2500 (85.0000) lr 1.8090e-03 eta 0:01:45
epoch [12/50] batch [10/12] time 0.064 (0.146) data 0.000 (0.082) loss 0.7275 (0.7241) acc 87.5000 (87.1875) lr 1.8090e-03 eta 0:01:06
epoch [13/50] batch [5/12] time 0.062 (0.234) data 0.000 (0.170) loss 0.6548 (0.7377) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:01:45
epoch [13/50] batch [10/12] time 0.063 (0.149) data 0.000 (0.085) loss 0.7583 (0.6908) acc 87.5000 (89.3750) lr 1.7705e-03 eta 0:01:06
epoch [14/50] batch [5/12] time 0.062 (0.233) data 0.000 (0.167) loss 0.4849 (0.5949) acc 96.8750 (94.3750) lr 1.7290e-03 eta 0:01:42
epoch [14/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.083) loss 0.6616 (0.6511) acc 87.5000 (90.3125) lr 1.7290e-03 eta 0:01:04
epoch [15/50] batch [5/12] time 0.063 (0.222) data 0.000 (0.159) loss 0.6885 (0.5823) acc 90.6250 (94.3750) lr 1.6845e-03 eta 0:01:34
epoch [15/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.080) loss 0.7739 (0.6305) acc 87.5000 (92.1875) lr 1.6845e-03 eta 0:00:59
epoch [16/50] batch [5/12] time 0.064 (0.228) data 0.000 (0.162) loss 0.7520 (0.6454) acc 84.3750 (88.1250) lr 1.6374e-03 eta 0:01:34
epoch [16/50] batch [10/12] time 0.063 (0.145) data 0.000 (0.081) loss 0.6450 (0.6210) acc 93.7500 (90.3125) lr 1.6374e-03 eta 0:00:59
epoch [17/50] batch [5/12] time 0.064 (0.230) data 0.000 (0.164) loss 0.6104 (0.6566) acc 93.7500 (89.3750) lr 1.5878e-03 eta 0:01:32
epoch [17/50] batch [10/12] time 0.063 (0.147) data 0.000 (0.082) loss 0.5767 (0.6038) acc 93.7500 (92.1875) lr 1.5878e-03 eta 0:00:58
epoch [18/50] batch [5/12] time 0.064 (0.229) data 0.000 (0.163) loss 0.6162 (0.5988) acc 93.7500 (92.5000) lr 1.5358e-03 eta 0:01:29
epoch [18/50] batch [10/12] time 0.063 (0.146) data 0.000 (0.081) loss 0.5820 (0.5950) acc 90.6250 (92.1875) lr 1.5358e-03 eta 0:00:56
epoch [19/50] batch [5/12] time 0.063 (0.227) data 0.000 (0.163) loss 0.6802 (0.6081) acc 90.6250 (90.0000) lr 1.4818e-03 eta 0:01:25
epoch [19/50] batch [10/12] time 0.063 (0.144) data 0.000 (0.082) loss 0.4443 (0.5708) acc 93.7500 (92.1875) lr 1.4818e-03 eta 0:00:54
epoch [20/50] batch [5/12] time 0.064 (0.235) data 0.000 (0.171) loss 0.7578 (0.6369) acc 81.2500 (88.7500) lr 1.4258e-03 eta 0:01:26
epoch [20/50] batch [10/12] time 0.062 (0.149) data 0.000 (0.086) loss 0.6787 (0.6049) acc 90.6250 (90.3125) lr 1.4258e-03 eta 0:00:53
epoch [21/50] batch [5/12] time 0.065 (0.226) data 0.000 (0.162) loss 0.5195 (0.5522) acc 96.8750 (93.1250) lr 1.3681e-03 eta 0:01:20
epoch [21/50] batch [10/12] time 0.063 (0.144) data 0.000 (0.081) loss 0.5913 (0.5511) acc 93.7500 (93.1250) lr 1.3681e-03 eta 0:00:50
epoch [22/50] batch [5/12] time 0.064 (0.232) data 0.000 (0.169) loss 0.3140 (0.5290) acc 100.0000 (91.8750) lr 1.3090e-03 eta 0:01:19
epoch [22/50] batch [10/12] time 0.061 (0.147) data 0.000 (0.085) loss 0.6172 (0.5692) acc 93.7500 (91.5625) lr 1.3090e-03 eta 0:00:49
epoch [23/50] batch [5/12] time 0.065 (0.236) data 0.000 (0.170) loss 0.5977 (0.5211) acc 87.5000 (91.8750) lr 1.2487e-03 eta 0:01:17
epoch [23/50] batch [10/12] time 0.063 (0.149) data 0.000 (0.085) loss 0.5068 (0.5217) acc 93.7500 (92.1875) lr 1.2487e-03 eta 0:00:48
epoch [24/50] batch [5/12] time 0.062 (0.222) data 0.000 (0.158) loss 0.4795 (0.4526) acc 100.0000 (99.3750) lr 1.1874e-03 eta 0:01:10
epoch [24/50] batch [10/12] time 0.062 (0.142) data 0.000 (0.079) loss 0.5933 (0.5161) acc 87.5000 (95.6250) lr 1.1874e-03 eta 0:00:44
epoch [25/50] batch [5/12] time 0.064 (0.243) data 0.000 (0.177) loss 0.6289 (0.4824) acc 87.5000 (95.0000) lr 1.1253e-03 eta 0:01:14
epoch [25/50] batch [10/12] time 0.062 (0.153) data 0.000 (0.089) loss 0.8237 (0.5284) acc 81.2500 (94.3750) lr 1.1253e-03 eta 0:00:46
epoch [26/50] batch [5/12] time 0.066 (0.235) data 0.000 (0.165) loss 0.5107 (0.5086) acc 93.7500 (95.6250) lr 1.0628e-03 eta 0:01:09
epoch [26/50] batch [10/12] time 0.063 (0.149) data 0.000 (0.083) loss 0.3911 (0.4898) acc 96.8750 (95.9375) lr 1.0628e-03 eta 0:00:43
epoch [27/50] batch [5/12] time 0.061 (0.231) data 0.000 (0.168) loss 0.5322 (0.5059) acc 96.8750 (95.0000) lr 1.0000e-03 eta 0:01:05
epoch [27/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 0.4517 (0.4959) acc 96.8750 (95.9375) lr 1.0000e-03 eta 0:00:40
epoch [28/50] batch [5/12] time 0.063 (0.224) data 0.000 (0.162) loss 0.4675 (0.4296) acc 93.7500 (96.2500) lr 9.3721e-04 eta 0:01:00
epoch [28/50] batch [10/12] time 0.064 (0.144) data 0.000 (0.081) loss 0.4934 (0.4647) acc 96.8750 (95.6250) lr 9.3721e-04 eta 0:00:38
epoch [29/50] batch [5/12] time 0.063 (0.240) data 0.000 (0.175) loss 0.4624 (0.4661) acc 96.8750 (95.6250) lr 8.7467e-04 eta 0:01:02
epoch [29/50] batch [10/12] time 0.063 (0.152) data 0.000 (0.088) loss 0.4150 (0.4711) acc 96.8750 (95.3125) lr 8.7467e-04 eta 0:00:38
epoch [30/50] batch [5/12] time 0.062 (0.227) data 0.000 (0.163) loss 0.4365 (0.4212) acc 96.8750 (97.5000) lr 8.1262e-04 eta 0:00:56
epoch [30/50] batch [10/12] time 0.062 (0.145) data 0.000 (0.082) loss 0.4238 (0.4604) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:00:35
epoch [31/50] batch [5/12] time 0.064 (0.232) data 0.000 (0.167) loss 0.3521 (0.4530) acc 100.0000 (93.1250) lr 7.5131e-04 eta 0:00:54
epoch [31/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 0.4146 (0.4698) acc 96.8750 (94.0625) lr 7.5131e-04 eta 0:00:33
epoch [32/50] batch [5/12] time 0.063 (0.228) data 0.000 (0.161) loss 0.4385 (0.4564) acc 96.8750 (96.2500) lr 6.9098e-04 eta 0:00:50
epoch [32/50] batch [10/12] time 0.063 (0.145) data 0.000 (0.081) loss 0.3911 (0.4369) acc 100.0000 (96.8750) lr 6.9098e-04 eta 0:00:31
epoch [33/50] batch [5/12] time 0.063 (0.228) data 0.000 (0.162) loss 0.3516 (0.4417) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [10/12] time 0.063 (0.145) data 0.000 (0.081) loss 0.5059 (0.4379) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:00:29
epoch [34/50] batch [5/12] time 0.061 (0.227) data 0.000 (0.164) loss 0.3950 (0.4542) acc 96.8750 (96.2500) lr 5.7422e-04 eta 0:00:45
epoch [34/50] batch [10/12] time 0.061 (0.144) data 0.000 (0.082) loss 0.5518 (0.4656) acc 90.6250 (95.6250) lr 5.7422e-04 eta 0:00:28
epoch [35/50] batch [5/12] time 0.062 (0.237) data 0.000 (0.174) loss 0.4180 (0.4600) acc 100.0000 (96.8750) lr 5.1825e-04 eta 0:00:44
epoch [35/50] batch [10/12] time 0.063 (0.150) data 0.000 (0.087) loss 0.4937 (0.4498) acc 96.8750 (96.5625) lr 5.1825e-04 eta 0:00:27
epoch [36/50] batch [5/12] time 0.063 (0.267) data 0.000 (0.201) loss 0.3711 (0.4729) acc 100.0000 (94.3750) lr 4.6417e-04 eta 0:00:46
epoch [36/50] batch [10/12] time 0.062 (0.165) data 0.000 (0.101) loss 0.3647 (0.4406) acc 96.8750 (96.5625) lr 4.6417e-04 eta 0:00:27
epoch [37/50] batch [5/12] time 0.061 (0.231) data 0.000 (0.168) loss 0.3521 (0.4539) acc 100.0000 (96.8750) lr 4.1221e-04 eta 0:00:37
epoch [37/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.084) loss 0.4338 (0.4535) acc 96.8750 (95.6250) lr 4.1221e-04 eta 0:00:23
epoch [38/50] batch [5/12] time 0.061 (0.219) data 0.000 (0.156) loss 0.3833 (0.4591) acc 100.0000 (96.2500) lr 3.6258e-04 eta 0:00:33
epoch [38/50] batch [10/12] time 0.062 (0.140) data 0.000 (0.078) loss 0.4561 (0.4517) acc 96.8750 (96.5625) lr 3.6258e-04 eta 0:00:20
epoch [39/50] batch [5/12] time 0.063 (0.255) data 0.000 (0.192) loss 0.3899 (0.4688) acc 96.8750 (95.0000) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [10/12] time 0.063 (0.158) data 0.000 (0.096) loss 0.4739 (0.4266) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:00:21
epoch [40/50] batch [5/12] time 0.061 (0.217) data 0.000 (0.153) loss 0.5078 (0.4821) acc 90.6250 (95.0000) lr 2.7103e-04 eta 0:00:27
epoch [40/50] batch [10/12] time 0.061 (0.140) data 0.000 (0.077) loss 0.3870 (0.4608) acc 100.0000 (95.9375) lr 2.7103e-04 eta 0:00:17
epoch [41/50] batch [5/12] time 0.062 (0.268) data 0.000 (0.204) loss 0.3906 (0.4187) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [10/12] time 0.065 (0.166) data 0.000 (0.102) loss 0.3491 (0.4150) acc 100.0000 (97.1875) lr 2.2949e-04 eta 0:00:18
epoch [42/50] batch [5/12] time 0.063 (0.238) data 0.000 (0.174) loss 0.4607 (0.4557) acc 96.8750 (96.2500) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [10/12] time 0.063 (0.150) data 0.000 (0.087) loss 0.4077 (0.4356) acc 100.0000 (96.8750) lr 1.9098e-04 eta 0:00:14
epoch [43/50] batch [5/12] time 0.063 (0.234) data 0.000 (0.170) loss 0.4490 (0.3915) acc 96.8750 (97.5000) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.085) loss 0.4382 (0.4013) acc 100.0000 (97.5000) lr 1.5567e-04 eta 0:00:12
epoch [44/50] batch [5/12] time 0.062 (0.247) data 0.000 (0.183) loss 0.3955 (0.4107) acc 96.8750 (98.1250) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [10/12] time 0.063 (0.155) data 0.000 (0.091) loss 0.4067 (0.4054) acc 100.0000 (97.5000) lr 1.2369e-04 eta 0:00:11
epoch [45/50] batch [5/12] time 0.063 (0.223) data 0.000 (0.159) loss 0.4023 (0.4174) acc 96.8750 (97.5000) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [10/12] time 0.063 (0.143) data 0.000 (0.080) loss 0.4116 (0.4092) acc 96.8750 (97.8125) lr 9.5173e-05 eta 0:00:08
epoch [46/50] batch [5/12] time 0.063 (0.231) data 0.000 (0.167) loss 0.4348 (0.4919) acc 90.6250 (93.7500) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [10/12] time 0.063 (0.147) data 0.000 (0.084) loss 0.3574 (0.4386) acc 100.0000 (95.9375) lr 7.0224e-05 eta 0:00:07
epoch [47/50] batch [5/12] time 0.062 (0.233) data 0.000 (0.170) loss 0.4214 (0.3876) acc 96.8750 (98.7500) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [10/12] time 0.062 (0.147) data 0.000 (0.085) loss 0.3682 (0.3996) acc 100.0000 (98.4375) lr 4.8943e-05 eta 0:00:05
epoch [48/50] batch [5/12] time 0.062 (0.230) data 0.000 (0.166) loss 0.4136 (0.3888) acc 96.8750 (98.7500) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [10/12] time 0.062 (0.146) data 0.000 (0.083) loss 0.3000 (0.4094) acc 100.0000 (98.1250) lr 3.1417e-05 eta 0:00:03
epoch [49/50] batch [5/12] time 0.062 (0.237) data 0.000 (0.173) loss 0.4287 (0.4256) acc 100.0000 (97.5000) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/12] time 0.062 (0.149) data 0.000 (0.087) loss 0.3560 (0.4111) acc 100.0000 (98.1250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/12] time 0.061 (0.226) data 0.000 (0.164) loss 0.3818 (0.3827) acc 96.8750 (98.1250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/12] time 0.062 (0.144) data 0.000 (0.082) loss 0.4807 (0.4076) acc 96.8750 (97.5000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:05<00:05,  5.13s/it]100%|██████████| 2/2 [00:05<00:00,  2.39s/it]100%|██████████| 2/2 [00:05<00:00,  2.86s/it]
=> result
* total: 864
* correct: 724
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 83.7%
Elapsed: 0:01:33
Run this job and save the output to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/12] time 0.063 (0.301) data 0.000 (0.223) loss 3.4688 (3.6348) acc 40.6250 (48.7500) lr 1.0000e-05 eta 0:02:59
epoch [1/50] batch [10/12] time 0.062 (0.183) data 0.000 (0.112) loss 3.7402 (3.6828) acc 31.2500 (45.6250) lr 1.0000e-05 eta 0:01:47
epoch [2/50] batch [5/12] time 0.063 (0.270) data 0.000 (0.205) loss 2.8008 (2.9941) acc 40.6250 (48.7500) lr 2.0000e-03 eta 0:02:37
epoch [2/50] batch [10/12] time 0.068 (0.168) data 0.000 (0.103) loss 1.5684 (2.5758) acc 68.7500 (54.3750) lr 2.0000e-03 eta 0:01:36
epoch [3/50] batch [5/12] time 0.063 (0.236) data 0.000 (0.169) loss 1.2520 (1.7877) acc 71.8750 (60.6250) lr 1.9980e-03 eta 0:02:14
epoch [3/50] batch [10/12] time 0.064 (0.150) data 0.000 (0.084) loss 1.9707 (1.6767) acc 53.1250 (62.5000) lr 1.9980e-03 eta 0:01:24
epoch [4/50] batch [5/12] time 0.063 (0.239) data 0.000 (0.172) loss 1.2676 (1.2676) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:02:13
epoch [4/50] batch [10/12] time 0.063 (0.151) data 0.000 (0.086) loss 1.1943 (1.3599) acc 71.8750 (70.6250) lr 1.9921e-03 eta 0:01:23
epoch [5/50] batch [5/12] time 0.063 (0.257) data 0.000 (0.192) loss 1.0586 (1.2604) acc 87.5000 (75.0000) lr 1.9823e-03 eta 0:02:20
epoch [5/50] batch [10/12] time 0.064 (0.161) data 0.000 (0.096) loss 1.1309 (1.2249) acc 75.0000 (75.9375) lr 1.9823e-03 eta 0:01:26
epoch [6/50] batch [5/12] time 0.061 (0.242) data 0.000 (0.179) loss 0.8394 (1.0029) acc 87.5000 (84.3750) lr 1.9686e-03 eta 0:02:09
epoch [6/50] batch [10/12] time 0.063 (0.153) data 0.000 (0.090) loss 1.0332 (1.0459) acc 81.2500 (80.0000) lr 1.9686e-03 eta 0:01:20
epoch [7/50] batch [5/12] time 0.069 (0.258) data 0.000 (0.188) loss 0.9531 (1.0564) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:02:14
epoch [7/50] batch [10/12] time 0.063 (0.161) data 0.000 (0.094) loss 0.8076 (0.9745) acc 90.6250 (81.5625) lr 1.9511e-03 eta 0:01:23
epoch [8/50] batch [5/12] time 0.063 (0.232) data 0.000 (0.165) loss 0.7104 (0.7858) acc 87.5000 (86.8750) lr 1.9298e-03 eta 0:01:58
epoch [8/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.082) loss 1.2471 (0.8751) acc 71.8750 (83.4375) lr 1.9298e-03 eta 0:01:14
epoch [9/50] batch [5/12] time 0.062 (0.246) data 0.000 (0.181) loss 0.7217 (0.7437) acc 84.3750 (86.8750) lr 1.9048e-03 eta 0:02:02
epoch [9/50] batch [10/12] time 0.064 (0.154) data 0.000 (0.091) loss 0.7646 (0.8226) acc 87.5000 (85.0000) lr 1.9048e-03 eta 0:01:16
epoch [10/50] batch [5/12] time 0.070 (0.244) data 0.000 (0.179) loss 0.7954 (0.7849) acc 78.1250 (86.2500) lr 1.8763e-03 eta 0:01:58
epoch [10/50] batch [10/12] time 0.066 (0.154) data 0.000 (0.089) loss 0.8960 (0.8394) acc 81.2500 (83.7500) lr 1.8763e-03 eta 0:01:14
epoch [11/50] batch [5/12] time 0.070 (0.267) data 0.000 (0.201) loss 0.7041 (0.8273) acc 87.5000 (83.1250) lr 1.8443e-03 eta 0:02:06
epoch [11/50] batch [10/12] time 0.066 (0.167) data 0.000 (0.100) loss 0.6987 (0.8240) acc 93.7500 (85.3125) lr 1.8443e-03 eta 0:01:18
epoch [12/50] batch [5/12] time 0.063 (0.246) data 0.000 (0.178) loss 0.8491 (0.7162) acc 78.1250 (87.5000) lr 1.8090e-03 eta 0:01:53
epoch [12/50] batch [10/12] time 0.065 (0.155) data 0.000 (0.089) loss 0.7432 (0.7103) acc 90.6250 (88.7500) lr 1.8090e-03 eta 0:01:10
epoch [13/50] batch [5/12] time 0.065 (0.252) data 0.000 (0.187) loss 0.5552 (0.6316) acc 96.8750 (93.7500) lr 1.7705e-03 eta 0:01:53
epoch [13/50] batch [10/12] time 0.062 (0.157) data 0.000 (0.094) loss 0.5688 (0.6638) acc 96.8750 (92.5000) lr 1.7705e-03 eta 0:01:10
epoch [14/50] batch [5/12] time 0.063 (0.251) data 0.000 (0.184) loss 0.4634 (0.6647) acc 100.0000 (88.7500) lr 1.7290e-03 eta 0:01:50
epoch [14/50] batch [10/12] time 0.065 (0.157) data 0.000 (0.092) loss 0.7588 (0.6656) acc 87.5000 (89.3750) lr 1.7290e-03 eta 0:01:08
epoch [15/50] batch [5/12] time 0.065 (0.245) data 0.000 (0.178) loss 0.6548 (0.6461) acc 90.6250 (90.0000) lr 1.6845e-03 eta 0:01:44
epoch [15/50] batch [10/12] time 0.064 (0.154) data 0.000 (0.089) loss 0.6357 (0.6545) acc 93.7500 (90.3125) lr 1.6845e-03 eta 0:01:05
epoch [16/50] batch [5/12] time 0.063 (0.241) data 0.000 (0.176) loss 0.5371 (0.5550) acc 93.7500 (95.0000) lr 1.6374e-03 eta 0:01:40
epoch [16/50] batch [10/12] time 0.071 (0.153) data 0.000 (0.088) loss 0.6748 (0.5879) acc 90.6250 (92.8125) lr 1.6374e-03 eta 0:01:02
epoch [17/50] batch [5/12] time 0.068 (0.262) data 0.000 (0.193) loss 0.5322 (0.6426) acc 93.7500 (90.0000) lr 1.5878e-03 eta 0:01:45
epoch [17/50] batch [10/12] time 0.064 (0.164) data 0.000 (0.096) loss 0.8369 (0.6456) acc 84.3750 (90.6250) lr 1.5878e-03 eta 0:01:05
epoch [18/50] batch [5/12] time 0.062 (0.239) data 0.000 (0.176) loss 0.6948 (0.5291) acc 87.5000 (93.1250) lr 1.5358e-03 eta 0:01:33
epoch [18/50] batch [10/12] time 0.065 (0.151) data 0.000 (0.088) loss 0.5518 (0.5548) acc 93.7500 (93.4375) lr 1.5358e-03 eta 0:00:58
epoch [19/50] batch [5/12] time 0.069 (0.239) data 0.000 (0.171) loss 0.6641 (0.5442) acc 78.1250 (90.0000) lr 1.4818e-03 eta 0:01:30
epoch [19/50] batch [10/12] time 0.066 (0.153) data 0.000 (0.086) loss 0.6865 (0.5779) acc 93.7500 (91.2500) lr 1.4818e-03 eta 0:00:57
epoch [20/50] batch [5/12] time 0.067 (0.263) data 0.000 (0.194) loss 0.5195 (0.5591) acc 100.0000 (93.7500) lr 1.4258e-03 eta 0:01:36
epoch [20/50] batch [10/12] time 0.068 (0.165) data 0.000 (0.097) loss 0.5190 (0.5787) acc 90.6250 (90.3125) lr 1.4258e-03 eta 0:00:59
epoch [21/50] batch [5/12] time 0.063 (0.242) data 0.000 (0.173) loss 0.4434 (0.5294) acc 100.0000 (95.6250) lr 1.3681e-03 eta 0:01:25
epoch [21/50] batch [10/12] time 0.066 (0.153) data 0.000 (0.087) loss 0.5767 (0.5414) acc 90.6250 (94.3750) lr 1.3681e-03 eta 0:00:53
epoch [22/50] batch [5/12] time 0.064 (0.258) data 0.000 (0.192) loss 0.6128 (0.5999) acc 93.7500 (93.1250) lr 1.3090e-03 eta 0:01:28
epoch [22/50] batch [10/12] time 0.066 (0.161) data 0.000 (0.096) loss 0.6504 (0.5677) acc 90.6250 (93.7500) lr 1.3090e-03 eta 0:00:54
epoch [23/50] batch [5/12] time 0.064 (0.255) data 0.000 (0.190) loss 0.5205 (0.4867) acc 96.8750 (96.2500) lr 1.2487e-03 eta 0:01:24
epoch [23/50] batch [10/12] time 0.071 (0.161) data 0.000 (0.095) loss 0.4961 (0.4976) acc 96.8750 (95.0000) lr 1.2487e-03 eta 0:00:52
epoch [24/50] batch [5/12] time 0.064 (0.248) data 0.000 (0.182) loss 0.7910 (0.5306) acc 78.1250 (91.8750) lr 1.1874e-03 eta 0:01:19
epoch [24/50] batch [10/12] time 0.062 (0.155) data 0.000 (0.091) loss 0.4541 (0.5159) acc 96.8750 (93.4375) lr 1.1874e-03 eta 0:00:48
epoch [25/50] batch [5/12] time 0.062 (0.233) data 0.000 (0.166) loss 0.7388 (0.5541) acc 87.5000 (92.5000) lr 1.1253e-03 eta 0:01:11
epoch [25/50] batch [10/12] time 0.062 (0.148) data 0.000 (0.083) loss 0.5005 (0.5336) acc 96.8750 (94.0625) lr 1.1253e-03 eta 0:00:44
epoch [26/50] batch [5/12] time 0.062 (0.253) data 0.000 (0.187) loss 0.5215 (0.4892) acc 93.7500 (95.6250) lr 1.0628e-03 eta 0:01:14
epoch [26/50] batch [10/12] time 0.063 (0.158) data 0.000 (0.094) loss 0.6299 (0.5164) acc 90.6250 (95.0000) lr 1.0628e-03 eta 0:00:45
epoch [27/50] batch [5/12] time 0.064 (0.243) data 0.000 (0.178) loss 0.4148 (0.4082) acc 96.8750 (97.5000) lr 1.0000e-03 eta 0:01:08
epoch [27/50] batch [10/12] time 0.064 (0.153) data 0.000 (0.089) loss 0.5786 (0.4878) acc 96.8750 (95.6250) lr 1.0000e-03 eta 0:00:42
epoch [28/50] batch [5/12] time 0.062 (0.252) data 0.000 (0.183) loss 0.5273 (0.4879) acc 93.7500 (96.2500) lr 9.3721e-04 eta 0:01:08
epoch [28/50] batch [10/12] time 0.064 (0.158) data 0.000 (0.092) loss 0.5010 (0.4918) acc 93.7500 (96.2500) lr 9.3721e-04 eta 0:00:41
epoch [29/50] batch [5/12] time 0.063 (0.236) data 0.000 (0.173) loss 0.4434 (0.5026) acc 96.8750 (96.2500) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [10/12] time 0.066 (0.150) data 0.000 (0.086) loss 0.5967 (0.5185) acc 90.6250 (94.6875) lr 8.7467e-04 eta 0:00:38
epoch [30/50] batch [5/12] time 0.064 (0.247) data 0.000 (0.182) loss 0.5244 (0.4593) acc 93.7500 (98.1250) lr 8.1262e-04 eta 0:01:00
epoch [30/50] batch [10/12] time 0.063 (0.155) data 0.000 (0.091) loss 0.5562 (0.4694) acc 93.7500 (96.2500) lr 8.1262e-04 eta 0:00:37
epoch [31/50] batch [5/12] time 0.062 (0.248) data 0.000 (0.183) loss 0.4556 (0.4390) acc 96.8750 (96.8750) lr 7.5131e-04 eta 0:00:58
epoch [31/50] batch [10/12] time 0.063 (0.155) data 0.000 (0.092) loss 0.4492 (0.4613) acc 100.0000 (96.8750) lr 7.5131e-04 eta 0:00:35
epoch [32/50] batch [5/12] time 0.064 (0.244) data 0.000 (0.180) loss 0.5244 (0.4605) acc 96.8750 (95.6250) lr 6.9098e-04 eta 0:00:54
epoch [32/50] batch [10/12] time 0.063 (0.154) data 0.000 (0.090) loss 0.6870 (0.4873) acc 90.6250 (94.6875) lr 6.9098e-04 eta 0:00:33
epoch [33/50] batch [5/12] time 0.062 (0.252) data 0.000 (0.186) loss 0.6143 (0.4980) acc 90.6250 (95.6250) lr 6.3188e-04 eta 0:00:53
epoch [33/50] batch [10/12] time 0.063 (0.158) data 0.000 (0.093) loss 0.4636 (0.4913) acc 96.8750 (95.9375) lr 6.3188e-04 eta 0:00:32
epoch [34/50] batch [5/12] time 0.062 (0.235) data 0.000 (0.170) loss 0.5186 (0.4657) acc 93.7500 (97.5000) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [10/12] time 0.064 (0.149) data 0.000 (0.085) loss 0.4514 (0.4513) acc 93.7500 (97.1875) lr 5.7422e-04 eta 0:00:28
epoch [35/50] batch [5/12] time 0.065 (0.230) data 0.000 (0.165) loss 0.3945 (0.4486) acc 96.8750 (98.1250) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [10/12] time 0.064 (0.147) data 0.000 (0.082) loss 0.4233 (0.4218) acc 100.0000 (98.1250) lr 5.1825e-04 eta 0:00:26
epoch [36/50] batch [5/12] time 0.062 (0.251) data 0.000 (0.185) loss 0.3730 (0.4137) acc 96.8750 (97.5000) lr 4.6417e-04 eta 0:00:43
epoch [36/50] batch [10/12] time 0.064 (0.157) data 0.000 (0.092) loss 0.4766 (0.4288) acc 93.7500 (96.2500) lr 4.6417e-04 eta 0:00:26
epoch [37/50] batch [5/12] time 0.065 (0.246) data 0.000 (0.179) loss 0.4053 (0.5483) acc 96.8750 (93.7500) lr 4.1221e-04 eta 0:00:40
epoch [37/50] batch [10/12] time 0.064 (0.155) data 0.000 (0.090) loss 0.4263 (0.4974) acc 96.8750 (95.3125) lr 4.1221e-04 eta 0:00:24
epoch [38/50] batch [5/12] time 0.064 (0.233) data 0.000 (0.167) loss 0.2844 (0.4391) acc 100.0000 (97.5000) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.084) loss 0.3762 (0.4556) acc 96.8750 (96.2500) lr 3.6258e-04 eta 0:00:21
epoch [39/50] batch [5/12] time 0.062 (0.245) data 0.000 (0.180) loss 0.4495 (0.4515) acc 93.7500 (96.2500) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [10/12] time 0.064 (0.154) data 0.000 (0.090) loss 0.4944 (0.4614) acc 96.8750 (95.9375) lr 3.1545e-04 eta 0:00:20
epoch [40/50] batch [5/12] time 0.064 (0.241) data 0.000 (0.174) loss 0.6582 (0.4955) acc 84.3750 (93.7500) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [10/12] time 0.063 (0.152) data 0.000 (0.087) loss 0.4368 (0.4650) acc 96.8750 (94.6875) lr 2.7103e-04 eta 0:00:18
epoch [41/50] batch [5/12] time 0.064 (0.250) data 0.000 (0.185) loss 0.4438 (0.4502) acc 100.0000 (96.8750) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [10/12] time 0.063 (0.157) data 0.000 (0.092) loss 0.3994 (0.4458) acc 100.0000 (97.1875) lr 2.2949e-04 eta 0:00:17
epoch [42/50] batch [5/12] time 0.063 (0.253) data 0.000 (0.188) loss 0.4658 (0.4069) acc 93.7500 (97.5000) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [10/12] time 0.066 (0.158) data 0.000 (0.094) loss 0.4985 (0.4400) acc 93.7500 (96.2500) lr 1.9098e-04 eta 0:00:15
epoch [43/50] batch [5/12] time 0.062 (0.241) data 0.000 (0.175) loss 0.4573 (0.4732) acc 100.0000 (97.5000) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [10/12] time 0.061 (0.152) data 0.000 (0.088) loss 0.5459 (0.4785) acc 100.0000 (97.1875) lr 1.5567e-04 eta 0:00:13
epoch [44/50] batch [5/12] time 0.063 (0.234) data 0.000 (0.169) loss 0.4414 (0.4480) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [10/12] time 0.063 (0.149) data 0.000 (0.085) loss 0.5156 (0.4279) acc 96.8750 (97.5000) lr 1.2369e-04 eta 0:00:11
epoch [45/50] batch [5/12] time 0.064 (0.245) data 0.000 (0.181) loss 0.4773 (0.4700) acc 93.7500 (96.8750) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [10/12] time 0.064 (0.154) data 0.000 (0.090) loss 0.4316 (0.4612) acc 100.0000 (97.1875) lr 9.5173e-05 eta 0:00:09
epoch [46/50] batch [5/12] time 0.063 (0.232) data 0.000 (0.166) loss 0.4675 (0.4345) acc 96.8750 (96.2500) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [10/12] time 0.063 (0.148) data 0.000 (0.083) loss 0.6973 (0.4628) acc 87.5000 (95.6250) lr 7.0224e-05 eta 0:00:07
epoch [47/50] batch [5/12] time 0.062 (0.248) data 0.000 (0.185) loss 0.4419 (0.4479) acc 93.7500 (96.8750) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [10/12] time 0.066 (0.156) data 0.000 (0.092) loss 0.4438 (0.4489) acc 100.0000 (96.5625) lr 4.8943e-05 eta 0:00:05
epoch [48/50] batch [5/12] time 0.062 (0.244) data 0.000 (0.179) loss 0.4490 (0.4187) acc 90.6250 (95.6250) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [10/12] time 0.065 (0.153) data 0.000 (0.090) loss 0.5791 (0.4607) acc 90.6250 (95.0000) lr 3.1417e-05 eta 0:00:03
epoch [49/50] batch [5/12] time 0.062 (0.251) data 0.000 (0.186) loss 0.4978 (0.4549) acc 93.7500 (96.2500) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/12] time 0.064 (0.157) data 0.000 (0.093) loss 0.4216 (0.4358) acc 100.0000 (97.1875) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/12] time 0.062 (0.239) data 0.000 (0.174) loss 0.4087 (0.4017) acc 96.8750 (98.1250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/12] time 0.064 (0.151) data 0.000 (0.087) loss 0.4287 (0.4255) acc 96.8750 (97.1875) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:04<00:04,  4.89s/it]100%|██████████| 2/2 [00:05<00:00,  2.30s/it]100%|██████████| 2/2 [00:05<00:00,  2.76s/it]
=> result
* total: 864
* correct: 715
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 82.5%
Elapsed: 0:01:36
Run this job and save the output to output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:06<00:06,  6.81s/it]100%|██████████| 2/2 [00:07<00:00,  3.06s/it]100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
=> result
* total: 828
* correct: 475
* accuracy: 57.4%
* error: 42.6%
* macro_f1: 55.5%
Run this job and save the output to output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:05<00:05,  5.14s/it]100%|██████████| 2/2 [00:05<00:00,  2.37s/it]100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
=> result
* total: 828
* correct: 480
* accuracy: 58.0%
* error: 42.0%
* macro_f1: 57.0%
Run this job and save the output to output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:04<00:04,  4.95s/it]100%|██████████| 2/2 [00:05<00:00,  2.29s/it]100%|██████████| 2/2 [00:05<00:00,  2.74s/it]
=> result
* total: 828
* correct: 464
* accuracy: 56.0%
* error: 44.0%
* macro_f1: 53.9%
Run this job and save the output to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [1/3] time 1.520 (1.520) data 1.399 (1.399) loss 3.7109 (3.7109) acc 65.6250 (65.6250) lr 1.0000e-05 eta 0:03:46
epoch [1/50] batch [2/3] time 0.061 (0.790) data 0.001 (0.700) loss 3.8359 (3.7734) acc 56.2500 (60.9375) lr 1.0000e-05 eta 0:01:56
epoch [1/50] batch [3/3] time 0.078 (0.553) data 0.000 (0.467) loss 3.9121 (3.8197) acc 50.0000 (57.2917) lr 2.0000e-03 eta 0:01:21
epoch [2/50] batch [1/3] time 0.859 (0.859) data 0.796 (0.796) loss 3.8066 (3.8066) acc 56.2500 (56.2500) lr 2.0000e-03 eta 0:02:05
epoch [2/50] batch [2/3] time 0.081 (0.470) data 0.024 (0.410) loss 3.2773 (3.5420) acc 46.8750 (51.5625) lr 2.0000e-03 eta 0:01:08
epoch [2/50] batch [3/3] time 0.044 (0.328) data 0.000 (0.274) loss 2.8164 (3.3001) acc 56.2500 (53.1250) lr 1.9980e-03 eta 0:00:47
epoch [3/50] batch [1/3] time 1.029 (1.029) data 0.965 (0.965) loss 2.0938 (2.0938) acc 75.0000 (75.0000) lr 1.9980e-03 eta 0:02:27
epoch [3/50] batch [2/3] time 0.061 (0.545) data 0.000 (0.483) loss 1.8662 (1.9800) acc 75.0000 (75.0000) lr 1.9980e-03 eta 0:01:17
epoch [3/50] batch [3/3] time 0.042 (0.377) data 0.000 (0.322) loss 1.7246 (1.8949) acc 81.2500 (77.0833) lr 1.9921e-03 eta 0:00:53
epoch [4/50] batch [1/3] time 0.853 (0.853) data 0.789 (0.789) loss 1.5967 (1.5967) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:01:59
epoch [4/50] batch [2/3] time 0.059 (0.456) data 0.000 (0.395) loss 1.4336 (1.5151) acc 68.7500 (71.8750) lr 1.9921e-03 eta 0:01:03
epoch [4/50] batch [3/3] time 0.041 (0.318) data 0.000 (0.263) loss 1.6895 (1.5732) acc 62.5000 (68.7500) lr 1.9823e-03 eta 0:00:43
epoch [5/50] batch [1/3] time 0.816 (0.816) data 0.754 (0.754) loss 1.3984 (1.3984) acc 81.2500 (81.2500) lr 1.9823e-03 eta 0:01:51
epoch [5/50] batch [2/3] time 0.059 (0.438) data 0.000 (0.377) loss 1.1895 (1.2939) acc 78.1250 (79.6875) lr 1.9823e-03 eta 0:00:59
epoch [5/50] batch [3/3] time 0.042 (0.306) data 0.000 (0.251) loss 1.0166 (1.2015) acc 87.5000 (82.2917) lr 1.9686e-03 eta 0:00:41
epoch [6/50] batch [1/3] time 0.819 (0.819) data 0.759 (0.759) loss 1.2051 (1.2051) acc 78.1250 (78.1250) lr 1.9686e-03 eta 0:01:49
epoch [6/50] batch [2/3] time 0.058 (0.439) data 0.000 (0.380) loss 0.9014 (1.0532) acc 87.5000 (82.8125) lr 1.9686e-03 eta 0:00:58
epoch [6/50] batch [3/3] time 0.040 (0.306) data 0.000 (0.253) loss 1.0215 (1.0426) acc 81.2500 (82.2917) lr 1.9511e-03 eta 0:00:40
epoch [7/50] batch [1/3] time 0.821 (0.821) data 0.760 (0.760) loss 0.8037 (0.8037) acc 96.8750 (96.8750) lr 1.9511e-03 eta 0:01:47
epoch [7/50] batch [2/3] time 0.059 (0.440) data 0.000 (0.380) loss 1.1211 (0.9624) acc 75.0000 (85.9375) lr 1.9511e-03 eta 0:00:57
epoch [7/50] batch [3/3] time 0.042 (0.307) data 0.000 (0.254) loss 0.5674 (0.8307) acc 93.7500 (88.5417) lr 1.9298e-03 eta 0:00:39
epoch [8/50] batch [1/3] time 0.743 (0.743) data 0.684 (0.684) loss 0.8940 (0.8940) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:01:35
epoch [8/50] batch [2/3] time 0.060 (0.401) data 0.000 (0.342) loss 0.8218 (0.8579) acc 84.3750 (85.9375) lr 1.9298e-03 eta 0:00:50
epoch [8/50] batch [3/3] time 0.040 (0.281) data 0.000 (0.228) loss 0.9438 (0.8866) acc 81.2500 (84.3750) lr 1.9048e-03 eta 0:00:35
epoch [9/50] batch [1/3] time 0.834 (0.834) data 0.776 (0.776) loss 0.7324 (0.7324) acc 90.6250 (90.6250) lr 1.9048e-03 eta 0:01:44
epoch [9/50] batch [2/3] time 0.058 (0.446) data 0.000 (0.388) loss 0.8740 (0.8032) acc 81.2500 (85.9375) lr 1.9048e-03 eta 0:00:55
epoch [9/50] batch [3/3] time 0.042 (0.312) data 0.000 (0.259) loss 0.6621 (0.7562) acc 81.2500 (84.3750) lr 1.8763e-03 eta 0:00:38
epoch [10/50] batch [1/3] time 0.733 (0.733) data 0.673 (0.673) loss 0.9331 (0.9331) acc 71.8750 (71.8750) lr 1.8763e-03 eta 0:01:29
epoch [10/50] batch [2/3] time 0.057 (0.395) data 0.000 (0.337) loss 0.7222 (0.8276) acc 87.5000 (79.6875) lr 1.8763e-03 eta 0:00:47
epoch [10/50] batch [3/3] time 0.041 (0.277) data 0.000 (0.225) loss 0.5225 (0.7259) acc 100.0000 (86.4583) lr 1.8443e-03 eta 0:00:33
epoch [11/50] batch [1/3] time 0.746 (0.746) data 0.686 (0.686) loss 0.4688 (0.4688) acc 93.7500 (93.7500) lr 1.8443e-03 eta 0:01:28
epoch [11/50] batch [2/3] time 0.058 (0.402) data 0.000 (0.343) loss 0.6333 (0.5510) acc 87.5000 (90.6250) lr 1.8443e-03 eta 0:00:47
epoch [11/50] batch [3/3] time 0.042 (0.282) data 0.000 (0.229) loss 0.7344 (0.6121) acc 87.5000 (89.5833) lr 1.8090e-03 eta 0:00:32
epoch [12/50] batch [1/3] time 0.829 (0.829) data 0.768 (0.768) loss 0.4736 (0.4736) acc 96.8750 (96.8750) lr 1.8090e-03 eta 0:01:36
epoch [12/50] batch [2/3] time 0.057 (0.443) data 0.000 (0.384) loss 0.7646 (0.6191) acc 84.3750 (90.6250) lr 1.8090e-03 eta 0:00:50
epoch [12/50] batch [3/3] time 0.043 (0.310) data 0.000 (0.256) loss 0.6318 (0.6234) acc 81.2500 (87.5000) lr 1.7705e-03 eta 0:00:35
epoch [13/50] batch [1/3] time 0.746 (0.746) data 0.687 (0.687) loss 0.6353 (0.6353) acc 90.6250 (90.6250) lr 1.7705e-03 eta 0:01:24
epoch [13/50] batch [2/3] time 0.058 (0.402) data 0.000 (0.343) loss 0.6621 (0.6487) acc 81.2500 (85.9375) lr 1.7705e-03 eta 0:00:45
epoch [13/50] batch [3/3] time 0.043 (0.282) data 0.000 (0.229) loss 0.7266 (0.6746) acc 87.5000 (86.4583) lr 1.7290e-03 eta 0:00:31
epoch [14/50] batch [1/3] time 0.764 (0.764) data 0.705 (0.705) loss 0.5801 (0.5801) acc 93.7500 (93.7500) lr 1.7290e-03 eta 0:01:24
epoch [14/50] batch [2/3] time 0.058 (0.411) data 0.000 (0.353) loss 0.7217 (0.6509) acc 78.1250 (85.9375) lr 1.7290e-03 eta 0:00:44
epoch [14/50] batch [3/3] time 0.041 (0.288) data 0.000 (0.235) loss 0.6992 (0.6670) acc 87.5000 (86.4583) lr 1.6845e-03 eta 0:00:31
epoch [15/50] batch [1/3] time 0.911 (0.911) data 0.850 (0.850) loss 0.5752 (0.5752) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:01:37
epoch [15/50] batch [2/3] time 0.059 (0.485) data 0.000 (0.425) loss 0.5342 (0.5547) acc 90.6250 (87.5000) lr 1.6845e-03 eta 0:00:51
epoch [15/50] batch [3/3] time 0.050 (0.340) data 0.000 (0.284) loss 1.0674 (0.7256) acc 75.0000 (83.3333) lr 1.6374e-03 eta 0:00:35
epoch [16/50] batch [1/3] time 0.742 (0.742) data 0.681 (0.681) loss 0.5371 (0.5371) acc 90.6250 (90.6250) lr 1.6374e-03 eta 0:01:17
epoch [16/50] batch [2/3] time 0.058 (0.400) data 0.000 (0.341) loss 0.6265 (0.5818) acc 84.3750 (87.5000) lr 1.6374e-03 eta 0:00:41
epoch [16/50] batch [3/3] time 0.041 (0.280) data 0.000 (0.227) loss 0.6914 (0.6183) acc 81.2500 (85.4167) lr 1.5878e-03 eta 0:00:28
epoch [17/50] batch [1/3] time 0.743 (0.743) data 0.683 (0.683) loss 0.4875 (0.4875) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:01:15
epoch [17/50] batch [2/3] time 0.059 (0.401) data 0.000 (0.342) loss 0.6294 (0.5585) acc 87.5000 (90.6250) lr 1.5878e-03 eta 0:00:40
epoch [17/50] batch [3/3] time 0.040 (0.281) data 0.000 (0.228) loss 0.6953 (0.6041) acc 87.5000 (89.5833) lr 1.5358e-03 eta 0:00:27
epoch [18/50] batch [1/3] time 0.750 (0.750) data 0.697 (0.697) loss 0.6396 (0.6396) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:01:13
epoch [18/50] batch [2/3] time 0.057 (0.404) data 0.001 (0.349) loss 0.4668 (0.5532) acc 90.6250 (89.0625) lr 1.5358e-03 eta 0:00:39
epoch [18/50] batch [3/3] time 0.043 (0.283) data 0.000 (0.233) loss 0.3970 (0.5011) acc 100.0000 (92.7083) lr 1.4818e-03 eta 0:00:27
epoch [19/50] batch [1/3] time 0.850 (0.850) data 0.791 (0.791) loss 0.4578 (0.4578) acc 96.8750 (96.8750) lr 1.4818e-03 eta 0:01:20
epoch [19/50] batch [2/3] time 0.059 (0.455) data 0.000 (0.396) loss 0.4263 (0.4420) acc 96.8750 (96.8750) lr 1.4818e-03 eta 0:00:42
epoch [19/50] batch [3/3] time 0.048 (0.319) data 0.000 (0.264) loss 0.9536 (0.6125) acc 68.7500 (87.5000) lr 1.4258e-03 eta 0:00:29
epoch [20/50] batch [1/3] time 0.800 (0.800) data 0.735 (0.735) loss 0.4668 (0.4668) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:01:13
epoch [20/50] batch [2/3] time 0.057 (0.429) data 0.000 (0.368) loss 0.6582 (0.5625) acc 87.5000 (90.6250) lr 1.4258e-03 eta 0:00:39
epoch [20/50] batch [3/3] time 0.050 (0.303) data 0.000 (0.245) loss 0.5669 (0.5640) acc 87.5000 (89.5833) lr 1.3681e-03 eta 0:00:27
epoch [21/50] batch [1/3] time 0.772 (0.772) data 0.704 (0.704) loss 0.4436 (0.4436) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:01:08
epoch [21/50] batch [2/3] time 0.059 (0.416) data 0.000 (0.352) loss 0.4646 (0.4541) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:00:36
epoch [21/50] batch [3/3] time 0.049 (0.294) data 0.000 (0.235) loss 0.6396 (0.5160) acc 81.2500 (87.5000) lr 1.3090e-03 eta 0:00:25
epoch [22/50] batch [1/3] time 0.773 (0.773) data 0.713 (0.713) loss 0.5659 (0.5659) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:01:06
epoch [22/50] batch [2/3] time 0.058 (0.416) data 0.000 (0.357) loss 0.4692 (0.5176) acc 96.8750 (95.3125) lr 1.3090e-03 eta 0:00:35
epoch [22/50] batch [3/3] time 0.041 (0.291) data 0.000 (0.238) loss 0.5215 (0.5189) acc 93.7500 (94.7917) lr 1.2487e-03 eta 0:00:24
epoch [23/50] batch [1/3] time 0.843 (0.843) data 0.780 (0.780) loss 0.4253 (0.4253) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:01:09
epoch [23/50] batch [2/3] time 0.061 (0.452) data 0.000 (0.390) loss 0.4314 (0.4283) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:00:37
epoch [23/50] batch [3/3] time 0.041 (0.315) data 0.000 (0.260) loss 0.5835 (0.4801) acc 87.5000 (89.5833) lr 1.1874e-03 eta 0:00:25
epoch [24/50] batch [1/3] time 0.789 (0.789) data 0.730 (0.730) loss 0.4880 (0.4880) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:01:03
epoch [24/50] batch [2/3] time 0.057 (0.423) data 0.000 (0.365) loss 0.4124 (0.4502) acc 90.6250 (89.0625) lr 1.1874e-03 eta 0:00:33
epoch [24/50] batch [3/3] time 0.037 (0.294) data 0.000 (0.243) loss 0.6240 (0.5081) acc 87.5000 (88.5417) lr 1.1253e-03 eta 0:00:22
epoch [25/50] batch [1/3] time 0.756 (0.756) data 0.693 (0.693) loss 0.6729 (0.6729) acc 78.1250 (78.1250) lr 1.1253e-03 eta 0:00:58
epoch [25/50] batch [2/3] time 0.061 (0.409) data 0.000 (0.347) loss 0.4771 (0.5750) acc 90.6250 (84.3750) lr 1.1253e-03 eta 0:00:31
epoch [25/50] batch [3/3] time 0.049 (0.289) data 0.000 (0.231) loss 0.3267 (0.4922) acc 100.0000 (89.5833) lr 1.0628e-03 eta 0:00:21
epoch [26/50] batch [1/3] time 0.765 (0.765) data 0.705 (0.705) loss 0.5049 (0.5049) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:00:56
epoch [26/50] batch [2/3] time 0.060 (0.412) data 0.001 (0.353) loss 0.4294 (0.4672) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:00:30
epoch [26/50] batch [3/3] time 0.041 (0.288) data 0.000 (0.235) loss 0.3872 (0.4405) acc 93.7500 (91.6667) lr 1.0000e-03 eta 0:00:20
epoch [27/50] batch [1/3] time 0.776 (0.776) data 0.716 (0.716) loss 0.4736 (0.4736) acc 96.8750 (96.8750) lr 1.0000e-03 eta 0:00:55
epoch [27/50] batch [2/3] time 0.058 (0.417) data 0.000 (0.358) loss 0.5752 (0.5244) acc 84.3750 (90.6250) lr 1.0000e-03 eta 0:00:29
epoch [27/50] batch [3/3] time 0.048 (0.294) data 0.000 (0.239) loss 0.5669 (0.5386) acc 87.5000 (89.5833) lr 9.3721e-04 eta 0:00:20
epoch [28/50] batch [1/3] time 0.831 (0.831) data 0.770 (0.770) loss 0.4976 (0.4976) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:00:56
epoch [28/50] batch [2/3] time 0.060 (0.446) data 0.000 (0.385) loss 0.3608 (0.4292) acc 96.8750 (90.6250) lr 9.3721e-04 eta 0:00:29
epoch [28/50] batch [3/3] time 0.041 (0.311) data 0.000 (0.257) loss 0.8086 (0.5557) acc 87.5000 (89.5833) lr 8.7467e-04 eta 0:00:20
epoch [29/50] batch [1/3] time 0.751 (0.751) data 0.684 (0.684) loss 0.4097 (0.4097) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:00:48
epoch [29/50] batch [2/3] time 0.060 (0.405) data 0.000 (0.342) loss 0.4587 (0.4342) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:00:25
epoch [29/50] batch [3/3] time 0.044 (0.285) data 0.000 (0.228) loss 0.3994 (0.4226) acc 93.7500 (95.8333) lr 8.1262e-04 eta 0:00:17
epoch [30/50] batch [1/3] time 0.771 (0.771) data 0.712 (0.712) loss 0.5659 (0.5659) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:00:47
epoch [30/50] batch [2/3] time 0.058 (0.414) data 0.000 (0.356) loss 0.4788 (0.5223) acc 93.7500 (89.0625) lr 8.1262e-04 eta 0:00:25
epoch [30/50] batch [3/3] time 0.041 (0.290) data 0.000 (0.237) loss 0.4668 (0.5038) acc 93.7500 (90.6250) lr 7.5131e-04 eta 0:00:17
epoch [31/50] batch [1/3] time 0.760 (0.760) data 0.701 (0.701) loss 0.4121 (0.4121) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:44
epoch [31/50] batch [2/3] time 0.060 (0.410) data 0.002 (0.352) loss 0.4382 (0.4252) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:23
epoch [31/50] batch [3/3] time 0.041 (0.287) data 0.000 (0.234) loss 0.3975 (0.4159) acc 93.7500 (93.7500) lr 6.9098e-04 eta 0:00:16
epoch [32/50] batch [1/3] time 0.779 (0.779) data 0.720 (0.720) loss 0.3496 (0.3496) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:00:43
epoch [32/50] batch [2/3] time 0.059 (0.419) data 0.000 (0.360) loss 0.5112 (0.4304) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:00:23
epoch [32/50] batch [3/3] time 0.042 (0.293) data 0.000 (0.240) loss 0.3987 (0.4198) acc 93.7500 (95.8333) lr 6.3188e-04 eta 0:00:15
epoch [33/50] batch [1/3] time 0.843 (0.843) data 0.781 (0.781) loss 0.5215 (0.5215) acc 84.3750 (84.3750) lr 6.3188e-04 eta 0:00:44
epoch [33/50] batch [2/3] time 0.060 (0.452) data 0.000 (0.391) loss 0.3389 (0.4302) acc 96.8750 (90.6250) lr 6.3188e-04 eta 0:00:23
epoch [33/50] batch [3/3] time 0.041 (0.315) data 0.000 (0.261) loss 0.3848 (0.4150) acc 100.0000 (93.7500) lr 5.7422e-04 eta 0:00:16
epoch [34/50] batch [1/3] time 0.770 (0.770) data 0.708 (0.708) loss 0.4541 (0.4541) acc 93.7500 (93.7500) lr 5.7422e-04 eta 0:00:38
epoch [34/50] batch [2/3] time 0.061 (0.415) data 0.000 (0.354) loss 0.3762 (0.4152) acc 93.7500 (93.7500) lr 5.7422e-04 eta 0:00:20
epoch [34/50] batch [3/3] time 0.042 (0.291) data 0.000 (0.236) loss 0.3823 (0.4042) acc 100.0000 (95.8333) lr 5.1825e-04 eta 0:00:13
epoch [35/50] batch [1/3] time 0.774 (0.774) data 0.716 (0.716) loss 0.3711 (0.3711) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:00:36
epoch [35/50] batch [2/3] time 0.059 (0.416) data 0.000 (0.358) loss 0.4504 (0.4108) acc 84.3750 (90.6250) lr 5.1825e-04 eta 0:00:19
epoch [35/50] batch [3/3] time 0.044 (0.292) data 0.000 (0.239) loss 0.2515 (0.3577) acc 100.0000 (93.7500) lr 4.6417e-04 eta 0:00:13
epoch [36/50] batch [1/3] time 0.770 (0.770) data 0.710 (0.710) loss 0.4905 (0.4905) acc 81.2500 (81.2500) lr 4.6417e-04 eta 0:00:33
epoch [36/50] batch [2/3] time 0.058 (0.414) data 0.000 (0.355) loss 0.3687 (0.4296) acc 93.7500 (87.5000) lr 4.6417e-04 eta 0:00:17
epoch [36/50] batch [3/3] time 0.040 (0.289) data 0.000 (0.237) loss 0.4319 (0.4303) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:12
epoch [37/50] batch [1/3] time 0.759 (0.759) data 0.702 (0.702) loss 0.5811 (0.5811) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:31
epoch [37/50] batch [2/3] time 0.052 (0.406) data 0.000 (0.351) loss 0.3613 (0.4712) acc 93.7500 (90.6250) lr 4.1221e-04 eta 0:00:16
epoch [37/50] batch [3/3] time 0.035 (0.282) data 0.000 (0.234) loss 0.3657 (0.4360) acc 100.0000 (93.7500) lr 3.6258e-04 eta 0:00:10
epoch [38/50] batch [1/3] time 0.759 (0.759) data 0.700 (0.700) loss 0.3218 (0.3218) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:00:28
epoch [38/50] batch [2/3] time 0.058 (0.408) data 0.000 (0.350) loss 0.5176 (0.4197) acc 87.5000 (92.1875) lr 3.6258e-04 eta 0:00:15
epoch [38/50] batch [3/3] time 0.041 (0.286) data 0.000 (0.233) loss 0.3384 (0.3926) acc 100.0000 (94.7917) lr 3.1545e-04 eta 0:00:10
epoch [39/50] batch [1/3] time 0.861 (0.861) data 0.804 (0.804) loss 0.3857 (0.3857) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:30
epoch [39/50] batch [2/3] time 0.058 (0.459) data 0.000 (0.402) loss 0.3643 (0.3750) acc 96.8750 (93.7500) lr 3.1545e-04 eta 0:00:15
epoch [39/50] batch [3/3] time 0.045 (0.321) data 0.000 (0.268) loss 0.4556 (0.4019) acc 93.7500 (93.7500) lr 2.7103e-04 eta 0:00:10
epoch [40/50] batch [1/3] time 0.805 (0.805) data 0.745 (0.745) loss 0.3906 (0.3906) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:25
epoch [40/50] batch [2/3] time 0.059 (0.432) data 0.000 (0.373) loss 0.4131 (0.4019) acc 93.7500 (92.1875) lr 2.7103e-04 eta 0:00:13
epoch [40/50] batch [3/3] time 0.048 (0.304) data 0.000 (0.249) loss 0.5229 (0.4422) acc 87.5000 (90.6250) lr 2.2949e-04 eta 0:00:09
epoch [41/50] batch [1/3] time 0.812 (0.812) data 0.751 (0.751) loss 0.4863 (0.4863) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:23
epoch [41/50] batch [2/3] time 0.057 (0.435) data 0.000 (0.376) loss 0.3467 (0.4165) acc 93.7500 (92.1875) lr 2.2949e-04 eta 0:00:12
epoch [41/50] batch [3/3] time 0.039 (0.303) data 0.000 (0.251) loss 0.2749 (0.3693) acc 100.0000 (94.7917) lr 1.9098e-04 eta 0:00:08
epoch [42/50] batch [1/3] time 0.751 (0.751) data 0.697 (0.697) loss 0.3389 (0.3389) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:19
epoch [42/50] batch [2/3] time 0.057 (0.404) data 0.000 (0.349) loss 0.5664 (0.4526) acc 84.3750 (90.6250) lr 1.9098e-04 eta 0:00:10
epoch [42/50] batch [3/3] time 0.043 (0.284) data 0.000 (0.233) loss 0.3032 (0.4028) acc 93.7500 (91.6667) lr 1.5567e-04 eta 0:00:06
epoch [43/50] batch [1/3] time 1.023 (1.023) data 0.962 (0.962) loss 0.5190 (0.5190) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [2/3] time 0.060 (0.541) data 0.000 (0.481) loss 0.3740 (0.4465) acc 93.7500 (92.1875) lr 1.5567e-04 eta 0:00:11
epoch [43/50] batch [3/3] time 0.051 (0.378) data 0.000 (0.321) loss 0.4722 (0.4551) acc 87.5000 (90.6250) lr 1.2369e-04 eta 0:00:07
epoch [44/50] batch [1/3] time 0.772 (0.772) data 0.716 (0.716) loss 0.4451 (0.4451) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:15
epoch [44/50] batch [2/3] time 0.057 (0.415) data 0.000 (0.358) loss 0.4160 (0.4305) acc 87.5000 (89.0625) lr 1.2369e-04 eta 0:00:07
epoch [44/50] batch [3/3] time 0.040 (0.290) data 0.000 (0.239) loss 0.3740 (0.4117) acc 93.7500 (90.6250) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.747 (0.747) data 0.688 (0.688) loss 0.3662 (0.3662) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [2/3] time 0.057 (0.402) data 0.000 (0.344) loss 0.3452 (0.3557) acc 96.8750 (96.8750) lr 9.5173e-05 eta 0:00:06
epoch [45/50] batch [3/3] time 0.040 (0.282) data 0.000 (0.230) loss 0.3193 (0.3436) acc 100.0000 (97.9167) lr 7.0224e-05 eta 0:00:04
epoch [46/50] batch [1/3] time 0.781 (0.781) data 0.723 (0.723) loss 0.4189 (0.4189) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:00:10
epoch [46/50] batch [2/3] time 0.058 (0.420) data 0.000 (0.361) loss 0.3755 (0.3972) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [3/3] time 0.041 (0.293) data 0.000 (0.241) loss 0.3159 (0.3701) acc 93.7500 (95.8333) lr 4.8943e-05 eta 0:00:03
epoch [47/50] batch [1/3] time 0.915 (0.915) data 0.856 (0.856) loss 0.2861 (0.2861) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [2/3] time 0.062 (0.489) data 0.001 (0.428) loss 0.3115 (0.2988) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [3/3] time 0.047 (0.341) data 0.000 (0.286) loss 0.4412 (0.3463) acc 93.7500 (95.8333) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [1/3] time 0.773 (0.773) data 0.714 (0.714) loss 0.5098 (0.5098) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [2/3] time 0.060 (0.416) data 0.001 (0.357) loss 0.3228 (0.4163) acc 96.8750 (92.1875) lr 3.1417e-05 eta 0:00:02
epoch [48/50] batch [3/3] time 0.042 (0.292) data 0.000 (0.238) loss 0.3447 (0.3924) acc 93.7500 (92.7083) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [1/3] time 0.798 (0.798) data 0.739 (0.739) loss 0.6016 (0.6016) acc 87.5000 (87.5000) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [2/3] time 0.060 (0.429) data 0.000 (0.370) loss 0.3911 (0.4963) acc 93.7500 (90.6250) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.041 (0.300) data 0.000 (0.247) loss 0.4482 (0.4803) acc 93.7500 (91.6667) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [1/3] time 0.774 (0.774) data 0.713 (0.713) loss 0.5625 (0.5625) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [2/3] time 0.060 (0.417) data 0.000 (0.357) loss 0.4482 (0.5054) acc 90.6250 (89.0625) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.043 (0.293) data 0.000 (0.238) loss 0.3037 (0.4382) acc 100.0000 (92.7083) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:33,  4.16s/it] 22%|██▏       | 2/9 [00:04<00:14,  2.04s/it] 33%|███▎      | 3/9 [00:05<00:08,  1.36s/it] 44%|████▍     | 4/9 [00:05<00:05,  1.05s/it] 56%|█████▌    | 5/9 [00:06<00:03,  1.15it/s] 67%|██████▋   | 6/9 [00:06<00:02,  1.31it/s] 78%|███████▊  | 7/9 [00:07<00:01,  1.43it/s] 89%|████████▉ | 8/9 [00:08<00:00,  1.53it/s]100%|██████████| 9/9 [00:08<00:00,  1.87it/s]100%|██████████| 9/9 [00:08<00:00,  1.06it/s]
=> result
* total: 4,200
* correct: 3,780
* accuracy: 90.0%
* error: 10.0%
* macro_f1: 90.0%
Elapsed: 0:01:00
Run this job and save the output to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [1/3] time 1.335 (1.335) data 1.212 (1.212) loss 3.3867 (3.3867) acc 56.2500 (56.2500) lr 1.0000e-05 eta 0:03:18
epoch [1/50] batch [2/3] time 0.060 (0.698) data 0.000 (0.606) loss 3.6016 (3.4941) acc 46.8750 (51.5625) lr 1.0000e-05 eta 0:01:43
epoch [1/50] batch [3/3] time 0.065 (0.487) data 0.000 (0.404) loss 3.2148 (3.4010) acc 56.2500 (53.1250) lr 2.0000e-03 eta 0:01:11
epoch [2/50] batch [1/3] time 0.830 (0.830) data 0.770 (0.770) loss 3.4453 (3.4453) acc 46.8750 (46.8750) lr 2.0000e-03 eta 0:02:01
epoch [2/50] batch [2/3] time 0.059 (0.444) data 0.000 (0.385) loss 2.6777 (3.0615) acc 62.5000 (54.6875) lr 2.0000e-03 eta 0:01:04
epoch [2/50] batch [3/3] time 0.041 (0.310) data 0.000 (0.257) loss 2.8965 (3.0065) acc 43.7500 (51.0417) lr 1.9980e-03 eta 0:00:44
epoch [3/50] batch [1/3] time 0.825 (0.825) data 0.766 (0.766) loss 3.0801 (3.0801) acc 62.5000 (62.5000) lr 1.9980e-03 eta 0:01:57
epoch [3/50] batch [2/3] time 0.058 (0.441) data 0.001 (0.383) loss 2.5195 (2.7998) acc 53.1250 (57.8125) lr 1.9980e-03 eta 0:01:02
epoch [3/50] batch [3/3] time 0.043 (0.309) data 0.000 (0.255) loss 2.3027 (2.6341) acc 50.0000 (55.2083) lr 1.9921e-03 eta 0:00:43
epoch [4/50] batch [1/3] time 0.817 (0.817) data 0.756 (0.756) loss 1.7871 (1.7871) acc 68.7500 (68.7500) lr 1.9921e-03 eta 0:01:54
epoch [4/50] batch [2/3] time 0.058 (0.438) data 0.001 (0.378) loss 1.7988 (1.7930) acc 56.2500 (62.5000) lr 1.9921e-03 eta 0:01:00
epoch [4/50] batch [3/3] time 0.043 (0.306) data 0.000 (0.252) loss 1.6113 (1.7324) acc 68.7500 (64.5833) lr 1.9823e-03 eta 0:00:42
epoch [5/50] batch [1/3] time 0.811 (0.811) data 0.747 (0.747) loss 1.2129 (1.2129) acc 87.5000 (87.5000) lr 1.9823e-03 eta 0:01:51
epoch [5/50] batch [2/3] time 0.059 (0.435) data 0.000 (0.374) loss 1.6035 (1.4082) acc 62.5000 (75.0000) lr 1.9823e-03 eta 0:00:59
epoch [5/50] batch [3/3] time 0.048 (0.306) data 0.000 (0.249) loss 1.4492 (1.4219) acc 62.5000 (70.8333) lr 1.9686e-03 eta 0:00:41
epoch [6/50] batch [1/3] time 0.914 (0.914) data 0.856 (0.856) loss 1.0996 (1.0996) acc 93.7500 (93.7500) lr 1.9686e-03 eta 0:02:02
epoch [6/50] batch [2/3] time 0.058 (0.486) data 0.001 (0.428) loss 1.2656 (1.1826) acc 71.8750 (82.8125) lr 1.9686e-03 eta 0:01:04
epoch [6/50] batch [3/3] time 0.059 (0.344) data 0.000 (0.286) loss 1.2148 (1.1934) acc 75.0000 (80.2083) lr 1.9511e-03 eta 0:00:45
epoch [7/50] batch [1/3] time 0.779 (0.779) data 0.719 (0.719) loss 1.1074 (1.1074) acc 75.0000 (75.0000) lr 1.9511e-03 eta 0:01:41
epoch [7/50] batch [2/3] time 0.058 (0.418) data 0.000 (0.360) loss 1.0215 (1.0645) acc 81.2500 (78.1250) lr 1.9511e-03 eta 0:00:54
epoch [7/50] batch [3/3] time 0.040 (0.292) data 0.000 (0.240) loss 1.2598 (1.1296) acc 62.5000 (72.9167) lr 1.9298e-03 eta 0:00:37
epoch [8/50] batch [1/3] time 0.755 (0.755) data 0.693 (0.693) loss 0.9868 (0.9868) acc 84.3750 (84.3750) lr 1.9298e-03 eta 0:01:36
epoch [8/50] batch [2/3] time 0.061 (0.408) data 0.000 (0.346) loss 0.9136 (0.9502) acc 75.0000 (79.6875) lr 1.9298e-03 eta 0:00:51
epoch [8/50] batch [3/3] time 0.042 (0.286) data 0.000 (0.231) loss 0.9766 (0.9590) acc 81.2500 (80.2083) lr 1.9048e-03 eta 0:00:36
epoch [9/50] batch [1/3] time 0.800 (0.800) data 0.742 (0.742) loss 0.9658 (0.9658) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:01:39
epoch [9/50] batch [2/3] time 0.059 (0.429) data 0.001 (0.371) loss 0.8799 (0.9229) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:00:53
epoch [9/50] batch [3/3] time 0.050 (0.303) data 0.000 (0.248) loss 0.8628 (0.9028) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:00:37
epoch [10/50] batch [1/3] time 0.768 (0.768) data 0.708 (0.708) loss 0.8521 (0.8521) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:01:33
epoch [10/50] batch [2/3] time 0.057 (0.413) data 0.000 (0.354) loss 0.8486 (0.8503) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:00:49
epoch [10/50] batch [3/3] time 0.040 (0.289) data 0.000 (0.236) loss 1.3955 (1.0321) acc 81.2500 (79.1667) lr 1.8443e-03 eta 0:00:34
epoch [11/50] batch [1/3] time 0.745 (0.745) data 0.687 (0.687) loss 0.7568 (0.7568) acc 87.5000 (87.5000) lr 1.8443e-03 eta 0:01:28
epoch [11/50] batch [2/3] time 0.058 (0.402) data 0.000 (0.344) loss 0.8706 (0.8137) acc 75.0000 (81.2500) lr 1.8443e-03 eta 0:00:47
epoch [11/50] batch [3/3] time 0.042 (0.282) data 0.000 (0.229) loss 0.9160 (0.8478) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:00:32
epoch [12/50] batch [1/3] time 0.781 (0.781) data 0.723 (0.723) loss 0.6729 (0.6729) acc 90.6250 (90.6250) lr 1.8090e-03 eta 0:01:30
epoch [12/50] batch [2/3] time 0.057 (0.419) data 0.000 (0.362) loss 0.9575 (0.8152) acc 75.0000 (82.8125) lr 1.8090e-03 eta 0:00:48
epoch [12/50] batch [3/3] time 0.040 (0.293) data 0.000 (0.241) loss 0.7646 (0.7983) acc 81.2500 (82.2917) lr 1.7705e-03 eta 0:00:33
epoch [13/50] batch [1/3] time 0.813 (0.813) data 0.751 (0.751) loss 0.8960 (0.8960) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:01:31
epoch [13/50] batch [2/3] time 0.057 (0.435) data 0.000 (0.376) loss 0.6299 (0.7629) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:00:48
epoch [13/50] batch [3/3] time 0.044 (0.305) data 0.000 (0.251) loss 0.7578 (0.7612) acc 75.0000 (83.3333) lr 1.7290e-03 eta 0:00:33
epoch [14/50] batch [1/3] time 0.759 (0.759) data 0.697 (0.697) loss 0.6250 (0.6250) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:01:23
epoch [14/50] batch [2/3] time 0.059 (0.409) data 0.001 (0.349) loss 0.7368 (0.6809) acc 87.5000 (89.0625) lr 1.7290e-03 eta 0:00:44
epoch [14/50] batch [3/3] time 0.047 (0.288) data 0.000 (0.233) loss 0.7295 (0.6971) acc 81.2500 (86.4583) lr 1.6845e-03 eta 0:00:31
epoch [15/50] batch [1/3] time 0.811 (0.811) data 0.751 (0.751) loss 0.7109 (0.7109) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:01:26
epoch [15/50] batch [2/3] time 0.059 (0.435) data 0.000 (0.376) loss 0.6152 (0.6631) acc 96.8750 (90.6250) lr 1.6845e-03 eta 0:00:46
epoch [15/50] batch [3/3] time 0.044 (0.305) data 0.000 (0.250) loss 0.6826 (0.6696) acc 93.7500 (91.6667) lr 1.6374e-03 eta 0:00:31
epoch [16/50] batch [1/3] time 0.777 (0.777) data 0.720 (0.720) loss 0.8398 (0.8398) acc 78.1250 (78.1250) lr 1.6374e-03 eta 0:01:20
epoch [16/50] batch [2/3] time 0.058 (0.418) data 0.001 (0.360) loss 0.6733 (0.7566) acc 93.7500 (85.9375) lr 1.6374e-03 eta 0:00:43
epoch [16/50] batch [3/3] time 0.040 (0.292) data 0.000 (0.240) loss 0.5879 (0.7004) acc 87.5000 (86.4583) lr 1.5878e-03 eta 0:00:29
epoch [17/50] batch [1/3] time 0.786 (0.786) data 0.725 (0.725) loss 0.5693 (0.5693) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:01:19
epoch [17/50] batch [2/3] time 0.058 (0.422) data 0.001 (0.363) loss 0.7939 (0.6816) acc 87.5000 (85.9375) lr 1.5878e-03 eta 0:00:42
epoch [17/50] batch [3/3] time 0.050 (0.298) data 0.000 (0.242) loss 0.4333 (0.5989) acc 100.0000 (90.6250) lr 1.5358e-03 eta 0:00:29
epoch [18/50] batch [1/3] time 0.804 (0.804) data 0.745 (0.745) loss 0.8325 (0.8325) acc 75.0000 (75.0000) lr 1.5358e-03 eta 0:01:18
epoch [18/50] batch [2/3] time 0.060 (0.432) data 0.000 (0.373) loss 0.6899 (0.7612) acc 81.2500 (78.1250) lr 1.5358e-03 eta 0:00:41
epoch [18/50] batch [3/3] time 0.042 (0.302) data 0.000 (0.249) loss 0.6250 (0.7158) acc 87.5000 (81.2500) lr 1.4818e-03 eta 0:00:28
epoch [19/50] batch [1/3] time 0.779 (0.779) data 0.719 (0.719) loss 0.6553 (0.6553) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:01:14
epoch [19/50] batch [2/3] time 0.058 (0.419) data 0.000 (0.360) loss 0.5752 (0.6152) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:00:39
epoch [19/50] batch [3/3] time 0.042 (0.293) data 0.000 (0.240) loss 0.7168 (0.6491) acc 81.2500 (85.4167) lr 1.4258e-03 eta 0:00:27
epoch [20/50] batch [1/3] time 0.773 (0.773) data 0.712 (0.712) loss 0.6045 (0.6045) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:01:11
epoch [20/50] batch [2/3] time 0.060 (0.417) data 0.000 (0.356) loss 0.6870 (0.6458) acc 81.2500 (87.5000) lr 1.4258e-03 eta 0:00:37
epoch [20/50] batch [3/3] time 0.043 (0.292) data 0.000 (0.238) loss 0.5903 (0.6273) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:00:26
epoch [21/50] batch [1/3] time 0.805 (0.805) data 0.743 (0.743) loss 0.6611 (0.6611) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:01:11
epoch [21/50] batch [2/3] time 0.059 (0.432) data 0.000 (0.372) loss 0.5781 (0.6196) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:00:38
epoch [21/50] batch [3/3] time 0.050 (0.305) data 0.000 (0.248) loss 0.5854 (0.6082) acc 93.7500 (89.5833) lr 1.3090e-03 eta 0:00:26
epoch [22/50] batch [1/3] time 0.835 (0.835) data 0.775 (0.775) loss 0.5957 (0.5957) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:11
epoch [22/50] batch [2/3] time 0.059 (0.447) data 0.000 (0.388) loss 0.5830 (0.5894) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:00:37
epoch [22/50] batch [3/3] time 0.043 (0.312) data 0.000 (0.258) loss 0.5210 (0.5666) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:00:26
epoch [23/50] batch [1/3] time 0.792 (0.792) data 0.726 (0.726) loss 0.5264 (0.5264) acc 96.8750 (96.8750) lr 1.2487e-03 eta 0:01:05
epoch [23/50] batch [2/3] time 0.062 (0.427) data 0.000 (0.363) loss 0.5898 (0.5581) acc 87.5000 (92.1875) lr 1.2487e-03 eta 0:00:35
epoch [23/50] batch [3/3] time 0.058 (0.304) data 0.000 (0.242) loss 0.5059 (0.5407) acc 93.7500 (92.7083) lr 1.1874e-03 eta 0:00:24
epoch [24/50] batch [1/3] time 0.845 (0.845) data 0.790 (0.790) loss 0.5420 (0.5420) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:01:07
epoch [24/50] batch [2/3] time 0.056 (0.451) data 0.000 (0.395) loss 0.5312 (0.5366) acc 96.8750 (96.8750) lr 1.1874e-03 eta 0:00:35
epoch [24/50] batch [3/3] time 0.040 (0.314) data 0.000 (0.264) loss 0.6006 (0.5579) acc 87.5000 (93.7500) lr 1.1253e-03 eta 0:00:24
epoch [25/50] batch [1/3] time 0.773 (0.773) data 0.706 (0.706) loss 0.5049 (0.5049) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:00:59
epoch [25/50] batch [2/3] time 0.061 (0.417) data 0.000 (0.353) loss 0.5439 (0.5244) acc 90.6250 (93.7500) lr 1.1253e-03 eta 0:00:31
epoch [25/50] batch [3/3] time 0.048 (0.294) data 0.000 (0.236) loss 0.3999 (0.4829) acc 100.0000 (95.8333) lr 1.0628e-03 eta 0:00:22
epoch [26/50] batch [1/3] time 0.787 (0.787) data 0.725 (0.725) loss 0.5620 (0.5620) acc 96.8750 (96.8750) lr 1.0628e-03 eta 0:00:58
epoch [26/50] batch [2/3] time 0.060 (0.424) data 0.000 (0.363) loss 0.4763 (0.5192) acc 90.6250 (93.7500) lr 1.0628e-03 eta 0:00:30
epoch [26/50] batch [3/3] time 0.040 (0.296) data 0.000 (0.242) loss 0.7129 (0.5837) acc 87.5000 (91.6667) lr 1.0000e-03 eta 0:00:21
epoch [27/50] batch [1/3] time 0.764 (0.764) data 0.703 (0.703) loss 0.4875 (0.4875) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:00:54
epoch [27/50] batch [2/3] time 0.059 (0.411) data 0.001 (0.352) loss 0.6289 (0.5582) acc 84.3750 (89.0625) lr 1.0000e-03 eta 0:00:28
epoch [27/50] batch [3/3] time 0.050 (0.291) data 0.000 (0.235) loss 0.3960 (0.5042) acc 93.7500 (90.6250) lr 9.3721e-04 eta 0:00:20
epoch [28/50] batch [1/3] time 0.970 (0.970) data 0.906 (0.906) loss 0.5669 (0.5669) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:01:05
epoch [28/50] batch [2/3] time 0.059 (0.515) data 0.000 (0.453) loss 0.5889 (0.5779) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:00:34
epoch [28/50] batch [3/3] time 0.051 (0.360) data 0.000 (0.302) loss 0.4607 (0.5388) acc 87.5000 (87.5000) lr 8.7467e-04 eta 0:00:23
epoch [29/50] batch [1/3] time 0.807 (0.807) data 0.743 (0.743) loss 0.4282 (0.4282) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:00:52
epoch [29/50] batch [2/3] time 0.059 (0.433) data 0.000 (0.372) loss 0.5439 (0.4861) acc 84.3750 (90.6250) lr 8.7467e-04 eta 0:00:27
epoch [29/50] batch [3/3] time 0.050 (0.305) data 0.000 (0.248) loss 0.5889 (0.5203) acc 87.5000 (89.5833) lr 8.1262e-04 eta 0:00:19
epoch [30/50] batch [1/3] time 0.905 (0.905) data 0.843 (0.843) loss 0.5645 (0.5645) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:00:56
epoch [30/50] batch [2/3] time 0.060 (0.482) data 0.001 (0.422) loss 0.4365 (0.5005) acc 93.7500 (90.6250) lr 8.1262e-04 eta 0:00:29
epoch [30/50] batch [3/3] time 0.062 (0.342) data 0.000 (0.281) loss 0.4036 (0.4682) acc 93.7500 (91.6667) lr 7.5131e-04 eta 0:00:20
epoch [31/50] batch [1/3] time 0.769 (0.769) data 0.707 (0.707) loss 0.5693 (0.5693) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:00:45
epoch [31/50] batch [2/3] time 0.061 (0.415) data 0.000 (0.353) loss 0.4697 (0.5195) acc 93.7500 (90.6250) lr 7.5131e-04 eta 0:00:24
epoch [31/50] batch [3/3] time 0.040 (0.290) data 0.000 (0.236) loss 0.5928 (0.5439) acc 87.5000 (89.5833) lr 6.9098e-04 eta 0:00:16
epoch [32/50] batch [1/3] time 0.786 (0.786) data 0.724 (0.724) loss 0.5181 (0.5181) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:00:44
epoch [32/50] batch [2/3] time 0.062 (0.424) data 0.000 (0.362) loss 0.4949 (0.5065) acc 90.6250 (89.0625) lr 6.9098e-04 eta 0:00:23
epoch [32/50] batch [3/3] time 0.041 (0.296) data 0.000 (0.242) loss 0.5557 (0.5229) acc 93.7500 (90.6250) lr 6.3188e-04 eta 0:00:16
epoch [33/50] batch [1/3] time 0.820 (0.820) data 0.763 (0.763) loss 0.4956 (0.4956) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:00:43
epoch [33/50] batch [2/3] time 0.058 (0.439) data 0.000 (0.381) loss 0.4453 (0.4705) acc 93.7500 (92.1875) lr 6.3188e-04 eta 0:00:22
epoch [33/50] batch [3/3] time 0.040 (0.306) data 0.000 (0.254) loss 0.6270 (0.5226) acc 81.2500 (88.5417) lr 5.7422e-04 eta 0:00:15
epoch [34/50] batch [1/3] time 0.897 (0.897) data 0.834 (0.834) loss 0.4424 (0.4424) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [2/3] time 0.058 (0.477) data 0.000 (0.417) loss 0.4607 (0.4515) acc 93.7500 (95.3125) lr 5.7422e-04 eta 0:00:23
epoch [34/50] batch [3/3] time 0.042 (0.332) data 0.000 (0.278) loss 0.3926 (0.4319) acc 100.0000 (96.8750) lr 5.1825e-04 eta 0:00:15
epoch [35/50] batch [1/3] time 0.954 (0.954) data 0.890 (0.890) loss 0.4819 (0.4819) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:00:44
epoch [35/50] batch [2/3] time 0.060 (0.507) data 0.001 (0.445) loss 0.4341 (0.4580) acc 96.8750 (93.7500) lr 5.1825e-04 eta 0:00:23
epoch [35/50] batch [3/3] time 0.047 (0.354) data 0.000 (0.297) loss 0.4541 (0.4567) acc 100.0000 (95.8333) lr 4.6417e-04 eta 0:00:15
epoch [36/50] batch [1/3] time 1.250 (1.250) data 1.189 (1.189) loss 0.5430 (0.5430) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:00:54
epoch [36/50] batch [2/3] time 0.060 (0.655) data 0.001 (0.595) loss 0.4800 (0.5115) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:00:28
epoch [36/50] batch [3/3] time 0.047 (0.452) data 0.000 (0.397) loss 0.5557 (0.5262) acc 93.7500 (91.6667) lr 4.1221e-04 eta 0:00:18
epoch [37/50] batch [1/3] time 1.657 (1.657) data 1.594 (1.594) loss 0.6108 (0.6108) acc 84.3750 (84.3750) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [2/3] time 0.061 (0.859) data 0.000 (0.797) loss 0.5757 (0.5933) acc 90.6250 (87.5000) lr 4.1221e-04 eta 0:00:34
epoch [37/50] batch [3/3] time 0.042 (0.587) data 0.000 (0.532) loss 0.4922 (0.5596) acc 87.5000 (87.5000) lr 3.6258e-04 eta 0:00:22
epoch [38/50] batch [1/3] time 1.313 (1.313) data 1.248 (1.248) loss 0.5889 (0.5889) acc 84.3750 (84.3750) lr 3.6258e-04 eta 0:00:49
epoch [38/50] batch [2/3] time 0.059 (0.686) data 0.001 (0.624) loss 0.4421 (0.5155) acc 96.8750 (90.6250) lr 3.6258e-04 eta 0:00:25
epoch [38/50] batch [3/3] time 0.052 (0.475) data 0.000 (0.416) loss 0.5000 (0.5103) acc 87.5000 (89.5833) lr 3.1545e-04 eta 0:00:17
epoch [39/50] batch [1/3] time 1.129 (1.129) data 1.067 (1.067) loss 0.5098 (0.5098) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:39
epoch [39/50] batch [2/3] time 0.057 (0.593) data 0.001 (0.534) loss 0.4424 (0.4761) acc 96.8750 (93.7500) lr 3.1545e-04 eta 0:00:20
epoch [39/50] batch [3/3] time 0.047 (0.411) data 0.000 (0.356) loss 0.4836 (0.4786) acc 93.7500 (93.7500) lr 2.7103e-04 eta 0:00:13
epoch [40/50] batch [1/3] time 1.027 (1.027) data 0.964 (0.964) loss 0.5381 (0.5381) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [2/3] time 0.059 (0.543) data 0.000 (0.482) loss 0.7466 (0.6423) acc 75.0000 (82.8125) lr 2.7103e-04 eta 0:00:16
epoch [40/50] batch [3/3] time 0.047 (0.378) data 0.000 (0.322) loss 0.5205 (0.6017) acc 100.0000 (88.5417) lr 2.2949e-04 eta 0:00:11
epoch [41/50] batch [1/3] time 1.176 (1.176) data 1.115 (1.115) loss 0.3730 (0.3730) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:00:34
epoch [41/50] batch [2/3] time 0.059 (0.617) data 0.000 (0.558) loss 0.7603 (0.5667) acc 84.3750 (90.6250) lr 2.2949e-04 eta 0:00:17
epoch [41/50] batch [3/3] time 0.042 (0.426) data 0.000 (0.372) loss 0.4023 (0.5119) acc 100.0000 (93.7500) lr 1.9098e-04 eta 0:00:11
epoch [42/50] batch [1/3] time 1.396 (1.396) data 1.331 (1.331) loss 0.4290 (0.4290) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:00:36
epoch [42/50] batch [2/3] time 0.066 (0.731) data 0.007 (0.669) loss 0.6040 (0.5165) acc 84.3750 (87.5000) lr 1.9098e-04 eta 0:00:18
epoch [42/50] batch [3/3] time 0.040 (0.501) data 0.000 (0.446) loss 0.4512 (0.4947) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:00:12
epoch [43/50] batch [1/3] time 0.977 (0.977) data 0.915 (0.915) loss 0.5146 (0.5146) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [2/3] time 0.060 (0.518) data 0.000 (0.458) loss 0.4492 (0.4819) acc 93.7500 (90.6250) lr 1.5567e-04 eta 0:00:11
epoch [43/50] batch [3/3] time 0.042 (0.360) data 0.000 (0.305) loss 0.5562 (0.5067) acc 87.5000 (89.5833) lr 1.2369e-04 eta 0:00:07
epoch [44/50] batch [1/3] time 0.840 (0.840) data 0.779 (0.779) loss 0.3760 (0.3760) acc 100.0000 (100.0000) lr 1.2369e-04 eta 0:00:16
epoch [44/50] batch [2/3] time 0.059 (0.449) data 0.000 (0.390) loss 0.5049 (0.4404) acc 93.7500 (96.8750) lr 1.2369e-04 eta 0:00:08
epoch [44/50] batch [3/3] time 0.048 (0.316) data 0.000 (0.260) loss 0.4268 (0.4359) acc 87.5000 (93.7500) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.952 (0.952) data 0.889 (0.889) loss 0.4897 (0.4897) acc 87.5000 (87.5000) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [2/3] time 0.060 (0.506) data 0.000 (0.445) loss 0.4265 (0.4581) acc 96.8750 (92.1875) lr 9.5173e-05 eta 0:00:08
epoch [45/50] batch [3/3] time 0.048 (0.354) data 0.000 (0.297) loss 0.4346 (0.4503) acc 100.0000 (94.7917) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [1/3] time 1.259 (1.259) data 1.198 (1.198) loss 0.5557 (0.5557) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:17
epoch [46/50] batch [2/3] time 0.059 (0.659) data 0.000 (0.599) loss 0.4592 (0.5074) acc 96.8750 (92.1875) lr 7.0224e-05 eta 0:00:08
epoch [46/50] batch [3/3] time 0.041 (0.453) data 0.000 (0.399) loss 0.4009 (0.4719) acc 93.7500 (92.7083) lr 4.8943e-05 eta 0:00:05
epoch [47/50] batch [1/3] time 1.035 (1.035) data 0.973 (0.973) loss 0.4180 (0.4180) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [2/3] time 0.061 (0.548) data 0.001 (0.487) loss 0.4739 (0.4459) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:05
epoch [47/50] batch [3/3] time 0.044 (0.380) data 0.000 (0.325) loss 0.4756 (0.4558) acc 100.0000 (95.8333) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [1/3] time 1.174 (1.174) data 1.109 (1.109) loss 0.4692 (0.4692) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [2/3] time 0.062 (0.618) data 0.000 (0.555) loss 0.4604 (0.4648) acc 96.8750 (93.7500) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [3/3] time 0.042 (0.426) data 0.000 (0.370) loss 0.5918 (0.5072) acc 87.5000 (91.6667) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [1/3] time 0.884 (0.884) data 0.814 (0.814) loss 0.5459 (0.5459) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [2/3] time 0.062 (0.473) data 0.000 (0.407) loss 0.4849 (0.5154) acc 93.7500 (92.1875) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.044 (0.330) data 0.000 (0.272) loss 0.4294 (0.4867) acc 93.7500 (92.7083) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [1/3] time 1.220 (1.220) data 1.153 (1.153) loss 0.4634 (0.4634) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [2/3] time 0.059 (0.640) data 0.001 (0.577) loss 0.5088 (0.4861) acc 87.5000 (89.0625) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.042 (0.440) data 0.000 (0.385) loss 0.5342 (0.5021) acc 87.5000 (88.5417) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:03<00:25,  3.21s/it] 22%|██▏       | 2/9 [00:03<00:11,  1.66s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.16s/it] 44%|████▍     | 4/9 [00:04<00:04,  1.08it/s] 56%|█████▌    | 5/9 [00:05<00:03,  1.26it/s] 67%|██████▋   | 6/9 [00:06<00:02,  1.40it/s] 78%|███████▊  | 7/9 [00:06<00:01,  1.51it/s] 89%|████████▉ | 8/9 [00:07<00:00,  1.59it/s]100%|██████████| 9/9 [00:07<00:00,  1.93it/s]100%|██████████| 9/9 [00:07<00:00,  1.19it/s]
=> result
* total: 4,200
* correct: 3,860
* accuracy: 91.9%
* error: 8.1%
* macro_f1: 92.0%
Elapsed: 0:01:05
Run this job and save the output to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [1/3] time 5.695 (5.695) data 5.575 (5.575) loss 3.7656 (3.7656) acc 65.6250 (65.6250) lr 1.0000e-05 eta 0:14:08
epoch [1/50] batch [2/3] time 0.065 (2.880) data 0.000 (2.788) loss 3.9316 (3.8486) acc 59.3750 (62.5000) lr 1.0000e-05 eta 0:07:06
epoch [1/50] batch [3/3] time 0.075 (1.945) data 0.000 (1.858) loss 3.9414 (3.8796) acc 68.7500 (64.5833) lr 2.0000e-03 eta 0:04:45
epoch [2/50] batch [1/3] time 1.238 (1.238) data 1.170 (1.170) loss 3.8496 (3.8496) acc 46.8750 (46.8750) lr 2.0000e-03 eta 0:03:00
epoch [2/50] batch [2/3] time 0.278 (0.758) data 0.219 (0.695) loss 2.9902 (3.4199) acc 75.0000 (60.9375) lr 2.0000e-03 eta 0:01:49
epoch [2/50] batch [3/3] time 0.044 (0.520) data 0.000 (0.463) loss 2.4805 (3.1068) acc 75.0000 (65.6250) lr 1.9980e-03 eta 0:01:14
epoch [3/50] batch [1/3] time 1.193 (1.193) data 1.126 (1.126) loss 2.3906 (2.3906) acc 71.8750 (71.8750) lr 1.9980e-03 eta 0:02:50
epoch [3/50] batch [2/3] time 0.067 (0.630) data 0.004 (0.565) loss 2.1074 (2.2490) acc 65.6250 (68.7500) lr 1.9980e-03 eta 0:01:29
epoch [3/50] batch [3/3] time 0.049 (0.436) data 0.000 (0.377) loss 1.7285 (2.0755) acc 75.0000 (70.8333) lr 1.9921e-03 eta 0:01:01
epoch [4/50] batch [1/3] time 1.206 (1.206) data 1.145 (1.145) loss 1.6221 (1.6221) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:02:48
epoch [4/50] batch [2/3] time 0.055 (0.631) data 0.000 (0.573) loss 1.6348 (1.6284) acc 68.7500 (70.3125) lr 1.9921e-03 eta 0:01:27
epoch [4/50] batch [3/3] time 0.043 (0.435) data 0.000 (0.382) loss 1.3262 (1.5277) acc 81.2500 (73.9583) lr 1.9823e-03 eta 0:00:59
epoch [5/50] batch [1/3] time 1.175 (1.175) data 1.115 (1.115) loss 1.2578 (1.2578) acc 84.3750 (84.3750) lr 1.9823e-03 eta 0:02:40
epoch [5/50] batch [2/3] time 0.058 (0.617) data 0.000 (0.557) loss 1.2480 (1.2529) acc 78.1250 (81.2500) lr 1.9823e-03 eta 0:01:23
epoch [5/50] batch [3/3] time 0.048 (0.427) data 0.000 (0.372) loss 1.3857 (1.2972) acc 68.7500 (77.0833) lr 1.9686e-03 eta 0:00:57
epoch [6/50] batch [1/3] time 1.250 (1.250) data 1.182 (1.182) loss 1.1719 (1.1719) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:02:47
epoch [6/50] batch [2/3] time 0.059 (0.654) data 0.000 (0.591) loss 0.9819 (1.0769) acc 84.3750 (79.6875) lr 1.9686e-03 eta 0:01:27
epoch [6/50] batch [3/3] time 0.046 (0.451) data 0.001 (0.394) loss 1.3066 (1.1535) acc 62.5000 (73.9583) lr 1.9511e-03 eta 0:00:59
epoch [7/50] batch [1/3] time 1.049 (1.049) data 0.987 (0.987) loss 1.0234 (1.0234) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:02:17
epoch [7/50] batch [2/3] time 0.059 (0.554) data 0.000 (0.494) loss 1.0449 (1.0342) acc 71.8750 (76.5625) lr 1.9511e-03 eta 0:01:12
epoch [7/50] batch [3/3] time 0.044 (0.384) data 0.000 (0.329) loss 0.8721 (0.9801) acc 81.2500 (78.1250) lr 1.9298e-03 eta 0:00:49
epoch [8/50] batch [1/3] time 1.063 (1.063) data 0.996 (0.996) loss 0.7646 (0.7646) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:02:16
epoch [8/50] batch [2/3] time 0.054 (0.558) data 0.000 (0.498) loss 0.8770 (0.8208) acc 81.2500 (84.3750) lr 1.9298e-03 eta 0:01:10
epoch [8/50] batch [3/3] time 0.046 (0.387) data 0.000 (0.332) loss 1.0566 (0.8994) acc 75.0000 (81.2500) lr 1.9048e-03 eta 0:00:48
epoch [9/50] batch [1/3] time 0.910 (0.910) data 0.848 (0.848) loss 0.9321 (0.9321) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:01:53
epoch [9/50] batch [2/3] time 0.059 (0.485) data 0.001 (0.424) loss 1.0449 (0.9885) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:01:00
epoch [9/50] batch [3/3] time 0.043 (0.337) data 0.000 (0.283) loss 0.7666 (0.9146) acc 87.5000 (81.2500) lr 1.8763e-03 eta 0:00:41
epoch [10/50] batch [1/3] time 1.230 (1.230) data 1.165 (1.165) loss 0.9238 (0.9238) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:02:30
epoch [10/50] batch [2/3] time 0.058 (0.644) data 0.000 (0.583) loss 0.8174 (0.8706) acc 78.1250 (79.6875) lr 1.8763e-03 eta 0:01:17
epoch [10/50] batch [3/3] time 0.040 (0.443) data 0.000 (0.389) loss 0.6719 (0.8044) acc 93.7500 (84.3750) lr 1.8443e-03 eta 0:00:53
epoch [11/50] batch [1/3] time 1.280 (1.280) data 1.223 (1.223) loss 0.9639 (0.9639) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:02:32
epoch [11/50] batch [2/3] time 0.058 (0.669) data 0.000 (0.612) loss 0.7402 (0.8521) acc 78.1250 (79.6875) lr 1.8443e-03 eta 0:01:18
epoch [11/50] batch [3/3] time 0.043 (0.460) data 0.000 (0.408) loss 0.5151 (0.7397) acc 93.7500 (84.3750) lr 1.8090e-03 eta 0:00:53
epoch [12/50] batch [1/3] time 1.214 (1.214) data 1.149 (1.149) loss 0.8052 (0.8052) acc 81.2500 (81.2500) lr 1.8090e-03 eta 0:02:20
epoch [12/50] batch [2/3] time 0.059 (0.637) data 0.000 (0.575) loss 0.6548 (0.7300) acc 90.6250 (85.9375) lr 1.8090e-03 eta 0:01:13
epoch [12/50] batch [3/3] time 0.043 (0.439) data 0.000 (0.383) loss 0.5269 (0.6623) acc 100.0000 (90.6250) lr 1.7705e-03 eta 0:00:50
epoch [13/50] batch [1/3] time 1.118 (1.118) data 1.052 (1.052) loss 0.8950 (0.8950) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:02:06
epoch [13/50] batch [2/3] time 0.062 (0.590) data 0.001 (0.526) loss 0.7207 (0.8079) acc 84.3750 (82.8125) lr 1.7705e-03 eta 0:01:06
epoch [13/50] batch [3/3] time 0.045 (0.409) data 0.000 (0.351) loss 0.4497 (0.6885) acc 100.0000 (88.5417) lr 1.7290e-03 eta 0:00:45
epoch [14/50] batch [1/3] time 1.406 (1.406) data 1.345 (1.345) loss 0.7329 (0.7329) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:02:34
epoch [14/50] batch [2/3] time 0.059 (0.732) data 0.000 (0.673) loss 0.9126 (0.8228) acc 71.8750 (79.6875) lr 1.7290e-03 eta 0:01:19
epoch [14/50] batch [3/3] time 0.042 (0.502) data 0.000 (0.449) loss 0.9346 (0.8600) acc 75.0000 (78.1250) lr 1.6845e-03 eta 0:00:54
epoch [15/50] batch [1/3] time 1.076 (1.076) data 1.014 (1.014) loss 0.6162 (0.6162) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:01:55
epoch [15/50] batch [2/3] time 0.059 (0.567) data 0.000 (0.507) loss 0.7686 (0.6924) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:01:00
epoch [15/50] batch [3/3] time 0.042 (0.392) data 0.000 (0.338) loss 0.7578 (0.7142) acc 75.0000 (83.3333) lr 1.6374e-03 eta 0:00:41
epoch [16/50] batch [1/3] time 1.325 (1.325) data 1.264 (1.264) loss 0.6934 (0.6934) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:02:17
epoch [16/50] batch [2/3] time 0.059 (0.692) data 0.000 (0.632) loss 0.5684 (0.6309) acc 96.8750 (92.1875) lr 1.6374e-03 eta 0:01:11
epoch [16/50] batch [3/3] time 0.043 (0.476) data 0.000 (0.421) loss 0.6494 (0.6370) acc 87.5000 (90.6250) lr 1.5878e-03 eta 0:00:48
epoch [17/50] batch [1/3] time 1.223 (1.223) data 1.160 (1.160) loss 0.4849 (0.4849) acc 96.8750 (96.8750) lr 1.5878e-03 eta 0:02:03
epoch [17/50] batch [2/3] time 0.061 (0.642) data 0.000 (0.580) loss 0.6113 (0.5481) acc 90.6250 (93.7500) lr 1.5878e-03 eta 0:01:04
epoch [17/50] batch [3/3] time 0.043 (0.442) data 0.000 (0.387) loss 0.6553 (0.5838) acc 81.2500 (89.5833) lr 1.5358e-03 eta 0:00:43
epoch [18/50] batch [1/3] time 1.134 (1.134) data 1.068 (1.068) loss 0.6035 (0.6035) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:01:51
epoch [18/50] batch [2/3] time 0.063 (0.598) data 0.000 (0.534) loss 0.4812 (0.5424) acc 90.6250 (89.0625) lr 1.5358e-03 eta 0:00:58
epoch [18/50] batch [3/3] time 0.054 (0.417) data 0.000 (0.356) loss 0.7241 (0.6029) acc 81.2500 (86.4583) lr 1.4818e-03 eta 0:00:40
epoch [19/50] batch [1/3] time 1.292 (1.292) data 1.233 (1.233) loss 0.4912 (0.4912) acc 96.8750 (96.8750) lr 1.4818e-03 eta 0:02:02
epoch [19/50] batch [2/3] time 0.058 (0.675) data 0.000 (0.617) loss 0.7695 (0.6304) acc 81.2500 (89.0625) lr 1.4818e-03 eta 0:01:03
epoch [19/50] batch [3/3] time 0.042 (0.464) data 0.000 (0.411) loss 0.4268 (0.5625) acc 93.7500 (90.6250) lr 1.4258e-03 eta 0:00:43
epoch [20/50] batch [1/3] time 1.167 (1.167) data 1.103 (1.103) loss 0.5596 (0.5596) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:01:47
epoch [20/50] batch [2/3] time 0.061 (0.614) data 0.000 (0.552) loss 0.6582 (0.6089) acc 84.3750 (87.5000) lr 1.4258e-03 eta 0:00:55
epoch [20/50] batch [3/3] time 0.043 (0.424) data 0.000 (0.368) loss 0.7764 (0.6647) acc 75.0000 (83.3333) lr 1.3681e-03 eta 0:00:38
epoch [21/50] batch [1/3] time 1.186 (1.186) data 1.124 (1.124) loss 0.4949 (0.4949) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:01:45
epoch [21/50] batch [2/3] time 0.059 (0.622) data 0.000 (0.562) loss 0.4958 (0.4954) acc 93.7500 (92.1875) lr 1.3681e-03 eta 0:00:54
epoch [21/50] batch [3/3] time 0.043 (0.429) data 0.000 (0.375) loss 0.5693 (0.5200) acc 93.7500 (92.7083) lr 1.3090e-03 eta 0:00:37
epoch [22/50] batch [1/3] time 2.133 (2.133) data 2.069 (2.069) loss 0.6226 (0.6226) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:03:03
epoch [22/50] batch [2/3] time 0.062 (1.098) data 0.001 (1.035) loss 0.6929 (0.6577) acc 84.3750 (87.5000) lr 1.3090e-03 eta 0:01:33
epoch [22/50] batch [3/3] time 0.043 (0.746) data 0.000 (0.690) loss 0.6333 (0.6496) acc 93.7500 (89.5833) lr 1.2487e-03 eta 0:01:02
epoch [23/50] batch [1/3] time 1.485 (1.485) data 1.425 (1.425) loss 0.6665 (0.6665) acc 81.2500 (81.2500) lr 1.2487e-03 eta 0:02:03
epoch [23/50] batch [2/3] time 0.061 (0.773) data 0.000 (0.713) loss 0.5010 (0.5837) acc 93.7500 (87.5000) lr 1.2487e-03 eta 0:01:03
epoch [23/50] batch [3/3] time 0.051 (0.532) data 0.000 (0.475) loss 0.3696 (0.5124) acc 100.0000 (91.6667) lr 1.1874e-03 eta 0:00:43
epoch [24/50] batch [1/3] time 1.777 (1.777) data 1.712 (1.712) loss 0.5815 (0.5815) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:02:22
epoch [24/50] batch [2/3] time 0.060 (0.919) data 0.000 (0.856) loss 0.5625 (0.5720) acc 87.5000 (87.5000) lr 1.1874e-03 eta 0:01:12
epoch [24/50] batch [3/3] time 0.048 (0.628) data 0.000 (0.571) loss 0.4829 (0.5423) acc 93.7500 (89.5833) lr 1.1253e-03 eta 0:00:49
epoch [25/50] batch [1/3] time 2.428 (2.428) data 2.363 (2.363) loss 0.5098 (0.5098) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:03:06
epoch [25/50] batch [2/3] time 0.060 (1.244) data 0.000 (1.182) loss 0.4097 (0.4597) acc 96.8750 (95.3125) lr 1.1253e-03 eta 0:01:34
epoch [25/50] batch [3/3] time 0.041 (0.843) data 0.000 (0.788) loss 0.5767 (0.4987) acc 87.5000 (92.7083) lr 1.0628e-03 eta 0:01:03
epoch [26/50] batch [1/3] time 0.892 (0.892) data 0.834 (0.834) loss 0.4636 (0.4636) acc 96.8750 (96.8750) lr 1.0628e-03 eta 0:01:06
epoch [26/50] batch [2/3] time 0.059 (0.476) data 0.001 (0.417) loss 0.4832 (0.4734) acc 93.7500 (95.3125) lr 1.0628e-03 eta 0:00:34
epoch [26/50] batch [3/3] time 0.043 (0.331) data 0.000 (0.278) loss 0.3748 (0.4405) acc 93.7500 (94.7917) lr 1.0000e-03 eta 0:00:23
epoch [27/50] batch [1/3] time 1.506 (1.506) data 1.439 (1.439) loss 0.4937 (0.4937) acc 93.7500 (93.7500) lr 1.0000e-03 eta 0:01:46
epoch [27/50] batch [2/3] time 0.064 (0.785) data 0.000 (0.720) loss 0.6953 (0.5945) acc 81.2500 (87.5000) lr 1.0000e-03 eta 0:00:54
epoch [27/50] batch [3/3] time 0.043 (0.538) data 0.000 (0.480) loss 0.3625 (0.5172) acc 100.0000 (91.6667) lr 9.3721e-04 eta 0:00:37
epoch [28/50] batch [1/3] time 1.296 (1.296) data 1.237 (1.237) loss 0.4673 (0.4673) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:01:28
epoch [28/50] batch [2/3] time 0.057 (0.677) data 0.000 (0.619) loss 0.5742 (0.5208) acc 90.6250 (92.1875) lr 9.3721e-04 eta 0:00:45
epoch [28/50] batch [3/3] time 0.049 (0.467) data 0.000 (0.413) loss 0.4844 (0.5086) acc 93.7500 (92.7083) lr 8.7467e-04 eta 0:00:30
epoch [29/50] batch [1/3] time 2.374 (2.374) data 2.308 (2.308) loss 0.4626 (0.4626) acc 90.6250 (90.6250) lr 8.7467e-04 eta 0:02:34
epoch [29/50] batch [2/3] time 0.062 (1.218) data 0.001 (1.154) loss 0.6470 (0.5548) acc 78.1250 (84.3750) lr 8.7467e-04 eta 0:01:17
epoch [29/50] batch [3/3] time 0.041 (0.826) data 0.000 (0.770) loss 0.3958 (0.5018) acc 93.7500 (87.5000) lr 8.1262e-04 eta 0:00:52
epoch [30/50] batch [1/3] time 1.435 (1.435) data 1.375 (1.375) loss 0.4460 (0.4460) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:01:28
epoch [30/50] batch [2/3] time 0.059 (0.747) data 0.000 (0.687) loss 0.4636 (0.4548) acc 93.7500 (90.6250) lr 8.1262e-04 eta 0:00:45
epoch [30/50] batch [3/3] time 0.042 (0.512) data 0.000 (0.458) loss 0.4890 (0.4662) acc 93.7500 (91.6667) lr 7.5131e-04 eta 0:00:30
epoch [31/50] batch [1/3] time 1.282 (1.282) data 1.221 (1.221) loss 0.6729 (0.6729) acc 84.3750 (84.3750) lr 7.5131e-04 eta 0:01:15
epoch [31/50] batch [2/3] time 0.058 (0.670) data 0.000 (0.610) loss 0.4626 (0.5677) acc 90.6250 (87.5000) lr 7.5131e-04 eta 0:00:38
epoch [31/50] batch [3/3] time 0.040 (0.460) data 0.000 (0.407) loss 0.3809 (0.5055) acc 100.0000 (91.6667) lr 6.9098e-04 eta 0:00:26
epoch [32/50] batch [1/3] time 1.581 (1.581) data 1.520 (1.520) loss 0.4207 (0.4207) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:01:28
epoch [32/50] batch [2/3] time 0.059 (0.820) data 0.000 (0.760) loss 0.6504 (0.5355) acc 78.1250 (87.5000) lr 6.9098e-04 eta 0:00:45
epoch [32/50] batch [3/3] time 0.043 (0.561) data 0.000 (0.507) loss 0.5322 (0.5344) acc 93.7500 (89.5833) lr 6.3188e-04 eta 0:00:30
epoch [33/50] batch [1/3] time 0.968 (0.968) data 0.907 (0.907) loss 0.5098 (0.5098) acc 87.5000 (87.5000) lr 6.3188e-04 eta 0:00:51
epoch [33/50] batch [2/3] time 0.063 (0.515) data 0.000 (0.454) loss 0.4663 (0.4880) acc 93.7500 (90.6250) lr 6.3188e-04 eta 0:00:26
epoch [33/50] batch [3/3] time 0.045 (0.359) data 0.000 (0.302) loss 0.5625 (0.5129) acc 87.5000 (89.5833) lr 5.7422e-04 eta 0:00:18
epoch [34/50] batch [1/3] time 0.937 (0.937) data 0.874 (0.874) loss 0.5767 (0.5767) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [2/3] time 0.062 (0.499) data 0.000 (0.437) loss 0.4111 (0.4939) acc 96.8750 (89.0625) lr 5.7422e-04 eta 0:00:24
epoch [34/50] batch [3/3] time 0.042 (0.347) data 0.000 (0.291) loss 0.3867 (0.4582) acc 93.7500 (90.6250) lr 5.1825e-04 eta 0:00:16
epoch [35/50] batch [1/3] time 0.845 (0.845) data 0.785 (0.785) loss 0.4097 (0.4097) acc 90.6250 (90.6250) lr 5.1825e-04 eta 0:00:39
epoch [35/50] batch [2/3] time 0.058 (0.451) data 0.000 (0.393) loss 0.5146 (0.4622) acc 93.7500 (92.1875) lr 5.1825e-04 eta 0:00:20
epoch [35/50] batch [3/3] time 0.043 (0.315) data 0.000 (0.262) loss 0.4607 (0.4617) acc 93.7500 (92.7083) lr 4.6417e-04 eta 0:00:14
epoch [36/50] batch [1/3] time 0.838 (0.838) data 0.778 (0.778) loss 0.4292 (0.4292) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:00:36
epoch [36/50] batch [2/3] time 0.058 (0.448) data 0.000 (0.389) loss 0.5132 (0.4712) acc 90.6250 (90.6250) lr 4.6417e-04 eta 0:00:19
epoch [36/50] batch [3/3] time 0.043 (0.313) data 0.000 (0.260) loss 0.3142 (0.4189) acc 100.0000 (93.7500) lr 4.1221e-04 eta 0:00:13
epoch [37/50] batch [1/3] time 0.903 (0.903) data 0.837 (0.837) loss 0.5103 (0.5103) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:00:37
epoch [37/50] batch [2/3] time 0.057 (0.480) data 0.000 (0.419) loss 0.4836 (0.4969) acc 84.3750 (90.6250) lr 4.1221e-04 eta 0:00:19
epoch [37/50] batch [3/3] time 0.052 (0.337) data 0.000 (0.279) loss 0.3662 (0.4534) acc 93.7500 (91.6667) lr 3.6258e-04 eta 0:00:13
epoch [38/50] batch [1/3] time 0.869 (0.869) data 0.802 (0.802) loss 0.4194 (0.4194) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:00:33
epoch [38/50] batch [2/3] time 0.060 (0.464) data 0.000 (0.401) loss 0.3843 (0.4019) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:00:17
epoch [38/50] batch [3/3] time 0.046 (0.325) data 0.000 (0.267) loss 0.5654 (0.4564) acc 81.2500 (91.6667) lr 3.1545e-04 eta 0:00:11
epoch [39/50] batch [1/3] time 0.845 (0.845) data 0.785 (0.785) loss 0.5005 (0.5005) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:29
epoch [39/50] batch [2/3] time 0.059 (0.452) data 0.000 (0.393) loss 0.5371 (0.5188) acc 87.5000 (90.6250) lr 3.1545e-04 eta 0:00:15
epoch [39/50] batch [3/3] time 0.042 (0.315) data 0.000 (0.262) loss 0.2795 (0.4390) acc 100.0000 (93.7500) lr 2.7103e-04 eta 0:00:10
epoch [40/50] batch [1/3] time 0.846 (0.846) data 0.786 (0.786) loss 0.4849 (0.4849) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:27
epoch [40/50] batch [2/3] time 0.060 (0.453) data 0.000 (0.393) loss 0.4094 (0.4471) acc 93.7500 (92.1875) lr 2.7103e-04 eta 0:00:14
epoch [40/50] batch [3/3] time 0.044 (0.317) data 0.000 (0.262) loss 0.3923 (0.4289) acc 93.7500 (92.7083) lr 2.2949e-04 eta 0:00:09
epoch [41/50] batch [1/3] time 0.843 (0.843) data 0.783 (0.783) loss 0.5938 (0.5938) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:24
epoch [41/50] batch [2/3] time 0.058 (0.451) data 0.000 (0.392) loss 0.4719 (0.5328) acc 87.5000 (89.0625) lr 2.2949e-04 eta 0:00:12
epoch [41/50] batch [3/3] time 0.044 (0.315) data 0.000 (0.261) loss 0.3325 (0.4661) acc 100.0000 (92.7083) lr 1.9098e-04 eta 0:00:08
epoch [42/50] batch [1/3] time 0.850 (0.850) data 0.789 (0.789) loss 0.4854 (0.4854) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:00:22
epoch [42/50] batch [2/3] time 0.061 (0.456) data 0.000 (0.395) loss 0.4756 (0.4805) acc 84.3750 (85.9375) lr 1.9098e-04 eta 0:00:11
epoch [42/50] batch [3/3] time 0.040 (0.317) data 0.000 (0.263) loss 0.6074 (0.5228) acc 81.2500 (84.3750) lr 1.5567e-04 eta 0:00:07
epoch [43/50] batch [1/3] time 0.849 (0.849) data 0.783 (0.783) loss 0.5078 (0.5078) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:19
epoch [43/50] batch [2/3] time 0.063 (0.456) data 0.000 (0.392) loss 0.3516 (0.4297) acc 96.8750 (93.7500) lr 1.5567e-04 eta 0:00:10
epoch [43/50] batch [3/3] time 0.058 (0.323) data 0.000 (0.261) loss 0.3857 (0.4150) acc 87.5000 (91.6667) lr 1.2369e-04 eta 0:00:06
epoch [44/50] batch [1/3] time 0.895 (0.895) data 0.830 (0.830) loss 0.3926 (0.3926) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [2/3] time 0.061 (0.478) data 0.000 (0.415) loss 0.4543 (0.4235) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:09
epoch [44/50] batch [3/3] time 0.043 (0.333) data 0.000 (0.277) loss 0.4893 (0.4454) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.871 (0.871) data 0.810 (0.810) loss 0.4561 (0.4561) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [2/3] time 0.059 (0.465) data 0.000 (0.405) loss 0.4546 (0.4553) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:07
epoch [45/50] batch [3/3] time 0.043 (0.324) data 0.000 (0.270) loss 0.3206 (0.4104) acc 100.0000 (93.7500) lr 7.0224e-05 eta 0:00:04
epoch [46/50] batch [1/3] time 0.850 (0.850) data 0.788 (0.788) loss 0.4883 (0.4883) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [2/3] time 0.061 (0.456) data 0.001 (0.394) loss 0.4565 (0.4724) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [3/3] time 0.056 (0.322) data 0.000 (0.263) loss 0.4780 (0.4743) acc 87.5000 (91.6667) lr 4.8943e-05 eta 0:00:03
epoch [47/50] batch [1/3] time 0.873 (0.873) data 0.813 (0.813) loss 0.5454 (0.5454) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [2/3] time 0.058 (0.466) data 0.000 (0.407) loss 0.3423 (0.4438) acc 100.0000 (95.3125) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [3/3] time 0.044 (0.325) data 0.000 (0.271) loss 0.4773 (0.4550) acc 93.7500 (94.7917) lr 3.1417e-05 eta 0:00:02
epoch [48/50] batch [1/3] time 0.866 (0.866) data 0.806 (0.806) loss 0.3496 (0.3496) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [2/3] time 0.101 (0.484) data 0.043 (0.424) loss 0.5654 (0.4575) acc 87.5000 (92.1875) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [3/3] time 0.044 (0.337) data 0.000 (0.283) loss 0.3267 (0.4139) acc 100.0000 (94.7917) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [1/3] time 0.860 (0.860) data 0.797 (0.797) loss 0.4177 (0.4177) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [2/3] time 0.060 (0.460) data 0.000 (0.399) loss 0.4722 (0.4449) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.041 (0.320) data 0.000 (0.266) loss 0.6416 (0.5105) acc 75.0000 (87.5000) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [1/3] time 0.842 (0.842) data 0.783 (0.783) loss 0.5020 (0.5020) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [2/3] time 0.058 (0.450) data 0.000 (0.392) loss 0.3584 (0.4302) acc 96.8750 (92.1875) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.043 (0.314) data 0.000 (0.261) loss 0.5078 (0.4561) acc 87.5000 (90.6250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:03<00:28,  3.59s/it] 22%|██▏       | 2/9 [00:04<00:12,  1.81s/it] 33%|███▎      | 3/9 [00:04<00:07,  1.24s/it] 44%|████▍     | 4/9 [00:05<00:04,  1.03it/s] 56%|█████▌    | 5/9 [00:05<00:03,  1.22it/s] 67%|██████▋   | 6/9 [00:06<00:02,  1.37it/s] 78%|███████▊  | 7/9 [00:06<00:01,  1.48it/s] 89%|████████▉ | 8/9 [00:07<00:00,  1.57it/s]100%|██████████| 9/9 [00:07<00:00,  1.91it/s]100%|██████████| 9/9 [00:07<00:00,  1.14it/s]
=> result
* total: 4,200
* correct: 3,908
* accuracy: 93.0%
* error: 7.0%
* macro_f1: 93.1%
Elapsed: 0:01:23
Run this job and save the output to output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:24,  3.52s/it] 25%|██▌       | 2/8 [00:04<00:10,  1.78s/it] 38%|███▊      | 3/8 [00:04<00:06,  1.22s/it] 50%|█████     | 4/8 [00:05<00:03,  1.04it/s] 62%|██████▎   | 5/8 [00:05<00:02,  1.23it/s] 75%|███████▌  | 6/8 [00:06<00:01,  1.38it/s] 88%|████████▊ | 7/8 [00:06<00:00,  1.49it/s]100%|██████████| 8/8 [00:07<00:00,  1.61it/s]100%|██████████| 8/8 [00:07<00:00,  1.06it/s]
=> result
* total: 3,900
* correct: 3,106
* accuracy: 79.6%
* error: 20.4%
* macro_f1: 78.1%
Run this job and save the output to output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:24,  3.44s/it] 25%|██▌       | 2/8 [00:04<00:10,  1.80s/it] 38%|███▊      | 3/8 [00:04<00:06,  1.23s/it] 50%|█████     | 4/8 [00:05<00:03,  1.03it/s] 62%|██████▎   | 5/8 [00:05<00:02,  1.22it/s] 75%|███████▌  | 6/8 [00:06<00:01,  1.37it/s] 88%|████████▊ | 7/8 [00:06<00:00,  1.48it/s]100%|██████████| 8/8 [00:07<00:00,  1.60it/s]100%|██████████| 8/8 [00:07<00:00,  1.06it/s]
=> result
* total: 3,900
* correct: 2,631
* accuracy: 67.5%
* error: 32.5%
* macro_f1: 64.2%
Run this job and save the output to output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:04<00:29,  4.24s/it] 25%|██▌       | 2/8 [00:04<00:12,  2.07s/it] 38%|███▊      | 3/8 [00:05<00:06,  1.38s/it] 50%|█████     | 4/8 [00:05<00:04,  1.06s/it] 62%|██████▎   | 5/8 [00:06<00:02,  1.14it/s] 75%|███████▌  | 6/8 [00:07<00:01,  1.30it/s] 88%|████████▊ | 7/8 [00:07<00:00,  1.43it/s]100%|██████████| 8/8 [00:08<00:00,  1.56it/s]100%|██████████| 8/8 [00:08<00:00,  1.03s/it]
=> result
* total: 3,900
* correct: 3,007
* accuracy: 77.1%
* error: 22.9%
* macro_f1: 75.8%
Run this job and save the output to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/25] time 0.228 (0.575) data 0.158 (0.491) loss 5.7969 (5.5312) acc 6.2500 (11.8750) lr 1.0000e-05 eta 0:11:55
epoch [1/50] batch [10/25] time 0.069 (0.406) data 0.000 (0.329) loss 5.1875 (5.4754) acc 9.3750 (10.0000) lr 1.0000e-05 eta 0:08:22
epoch [1/50] batch [15/25] time 0.069 (0.301) data 0.000 (0.226) loss 5.1406 (5.3977) acc 9.3750 (9.7917) lr 1.0000e-05 eta 0:06:11
epoch [1/50] batch [20/25] time 0.069 (0.308) data 0.000 (0.234) loss 5.4609 (5.3869) acc 6.2500 (10.0000) lr 1.0000e-05 eta 0:06:18
epoch [1/50] batch [25/25] time 0.538 (0.283) data 0.468 (0.210) loss 5.2422 (5.3825) acc 9.3750 (10.2500) lr 2.0000e-03 eta 0:05:46
epoch [2/50] batch [5/25] time 0.071 (0.308) data 0.000 (0.233) loss 2.6367 (3.6027) acc 46.8750 (25.0000) lr 2.0000e-03 eta 0:06:15
epoch [2/50] batch [10/25] time 0.069 (0.191) data 0.000 (0.117) loss 3.0547 (3.2260) acc 18.7500 (23.4375) lr 2.0000e-03 eta 0:03:52
epoch [2/50] batch [15/25] time 0.069 (0.151) data 0.000 (0.078) loss 2.4336 (3.0245) acc 34.3750 (25.0000) lr 2.0000e-03 eta 0:03:02
epoch [2/50] batch [20/25] time 0.071 (0.131) data 0.000 (0.059) loss 2.8105 (2.8956) acc 31.2500 (27.3438) lr 2.0000e-03 eta 0:02:37
epoch [2/50] batch [25/25] time 0.070 (0.119) data 0.000 (0.047) loss 2.3438 (2.8192) acc 34.3750 (27.7500) lr 1.9980e-03 eta 0:02:22
epoch [3/50] batch [5/25] time 0.072 (0.291) data 0.000 (0.220) loss 2.7910 (2.4977) acc 18.7500 (21.8750) lr 1.9980e-03 eta 0:05:48
epoch [3/50] batch [10/25] time 0.070 (0.181) data 0.000 (0.110) loss 2.8008 (2.4841) acc 21.8750 (25.3125) lr 1.9980e-03 eta 0:03:35
epoch [3/50] batch [15/25] time 0.070 (0.144) data 0.000 (0.074) loss 2.6309 (2.4933) acc 31.2500 (25.0000) lr 1.9980e-03 eta 0:02:51
epoch [3/50] batch [20/25] time 0.069 (0.126) data 0.000 (0.055) loss 2.2910 (2.4589) acc 34.3750 (25.0000) lr 1.9980e-03 eta 0:02:28
epoch [3/50] batch [25/25] time 0.071 (0.114) data 0.000 (0.044) loss 2.1484 (2.4267) acc 43.7500 (27.1250) lr 1.9921e-03 eta 0:02:14
epoch [4/50] batch [5/25] time 0.070 (0.298) data 0.000 (0.228) loss 2.3926 (2.4813) acc 31.2500 (26.8750) lr 1.9921e-03 eta 0:05:48
epoch [4/50] batch [10/25] time 0.069 (0.185) data 0.000 (0.114) loss 2.2402 (2.4166) acc 31.2500 (28.1250) lr 1.9921e-03 eta 0:03:35
epoch [4/50] batch [15/25] time 0.069 (0.147) data 0.000 (0.076) loss 2.2793 (2.3655) acc 34.3750 (28.7500) lr 1.9921e-03 eta 0:02:50
epoch [4/50] batch [20/25] time 0.069 (0.127) data 0.000 (0.057) loss 2.9766 (2.4094) acc 15.6250 (28.1250) lr 1.9921e-03 eta 0:02:27
epoch [4/50] batch [25/25] time 0.069 (0.116) data 0.000 (0.046) loss 2.3359 (2.4063) acc 21.8750 (28.1250) lr 1.9823e-03 eta 0:02:13
epoch [5/50] batch [5/25] time 0.072 (0.339) data 0.000 (0.268) loss 2.2891 (2.4172) acc 34.3750 (23.1250) lr 1.9823e-03 eta 0:06:28
epoch [5/50] batch [10/25] time 0.070 (0.205) data 0.000 (0.134) loss 2.2031 (2.3969) acc 46.8750 (26.2500) lr 1.9823e-03 eta 0:03:53
epoch [5/50] batch [15/25] time 0.070 (0.160) data 0.000 (0.090) loss 2.4688 (2.3885) acc 34.3750 (29.5833) lr 1.9823e-03 eta 0:03:01
epoch [5/50] batch [20/25] time 0.070 (0.137) data 0.000 (0.067) loss 2.1406 (2.3675) acc 46.8750 (29.5312) lr 1.9823e-03 eta 0:02:35
epoch [5/50] batch [25/25] time 0.070 (0.124) data 0.000 (0.054) loss 2.3750 (2.3554) acc 25.0000 (29.0000) lr 1.9686e-03 eta 0:02:19
epoch [6/50] batch [5/25] time 0.069 (0.290) data 0.000 (0.221) loss 2.3262 (2.3676) acc 46.8750 (33.7500) lr 1.9686e-03 eta 0:05:24
epoch [6/50] batch [10/25] time 0.070 (0.180) data 0.000 (0.110) loss 2.2051 (2.2822) acc 28.1250 (35.0000) lr 1.9686e-03 eta 0:03:20
epoch [6/50] batch [15/25] time 0.070 (0.144) data 0.000 (0.074) loss 2.5605 (2.2714) acc 25.0000 (34.1667) lr 1.9686e-03 eta 0:02:40
epoch [6/50] batch [20/25] time 0.069 (0.126) data 0.000 (0.055) loss 2.2422 (2.2835) acc 25.0000 (32.5000) lr 1.9686e-03 eta 0:02:18
epoch [6/50] batch [25/25] time 0.071 (0.114) data 0.000 (0.044) loss 2.2949 (2.3000) acc 28.1250 (32.0000) lr 1.9511e-03 eta 0:02:05
epoch [7/50] batch [5/25] time 0.069 (0.292) data 0.000 (0.222) loss 2.2363 (2.2090) acc 37.5000 (36.8750) lr 1.9511e-03 eta 0:05:19
epoch [7/50] batch [10/25] time 0.070 (0.181) data 0.000 (0.111) loss 2.1211 (2.2168) acc 34.3750 (36.5625) lr 1.9511e-03 eta 0:03:17
epoch [7/50] batch [15/25] time 0.069 (0.144) data 0.000 (0.074) loss 2.2031 (2.2618) acc 25.0000 (34.1667) lr 1.9511e-03 eta 0:02:36
epoch [7/50] batch [20/25] time 0.069 (0.126) data 0.000 (0.056) loss 2.9043 (2.3052) acc 25.0000 (32.6562) lr 1.9511e-03 eta 0:02:15
epoch [7/50] batch [25/25] time 0.069 (0.114) data 0.000 (0.045) loss 2.4648 (2.3134) acc 28.1250 (31.1250) lr 1.9298e-03 eta 0:02:02
epoch [8/50] batch [5/25] time 0.070 (0.278) data 0.000 (0.207) loss 2.2402 (2.1426) acc 31.2500 (38.1250) lr 1.9298e-03 eta 0:04:57
epoch [8/50] batch [10/25] time 0.072 (0.175) data 0.000 (0.104) loss 2.2480 (2.1971) acc 28.1250 (37.1875) lr 1.9298e-03 eta 0:03:06
epoch [8/50] batch [15/25] time 0.069 (0.141) data 0.000 (0.069) loss 2.0215 (2.1598) acc 34.3750 (36.8750) lr 1.9298e-03 eta 0:02:28
epoch [8/50] batch [20/25] time 0.069 (0.123) data 0.000 (0.052) loss 2.2520 (2.2161) acc 34.3750 (34.3750) lr 1.9298e-03 eta 0:02:09
epoch [8/50] batch [25/25] time 0.069 (0.112) data 0.000 (0.042) loss 2.3652 (2.1990) acc 28.1250 (34.1250) lr 1.9048e-03 eta 0:01:57
epoch [9/50] batch [5/25] time 0.072 (0.287) data 0.000 (0.216) loss 1.9375 (2.1098) acc 37.5000 (38.7500) lr 1.9048e-03 eta 0:04:59
epoch [9/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.108) loss 2.3770 (2.1868) acc 18.7500 (33.4375) lr 1.9048e-03 eta 0:03:06
epoch [9/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.3027 (2.1759) acc 43.7500 (33.9583) lr 1.9048e-03 eta 0:02:28
epoch [9/50] batch [20/25] time 0.073 (0.125) data 0.000 (0.054) loss 2.6777 (2.1977) acc 21.8750 (32.5000) lr 1.9048e-03 eta 0:02:08
epoch [9/50] batch [25/25] time 0.070 (0.114) data 0.001 (0.043) loss 2.2441 (2.2202) acc 40.6250 (32.7500) lr 1.8763e-03 eta 0:01:56
epoch [10/50] batch [5/25] time 0.072 (0.287) data 0.000 (0.216) loss 2.3242 (2.1863) acc 31.2500 (40.0000) lr 1.8763e-03 eta 0:04:52
epoch [10/50] batch [10/25] time 0.071 (0.180) data 0.000 (0.108) loss 2.7422 (2.2270) acc 25.0000 (34.0625) lr 1.8763e-03 eta 0:03:02
epoch [10/50] batch [15/25] time 0.069 (0.143) data 0.000 (0.072) loss 2.0566 (2.2138) acc 34.3750 (33.7500) lr 1.8763e-03 eta 0:02:24
epoch [10/50] batch [20/25] time 0.069 (0.125) data 0.000 (0.054) loss 2.0117 (2.1824) acc 34.3750 (33.5938) lr 1.8763e-03 eta 0:02:05
epoch [10/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 2.5840 (2.1763) acc 25.0000 (34.2500) lr 1.8443e-03 eta 0:01:53
epoch [11/50] batch [5/25] time 0.071 (0.303) data 0.000 (0.233) loss 2.5176 (2.3238) acc 21.8750 (25.6250) lr 1.8443e-03 eta 0:05:01
epoch [11/50] batch [10/25] time 0.070 (0.187) data 0.000 (0.117) loss 2.0039 (2.1765) acc 37.5000 (32.8125) lr 1.8443e-03 eta 0:03:05
epoch [11/50] batch [15/25] time 0.070 (0.149) data 0.000 (0.078) loss 2.2129 (2.1873) acc 34.3750 (34.1667) lr 1.8443e-03 eta 0:02:26
epoch [11/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.059) loss 2.2324 (2.1822) acc 34.3750 (33.7500) lr 1.8443e-03 eta 0:02:06
epoch [11/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.047) loss 2.0430 (2.1846) acc 37.5000 (33.7500) lr 1.8090e-03 eta 0:01:54
epoch [12/50] batch [5/25] time 0.070 (0.296) data 0.000 (0.226) loss 2.1094 (2.2730) acc 28.1250 (30.6250) lr 1.8090e-03 eta 0:04:47
epoch [12/50] batch [10/25] time 0.070 (0.183) data 0.000 (0.113) loss 2.1816 (2.2936) acc 46.8750 (34.3750) lr 1.8090e-03 eta 0:02:56
epoch [12/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.075) loss 1.7412 (2.2316) acc 53.1250 (36.8750) lr 1.8090e-03 eta 0:02:19
epoch [12/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.057) loss 2.2812 (2.2160) acc 37.5000 (36.8750) lr 1.8090e-03 eta 0:02:01
epoch [12/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 1.9297 (2.1806) acc 43.7500 (37.7500) lr 1.7705e-03 eta 0:01:49
epoch [13/50] batch [5/25] time 0.070 (0.287) data 0.000 (0.217) loss 2.2773 (2.1057) acc 31.2500 (40.0000) lr 1.7705e-03 eta 0:04:31
epoch [13/50] batch [10/25] time 0.071 (0.179) data 0.000 (0.109) loss 2.0859 (2.1148) acc 28.1250 (36.2500) lr 1.7705e-03 eta 0:02:48
epoch [13/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.073) loss 2.3105 (2.0986) acc 25.0000 (36.2500) lr 1.7705e-03 eta 0:02:13
epoch [13/50] batch [20/25] time 0.070 (0.125) data 0.000 (0.054) loss 2.0156 (2.1143) acc 53.1250 (35.9375) lr 1.7705e-03 eta 0:01:55
epoch [13/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.044) loss 2.1680 (2.1230) acc 28.1250 (36.3750) lr 1.7290e-03 eta 0:01:45
epoch [14/50] batch [5/25] time 0.070 (0.321) data 0.000 (0.251) loss 1.8496 (2.0697) acc 53.1250 (40.6250) lr 1.7290e-03 eta 0:04:55
epoch [14/50] batch [10/25] time 0.070 (0.196) data 0.000 (0.126) loss 2.4453 (2.1578) acc 25.0000 (36.2500) lr 1.7290e-03 eta 0:02:59
epoch [14/50] batch [15/25] time 0.072 (0.155) data 0.000 (0.084) loss 2.0703 (2.1603) acc 40.6250 (37.2917) lr 1.7290e-03 eta 0:02:20
epoch [14/50] batch [20/25] time 0.070 (0.133) data 0.000 (0.063) loss 2.5039 (2.1542) acc 25.0000 (37.8125) lr 1.7290e-03 eta 0:02:00
epoch [14/50] batch [25/25] time 0.070 (0.121) data 0.000 (0.050) loss 2.0801 (2.1286) acc 40.6250 (39.1250) lr 1.6845e-03 eta 0:01:48
epoch [15/50] batch [5/25] time 0.070 (0.304) data 0.000 (0.234) loss 2.0840 (2.1121) acc 28.1250 (32.5000) lr 1.6845e-03 eta 0:04:32
epoch [15/50] batch [10/25] time 0.070 (0.188) data 0.000 (0.117) loss 1.7568 (2.1019) acc 43.7500 (35.0000) lr 1.6845e-03 eta 0:02:47
epoch [15/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.078) loss 2.0078 (2.1471) acc 37.5000 (34.5833) lr 1.6845e-03 eta 0:02:11
epoch [15/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.059) loss 2.4102 (2.1645) acc 21.8750 (34.6875) lr 1.6845e-03 eta 0:01:53
epoch [15/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.047) loss 2.0156 (2.1705) acc 37.5000 (34.3750) lr 1.6374e-03 eta 0:01:42
epoch [16/50] batch [5/25] time 0.070 (0.336) data 0.000 (0.264) loss 2.1270 (1.9941) acc 40.6250 (37.5000) lr 1.6374e-03 eta 0:04:52
epoch [16/50] batch [10/25] time 0.071 (0.204) data 0.000 (0.132) loss 2.0684 (2.0219) acc 31.2500 (38.1250) lr 1.6374e-03 eta 0:02:56
epoch [16/50] batch [15/25] time 0.070 (0.160) data 0.000 (0.088) loss 2.1426 (2.1161) acc 40.6250 (37.5000) lr 1.6374e-03 eta 0:02:18
epoch [16/50] batch [20/25] time 0.071 (0.138) data 0.000 (0.066) loss 1.9785 (2.1339) acc 43.7500 (38.1250) lr 1.6374e-03 eta 0:01:58
epoch [16/50] batch [25/25] time 0.071 (0.125) data 0.000 (0.053) loss 2.1211 (2.1055) acc 34.3750 (39.1250) lr 1.5878e-03 eta 0:01:46
epoch [17/50] batch [5/25] time 0.070 (0.337) data 0.000 (0.266) loss 1.9453 (2.0373) acc 43.7500 (39.3750) lr 1.5878e-03 eta 0:04:44
epoch [17/50] batch [10/25] time 0.071 (0.205) data 0.000 (0.134) loss 1.8232 (2.1137) acc 56.2500 (37.5000) lr 1.5878e-03 eta 0:02:51
epoch [17/50] batch [15/25] time 0.070 (0.160) data 0.000 (0.089) loss 2.3633 (2.1393) acc 31.2500 (36.4583) lr 1.5878e-03 eta 0:02:14
epoch [17/50] batch [20/25] time 0.071 (0.138) data 0.000 (0.067) loss 1.8008 (2.1148) acc 37.5000 (37.0312) lr 1.5878e-03 eta 0:01:54
epoch [17/50] batch [25/25] time 0.070 (0.124) data 0.000 (0.054) loss 2.0781 (2.0977) acc 31.2500 (37.0000) lr 1.5358e-03 eta 0:01:42
epoch [18/50] batch [5/25] time 0.070 (0.290) data 0.001 (0.218) loss 2.3418 (2.0830) acc 31.2500 (40.6250) lr 1.5358e-03 eta 0:03:57
epoch [18/50] batch [10/25] time 0.069 (0.180) data 0.000 (0.109) loss 2.1406 (2.1101) acc 53.1250 (37.1875) lr 1.5358e-03 eta 0:02:26
epoch [18/50] batch [15/25] time 0.069 (0.143) data 0.000 (0.073) loss 1.7793 (2.0499) acc 56.2500 (40.2083) lr 1.5358e-03 eta 0:01:56
epoch [18/50] batch [20/25] time 0.069 (0.125) data 0.000 (0.055) loss 2.1250 (2.0704) acc 28.1250 (39.5312) lr 1.5358e-03 eta 0:01:40
epoch [18/50] batch [25/25] time 0.069 (0.114) data 0.000 (0.044) loss 2.1270 (2.0637) acc 43.7500 (40.7500) lr 1.4818e-03 eta 0:01:30
epoch [19/50] batch [5/25] time 0.074 (0.311) data 0.000 (0.241) loss 2.0625 (2.0469) acc 37.5000 (43.7500) lr 1.4818e-03 eta 0:04:07
epoch [19/50] batch [10/25] time 0.070 (0.191) data 0.000 (0.121) loss 2.1680 (2.1301) acc 46.8750 (42.5000) lr 1.4818e-03 eta 0:02:30
epoch [19/50] batch [15/25] time 0.069 (0.151) data 0.000 (0.080) loss 2.1445 (2.0752) acc 34.3750 (42.0833) lr 1.4818e-03 eta 0:01:58
epoch [19/50] batch [20/25] time 0.069 (0.131) data 0.000 (0.060) loss 2.3477 (2.0729) acc 37.5000 (41.8750) lr 1.4818e-03 eta 0:01:41
epoch [19/50] batch [25/25] time 0.070 (0.118) data 0.000 (0.048) loss 2.0566 (2.0859) acc 50.0000 (41.1250) lr 1.4258e-03 eta 0:01:31
epoch [20/50] batch [5/25] time 0.070 (0.316) data 0.000 (0.245) loss 2.0898 (1.8707) acc 34.3750 (42.5000) lr 1.4258e-03 eta 0:04:03
epoch [20/50] batch [10/25] time 0.072 (0.195) data 0.000 (0.123) loss 1.8418 (1.8859) acc 46.8750 (42.8125) lr 1.4258e-03 eta 0:02:28
epoch [20/50] batch [15/25] time 0.072 (0.154) data 0.000 (0.082) loss 2.1387 (1.9888) acc 34.3750 (41.8750) lr 1.4258e-03 eta 0:01:56
epoch [20/50] batch [20/25] time 0.070 (0.133) data 0.000 (0.061) loss 2.7832 (2.0534) acc 25.0000 (40.9375) lr 1.4258e-03 eta 0:01:40
epoch [20/50] batch [25/25] time 0.069 (0.120) data 0.000 (0.049) loss 2.1895 (2.0551) acc 37.5000 (41.2500) lr 1.3681e-03 eta 0:01:30
epoch [21/50] batch [5/25] time 0.069 (0.310) data 0.000 (0.238) loss 1.7539 (2.0211) acc 50.0000 (41.8750) lr 1.3681e-03 eta 0:03:50
epoch [21/50] batch [10/25] time 0.070 (0.191) data 0.000 (0.119) loss 2.4941 (2.0090) acc 34.3750 (42.8125) lr 1.3681e-03 eta 0:02:21
epoch [21/50] batch [15/25] time 0.073 (0.152) data 0.000 (0.079) loss 2.0059 (1.9726) acc 43.7500 (44.5833) lr 1.3681e-03 eta 0:01:51
epoch [21/50] batch [20/25] time 0.072 (0.132) data 0.000 (0.060) loss 2.0664 (2.0014) acc 46.8750 (43.5938) lr 1.3681e-03 eta 0:01:36
epoch [21/50] batch [25/25] time 0.069 (0.120) data 0.000 (0.048) loss 1.9189 (2.0295) acc 37.5000 (42.0000) lr 1.3090e-03 eta 0:01:26
epoch [22/50] batch [5/25] time 0.070 (0.279) data 0.000 (0.208) loss 1.7861 (1.9062) acc 53.1250 (53.7500) lr 1.3090e-03 eta 0:03:20
epoch [22/50] batch [10/25] time 0.074 (0.176) data 0.000 (0.104) loss 2.1328 (1.9466) acc 37.5000 (45.9375) lr 1.3090e-03 eta 0:02:06
epoch [22/50] batch [15/25] time 0.071 (0.142) data 0.000 (0.070) loss 1.6133 (1.9633) acc 56.2500 (46.0417) lr 1.3090e-03 eta 0:01:40
epoch [22/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.052) loss 2.2832 (2.0011) acc 31.2500 (43.7500) lr 1.3090e-03 eta 0:01:27
epoch [22/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 2.1641 (2.0015) acc 37.5000 (43.0000) lr 1.2487e-03 eta 0:01:19
epoch [23/50] batch [5/25] time 0.073 (0.300) data 0.000 (0.229) loss 1.8994 (1.9281) acc 50.0000 (45.6250) lr 1.2487e-03 eta 0:03:28
epoch [23/50] batch [10/25] time 0.070 (0.186) data 0.000 (0.115) loss 2.3145 (2.0477) acc 34.3750 (42.8125) lr 1.2487e-03 eta 0:02:08
epoch [23/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.077) loss 2.2090 (2.0799) acc 37.5000 (41.0417) lr 1.2487e-03 eta 0:01:41
epoch [23/50] batch [20/25] time 0.070 (0.128) data 0.000 (0.058) loss 2.4102 (2.0743) acc 25.0000 (40.9375) lr 1.2487e-03 eta 0:01:27
epoch [23/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.046) loss 1.9824 (2.0594) acc 40.6250 (41.3750) lr 1.1874e-03 eta 0:01:18
epoch [24/50] batch [5/25] time 0.072 (0.281) data 0.000 (0.209) loss 2.4219 (2.2283) acc 37.5000 (38.7500) lr 1.1874e-03 eta 0:03:08
epoch [24/50] batch [10/25] time 0.070 (0.176) data 0.000 (0.105) loss 2.2949 (2.1829) acc 31.2500 (39.6875) lr 1.1874e-03 eta 0:01:57
epoch [24/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.070) loss 2.2500 (2.1502) acc 28.1250 (40.2083) lr 1.1874e-03 eta 0:01:32
epoch [24/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.053) loss 1.7754 (2.0878) acc 56.2500 (42.8125) lr 1.1874e-03 eta 0:01:20
epoch [24/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 2.1172 (2.0536) acc 46.8750 (44.3750) lr 1.1253e-03 eta 0:01:13
epoch [25/50] batch [5/25] time 0.070 (0.281) data 0.000 (0.210) loss 1.6807 (1.7551) acc 59.3750 (55.6250) lr 1.1253e-03 eta 0:03:01
epoch [25/50] batch [10/25] time 0.070 (0.176) data 0.000 (0.105) loss 2.0234 (1.8838) acc 46.8750 (51.2500) lr 1.1253e-03 eta 0:01:52
epoch [25/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.070) loss 2.2441 (1.9664) acc 34.3750 (46.4583) lr 1.1253e-03 eta 0:01:29
epoch [25/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.053) loss 1.8936 (1.9888) acc 50.0000 (45.7812) lr 1.1253e-03 eta 0:01:17
epoch [25/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.042) loss 2.0586 (2.0041) acc 50.0000 (46.2500) lr 1.0628e-03 eta 0:01:10
epoch [26/50] batch [5/25] time 0.070 (0.293) data 0.000 (0.223) loss 2.1504 (2.1621) acc 34.3750 (33.1250) lr 1.0628e-03 eta 0:03:01
epoch [26/50] batch [10/25] time 0.070 (0.182) data 0.000 (0.112) loss 2.4473 (2.1035) acc 31.2500 (39.0625) lr 1.0628e-03 eta 0:01:51
epoch [26/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.075) loss 1.7949 (2.0743) acc 53.1250 (41.2500) lr 1.0628e-03 eta 0:01:28
epoch [26/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.056) loss 2.1367 (2.0652) acc 37.5000 (40.6250) lr 1.0628e-03 eta 0:01:16
epoch [26/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 1.8564 (2.0360) acc 53.1250 (41.8750) lr 1.0000e-03 eta 0:01:08
epoch [27/50] batch [5/25] time 0.070 (0.284) data 0.000 (0.214) loss 1.7920 (1.7928) acc 56.2500 (51.8750) lr 1.0000e-03 eta 0:02:48
epoch [27/50] batch [10/25] time 0.070 (0.177) data 0.000 (0.107) loss 2.0117 (1.9401) acc 43.7500 (47.8125) lr 1.0000e-03 eta 0:01:44
epoch [27/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.071) loss 1.9355 (1.9356) acc 34.3750 (45.2083) lr 1.0000e-03 eta 0:01:22
epoch [27/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.054) loss 1.9883 (1.9806) acc 40.6250 (44.5312) lr 1.0000e-03 eta 0:01:11
epoch [27/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.043) loss 2.4180 (1.9810) acc 25.0000 (44.1250) lr 9.3721e-04 eta 0:01:04
epoch [28/50] batch [5/25] time 0.071 (0.274) data 0.000 (0.203) loss 2.0742 (1.8838) acc 34.3750 (43.1250) lr 9.3721e-04 eta 0:02:36
epoch [28/50] batch [10/25] time 0.071 (0.173) data 0.000 (0.102) loss 1.9502 (1.9720) acc 56.2500 (43.4375) lr 9.3721e-04 eta 0:01:37
epoch [28/50] batch [15/25] time 0.072 (0.139) data 0.000 (0.068) loss 2.0762 (1.9145) acc 46.8750 (46.2500) lr 9.3721e-04 eta 0:01:18
epoch [28/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 2.2168 (1.9305) acc 37.5000 (45.9375) lr 9.3721e-04 eta 0:01:07
epoch [28/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.9990 (1.9267) acc 46.8750 (46.0000) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [5/25] time 0.070 (0.276) data 0.000 (0.202) loss 2.0586 (1.9514) acc 53.1250 (48.1250) lr 8.7467e-04 eta 0:02:30
epoch [29/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.101) loss 1.9277 (1.9320) acc 37.5000 (46.2500) lr 8.7467e-04 eta 0:01:33
epoch [29/50] batch [15/25] time 0.070 (0.140) data 0.000 (0.067) loss 1.5879 (1.9354) acc 53.1250 (44.5833) lr 8.7467e-04 eta 0:01:14
epoch [29/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 2.0273 (1.9510) acc 40.6250 (44.5312) lr 8.7467e-04 eta 0:01:04
epoch [29/50] batch [25/25] time 0.071 (0.112) data 0.000 (0.041) loss 1.5898 (1.9134) acc 59.3750 (45.3750) lr 8.1262e-04 eta 0:00:58
epoch [30/50] batch [5/25] time 0.071 (0.273) data 0.001 (0.202) loss 2.0977 (1.8832) acc 43.7500 (48.1250) lr 8.1262e-04 eta 0:02:22
epoch [30/50] batch [10/25] time 0.070 (0.172) data 0.000 (0.101) loss 1.7188 (1.9285) acc 65.6250 (48.4375) lr 8.1262e-04 eta 0:01:28
epoch [30/50] batch [15/25] time 0.069 (0.138) data 0.000 (0.068) loss 2.0078 (1.9197) acc 53.1250 (49.7917) lr 8.1262e-04 eta 0:01:10
epoch [30/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.051) loss 1.9541 (1.9148) acc 43.7500 (48.7500) lr 8.1262e-04 eta 0:01:01
epoch [30/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 2.2930 (1.9237) acc 37.5000 (48.2500) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [5/25] time 0.070 (0.312) data 0.000 (0.241) loss 1.8369 (1.8037) acc 53.1250 (54.3750) lr 7.5131e-04 eta 0:02:34
epoch [31/50] batch [10/25] time 0.071 (0.192) data 0.000 (0.121) loss 2.1895 (1.9223) acc 37.5000 (50.3125) lr 7.5131e-04 eta 0:01:34
epoch [31/50] batch [15/25] time 0.070 (0.151) data 0.000 (0.081) loss 2.2969 (1.9545) acc 31.2500 (46.8750) lr 7.5131e-04 eta 0:01:13
epoch [31/50] batch [20/25] time 0.070 (0.131) data 0.000 (0.061) loss 2.0898 (1.9688) acc 31.2500 (45.6250) lr 7.5131e-04 eta 0:01:02
epoch [31/50] batch [25/25] time 0.070 (0.119) data 0.000 (0.049) loss 1.6934 (1.9296) acc 43.7500 (46.6250) lr 6.9098e-04 eta 0:00:56
epoch [32/50] batch [5/25] time 0.070 (0.275) data 0.000 (0.202) loss 1.8291 (1.7764) acc 53.1250 (54.3750) lr 6.9098e-04 eta 0:02:09
epoch [32/50] batch [10/25] time 0.071 (0.174) data 0.000 (0.102) loss 2.0293 (1.9168) acc 40.6250 (48.1250) lr 6.9098e-04 eta 0:01:20
epoch [32/50] batch [15/25] time 0.072 (0.140) data 0.000 (0.068) loss 1.9443 (1.9152) acc 43.7500 (47.7083) lr 6.9098e-04 eta 0:01:04
epoch [32/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 2.0918 (1.9134) acc 43.7500 (48.1250) lr 6.9098e-04 eta 0:00:55
epoch [32/50] batch [25/25] time 0.071 (0.112) data 0.000 (0.041) loss 1.8945 (1.9242) acc 50.0000 (47.2500) lr 6.3188e-04 eta 0:00:50
epoch [33/50] batch [5/25] time 0.070 (0.299) data 0.000 (0.229) loss 1.7295 (1.9604) acc 59.3750 (52.5000) lr 6.3188e-04 eta 0:02:12
epoch [33/50] batch [10/25] time 0.072 (0.186) data 0.000 (0.114) loss 1.8105 (1.9331) acc 56.2500 (50.3125) lr 6.3188e-04 eta 0:01:21
epoch [33/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.077) loss 1.9043 (1.9141) acc 50.0000 (49.1667) lr 6.3188e-04 eta 0:01:04
epoch [33/50] batch [20/25] time 0.073 (0.129) data 0.000 (0.058) loss 2.1504 (1.9244) acc 40.6250 (47.9688) lr 6.3188e-04 eta 0:00:55
epoch [33/50] batch [25/25] time 0.071 (0.117) data 0.000 (0.046) loss 1.4980 (1.8859) acc 65.6250 (48.6250) lr 5.7422e-04 eta 0:00:49
epoch [34/50] batch [5/25] time 0.070 (0.277) data 0.000 (0.207) loss 1.8438 (1.8924) acc 46.8750 (46.2500) lr 5.7422e-04 eta 0:01:56
epoch [34/50] batch [10/25] time 0.071 (0.175) data 0.000 (0.104) loss 1.9072 (1.8540) acc 46.8750 (45.9375) lr 5.7422e-04 eta 0:01:12
epoch [34/50] batch [15/25] time 0.070 (0.140) data 0.000 (0.069) loss 1.6514 (1.8327) acc 59.3750 (47.9167) lr 5.7422e-04 eta 0:00:57
epoch [34/50] batch [20/25] time 0.071 (0.123) data 0.000 (0.052) loss 2.2422 (1.8909) acc 37.5000 (46.8750) lr 5.7422e-04 eta 0:00:49
epoch [34/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.042) loss 2.4180 (1.9189) acc 28.1250 (46.8750) lr 5.1825e-04 eta 0:00:44
epoch [35/50] batch [5/25] time 0.071 (0.269) data 0.000 (0.197) loss 1.8955 (1.9049) acc 40.6250 (43.7500) lr 5.1825e-04 eta 0:01:46
epoch [35/50] batch [10/25] time 0.071 (0.170) data 0.000 (0.099) loss 1.9561 (1.8834) acc 31.2500 (45.6250) lr 5.1825e-04 eta 0:01:06
epoch [35/50] batch [15/25] time 0.070 (0.137) data 0.000 (0.066) loss 2.0820 (1.9189) acc 46.8750 (46.8750) lr 5.1825e-04 eta 0:00:52
epoch [35/50] batch [20/25] time 0.071 (0.121) data 0.000 (0.049) loss 2.0898 (1.9252) acc 59.3750 (47.8125) lr 5.1825e-04 eta 0:00:45
epoch [35/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.9941 (1.9213) acc 50.0000 (47.6250) lr 4.6417e-04 eta 0:00:41
epoch [36/50] batch [5/25] time 0.071 (0.284) data 0.000 (0.211) loss 1.4717 (1.8469) acc 68.7500 (48.1250) lr 4.6417e-04 eta 0:01:45
epoch [36/50] batch [10/25] time 0.072 (0.179) data 0.000 (0.106) loss 2.0664 (1.8549) acc 43.7500 (48.4375) lr 4.6417e-04 eta 0:01:05
epoch [36/50] batch [15/25] time 0.071 (0.144) data 0.000 (0.071) loss 2.0156 (1.8539) acc 50.0000 (47.0833) lr 4.6417e-04 eta 0:00:51
epoch [36/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.053) loss 1.9756 (1.9000) acc 53.1250 (47.3438) lr 4.6417e-04 eta 0:00:44
epoch [36/50] batch [25/25] time 0.071 (0.114) data 0.000 (0.042) loss 1.9453 (1.8836) acc 59.3750 (48.6250) lr 4.1221e-04 eta 0:00:40
epoch [37/50] batch [5/25] time 0.071 (0.288) data 0.000 (0.215) loss 1.9248 (1.8820) acc 43.7500 (47.5000) lr 4.1221e-04 eta 0:01:39
epoch [37/50] batch [10/25] time 0.070 (0.180) data 0.000 (0.108) loss 1.7451 (1.8960) acc 53.1250 (45.3125) lr 4.1221e-04 eta 0:01:01
epoch [37/50] batch [15/25] time 0.073 (0.144) data 0.000 (0.072) loss 2.1758 (1.9189) acc 37.5000 (46.2500) lr 4.1221e-04 eta 0:00:48
epoch [37/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.054) loss 1.9316 (1.8940) acc 40.6250 (46.8750) lr 4.1221e-04 eta 0:00:41
epoch [37/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.043) loss 1.7803 (1.8980) acc 59.3750 (47.7500) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [5/25] time 0.074 (0.276) data 0.000 (0.203) loss 2.1816 (1.9342) acc 31.2500 (45.0000) lr 3.6258e-04 eta 0:01:28
epoch [38/50] batch [10/25] time 0.070 (0.173) data 0.000 (0.102) loss 1.9424 (1.9357) acc 50.0000 (46.8750) lr 3.6258e-04 eta 0:00:54
epoch [38/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 1.8271 (1.9410) acc 40.6250 (45.8333) lr 3.6258e-04 eta 0:00:43
epoch [38/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 2.0117 (1.9658) acc 43.7500 (45.7812) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 2.0859 (1.9288) acc 40.6250 (48.1250) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [5/25] time 0.070 (0.262) data 0.000 (0.191) loss 1.7148 (1.9264) acc 56.2500 (46.2500) lr 3.1545e-04 eta 0:01:17
epoch [39/50] batch [10/25] time 0.070 (0.167) data 0.000 (0.096) loss 1.7402 (1.8607) acc 43.7500 (49.3750) lr 3.1545e-04 eta 0:00:48
epoch [39/50] batch [15/25] time 0.070 (0.135) data 0.000 (0.064) loss 2.1680 (1.8904) acc 40.6250 (48.9583) lr 3.1545e-04 eta 0:00:38
epoch [39/50] batch [20/25] time 0.070 (0.119) data 0.000 (0.048) loss 1.9043 (1.8676) acc 40.6250 (49.0625) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [25/25] time 0.070 (0.109) data 0.000 (0.038) loss 1.7607 (1.8417) acc 59.3750 (50.3750) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [5/25] time 0.071 (0.264) data 0.001 (0.194) loss 2.1719 (1.8707) acc 50.0000 (50.6250) lr 2.7103e-04 eta 0:01:11
epoch [40/50] batch [10/25] time 0.070 (0.168) data 0.000 (0.097) loss 2.2227 (1.9193) acc 43.7500 (48.1250) lr 2.7103e-04 eta 0:00:44
epoch [40/50] batch [15/25] time 0.070 (0.136) data 0.000 (0.065) loss 1.7979 (1.8758) acc 46.8750 (47.9167) lr 2.7103e-04 eta 0:00:35
epoch [40/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.049) loss 1.7305 (1.8542) acc 56.2500 (48.9062) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.039) loss 2.0039 (1.8582) acc 53.1250 (49.1250) lr 2.2949e-04 eta 0:00:27
epoch [41/50] batch [5/25] time 0.071 (0.283) data 0.000 (0.212) loss 1.8340 (1.8695) acc 37.5000 (50.6250) lr 2.2949e-04 eta 0:01:09
epoch [41/50] batch [10/25] time 0.070 (0.177) data 0.000 (0.106) loss 1.6445 (1.8224) acc 59.3750 (54.3750) lr 2.2949e-04 eta 0:00:42
epoch [41/50] batch [15/25] time 0.069 (0.142) data 0.000 (0.071) loss 2.1602 (1.8387) acc 50.0000 (54.3750) lr 2.2949e-04 eta 0:00:33
epoch [41/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.053) loss 1.4707 (1.8188) acc 62.5000 (53.9062) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [25/25] time 0.071 (0.113) data 0.000 (0.043) loss 2.0293 (1.8072) acc 50.0000 (54.7500) lr 1.9098e-04 eta 0:00:25
epoch [42/50] batch [5/25] time 0.072 (0.273) data 0.000 (0.199) loss 1.6582 (1.8469) acc 50.0000 (51.8750) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [10/25] time 0.071 (0.172) data 0.000 (0.100) loss 1.8623 (1.8143) acc 40.6250 (49.6875) lr 1.9098e-04 eta 0:00:36
epoch [42/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.066) loss 2.2070 (1.8746) acc 40.6250 (49.3750) lr 1.9098e-04 eta 0:00:29
epoch [42/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.050) loss 2.1855 (1.8577) acc 34.3750 (50.9375) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [25/25] time 0.071 (0.111) data 0.000 (0.040) loss 1.6309 (1.8570) acc 53.1250 (50.2500) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [5/25] time 0.071 (0.276) data 0.000 (0.205) loss 1.7080 (1.7756) acc 56.2500 (51.8750) lr 1.5567e-04 eta 0:00:53
epoch [43/50] batch [10/25] time 0.070 (0.173) data 0.000 (0.103) loss 1.8955 (1.8128) acc 50.0000 (50.3125) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [15/25] time 0.071 (0.139) data 0.000 (0.069) loss 1.6709 (1.7994) acc 53.1250 (51.8750) lr 1.5567e-04 eta 0:00:25
epoch [43/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.052) loss 1.8350 (1.7915) acc 53.1250 (53.2812) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [25/25] time 0.071 (0.112) data 0.000 (0.041) loss 2.0508 (1.7917) acc 43.7500 (53.3750) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [5/25] time 0.071 (0.274) data 0.000 (0.203) loss 1.6836 (1.7596) acc 46.8750 (54.3750) lr 1.2369e-04 eta 0:00:46
epoch [44/50] batch [10/25] time 0.071 (0.172) data 0.000 (0.102) loss 1.6064 (1.7952) acc 56.2500 (49.6875) lr 1.2369e-04 eta 0:00:28
epoch [44/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 2.2207 (1.8519) acc 40.6250 (50.0000) lr 1.2369e-04 eta 0:00:22
epoch [44/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 1.5557 (1.8272) acc 50.0000 (49.5312) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 2.0078 (1.8321) acc 46.8750 (49.8750) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [5/25] time 0.070 (0.270) data 0.000 (0.200) loss 2.0117 (1.9088) acc 50.0000 (50.0000) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [10/25] time 0.070 (0.171) data 0.000 (0.100) loss 1.8779 (1.8667) acc 43.7500 (52.1875) lr 9.5173e-05 eta 0:00:23
epoch [45/50] batch [15/25] time 0.070 (0.137) data 0.000 (0.067) loss 1.8232 (1.8229) acc 46.8750 (53.1250) lr 9.5173e-05 eta 0:00:18
epoch [45/50] batch [20/25] time 0.071 (0.121) data 0.000 (0.050) loss 1.7256 (1.8255) acc 62.5000 (52.5000) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.7578 (1.8520) acc 50.0000 (51.7500) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [5/25] time 0.069 (0.270) data 0.000 (0.201) loss 1.7500 (1.9023) acc 46.8750 (45.0000) lr 7.0224e-05 eta 0:00:32
epoch [46/50] batch [10/25] time 0.071 (0.171) data 0.000 (0.101) loss 1.8555 (1.8464) acc 53.1250 (50.6250) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [15/25] time 0.070 (0.137) data 0.000 (0.067) loss 1.9980 (1.8273) acc 46.8750 (51.8750) lr 7.0224e-05 eta 0:00:15
epoch [46/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.050) loss 1.8213 (1.8596) acc 56.2500 (50.9375) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.040) loss 1.8252 (1.8278) acc 53.1250 (52.1250) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [5/25] time 0.071 (0.262) data 0.000 (0.191) loss 2.1895 (1.9748) acc 40.6250 (46.2500) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [10/25] time 0.071 (0.166) data 0.000 (0.096) loss 1.7236 (1.8606) acc 56.2500 (49.0625) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [15/25] time 0.072 (0.135) data 0.000 (0.064) loss 1.8965 (1.8639) acc 46.8750 (51.2500) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [20/25] time 0.071 (0.119) data 0.000 (0.048) loss 1.7480 (1.8089) acc 62.5000 (54.2188) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/25] time 0.070 (0.109) data 0.000 (0.038) loss 1.8838 (1.8246) acc 56.2500 (53.1250) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [5/25] time 0.071 (0.288) data 0.000 (0.218) loss 1.6621 (1.6266) acc 50.0000 (56.2500) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.109) loss 1.9609 (1.7228) acc 46.8750 (54.3750) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [15/25] time 0.073 (0.143) data 0.000 (0.073) loss 1.9092 (1.7751) acc 43.7500 (53.1250) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [20/25] time 0.071 (0.125) data 0.000 (0.055) loss 1.7012 (1.7929) acc 59.3750 (54.0625) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/25] time 0.071 (0.114) data 0.000 (0.044) loss 1.7148 (1.7773) acc 37.5000 (54.2500) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [5/25] time 0.070 (0.272) data 0.000 (0.201) loss 1.4521 (1.7779) acc 62.5000 (51.2500) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/25] time 0.070 (0.171) data 0.000 (0.101) loss 2.1484 (1.8343) acc 46.8750 (51.2500) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.067) loss 1.9541 (1.8382) acc 43.7500 (52.2917) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.050) loss 1.8945 (1.8409) acc 50.0000 (52.3438) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.9121 (1.8318) acc 50.0000 (52.7500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.070 (0.258) data 0.000 (0.188) loss 1.6348 (1.7531) acc 65.6250 (56.2500) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/25] time 0.071 (0.165) data 0.000 (0.094) loss 1.3359 (1.7621) acc 68.7500 (55.6250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.070 (0.134) data 0.000 (0.063) loss 1.8438 (1.7563) acc 59.3750 (56.2500) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.071 (0.118) data 0.000 (0.047) loss 1.8838 (1.7628) acc 50.0000 (55.9375) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.070 (0.108) data 0.000 (0.038) loss 1.9795 (1.7764) acc 40.6250 (54.8750) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:21<01:04, 21.50s/it] 50%|█████     | 2/4 [00:22<00:18,  9.19s/it] 75%|███████▌  | 3/4 [00:22<00:05,  5.42s/it]100%|██████████| 4/4 [00:23<00:00,  3.37s/it]100%|██████████| 4/4 [00:23<00:00,  5.84s/it]
=> result
* total: 1,666
* correct: 711
* accuracy: 42.7%
* error: 57.3%
* macro_f1: 40.5%
Elapsed: 0:02:56
Run this job and save the output to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/25] time 0.070 (0.549) data 0.000 (0.467) loss 4.9219 (4.9617) acc 12.5000 (14.3750) lr 1.0000e-05 eta 0:11:23
epoch [1/50] batch [10/25] time 0.078 (0.333) data 0.000 (0.257) loss 4.9375 (4.9477) acc 15.6250 (14.0625) lr 1.0000e-05 eta 0:06:53
epoch [1/50] batch [15/25] time 0.070 (0.245) data 0.000 (0.171) loss 4.7227 (4.9094) acc 15.6250 (15.8333) lr 1.0000e-05 eta 0:05:03
epoch [1/50] batch [20/25] time 0.071 (0.236) data 0.000 (0.163) loss 5.0352 (4.8799) acc 25.0000 (17.1875) lr 1.0000e-05 eta 0:04:49
epoch [1/50] batch [25/25] time 0.244 (0.209) data 0.173 (0.137) loss 4.5234 (4.8255) acc 28.1250 (16.5000) lr 2.0000e-03 eta 0:04:16
epoch [2/50] batch [5/25] time 0.070 (0.314) data 0.000 (0.242) loss 3.1992 (3.6281) acc 18.7500 (19.3750) lr 2.0000e-03 eta 0:06:22
epoch [2/50] batch [10/25] time 0.070 (0.192) data 0.000 (0.121) loss 2.9316 (3.2633) acc 28.1250 (24.3750) lr 2.0000e-03 eta 0:03:53
epoch [2/50] batch [15/25] time 0.070 (0.152) data 0.000 (0.081) loss 2.6523 (3.0465) acc 34.3750 (25.8333) lr 2.0000e-03 eta 0:03:03
epoch [2/50] batch [20/25] time 0.071 (0.131) data 0.000 (0.061) loss 2.9121 (2.9634) acc 21.8750 (25.0000) lr 2.0000e-03 eta 0:02:38
epoch [2/50] batch [25/25] time 0.070 (0.119) data 0.000 (0.049) loss 2.7793 (2.8915) acc 34.3750 (26.3750) lr 1.9980e-03 eta 0:02:22
epoch [3/50] batch [5/25] time 0.070 (0.300) data 0.000 (0.230) loss 2.6895 (2.6703) acc 15.6250 (21.2500) lr 1.9980e-03 eta 0:05:58
epoch [3/50] batch [10/25] time 0.071 (0.185) data 0.000 (0.115) loss 2.2949 (2.5779) acc 34.3750 (25.3125) lr 1.9980e-03 eta 0:03:40
epoch [3/50] batch [15/25] time 0.071 (0.148) data 0.000 (0.077) loss 2.3223 (2.5091) acc 40.6250 (29.3750) lr 1.9980e-03 eta 0:02:55
epoch [3/50] batch [20/25] time 0.069 (0.128) data 0.000 (0.058) loss 2.2168 (2.4828) acc 31.2500 (30.1562) lr 1.9980e-03 eta 0:02:31
epoch [3/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.046) loss 2.4922 (2.4750) acc 34.3750 (30.0000) lr 1.9921e-03 eta 0:02:16
epoch [4/50] batch [5/25] time 0.070 (0.296) data 0.000 (0.226) loss 1.9619 (2.4689) acc 37.5000 (28.7500) lr 1.9921e-03 eta 0:05:45
epoch [4/50] batch [10/25] time 0.069 (0.184) data 0.000 (0.114) loss 2.3066 (2.3935) acc 34.3750 (28.4375) lr 1.9921e-03 eta 0:03:34
epoch [4/50] batch [15/25] time 0.069 (0.146) data 0.000 (0.076) loss 2.3652 (2.3814) acc 28.1250 (27.0833) lr 1.9921e-03 eta 0:02:49
epoch [4/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.057) loss 1.9092 (2.3748) acc 37.5000 (27.8125) lr 1.9921e-03 eta 0:02:26
epoch [4/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.046) loss 2.2480 (2.3709) acc 34.3750 (27.7500) lr 1.9823e-03 eta 0:02:12
epoch [5/50] batch [5/25] time 0.070 (0.297) data 0.000 (0.225) loss 2.0273 (2.3477) acc 40.6250 (31.8750) lr 1.9823e-03 eta 0:05:40
epoch [5/50] batch [10/25] time 0.071 (0.184) data 0.000 (0.113) loss 2.4336 (2.2951) acc 31.2500 (35.0000) lr 1.9823e-03 eta 0:03:29
epoch [5/50] batch [15/25] time 0.074 (0.147) data 0.000 (0.075) loss 2.0938 (2.3221) acc 34.3750 (33.7500) lr 1.9823e-03 eta 0:02:47
epoch [5/50] batch [20/25] time 0.071 (0.128) data 0.000 (0.057) loss 2.4961 (2.3376) acc 25.0000 (31.4062) lr 1.9823e-03 eta 0:02:24
epoch [5/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.045) loss 2.0547 (2.3489) acc 31.2500 (31.1250) lr 1.9686e-03 eta 0:02:11
epoch [6/50] batch [5/25] time 0.071 (0.294) data 0.000 (0.223) loss 2.3379 (2.2250) acc 37.5000 (35.6250) lr 1.9686e-03 eta 0:05:29
epoch [6/50] batch [10/25] time 0.071 (0.183) data 0.000 (0.112) loss 2.6699 (2.3439) acc 21.8750 (33.1250) lr 1.9686e-03 eta 0:03:23
epoch [6/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.075) loss 2.0957 (2.3370) acc 28.1250 (31.0417) lr 1.9686e-03 eta 0:02:41
epoch [6/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.056) loss 2.1719 (2.3289) acc 46.8750 (31.0938) lr 1.9686e-03 eta 0:02:20
epoch [6/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.045) loss 2.2988 (2.3238) acc 34.3750 (31.0000) lr 1.9511e-03 eta 0:02:07
epoch [7/50] batch [5/25] time 0.072 (0.299) data 0.001 (0.227) loss 2.4023 (2.4480) acc 31.2500 (26.2500) lr 1.9511e-03 eta 0:05:26
epoch [7/50] batch [10/25] time 0.070 (0.185) data 0.000 (0.114) loss 1.5303 (2.2952) acc 46.8750 (28.1250) lr 1.9511e-03 eta 0:03:21
epoch [7/50] batch [15/25] time 0.070 (0.147) data 0.000 (0.076) loss 2.0547 (2.2535) acc 46.8750 (31.6667) lr 1.9511e-03 eta 0:02:39
epoch [7/50] batch [20/25] time 0.069 (0.128) data 0.000 (0.057) loss 2.1680 (2.2458) acc 40.6250 (32.8125) lr 1.9511e-03 eta 0:02:18
epoch [7/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.046) loss 1.8506 (2.2590) acc 34.3750 (32.0000) lr 1.9298e-03 eta 0:02:05
epoch [8/50] batch [5/25] time 0.071 (0.287) data 0.000 (0.216) loss 2.7344 (2.2975) acc 12.5000 (33.7500) lr 1.9298e-03 eta 0:05:07
epoch [8/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.108) loss 2.2676 (2.2438) acc 25.0000 (37.1875) lr 1.9298e-03 eta 0:03:11
epoch [8/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.3594 (2.2757) acc 34.3750 (36.0417) lr 1.9298e-03 eta 0:02:31
epoch [8/50] batch [20/25] time 0.069 (0.125) data 0.000 (0.054) loss 1.8389 (2.2424) acc 40.6250 (35.4688) lr 1.9298e-03 eta 0:02:11
epoch [8/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 2.5098 (2.2799) acc 25.0000 (33.6250) lr 1.9048e-03 eta 0:01:59
epoch [9/50] batch [5/25] time 0.071 (0.320) data 0.000 (0.248) loss 2.3184 (2.2416) acc 34.3750 (36.2500) lr 1.9048e-03 eta 0:05:34
epoch [9/50] batch [10/25] time 0.070 (0.197) data 0.000 (0.124) loss 2.3281 (2.2417) acc 28.1250 (32.8125) lr 1.9048e-03 eta 0:03:24
epoch [9/50] batch [15/25] time 0.070 (0.155) data 0.000 (0.083) loss 2.4961 (2.2835) acc 31.2500 (31.8750) lr 1.9048e-03 eta 0:02:40
epoch [9/50] batch [20/25] time 0.070 (0.134) data 0.000 (0.062) loss 2.3711 (2.2940) acc 21.8750 (31.0938) lr 1.9048e-03 eta 0:02:17
epoch [9/50] batch [25/25] time 0.070 (0.121) data 0.000 (0.050) loss 2.1836 (2.2617) acc 25.0000 (31.3750) lr 1.8763e-03 eta 0:02:03
epoch [10/50] batch [5/25] time 0.071 (0.280) data 0.000 (0.208) loss 2.0918 (2.2109) acc 43.7500 (37.5000) lr 1.8763e-03 eta 0:04:45
epoch [10/50] batch [10/25] time 0.070 (0.176) data 0.000 (0.104) loss 2.0215 (2.2053) acc 37.5000 (34.6875) lr 1.8763e-03 eta 0:02:58
epoch [10/50] batch [15/25] time 0.073 (0.142) data 0.000 (0.070) loss 2.1504 (2.1951) acc 40.6250 (33.5417) lr 1.8763e-03 eta 0:02:23
epoch [10/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.052) loss 2.1035 (2.1858) acc 37.5000 (34.6875) lr 1.8763e-03 eta 0:02:04
epoch [10/50] batch [25/25] time 0.071 (0.113) data 0.000 (0.042) loss 2.3848 (2.2219) acc 15.6250 (32.7500) lr 1.8443e-03 eta 0:01:53
epoch [11/50] batch [5/25] time 0.070 (0.301) data 0.000 (0.230) loss 2.1035 (2.0455) acc 37.5000 (40.6250) lr 1.8443e-03 eta 0:04:59
epoch [11/50] batch [10/25] time 0.071 (0.186) data 0.000 (0.115) loss 1.9814 (2.1187) acc 43.7500 (38.1250) lr 1.8443e-03 eta 0:03:04
epoch [11/50] batch [15/25] time 0.070 (0.149) data 0.000 (0.077) loss 2.1230 (2.1645) acc 34.3750 (37.0833) lr 1.8443e-03 eta 0:02:26
epoch [11/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.058) loss 2.2520 (2.1769) acc 31.2500 (36.0938) lr 1.8443e-03 eta 0:02:06
epoch [11/50] batch [25/25] time 0.072 (0.117) data 0.000 (0.046) loss 2.4277 (2.1921) acc 21.8750 (35.6250) lr 1.8090e-03 eta 0:01:54
epoch [12/50] batch [5/25] time 0.073 (0.293) data 0.000 (0.221) loss 2.2148 (2.2463) acc 34.3750 (34.3750) lr 1.8090e-03 eta 0:04:44
epoch [12/50] batch [10/25] time 0.071 (0.183) data 0.000 (0.111) loss 2.2070 (2.2080) acc 34.3750 (35.3125) lr 1.8090e-03 eta 0:02:56
epoch [12/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.074) loss 2.0918 (2.1807) acc 34.3750 (35.4167) lr 1.8090e-03 eta 0:02:19
epoch [12/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.056) loss 1.8135 (2.1739) acc 50.0000 (35.7812) lr 1.8090e-03 eta 0:02:00
epoch [12/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.044) loss 2.1191 (2.1720) acc 31.2500 (35.2500) lr 1.7705e-03 eta 0:01:49
epoch [13/50] batch [5/25] time 0.071 (0.294) data 0.000 (0.223) loss 2.0156 (2.1840) acc 46.8750 (38.1250) lr 1.7705e-03 eta 0:04:37
epoch [13/50] batch [10/25] time 0.070 (0.183) data 0.000 (0.112) loss 1.9219 (2.1412) acc 34.3750 (36.8750) lr 1.7705e-03 eta 0:02:51
epoch [13/50] batch [15/25] time 0.072 (0.146) data 0.000 (0.075) loss 1.9160 (2.1197) acc 43.7500 (37.5000) lr 1.7705e-03 eta 0:02:16
epoch [13/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.056) loss 1.8906 (2.1333) acc 43.7500 (37.5000) lr 1.7705e-03 eta 0:01:57
epoch [13/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 1.8809 (2.1157) acc 43.7500 (38.2500) lr 1.7290e-03 eta 0:01:46
epoch [14/50] batch [5/25] time 0.071 (0.304) data 0.000 (0.234) loss 2.4219 (2.1248) acc 28.1250 (40.6250) lr 1.7290e-03 eta 0:04:39
epoch [14/50] batch [10/25] time 0.070 (0.187) data 0.000 (0.117) loss 2.3145 (2.1659) acc 28.1250 (38.7500) lr 1.7290e-03 eta 0:02:51
epoch [14/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.078) loss 2.3594 (2.1820) acc 21.8750 (36.8750) lr 1.7290e-03 eta 0:02:14
epoch [14/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.059) loss 2.4102 (2.2182) acc 25.0000 (35.1562) lr 1.7290e-03 eta 0:01:56
epoch [14/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.047) loss 2.3301 (2.1997) acc 28.1250 (35.5000) lr 1.6845e-03 eta 0:01:45
epoch [15/50] batch [5/25] time 0.071 (0.291) data 0.000 (0.219) loss 2.0879 (2.0662) acc 31.2500 (41.8750) lr 1.6845e-03 eta 0:04:20
epoch [15/50] batch [10/25] time 0.071 (0.181) data 0.000 (0.110) loss 2.7578 (2.1213) acc 18.7500 (39.3750) lr 1.6845e-03 eta 0:02:40
epoch [15/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.073) loss 1.9600 (2.1536) acc 43.7500 (39.1667) lr 1.6845e-03 eta 0:02:07
epoch [15/50] batch [20/25] time 0.071 (0.126) data 0.000 (0.055) loss 2.3398 (2.1583) acc 28.1250 (38.9062) lr 1.6845e-03 eta 0:01:50
epoch [15/50] batch [25/25] time 0.071 (0.115) data 0.000 (0.044) loss 2.1621 (2.1946) acc 31.2500 (37.8750) lr 1.6374e-03 eta 0:01:40
epoch [16/50] batch [5/25] time 0.071 (0.310) data 0.000 (0.239) loss 2.0586 (2.1926) acc 43.7500 (38.1250) lr 1.6374e-03 eta 0:04:29
epoch [16/50] batch [10/25] time 0.070 (0.191) data 0.000 (0.120) loss 1.8994 (2.0750) acc 56.2500 (39.6875) lr 1.6374e-03 eta 0:02:45
epoch [16/50] batch [15/25] time 0.070 (0.151) data 0.000 (0.080) loss 1.9111 (2.0661) acc 46.8750 (40.6250) lr 1.6374e-03 eta 0:02:09
epoch [16/50] batch [20/25] time 0.070 (0.131) data 0.000 (0.060) loss 2.0391 (2.0898) acc 43.7500 (39.6875) lr 1.6374e-03 eta 0:01:51
epoch [16/50] batch [25/25] time 0.070 (0.118) data 0.000 (0.048) loss 2.1973 (2.1162) acc 31.2500 (38.2500) lr 1.5878e-03 eta 0:01:40
epoch [17/50] batch [5/25] time 0.071 (0.284) data 0.000 (0.211) loss 2.0039 (2.1492) acc 37.5000 (40.0000) lr 1.5878e-03 eta 0:03:59
epoch [17/50] batch [10/25] time 0.071 (0.178) data 0.000 (0.105) loss 1.9775 (2.0945) acc 59.3750 (42.8125) lr 1.5878e-03 eta 0:02:29
epoch [17/50] batch [15/25] time 0.071 (0.143) data 0.000 (0.070) loss 2.2305 (2.1173) acc 31.2500 (41.0417) lr 1.5878e-03 eta 0:01:59
epoch [17/50] batch [20/25] time 0.070 (0.125) data 0.000 (0.053) loss 2.1133 (2.1079) acc 37.5000 (41.0938) lr 1.5878e-03 eta 0:01:43
epoch [17/50] batch [25/25] time 0.072 (0.114) data 0.000 (0.042) loss 2.1855 (2.1183) acc 25.0000 (39.3750) lr 1.5358e-03 eta 0:01:33
epoch [18/50] batch [5/25] time 0.073 (0.321) data 0.000 (0.248) loss 1.9834 (1.9646) acc 43.7500 (42.5000) lr 1.5358e-03 eta 0:04:23
epoch [18/50] batch [10/25] time 0.069 (0.196) data 0.000 (0.124) loss 2.0059 (2.0288) acc 34.3750 (41.8750) lr 1.5358e-03 eta 0:02:40
epoch [18/50] batch [15/25] time 0.070 (0.155) data 0.000 (0.083) loss 1.8691 (2.0729) acc 56.2500 (42.5000) lr 1.5358e-03 eta 0:02:05
epoch [18/50] batch [20/25] time 0.070 (0.134) data 0.000 (0.062) loss 2.0488 (2.0969) acc 43.7500 (42.1875) lr 1.5358e-03 eta 0:01:47
epoch [18/50] batch [25/25] time 0.070 (0.121) data 0.000 (0.050) loss 2.1582 (2.1200) acc 37.5000 (40.5000) lr 1.4818e-03 eta 0:01:36
epoch [19/50] batch [5/25] time 0.079 (0.322) data 0.000 (0.250) loss 1.8848 (1.9393) acc 43.7500 (43.1250) lr 1.4818e-03 eta 0:04:15
epoch [19/50] batch [10/25] time 0.075 (0.197) data 0.000 (0.125) loss 1.7100 (2.0040) acc 53.1250 (43.1250) lr 1.4818e-03 eta 0:02:35
epoch [19/50] batch [15/25] time 0.071 (0.156) data 0.000 (0.083) loss 1.8691 (2.0231) acc 50.0000 (44.3750) lr 1.4818e-03 eta 0:02:02
epoch [19/50] batch [20/25] time 0.071 (0.134) data 0.000 (0.063) loss 2.3770 (2.0794) acc 37.5000 (42.1875) lr 1.4818e-03 eta 0:01:44
epoch [19/50] batch [25/25] time 0.071 (0.122) data 0.000 (0.050) loss 2.0352 (2.0935) acc 43.7500 (41.0000) lr 1.4258e-03 eta 0:01:34
epoch [20/50] batch [5/25] time 0.075 (0.311) data 0.000 (0.239) loss 1.9580 (1.9375) acc 40.6250 (45.0000) lr 1.4258e-03 eta 0:03:59
epoch [20/50] batch [10/25] time 0.070 (0.191) data 0.000 (0.120) loss 2.2949 (2.0314) acc 31.2500 (42.1875) lr 1.4258e-03 eta 0:02:26
epoch [20/50] batch [15/25] time 0.070 (0.152) data 0.000 (0.080) loss 1.9277 (2.0529) acc 50.0000 (42.2917) lr 1.4258e-03 eta 0:01:55
epoch [20/50] batch [20/25] time 0.070 (0.131) data 0.000 (0.060) loss 2.1973 (2.0214) acc 31.2500 (42.5000) lr 1.4258e-03 eta 0:01:39
epoch [20/50] batch [25/25] time 0.070 (0.119) data 0.000 (0.048) loss 2.0508 (2.0482) acc 37.5000 (41.1250) lr 1.3681e-03 eta 0:01:29
epoch [21/50] batch [5/25] time 0.071 (0.321) data 0.001 (0.250) loss 2.1055 (2.0449) acc 43.7500 (42.5000) lr 1.3681e-03 eta 0:03:59
epoch [21/50] batch [10/25] time 0.071 (0.197) data 0.000 (0.125) loss 1.5693 (1.9321) acc 50.0000 (44.3750) lr 1.3681e-03 eta 0:02:25
epoch [21/50] batch [15/25] time 0.070 (0.155) data 0.000 (0.084) loss 2.1270 (1.9398) acc 37.5000 (43.7500) lr 1.3681e-03 eta 0:01:54
epoch [21/50] batch [20/25] time 0.070 (0.134) data 0.000 (0.063) loss 2.2773 (1.9471) acc 31.2500 (42.5000) lr 1.3681e-03 eta 0:01:37
epoch [21/50] batch [25/25] time 0.070 (0.121) data 0.000 (0.050) loss 2.1191 (1.9505) acc 46.8750 (43.5000) lr 1.3090e-03 eta 0:01:28
epoch [22/50] batch [5/25] time 0.072 (0.286) data 0.000 (0.215) loss 1.7041 (2.0682) acc 43.7500 (40.6250) lr 1.3090e-03 eta 0:03:26
epoch [22/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.108) loss 2.0820 (1.9665) acc 37.5000 (44.3750) lr 1.3090e-03 eta 0:02:08
epoch [22/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.2383 (2.0076) acc 43.7500 (42.0833) lr 1.3090e-03 eta 0:01:41
epoch [22/50] batch [20/25] time 0.070 (0.125) data 0.000 (0.054) loss 2.1641 (2.0467) acc 37.5000 (41.5625) lr 1.3090e-03 eta 0:01:28
epoch [22/50] batch [25/25] time 0.069 (0.114) data 0.000 (0.043) loss 2.0664 (2.0434) acc 25.0000 (41.2500) lr 1.2487e-03 eta 0:01:19
epoch [23/50] batch [5/25] time 0.071 (0.296) data 0.000 (0.224) loss 1.9307 (2.0223) acc 53.1250 (40.6250) lr 1.2487e-03 eta 0:03:25
epoch [23/50] batch [10/25] time 0.071 (0.184) data 0.000 (0.113) loss 1.8320 (1.9949) acc 50.0000 (42.5000) lr 1.2487e-03 eta 0:02:06
epoch [23/50] batch [15/25] time 0.071 (0.147) data 0.000 (0.075) loss 2.2852 (1.9880) acc 46.8750 (44.5833) lr 1.2487e-03 eta 0:01:40
epoch [23/50] batch [20/25] time 0.070 (0.128) data 0.000 (0.056) loss 1.8750 (1.9917) acc 43.7500 (44.0625) lr 1.2487e-03 eta 0:01:26
epoch [23/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.045) loss 2.4180 (2.0222) acc 34.3750 (42.8750) lr 1.1874e-03 eta 0:01:18
epoch [24/50] batch [5/25] time 0.070 (0.332) data 0.000 (0.262) loss 1.7920 (2.0113) acc 59.3750 (45.6250) lr 1.1874e-03 eta 0:03:42
epoch [24/50] batch [10/25] time 0.070 (0.203) data 0.000 (0.131) loss 2.3770 (2.0603) acc 34.3750 (42.5000) lr 1.1874e-03 eta 0:02:15
epoch [24/50] batch [15/25] time 0.071 (0.160) data 0.000 (0.087) loss 2.0527 (2.0247) acc 37.5000 (43.3333) lr 1.1874e-03 eta 0:01:45
epoch [24/50] batch [20/25] time 0.070 (0.137) data 0.000 (0.066) loss 1.8359 (2.0182) acc 46.8750 (42.5000) lr 1.1874e-03 eta 0:01:29
epoch [24/50] batch [25/25] time 0.071 (0.124) data 0.000 (0.053) loss 1.8994 (2.0058) acc 50.0000 (42.5000) lr 1.1253e-03 eta 0:01:20
epoch [25/50] batch [5/25] time 0.075 (0.318) data 0.004 (0.248) loss 1.9316 (1.9229) acc 43.7500 (44.3750) lr 1.1253e-03 eta 0:03:25
epoch [25/50] batch [10/25] time 0.070 (0.194) data 0.000 (0.124) loss 1.9453 (1.9620) acc 46.8750 (44.0625) lr 1.1253e-03 eta 0:02:04
epoch [25/50] batch [15/25] time 0.070 (0.153) data 0.000 (0.083) loss 1.9863 (1.9747) acc 40.6250 (43.7500) lr 1.1253e-03 eta 0:01:37
epoch [25/50] batch [20/25] time 0.070 (0.133) data 0.000 (0.062) loss 2.1660 (2.0140) acc 34.3750 (42.1875) lr 1.1253e-03 eta 0:01:23
epoch [25/50] batch [25/25] time 0.070 (0.120) data 0.000 (0.050) loss 1.8320 (2.0118) acc 50.0000 (43.2500) lr 1.0628e-03 eta 0:01:15
epoch [26/50] batch [5/25] time 0.071 (0.302) data 0.000 (0.230) loss 2.0879 (2.0221) acc 53.1250 (44.3750) lr 1.0628e-03 eta 0:03:07
epoch [26/50] batch [10/25] time 0.071 (0.188) data 0.000 (0.115) loss 1.8271 (1.9407) acc 50.0000 (45.9375) lr 1.0628e-03 eta 0:01:55
epoch [26/50] batch [15/25] time 0.070 (0.149) data 0.000 (0.077) loss 1.7773 (1.9447) acc 59.3750 (45.8333) lr 1.0628e-03 eta 0:01:30
epoch [26/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.058) loss 2.0254 (1.9679) acc 46.8750 (44.6875) lr 1.0628e-03 eta 0:01:18
epoch [26/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.046) loss 2.4453 (2.0027) acc 31.2500 (44.0000) lr 1.0000e-03 eta 0:01:10
epoch [27/50] batch [5/25] time 0.073 (0.297) data 0.000 (0.223) loss 2.0508 (2.0203) acc 43.7500 (42.5000) lr 1.0000e-03 eta 0:02:56
epoch [27/50] batch [10/25] time 0.071 (0.184) data 0.000 (0.111) loss 2.1562 (1.9731) acc 37.5000 (41.5625) lr 1.0000e-03 eta 0:01:48
epoch [27/50] batch [15/25] time 0.071 (0.147) data 0.000 (0.074) loss 1.7676 (1.9729) acc 59.3750 (42.5000) lr 1.0000e-03 eta 0:01:25
epoch [27/50] batch [20/25] time 0.070 (0.128) data 0.000 (0.056) loss 2.1484 (1.9706) acc 40.6250 (42.9688) lr 1.0000e-03 eta 0:01:13
epoch [27/50] batch [25/25] time 0.071 (0.116) data 0.000 (0.045) loss 1.7588 (1.9624) acc 59.3750 (43.7500) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [5/25] time 0.070 (0.265) data 0.000 (0.191) loss 1.8193 (1.9908) acc 43.7500 (44.3750) lr 9.3721e-04 eta 0:02:31
epoch [28/50] batch [10/25] time 0.071 (0.168) data 0.000 (0.096) loss 1.6094 (2.0157) acc 46.8750 (41.2500) lr 9.3721e-04 eta 0:01:35
epoch [28/50] batch [15/25] time 0.073 (0.136) data 0.000 (0.064) loss 1.8691 (1.9759) acc 53.1250 (43.7500) lr 9.3721e-04 eta 0:01:16
epoch [28/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.048) loss 2.1094 (1.9628) acc 37.5000 (43.9062) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.038) loss 1.6924 (1.9557) acc 59.3750 (44.5000) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [5/25] time 0.070 (0.269) data 0.000 (0.198) loss 2.0254 (1.9125) acc 43.7500 (46.2500) lr 8.7467e-04 eta 0:02:26
epoch [29/50] batch [10/25] time 0.071 (0.171) data 0.000 (0.099) loss 1.8984 (1.9515) acc 43.7500 (44.6875) lr 8.7467e-04 eta 0:01:32
epoch [29/50] batch [15/25] time 0.072 (0.139) data 0.000 (0.067) loss 1.9805 (1.9786) acc 50.0000 (44.1667) lr 8.7467e-04 eta 0:01:14
epoch [29/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.050) loss 1.6367 (1.9342) acc 53.1250 (46.2500) lr 8.7467e-04 eta 0:01:04
epoch [29/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 2.3242 (1.9526) acc 37.5000 (45.6250) lr 8.1262e-04 eta 0:00:58
epoch [30/50] batch [5/25] time 0.073 (0.274) data 0.000 (0.203) loss 2.1777 (2.0119) acc 34.3750 (40.6250) lr 8.1262e-04 eta 0:02:22
epoch [30/50] batch [10/25] time 0.070 (0.172) data 0.000 (0.102) loss 1.9375 (1.9131) acc 37.5000 (44.0625) lr 8.1262e-04 eta 0:01:28
epoch [30/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 1.8818 (1.9621) acc 43.7500 (41.6667) lr 8.1262e-04 eta 0:01:10
epoch [30/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 2.1328 (1.9457) acc 34.3750 (42.5000) lr 8.1262e-04 eta 0:01:01
epoch [30/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 1.8711 (1.9571) acc 46.8750 (43.2500) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [5/25] time 0.071 (0.303) data 0.000 (0.230) loss 1.7773 (1.8371) acc 46.8750 (49.3750) lr 7.5131e-04 eta 0:02:29
epoch [31/50] batch [10/25] time 0.070 (0.188) data 0.000 (0.115) loss 1.8857 (1.8534) acc 46.8750 (49.0625) lr 7.5131e-04 eta 0:01:31
epoch [31/50] batch [15/25] time 0.070 (0.149) data 0.000 (0.077) loss 2.4102 (1.9055) acc 31.2500 (47.5000) lr 7.5131e-04 eta 0:01:12
epoch [31/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.058) loss 1.6738 (1.9057) acc 40.6250 (47.1875) lr 7.5131e-04 eta 0:01:01
epoch [31/50] batch [25/25] time 0.071 (0.117) data 0.000 (0.046) loss 1.9365 (1.9114) acc 53.1250 (47.3750) lr 6.9098e-04 eta 0:00:55
epoch [32/50] batch [5/25] time 0.071 (0.266) data 0.000 (0.195) loss 1.9707 (2.0064) acc 31.2500 (40.0000) lr 6.9098e-04 eta 0:02:04
epoch [32/50] batch [10/25] time 0.070 (0.169) data 0.000 (0.098) loss 2.0078 (1.8993) acc 46.8750 (44.3750) lr 6.9098e-04 eta 0:01:18
epoch [32/50] batch [15/25] time 0.070 (0.136) data 0.000 (0.065) loss 1.7266 (1.9074) acc 56.2500 (44.3750) lr 6.9098e-04 eta 0:01:02
epoch [32/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.049) loss 1.6738 (1.8723) acc 65.6250 (47.0312) lr 6.9098e-04 eta 0:00:54
epoch [32/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.039) loss 1.6426 (1.8918) acc 46.8750 (46.2500) lr 6.3188e-04 eta 0:00:49
epoch [33/50] batch [5/25] time 0.071 (0.276) data 0.000 (0.205) loss 2.0117 (1.7937) acc 46.8750 (52.5000) lr 6.3188e-04 eta 0:02:02
epoch [33/50] batch [10/25] time 0.071 (0.174) data 0.000 (0.103) loss 1.4287 (1.8059) acc 65.6250 (49.6875) lr 6.3188e-04 eta 0:01:16
epoch [33/50] batch [15/25] time 0.071 (0.140) data 0.000 (0.069) loss 2.0586 (1.8535) acc 43.7500 (47.7083) lr 6.3188e-04 eta 0:01:00
epoch [33/50] batch [20/25] time 0.071 (0.123) data 0.000 (0.052) loss 2.0215 (1.8728) acc 40.6250 (47.6562) lr 6.3188e-04 eta 0:00:52
epoch [33/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.9922 (1.9096) acc 34.3750 (46.1250) lr 5.7422e-04 eta 0:00:47
epoch [34/50] batch [5/25] time 0.070 (0.255) data 0.000 (0.184) loss 1.8906 (1.6994) acc 56.2500 (56.8750) lr 5.7422e-04 eta 0:01:47
epoch [34/50] batch [10/25] time 0.071 (0.164) data 0.000 (0.092) loss 1.6230 (1.8159) acc 53.1250 (50.6250) lr 5.7422e-04 eta 0:01:07
epoch [34/50] batch [15/25] time 0.072 (0.133) data 0.000 (0.062) loss 2.1973 (1.8694) acc 40.6250 (50.2083) lr 5.7422e-04 eta 0:00:54
epoch [34/50] batch [20/25] time 0.070 (0.117) data 0.000 (0.046) loss 2.0801 (1.9135) acc 37.5000 (47.8125) lr 5.7422e-04 eta 0:00:47
epoch [34/50] batch [25/25] time 0.071 (0.108) data 0.000 (0.037) loss 2.0664 (1.9102) acc 40.6250 (48.5000) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [5/25] time 0.070 (0.265) data 0.000 (0.194) loss 1.8369 (1.8748) acc 34.3750 (45.6250) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [10/25] time 0.070 (0.169) data 0.000 (0.097) loss 1.7139 (1.8671) acc 50.0000 (44.0625) lr 5.1825e-04 eta 0:01:05
epoch [35/50] batch [15/25] time 0.072 (0.137) data 0.000 (0.065) loss 2.2266 (1.9061) acc 37.5000 (43.9583) lr 5.1825e-04 eta 0:00:52
epoch [35/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.049) loss 1.8066 (1.9093) acc 56.2500 (44.0625) lr 5.1825e-04 eta 0:00:45
epoch [35/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.039) loss 1.5664 (1.8744) acc 56.2500 (44.8750) lr 4.6417e-04 eta 0:00:41
epoch [36/50] batch [5/25] time 0.071 (0.277) data 0.000 (0.206) loss 1.9951 (1.8451) acc 40.6250 (46.8750) lr 4.6417e-04 eta 0:01:42
epoch [36/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.103) loss 1.8672 (1.8334) acc 46.8750 (49.0625) lr 4.6417e-04 eta 0:01:03
epoch [36/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.069) loss 1.7842 (1.8659) acc 50.0000 (47.9167) lr 4.6417e-04 eta 0:00:50
epoch [36/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.052) loss 1.7314 (1.8813) acc 50.0000 (47.5000) lr 4.6417e-04 eta 0:00:43
epoch [36/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.9893 (1.8862) acc 43.7500 (48.1250) lr 4.1221e-04 eta 0:00:39
epoch [37/50] batch [5/25] time 0.070 (0.277) data 0.000 (0.206) loss 1.9023 (1.7961) acc 46.8750 (48.1250) lr 4.1221e-04 eta 0:01:35
epoch [37/50] batch [10/25] time 0.071 (0.175) data 0.000 (0.103) loss 2.2812 (1.9242) acc 34.3750 (45.0000) lr 4.1221e-04 eta 0:00:59
epoch [37/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.069) loss 1.5234 (1.8519) acc 59.3750 (48.7500) lr 4.1221e-04 eta 0:00:47
epoch [37/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.052) loss 1.7617 (1.8484) acc 53.1250 (48.4375) lr 4.1221e-04 eta 0:00:40
epoch [37/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.041) loss 1.7734 (1.8733) acc 50.0000 (48.3750) lr 3.6258e-04 eta 0:00:36
epoch [38/50] batch [5/25] time 0.070 (0.273) data 0.000 (0.203) loss 1.8037 (2.0094) acc 53.1250 (46.8750) lr 3.6258e-04 eta 0:01:27
epoch [38/50] batch [10/25] time 0.070 (0.172) data 0.000 (0.101) loss 1.8320 (1.9110) acc 62.5000 (48.1250) lr 3.6258e-04 eta 0:00:54
epoch [38/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 1.7656 (1.9051) acc 56.2500 (47.5000) lr 3.6258e-04 eta 0:00:42
epoch [38/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 1.9619 (1.8587) acc 50.0000 (48.9062) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.041) loss 1.8262 (1.8684) acc 53.1250 (49.5000) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [5/25] time 0.070 (0.272) data 0.000 (0.202) loss 1.6953 (1.7348) acc 59.3750 (53.7500) lr 3.1545e-04 eta 0:01:20
epoch [39/50] batch [10/25] time 0.070 (0.172) data 0.000 (0.101) loss 2.0703 (1.7504) acc 37.5000 (52.1875) lr 3.1545e-04 eta 0:00:49
epoch [39/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.067) loss 2.1113 (1.8291) acc 50.0000 (51.2500) lr 3.1545e-04 eta 0:00:39
epoch [39/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.051) loss 1.9336 (1.8338) acc 46.8750 (50.1562) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [25/25] time 0.071 (0.111) data 0.000 (0.040) loss 1.5742 (1.8301) acc 53.1250 (49.8750) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [5/25] time 0.072 (0.268) data 0.000 (0.196) loss 1.7686 (1.9162) acc 53.1250 (51.8750) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [10/25] time 0.071 (0.170) data 0.000 (0.098) loss 1.9844 (1.9229) acc 56.2500 (53.1250) lr 2.7103e-04 eta 0:00:45
epoch [40/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.065) loss 1.7607 (1.8732) acc 56.2500 (53.1250) lr 2.7103e-04 eta 0:00:35
epoch [40/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.049) loss 1.9805 (1.8551) acc 46.8750 (51.7188) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.039) loss 1.8525 (1.8495) acc 46.8750 (51.8750) lr 2.2949e-04 eta 0:00:27
epoch [41/50] batch [5/25] time 0.070 (0.278) data 0.000 (0.207) loss 1.7021 (1.8814) acc 65.6250 (50.0000) lr 2.2949e-04 eta 0:01:08
epoch [41/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.104) loss 2.0039 (1.8636) acc 59.3750 (50.9375) lr 2.2949e-04 eta 0:00:41
epoch [41/50] batch [15/25] time 0.070 (0.140) data 0.000 (0.069) loss 1.8711 (1.8424) acc 50.0000 (51.2500) lr 2.2949e-04 eta 0:00:32
epoch [41/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.052) loss 1.7939 (1.8563) acc 43.7500 (50.3125) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [25/25] time 0.071 (0.112) data 0.000 (0.042) loss 1.7773 (1.8557) acc 59.3750 (51.0000) lr 1.9098e-04 eta 0:00:25
epoch [42/50] batch [5/25] time 0.070 (0.288) data 0.000 (0.217) loss 1.6465 (1.7682) acc 53.1250 (53.7500) lr 1.9098e-04 eta 0:01:03
epoch [42/50] batch [10/25] time 0.071 (0.180) data 0.000 (0.109) loss 2.2480 (1.7625) acc 40.6250 (55.9375) lr 1.9098e-04 eta 0:00:38
epoch [42/50] batch [15/25] time 0.069 (0.144) data 0.000 (0.073) loss 1.5449 (1.7609) acc 53.1250 (53.5417) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.055) loss 1.9395 (1.7997) acc 50.0000 (52.0312) lr 1.9098e-04 eta 0:00:25
epoch [42/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.044) loss 2.0195 (1.8198) acc 37.5000 (51.2500) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [5/25] time 0.071 (0.266) data 0.000 (0.195) loss 1.7715 (1.7275) acc 50.0000 (53.7500) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [10/25] time 0.070 (0.169) data 0.000 (0.098) loss 1.8643 (1.8487) acc 56.2500 (50.0000) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [15/25] time 0.073 (0.136) data 0.000 (0.065) loss 2.1309 (1.8234) acc 50.0000 (51.0417) lr 1.5567e-04 eta 0:00:25
epoch [43/50] batch [20/25] time 0.071 (0.120) data 0.000 (0.049) loss 1.8457 (1.8028) acc 59.3750 (52.0312) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [25/25] time 0.071 (0.110) data 0.000 (0.039) loss 1.5898 (1.8146) acc 59.3750 (50.7500) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [5/25] time 0.071 (0.287) data 0.000 (0.215) loss 1.9492 (1.8434) acc 59.3750 (55.6250) lr 1.2369e-04 eta 0:00:48
epoch [44/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.108) loss 1.8799 (1.8574) acc 50.0000 (53.1250) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [15/25] time 0.070 (0.144) data 0.000 (0.072) loss 2.0742 (1.8643) acc 40.6250 (51.2500) lr 1.2369e-04 eta 0:00:23
epoch [44/50] batch [20/25] time 0.070 (0.125) data 0.000 (0.054) loss 1.8662 (1.8564) acc 50.0000 (51.4062) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 1.6758 (1.8505) acc 53.1250 (51.1250) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [5/25] time 0.072 (0.313) data 0.000 (0.242) loss 1.7822 (1.6512) acc 53.1250 (58.1250) lr 9.5173e-05 eta 0:00:45
epoch [45/50] batch [10/25] time 0.070 (0.192) data 0.000 (0.121) loss 1.9219 (1.7700) acc 59.3750 (55.6250) lr 9.5173e-05 eta 0:00:26
epoch [45/50] batch [15/25] time 0.069 (0.151) data 0.000 (0.081) loss 1.9502 (1.8122) acc 34.3750 (52.7083) lr 9.5173e-05 eta 0:00:20
epoch [45/50] batch [20/25] time 0.070 (0.131) data 0.000 (0.061) loss 1.8574 (1.8271) acc 37.5000 (51.8750) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [25/25] time 0.071 (0.119) data 0.000 (0.049) loss 1.6924 (1.8121) acc 59.3750 (52.3750) lr 7.0224e-05 eta 0:00:14
epoch [46/50] batch [5/25] time 0.070 (0.276) data 0.000 (0.206) loss 2.2168 (2.0359) acc 40.6250 (46.2500) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.103) loss 2.0195 (1.9031) acc 37.5000 (47.5000) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.069) loss 2.0742 (1.8967) acc 43.7500 (48.5417) lr 7.0224e-05 eta 0:00:15
epoch [46/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.052) loss 1.5977 (1.8699) acc 43.7500 (50.6250) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.7158 (1.8518) acc 56.2500 (51.1250) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [5/25] time 0.070 (0.299) data 0.000 (0.226) loss 1.7744 (1.8027) acc 56.2500 (52.5000) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [10/25] time 0.072 (0.185) data 0.000 (0.113) loss 1.8047 (1.8003) acc 56.2500 (53.4375) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [15/25] time 0.071 (0.147) data 0.000 (0.075) loss 1.5430 (1.8135) acc 59.3750 (53.1250) lr 4.8943e-05 eta 0:00:12
epoch [47/50] batch [20/25] time 0.070 (0.128) data 0.000 (0.057) loss 1.7207 (1.8067) acc 53.1250 (53.2812) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [25/25] time 0.071 (0.116) data 0.000 (0.045) loss 2.5078 (1.8635) acc 37.5000 (51.0000) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [5/25] time 0.078 (0.280) data 0.000 (0.207) loss 1.9033 (1.9172) acc 46.8750 (47.5000) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [10/25] time 0.071 (0.176) data 0.000 (0.104) loss 1.8721 (1.7974) acc 50.0000 (51.2500) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.069) loss 2.2246 (1.8193) acc 46.8750 (52.9167) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.052) loss 1.6885 (1.8158) acc 43.7500 (52.0312) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/25] time 0.074 (0.113) data 0.000 (0.042) loss 1.8135 (1.8143) acc 62.5000 (53.1250) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [5/25] time 0.071 (0.266) data 0.000 (0.195) loss 1.9941 (1.7049) acc 46.8750 (56.2500) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [10/25] time 0.074 (0.169) data 0.000 (0.097) loss 2.2441 (1.7769) acc 37.5000 (54.0625) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/25] time 0.074 (0.137) data 0.000 (0.065) loss 1.5713 (1.7834) acc 68.7500 (53.9583) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.049) loss 1.7070 (1.7684) acc 62.5000 (53.9062) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.071 (0.111) data 0.000 (0.039) loss 1.7959 (1.7766) acc 62.5000 (53.8750) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.071 (0.311) data 0.001 (0.238) loss 1.9502 (1.8660) acc 40.6250 (46.8750) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [10/25] time 0.072 (0.192) data 0.000 (0.119) loss 1.8516 (1.8415) acc 50.0000 (50.0000) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.070 (0.152) data 0.000 (0.080) loss 1.7549 (1.8165) acc 65.6250 (52.5000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.071 (0.132) data 0.000 (0.060) loss 2.0352 (1.7882) acc 46.8750 (54.5312) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.071 (0.119) data 0.000 (0.048) loss 1.4619 (1.7720) acc 65.6250 (55.6250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.31s/it] 50%|█████     | 2/4 [00:10<00:09,  4.58s/it] 75%|███████▌  | 3/4 [00:11<00:02,  2.75s/it]100%|██████████| 4/4 [00:11<00:00,  1.76s/it]100%|██████████| 4/4 [00:11<00:00,  2.95s/it]
=> result
* total: 1,666
* correct: 709
* accuracy: 42.6%
* error: 57.4%
* macro_f1: 40.5%
Elapsed: 0:02:44
Run this job and save the output to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/25] time 0.078 (0.384) data 0.001 (0.294) loss 3.8027 (3.9926) acc 21.8750 (18.7500) lr 1.0000e-05 eta 0:07:58
epoch [1/50] batch [10/25] time 0.070 (0.228) data 0.000 (0.147) loss 3.9980 (3.9813) acc 9.3750 (19.0625) lr 1.0000e-05 eta 0:04:43
epoch [1/50] batch [15/25] time 0.070 (0.183) data 0.000 (0.105) loss 4.1953 (3.9841) acc 15.6250 (21.4583) lr 1.0000e-05 eta 0:03:45
epoch [1/50] batch [20/25] time 0.070 (0.156) data 0.000 (0.080) loss 4.2695 (3.9945) acc 9.3750 (19.5312) lr 1.0000e-05 eta 0:03:11
epoch [1/50] batch [25/25] time 0.070 (0.139) data 0.000 (0.064) loss 3.9824 (3.9876) acc 21.8750 (19.5000) lr 2.0000e-03 eta 0:02:50
epoch [2/50] batch [5/25] time 0.079 (0.297) data 0.000 (0.223) loss 3.1230 (3.3418) acc 21.8750 (16.8750) lr 2.0000e-03 eta 0:06:02
epoch [2/50] batch [10/25] time 0.071 (0.186) data 0.000 (0.112) loss 2.7129 (3.0953) acc 18.7500 (19.3750) lr 2.0000e-03 eta 0:03:46
epoch [2/50] batch [15/25] time 0.069 (0.148) data 0.000 (0.075) loss 2.3828 (2.9087) acc 28.1250 (23.7500) lr 2.0000e-03 eta 0:02:58
epoch [2/50] batch [20/25] time 0.069 (0.128) data 0.000 (0.056) loss 2.7461 (2.8114) acc 25.0000 (25.3125) lr 2.0000e-03 eta 0:02:34
epoch [2/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.045) loss 2.7441 (2.7672) acc 18.7500 (26.1250) lr 1.9980e-03 eta 0:02:20
epoch [3/50] batch [5/25] time 0.070 (0.283) data 0.000 (0.212) loss 2.4395 (2.5922) acc 25.0000 (26.8750) lr 1.9980e-03 eta 0:05:37
epoch [3/50] batch [10/25] time 0.070 (0.177) data 0.000 (0.106) loss 2.1797 (2.5029) acc 40.6250 (28.4375) lr 1.9980e-03 eta 0:03:30
epoch [3/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.071) loss 2.4258 (2.4737) acc 28.1250 (28.3333) lr 1.9980e-03 eta 0:02:48
epoch [3/50] batch [20/25] time 0.069 (0.124) data 0.000 (0.053) loss 2.2988 (2.4538) acc 37.5000 (28.9062) lr 1.9980e-03 eta 0:02:26
epoch [3/50] batch [25/25] time 0.069 (0.113) data 0.000 (0.043) loss 2.4004 (2.4223) acc 21.8750 (28.7500) lr 1.9921e-03 eta 0:02:12
epoch [4/50] batch [5/25] time 0.071 (0.288) data 0.000 (0.217) loss 2.0820 (2.2863) acc 40.6250 (30.6250) lr 1.9921e-03 eta 0:05:36
epoch [4/50] batch [10/25] time 0.070 (0.181) data 0.000 (0.108) loss 2.3281 (2.3178) acc 46.8750 (32.5000) lr 1.9921e-03 eta 0:03:30
epoch [4/50] batch [15/25] time 0.070 (0.144) data 0.000 (0.072) loss 1.8984 (2.3302) acc 56.2500 (31.6667) lr 1.9921e-03 eta 0:02:46
epoch [4/50] batch [20/25] time 0.072 (0.126) data 0.000 (0.054) loss 2.6777 (2.3340) acc 25.0000 (33.7500) lr 1.9921e-03 eta 0:02:25
epoch [4/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.043) loss 2.9297 (2.3412) acc 15.6250 (32.1250) lr 1.9823e-03 eta 0:02:11
epoch [5/50] batch [5/25] time 0.070 (0.283) data 0.000 (0.213) loss 2.5059 (2.2348) acc 37.5000 (36.2500) lr 1.9823e-03 eta 0:05:24
epoch [5/50] batch [10/25] time 0.070 (0.177) data 0.000 (0.107) loss 2.4121 (2.2938) acc 18.7500 (30.9375) lr 1.9823e-03 eta 0:03:21
epoch [5/50] batch [15/25] time 0.069 (0.141) data 0.000 (0.071) loss 2.4609 (2.2601) acc 28.1250 (32.0833) lr 1.9823e-03 eta 0:02:40
epoch [5/50] batch [20/25] time 0.069 (0.123) data 0.000 (0.053) loss 2.5410 (2.2843) acc 25.0000 (32.1875) lr 1.9823e-03 eta 0:02:19
epoch [5/50] batch [25/25] time 0.069 (0.112) data 0.000 (0.043) loss 2.2715 (2.2788) acc 28.1250 (33.2500) lr 1.9686e-03 eta 0:02:06
epoch [6/50] batch [5/25] time 0.073 (0.306) data 0.000 (0.235) loss 2.1582 (2.2273) acc 37.5000 (37.5000) lr 1.9686e-03 eta 0:05:42
epoch [6/50] batch [10/25] time 0.070 (0.188) data 0.000 (0.118) loss 2.2520 (2.2088) acc 28.1250 (35.6250) lr 1.9686e-03 eta 0:03:29
epoch [6/50] batch [15/25] time 0.070 (0.149) data 0.000 (0.079) loss 2.1680 (2.2738) acc 37.5000 (33.7500) lr 1.9686e-03 eta 0:02:45
epoch [6/50] batch [20/25] time 0.071 (0.129) data 0.000 (0.059) loss 1.9453 (2.2041) acc 43.7500 (35.4688) lr 1.9686e-03 eta 0:02:22
epoch [6/50] batch [25/25] time 0.070 (0.118) data 0.000 (0.047) loss 2.4570 (2.2227) acc 31.2500 (35.1250) lr 1.9511e-03 eta 0:02:09
epoch [7/50] batch [5/25] time 0.072 (0.293) data 0.000 (0.222) loss 2.5410 (2.2945) acc 34.3750 (34.3750) lr 1.9511e-03 eta 0:05:21
epoch [7/50] batch [10/25] time 0.071 (0.182) data 0.000 (0.111) loss 2.2617 (2.3043) acc 34.3750 (34.6875) lr 1.9511e-03 eta 0:03:18
epoch [7/50] batch [15/25] time 0.072 (0.145) data 0.000 (0.074) loss 2.2012 (2.3060) acc 40.6250 (34.5833) lr 1.9511e-03 eta 0:02:37
epoch [7/50] batch [20/25] time 0.070 (0.126) data 0.000 (0.056) loss 1.9795 (2.2669) acc 50.0000 (35.9375) lr 1.9511e-03 eta 0:02:16
epoch [7/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 2.2617 (2.2647) acc 34.3750 (35.8750) lr 1.9298e-03 eta 0:02:03
epoch [8/50] batch [5/25] time 0.070 (0.295) data 0.000 (0.224) loss 2.4883 (2.1221) acc 15.6250 (34.3750) lr 1.9298e-03 eta 0:05:15
epoch [8/50] batch [10/25] time 0.070 (0.183) data 0.000 (0.112) loss 2.0996 (2.1794) acc 43.7500 (35.6250) lr 1.9298e-03 eta 0:03:14
epoch [8/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.075) loss 1.9863 (2.2003) acc 40.6250 (34.5833) lr 1.9298e-03 eta 0:02:33
epoch [8/50] batch [20/25] time 0.071 (0.126) data 0.000 (0.056) loss 2.1172 (2.1962) acc 46.8750 (35.0000) lr 1.9298e-03 eta 0:02:13
epoch [8/50] batch [25/25] time 0.072 (0.115) data 0.000 (0.045) loss 2.0723 (2.2082) acc 43.7500 (34.7500) lr 1.9048e-03 eta 0:02:00
epoch [9/50] batch [5/25] time 0.070 (0.282) data 0.000 (0.212) loss 2.3164 (2.1939) acc 40.6250 (36.2500) lr 1.9048e-03 eta 0:04:54
epoch [9/50] batch [10/25] time 0.070 (0.177) data 0.000 (0.106) loss 2.2656 (2.1931) acc 34.3750 (34.3750) lr 1.9048e-03 eta 0:03:03
epoch [9/50] batch [15/25] time 0.069 (0.142) data 0.000 (0.071) loss 2.0957 (2.1434) acc 31.2500 (36.2500) lr 1.9048e-03 eta 0:02:26
epoch [9/50] batch [20/25] time 0.069 (0.124) data 0.000 (0.053) loss 1.8887 (2.1660) acc 31.2500 (35.3125) lr 1.9048e-03 eta 0:02:07
epoch [9/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.043) loss 2.4746 (2.1868) acc 25.0000 (35.3750) lr 1.8763e-03 eta 0:01:55
epoch [10/50] batch [5/25] time 0.070 (0.283) data 0.000 (0.211) loss 2.1797 (2.1480) acc 31.2500 (34.3750) lr 1.8763e-03 eta 0:04:48
epoch [10/50] batch [10/25] time 0.072 (0.178) data 0.000 (0.106) loss 2.0195 (2.1750) acc 50.0000 (36.8750) lr 1.8763e-03 eta 0:03:00
epoch [10/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.071) loss 1.7939 (2.1667) acc 46.8750 (37.9167) lr 1.8763e-03 eta 0:02:23
epoch [10/50] batch [20/25] time 0.072 (0.124) data 0.000 (0.053) loss 2.2051 (2.1944) acc 28.1250 (36.5625) lr 1.8763e-03 eta 0:02:04
epoch [10/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 2.3574 (2.1791) acc 31.2500 (36.5000) lr 1.8443e-03 eta 0:01:53
epoch [11/50] batch [5/25] time 0.070 (0.295) data 0.000 (0.225) loss 2.2324 (2.1715) acc 31.2500 (36.8750) lr 1.8443e-03 eta 0:04:53
epoch [11/50] batch [10/25] time 0.070 (0.183) data 0.000 (0.113) loss 1.9258 (2.1213) acc 43.7500 (36.5625) lr 1.8443e-03 eta 0:03:00
epoch [11/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.075) loss 2.0625 (2.1371) acc 40.6250 (36.6667) lr 1.8443e-03 eta 0:02:23
epoch [11/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.057) loss 1.9004 (2.1438) acc 43.7500 (35.7812) lr 1.8443e-03 eta 0:02:04
epoch [11/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 2.3008 (2.1404) acc 34.3750 (35.7500) lr 1.8090e-03 eta 0:01:52
epoch [12/50] batch [5/25] time 0.076 (0.302) data 0.000 (0.229) loss 1.9482 (2.1490) acc 46.8750 (36.8750) lr 1.8090e-03 eta 0:04:52
epoch [12/50] batch [10/25] time 0.071 (0.187) data 0.000 (0.115) loss 1.8086 (2.1544) acc 53.1250 (39.0625) lr 1.8090e-03 eta 0:03:00
epoch [12/50] batch [15/25] time 0.069 (0.148) data 0.000 (0.077) loss 1.7217 (2.1521) acc 62.5000 (38.7500) lr 1.8090e-03 eta 0:02:22
epoch [12/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.058) loss 2.1836 (2.1472) acc 37.5000 (37.9688) lr 1.8090e-03 eta 0:02:02
epoch [12/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.046) loss 2.2227 (2.1515) acc 34.3750 (37.3750) lr 1.7705e-03 eta 0:01:51
epoch [13/50] batch [5/25] time 0.070 (0.292) data 0.000 (0.222) loss 2.2168 (2.0832) acc 34.3750 (39.3750) lr 1.7705e-03 eta 0:04:36
epoch [13/50] batch [10/25] time 0.071 (0.182) data 0.000 (0.111) loss 1.8018 (2.0983) acc 53.1250 (40.0000) lr 1.7705e-03 eta 0:02:51
epoch [13/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.074) loss 2.3711 (2.1239) acc 31.2500 (39.7917) lr 1.7705e-03 eta 0:02:16
epoch [13/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.056) loss 2.1738 (2.1269) acc 40.6250 (38.7500) lr 1.7705e-03 eta 0:01:57
epoch [13/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.045) loss 2.2539 (2.1321) acc 34.3750 (37.8750) lr 1.7290e-03 eta 0:01:46
epoch [14/50] batch [5/25] time 0.072 (0.305) data 0.000 (0.234) loss 1.9707 (2.1943) acc 46.8750 (37.5000) lr 1.7290e-03 eta 0:04:40
epoch [14/50] batch [10/25] time 0.070 (0.189) data 0.000 (0.117) loss 2.0762 (2.1299) acc 56.2500 (40.6250) lr 1.7290e-03 eta 0:02:52
epoch [14/50] batch [15/25] time 0.074 (0.150) data 0.004 (0.078) loss 2.0508 (2.1331) acc 40.6250 (39.3750) lr 1.7290e-03 eta 0:02:16
epoch [14/50] batch [20/25] time 0.070 (0.130) data 0.000 (0.059) loss 2.2754 (2.1507) acc 46.8750 (38.4375) lr 1.7290e-03 eta 0:01:57
epoch [14/50] batch [25/25] time 0.073 (0.118) data 0.000 (0.047) loss 2.5312 (2.1514) acc 25.0000 (38.5000) lr 1.6845e-03 eta 0:01:46
epoch [15/50] batch [5/25] time 0.071 (0.286) data 0.000 (0.214) loss 2.0215 (2.1674) acc 43.7500 (41.2500) lr 1.6845e-03 eta 0:04:15
epoch [15/50] batch [10/25] time 0.070 (0.179) data 0.000 (0.107) loss 1.5732 (2.1391) acc 56.2500 (39.6875) lr 1.6845e-03 eta 0:02:39
epoch [15/50] batch [15/25] time 0.071 (0.143) data 0.000 (0.072) loss 2.5410 (2.1754) acc 25.0000 (37.9167) lr 1.6845e-03 eta 0:02:06
epoch [15/50] batch [20/25] time 0.074 (0.125) data 0.000 (0.054) loss 2.0195 (2.1539) acc 53.1250 (37.8125) lr 1.6845e-03 eta 0:01:49
epoch [15/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 2.0312 (2.1384) acc 46.8750 (38.5000) lr 1.6374e-03 eta 0:01:39
epoch [16/50] batch [5/25] time 0.072 (0.293) data 0.000 (0.222) loss 2.2793 (2.1055) acc 40.6250 (38.1250) lr 1.6374e-03 eta 0:04:15
epoch [16/50] batch [10/25] time 0.071 (0.182) data 0.000 (0.111) loss 1.5088 (2.0763) acc 65.6250 (43.4375) lr 1.6374e-03 eta 0:02:37
epoch [16/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.074) loss 1.7744 (2.0506) acc 53.1250 (44.3750) lr 1.6374e-03 eta 0:02:05
epoch [16/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.056) loss 1.9238 (2.0879) acc 37.5000 (42.5000) lr 1.6374e-03 eta 0:01:48
epoch [16/50] batch [25/25] time 0.070 (0.116) data 0.000 (0.045) loss 1.9043 (2.0930) acc 50.0000 (41.8750) lr 1.5878e-03 eta 0:01:38
epoch [17/50] batch [5/25] time 0.070 (0.292) data 0.000 (0.219) loss 2.1699 (2.0631) acc 40.6250 (41.8750) lr 1.5878e-03 eta 0:04:06
epoch [17/50] batch [10/25] time 0.070 (0.182) data 0.000 (0.110) loss 1.7246 (2.1044) acc 46.8750 (40.0000) lr 1.5878e-03 eta 0:02:33
epoch [17/50] batch [15/25] time 0.070 (0.145) data 0.000 (0.073) loss 2.1992 (2.1242) acc 25.0000 (39.5833) lr 1.5878e-03 eta 0:02:01
epoch [17/50] batch [20/25] time 0.069 (0.126) data 0.000 (0.055) loss 2.0742 (2.1310) acc 50.0000 (40.6250) lr 1.5878e-03 eta 0:01:44
epoch [17/50] batch [25/25] time 0.070 (0.115) data 0.000 (0.044) loss 2.2715 (2.1132) acc 43.7500 (40.7500) lr 1.5358e-03 eta 0:01:34
epoch [18/50] batch [5/25] time 0.070 (0.292) data 0.000 (0.221) loss 1.9785 (2.0547) acc 37.5000 (38.7500) lr 1.5358e-03 eta 0:03:59
epoch [18/50] batch [10/25] time 0.071 (0.182) data 0.000 (0.111) loss 1.9941 (2.0507) acc 37.5000 (37.5000) lr 1.5358e-03 eta 0:02:28
epoch [18/50] batch [15/25] time 0.071 (0.145) data 0.000 (0.074) loss 2.0391 (2.0491) acc 46.8750 (38.5417) lr 1.5358e-03 eta 0:01:57
epoch [18/50] batch [20/25] time 0.071 (0.127) data 0.000 (0.056) loss 1.9541 (2.0590) acc 40.6250 (38.1250) lr 1.5358e-03 eta 0:01:41
epoch [18/50] batch [25/25] time 0.071 (0.115) data 0.000 (0.045) loss 2.0352 (2.0493) acc 43.7500 (39.1250) lr 1.4818e-03 eta 0:01:32
epoch [19/50] batch [5/25] time 0.078 (0.301) data 0.000 (0.229) loss 1.9053 (1.9203) acc 46.8750 (41.2500) lr 1.4818e-03 eta 0:03:59
epoch [19/50] batch [10/25] time 0.073 (0.186) data 0.000 (0.115) loss 2.0508 (2.0170) acc 31.2500 (40.6250) lr 1.4818e-03 eta 0:02:27
epoch [19/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.077) loss 2.2559 (2.0382) acc 34.3750 (40.8333) lr 1.4818e-03 eta 0:01:55
epoch [19/50] batch [20/25] time 0.072 (0.128) data 0.000 (0.058) loss 2.0742 (2.0279) acc 37.5000 (42.9688) lr 1.4818e-03 eta 0:01:40
epoch [19/50] batch [25/25] time 0.071 (0.117) data 0.000 (0.046) loss 2.3672 (2.0610) acc 37.5000 (41.7500) lr 1.4258e-03 eta 0:01:30
epoch [20/50] batch [5/25] time 0.070 (0.287) data 0.000 (0.216) loss 1.7695 (2.0838) acc 56.2500 (45.0000) lr 1.4258e-03 eta 0:03:40
epoch [20/50] batch [10/25] time 0.070 (0.180) data 0.000 (0.108) loss 2.0605 (2.0543) acc 46.8750 (43.7500) lr 1.4258e-03 eta 0:02:17
epoch [20/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.2070 (2.0590) acc 31.2500 (42.7083) lr 1.4258e-03 eta 0:01:49
epoch [20/50] batch [20/25] time 0.072 (0.125) data 0.000 (0.054) loss 2.0078 (2.0429) acc 37.5000 (42.3438) lr 1.4258e-03 eta 0:01:34
epoch [20/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 2.0586 (2.0401) acc 43.7500 (43.3750) lr 1.3681e-03 eta 0:01:25
epoch [21/50] batch [5/25] time 0.070 (0.299) data 0.000 (0.227) loss 1.8232 (2.0111) acc 56.2500 (43.1250) lr 1.3681e-03 eta 0:03:42
epoch [21/50] batch [10/25] time 0.070 (0.185) data 0.000 (0.114) loss 2.2695 (2.0445) acc 43.7500 (43.7500) lr 1.3681e-03 eta 0:02:17
epoch [21/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.076) loss 2.1934 (2.0008) acc 40.6250 (45.4167) lr 1.3681e-03 eta 0:01:48
epoch [21/50] batch [20/25] time 0.070 (0.129) data 0.000 (0.057) loss 1.8604 (2.0016) acc 59.3750 (44.5312) lr 1.3681e-03 eta 0:01:33
epoch [21/50] batch [25/25] time 0.071 (0.117) data 0.000 (0.046) loss 2.0391 (2.0142) acc 34.3750 (43.8750) lr 1.3090e-03 eta 0:01:24
epoch [22/50] batch [5/25] time 0.070 (0.299) data 0.000 (0.227) loss 2.1289 (2.0629) acc 37.5000 (41.8750) lr 1.3090e-03 eta 0:03:35
epoch [22/50] batch [10/25] time 0.075 (0.187) data 0.000 (0.114) loss 1.9014 (2.0323) acc 43.7500 (42.5000) lr 1.3090e-03 eta 0:02:13
epoch [22/50] batch [15/25] time 0.070 (0.148) data 0.000 (0.076) loss 2.0801 (2.0148) acc 31.2500 (43.1250) lr 1.3090e-03 eta 0:01:45
epoch [22/50] batch [20/25] time 0.071 (0.129) data 0.000 (0.057) loss 1.9199 (2.0103) acc 53.1250 (44.5312) lr 1.3090e-03 eta 0:01:30
epoch [22/50] batch [25/25] time 0.070 (0.117) data 0.000 (0.046) loss 1.9580 (1.9968) acc 34.3750 (44.6250) lr 1.2487e-03 eta 0:01:22
epoch [23/50] batch [5/25] time 0.070 (0.286) data 0.000 (0.217) loss 1.9160 (1.9971) acc 37.5000 (39.3750) lr 1.2487e-03 eta 0:03:19
epoch [23/50] batch [10/25] time 0.071 (0.179) data 0.000 (0.109) loss 2.1719 (1.9818) acc 28.1250 (42.5000) lr 1.2487e-03 eta 0:02:03
epoch [23/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 1.6738 (1.9611) acc 59.3750 (43.9583) lr 1.2487e-03 eta 0:01:37
epoch [23/50] batch [20/25] time 0.071 (0.125) data 0.000 (0.054) loss 2.0508 (1.9677) acc 34.3750 (43.9062) lr 1.2487e-03 eta 0:01:24
epoch [23/50] batch [25/25] time 0.071 (0.114) data 0.000 (0.044) loss 2.0664 (1.9692) acc 46.8750 (44.0000) lr 1.1874e-03 eta 0:01:16
epoch [24/50] batch [5/25] time 0.070 (0.292) data 0.000 (0.220) loss 1.7354 (2.0342) acc 53.1250 (45.0000) lr 1.1874e-03 eta 0:03:15
epoch [24/50] batch [10/25] time 0.072 (0.182) data 0.000 (0.110) loss 2.2070 (2.0383) acc 43.7500 (44.6875) lr 1.1874e-03 eta 0:02:01
epoch [24/50] batch [15/25] time 0.071 (0.145) data 0.000 (0.073) loss 1.9072 (2.0314) acc 53.1250 (45.0000) lr 1.1874e-03 eta 0:01:35
epoch [24/50] batch [20/25] time 0.071 (0.126) data 0.000 (0.055) loss 1.9814 (2.0225) acc 43.7500 (46.2500) lr 1.1874e-03 eta 0:01:22
epoch [24/50] batch [25/25] time 0.071 (0.116) data 0.001 (0.044) loss 1.9180 (2.0222) acc 34.3750 (45.3750) lr 1.1253e-03 eta 0:01:15
epoch [25/50] batch [5/25] time 0.077 (0.317) data 0.000 (0.243) loss 1.8828 (1.9453) acc 53.1250 (46.8750) lr 1.1253e-03 eta 0:03:24
epoch [25/50] batch [10/25] time 0.070 (0.195) data 0.000 (0.122) loss 1.8330 (1.9471) acc 56.2500 (46.8750) lr 1.1253e-03 eta 0:02:04
epoch [25/50] batch [15/25] time 0.070 (0.154) data 0.000 (0.081) loss 1.6367 (1.9456) acc 59.3750 (47.9167) lr 1.1253e-03 eta 0:01:37
epoch [25/50] batch [20/25] time 0.071 (0.133) data 0.000 (0.061) loss 2.2656 (1.9855) acc 43.7500 (46.8750) lr 1.1253e-03 eta 0:01:23
epoch [25/50] batch [25/25] time 0.070 (0.121) data 0.000 (0.049) loss 1.9111 (1.9771) acc 50.0000 (47.1250) lr 1.0628e-03 eta 0:01:15
epoch [26/50] batch [5/25] time 0.071 (0.287) data 0.001 (0.216) loss 2.2598 (1.9729) acc 37.5000 (51.2500) lr 1.0628e-03 eta 0:02:58
epoch [26/50] batch [10/25] time 0.071 (0.179) data 0.000 (0.108) loss 1.8984 (2.0285) acc 40.6250 (46.2500) lr 1.0628e-03 eta 0:01:50
epoch [26/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.0293 (1.9887) acc 40.6250 (45.4167) lr 1.0628e-03 eta 0:01:27
epoch [26/50] batch [20/25] time 0.070 (0.125) data 0.000 (0.054) loss 1.9688 (1.9831) acc 40.6250 (46.2500) lr 1.0628e-03 eta 0:01:15
epoch [26/50] batch [25/25] time 0.069 (0.114) data 0.000 (0.043) loss 1.6211 (1.9537) acc 59.3750 (47.0000) lr 1.0000e-03 eta 0:01:08
epoch [27/50] batch [5/25] time 0.070 (0.296) data 0.000 (0.226) loss 2.1191 (1.9771) acc 37.5000 (45.6250) lr 1.0000e-03 eta 0:02:56
epoch [27/50] batch [10/25] time 0.070 (0.183) data 0.000 (0.113) loss 2.0742 (1.9379) acc 37.5000 (47.8125) lr 1.0000e-03 eta 0:01:48
epoch [27/50] batch [15/25] time 0.070 (0.146) data 0.000 (0.076) loss 1.6094 (1.9242) acc 59.3750 (47.2917) lr 1.0000e-03 eta 0:01:25
epoch [27/50] batch [20/25] time 0.070 (0.127) data 0.000 (0.057) loss 2.0430 (1.9555) acc 37.5000 (46.5625) lr 1.0000e-03 eta 0:01:13
epoch [27/50] batch [25/25] time 0.073 (0.116) data 0.000 (0.045) loss 2.3027 (1.9762) acc 31.2500 (45.0000) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [5/25] time 0.070 (0.275) data 0.000 (0.204) loss 2.0469 (2.0432) acc 34.3750 (43.1250) lr 9.3721e-04 eta 0:02:36
epoch [28/50] batch [10/25] time 0.071 (0.173) data 0.000 (0.103) loss 1.5293 (1.9359) acc 56.2500 (46.5625) lr 9.3721e-04 eta 0:01:37
epoch [28/50] batch [15/25] time 0.074 (0.139) data 0.000 (0.069) loss 2.0312 (1.9674) acc 37.5000 (46.2500) lr 9.3721e-04 eta 0:01:18
epoch [28/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 1.6973 (1.9880) acc 53.1250 (45.9375) lr 9.3721e-04 eta 0:01:07
epoch [28/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.9160 (1.9809) acc 43.7500 (45.5000) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [5/25] time 0.072 (0.288) data 0.000 (0.217) loss 1.9053 (1.9209) acc 56.2500 (46.2500) lr 8.7467e-04 eta 0:02:37
epoch [29/50] batch [10/25] time 0.069 (0.179) data 0.000 (0.109) loss 1.3896 (1.9271) acc 56.2500 (47.1875) lr 8.7467e-04 eta 0:01:36
epoch [29/50] batch [15/25] time 0.069 (0.143) data 0.000 (0.073) loss 1.7568 (1.9595) acc 40.6250 (45.6250) lr 8.7467e-04 eta 0:01:16
epoch [29/50] batch [20/25] time 0.069 (0.124) data 0.000 (0.054) loss 1.7666 (1.9672) acc 46.8750 (45.3125) lr 8.7467e-04 eta 0:01:05
epoch [29/50] batch [25/25] time 0.069 (0.113) data 0.000 (0.044) loss 1.8682 (1.9822) acc 56.2500 (45.6250) lr 8.1262e-04 eta 0:00:59
epoch [30/50] batch [5/25] time 0.071 (0.272) data 0.000 (0.201) loss 2.2930 (2.0215) acc 40.6250 (54.3750) lr 8.1262e-04 eta 0:02:21
epoch [30/50] batch [10/25] time 0.071 (0.172) data 0.000 (0.101) loss 1.9551 (1.9718) acc 53.1250 (50.9375) lr 8.1262e-04 eta 0:01:28
epoch [30/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.067) loss 2.1719 (1.9372) acc 50.0000 (50.4167) lr 8.1262e-04 eta 0:01:10
epoch [30/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.050) loss 2.0742 (1.9643) acc 34.3750 (47.6562) lr 8.1262e-04 eta 0:01:01
epoch [30/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 2.0234 (1.9507) acc 43.7500 (47.7500) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [5/25] time 0.073 (0.271) data 0.000 (0.198) loss 2.0430 (2.0602) acc 37.5000 (48.1250) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [10/25] time 0.070 (0.172) data 0.000 (0.100) loss 1.9941 (1.9689) acc 46.8750 (48.4375) lr 7.5131e-04 eta 0:01:24
epoch [31/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.067) loss 1.7510 (1.8969) acc 53.1250 (49.7917) lr 7.5131e-04 eta 0:01:07
epoch [31/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.050) loss 1.9688 (1.8979) acc 65.6250 (51.5625) lr 7.5131e-04 eta 0:00:58
epoch [31/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 2.0508 (1.9152) acc 50.0000 (51.2500) lr 6.9098e-04 eta 0:00:52
epoch [32/50] batch [5/25] time 0.073 (0.277) data 0.000 (0.206) loss 1.6719 (1.9469) acc 53.1250 (46.2500) lr 6.9098e-04 eta 0:02:10
epoch [32/50] batch [10/25] time 0.070 (0.174) data 0.000 (0.103) loss 1.8291 (1.8762) acc 56.2500 (47.5000) lr 6.9098e-04 eta 0:01:20
epoch [32/50] batch [15/25] time 0.070 (0.140) data 0.000 (0.069) loss 1.7979 (1.8412) acc 46.8750 (49.1667) lr 6.9098e-04 eta 0:01:04
epoch [32/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.052) loss 1.6758 (1.8804) acc 59.3750 (48.4375) lr 6.9098e-04 eta 0:00:55
epoch [32/50] batch [25/25] time 0.074 (0.112) data 0.000 (0.041) loss 1.7578 (1.8910) acc 53.1250 (48.0000) lr 6.3188e-04 eta 0:00:50
epoch [33/50] batch [5/25] time 0.070 (0.266) data 0.000 (0.196) loss 2.0039 (1.8594) acc 46.8750 (55.0000) lr 6.3188e-04 eta 0:01:58
epoch [33/50] batch [10/25] time 0.070 (0.169) data 0.000 (0.098) loss 1.7246 (1.8952) acc 50.0000 (51.2500) lr 6.3188e-04 eta 0:01:14
epoch [33/50] batch [15/25] time 0.069 (0.136) data 0.000 (0.066) loss 1.8027 (1.8880) acc 62.5000 (51.0417) lr 6.3188e-04 eta 0:00:59
epoch [33/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.049) loss 1.9062 (1.8712) acc 59.3750 (52.1875) lr 6.3188e-04 eta 0:00:51
epoch [33/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.039) loss 2.0430 (1.8734) acc 37.5000 (51.8750) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [5/25] time 0.071 (0.272) data 0.000 (0.202) loss 2.1719 (1.9195) acc 43.7500 (50.6250) lr 5.7422e-04 eta 0:01:54
epoch [34/50] batch [10/25] time 0.070 (0.171) data 0.000 (0.101) loss 2.1973 (1.9192) acc 31.2500 (49.3750) lr 5.7422e-04 eta 0:01:11
epoch [34/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.067) loss 1.8604 (1.9161) acc 46.8750 (50.4167) lr 5.7422e-04 eta 0:00:56
epoch [34/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.051) loss 1.9326 (1.8982) acc 53.1250 (50.1562) lr 5.7422e-04 eta 0:00:49
epoch [34/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.7861 (1.8881) acc 56.2500 (50.6250) lr 5.1825e-04 eta 0:00:44
epoch [35/50] batch [5/25] time 0.071 (0.274) data 0.000 (0.204) loss 2.1445 (1.9549) acc 31.2500 (46.2500) lr 5.1825e-04 eta 0:01:48
epoch [35/50] batch [10/25] time 0.071 (0.173) data 0.000 (0.102) loss 1.5957 (1.9096) acc 59.3750 (46.2500) lr 5.1825e-04 eta 0:01:07
epoch [35/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 1.6182 (1.9251) acc 62.5000 (47.2917) lr 5.1825e-04 eta 0:00:53
epoch [35/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 1.9492 (1.9057) acc 53.1250 (48.9062) lr 5.1825e-04 eta 0:00:46
epoch [35/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 2.1211 (1.8984) acc 37.5000 (49.2500) lr 4.6417e-04 eta 0:00:41
epoch [36/50] batch [5/25] time 0.071 (0.280) data 0.000 (0.209) loss 1.8252 (1.8482) acc 50.0000 (50.6250) lr 4.6417e-04 eta 0:01:43
epoch [36/50] batch [10/25] time 0.070 (0.176) data 0.000 (0.105) loss 1.7861 (1.8398) acc 50.0000 (51.2500) lr 4.6417e-04 eta 0:01:04
epoch [36/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.070) loss 2.1641 (1.8445) acc 31.2500 (50.4167) lr 4.6417e-04 eta 0:00:50
epoch [36/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.053) loss 1.8076 (1.8581) acc 46.8750 (50.0000) lr 4.6417e-04 eta 0:00:43
epoch [36/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 1.6592 (1.8671) acc 46.8750 (49.7500) lr 4.1221e-04 eta 0:00:39
epoch [37/50] batch [5/25] time 0.070 (0.262) data 0.000 (0.191) loss 1.9980 (1.8100) acc 37.5000 (52.5000) lr 4.1221e-04 eta 0:01:30
epoch [37/50] batch [10/25] time 0.077 (0.168) data 0.000 (0.096) loss 1.9463 (1.7765) acc 37.5000 (51.8750) lr 4.1221e-04 eta 0:00:57
epoch [37/50] batch [15/25] time 0.073 (0.136) data 0.000 (0.064) loss 2.0312 (1.8113) acc 43.7500 (52.7083) lr 4.1221e-04 eta 0:00:45
epoch [37/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.048) loss 1.8594 (1.8451) acc 59.3750 (51.2500) lr 4.1221e-04 eta 0:00:39
epoch [37/50] batch [25/25] time 0.070 (0.110) data 0.000 (0.038) loss 2.0742 (1.8443) acc 40.6250 (50.6250) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [5/25] time 0.071 (0.280) data 0.000 (0.210) loss 1.8848 (1.7549) acc 43.7500 (51.8750) lr 3.6258e-04 eta 0:01:29
epoch [38/50] batch [10/25] time 0.070 (0.175) data 0.000 (0.105) loss 2.0898 (1.8179) acc 46.8750 (53.1250) lr 3.6258e-04 eta 0:00:55
epoch [38/50] batch [15/25] time 0.070 (0.141) data 0.000 (0.070) loss 1.8945 (1.8732) acc 37.5000 (48.3333) lr 3.6258e-04 eta 0:00:43
epoch [38/50] batch [20/25] time 0.070 (0.123) data 0.000 (0.053) loss 1.8633 (1.8709) acc 56.2500 (49.0625) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 1.5918 (1.8712) acc 62.5000 (49.8750) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [5/25] time 0.072 (0.285) data 0.000 (0.214) loss 1.7754 (1.7236) acc 68.7500 (60.6250) lr 3.1545e-04 eta 0:01:24
epoch [39/50] batch [10/25] time 0.071 (0.178) data 0.000 (0.107) loss 2.0566 (1.8057) acc 28.1250 (54.6875) lr 3.1545e-04 eta 0:00:51
epoch [39/50] batch [15/25] time 0.070 (0.143) data 0.000 (0.072) loss 2.0664 (1.8387) acc 43.7500 (53.3333) lr 3.1545e-04 eta 0:00:40
epoch [39/50] batch [20/25] time 0.072 (0.125) data 0.000 (0.054) loss 2.1172 (1.8574) acc 43.7500 (52.1875) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [25/25] time 0.070 (0.114) data 0.000 (0.043) loss 1.7598 (1.8230) acc 46.8750 (52.8750) lr 2.7103e-04 eta 0:00:31
epoch [40/50] batch [5/25] time 0.072 (0.281) data 0.000 (0.209) loss 1.5840 (1.8637) acc 56.2500 (51.2500) lr 2.7103e-04 eta 0:01:15
epoch [40/50] batch [10/25] time 0.070 (0.176) data 0.000 (0.105) loss 1.9834 (1.7964) acc 46.8750 (52.8125) lr 2.7103e-04 eta 0:00:46
epoch [40/50] batch [15/25] time 0.071 (0.142) data 0.000 (0.070) loss 2.2344 (1.8592) acc 46.8750 (50.6250) lr 2.7103e-04 eta 0:00:36
epoch [40/50] batch [20/25] time 0.071 (0.124) data 0.000 (0.052) loss 1.8857 (1.8406) acc 53.1250 (51.7188) lr 2.7103e-04 eta 0:00:31
epoch [40/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 1.7637 (1.8170) acc 68.7500 (53.7500) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [5/25] time 0.070 (0.281) data 0.000 (0.211) loss 1.7637 (1.7475) acc 56.2500 (58.7500) lr 2.2949e-04 eta 0:01:08
epoch [41/50] batch [10/25] time 0.070 (0.178) data 0.000 (0.105) loss 2.0391 (1.7485) acc 56.2500 (58.1250) lr 2.2949e-04 eta 0:00:42
epoch [41/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.070) loss 1.6826 (1.7747) acc 65.6250 (56.2500) lr 2.2949e-04 eta 0:00:33
epoch [41/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.053) loss 1.8701 (1.7994) acc 59.3750 (54.3750) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.042) loss 1.7783 (1.7968) acc 53.1250 (54.2500) lr 1.9098e-04 eta 0:00:25
epoch [42/50] batch [5/25] time 0.071 (0.296) data 0.000 (0.225) loss 1.9561 (1.7598) acc 43.7500 (54.3750) lr 1.9098e-04 eta 0:01:05
epoch [42/50] batch [10/25] time 0.076 (0.185) data 0.000 (0.113) loss 1.9004 (1.7680) acc 50.0000 (53.4375) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [15/25] time 0.071 (0.148) data 0.000 (0.075) loss 1.7480 (1.7826) acc 53.1250 (53.3333) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [20/25] time 0.071 (0.128) data 0.000 (0.057) loss 1.6943 (1.8228) acc 53.1250 (52.5000) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [25/25] time 0.073 (0.117) data 0.000 (0.045) loss 2.0703 (1.8413) acc 43.7500 (52.8750) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [5/25] time 0.070 (0.264) data 0.000 (0.195) loss 2.0703 (1.7855) acc 50.0000 (55.6250) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [10/25] time 0.070 (0.169) data 0.000 (0.098) loss 1.9570 (1.8674) acc 59.3750 (53.1250) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [15/25] time 0.070 (0.136) data 0.000 (0.065) loss 1.9199 (1.7832) acc 53.1250 (56.4583) lr 1.5567e-04 eta 0:00:25
epoch [43/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.049) loss 1.9355 (1.8208) acc 46.8750 (54.5312) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [25/25] time 0.071 (0.110) data 0.000 (0.039) loss 1.7754 (1.7967) acc 59.3750 (55.1250) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [5/25] time 0.070 (0.274) data 0.000 (0.201) loss 1.4229 (1.8000) acc 68.7500 (56.2500) lr 1.2369e-04 eta 0:00:46
epoch [44/50] batch [10/25] time 0.072 (0.173) data 0.000 (0.101) loss 1.6299 (1.7628) acc 75.0000 (55.3125) lr 1.2369e-04 eta 0:00:28
epoch [44/50] batch [15/25] time 0.069 (0.139) data 0.000 (0.067) loss 1.5137 (1.7552) acc 71.8750 (54.7917) lr 1.2369e-04 eta 0:00:22
epoch [44/50] batch [20/25] time 0.069 (0.122) data 0.000 (0.051) loss 1.9131 (1.7956) acc 50.0000 (53.5938) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.6924 (1.7884) acc 62.5000 (53.7500) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [5/25] time 0.073 (0.271) data 0.000 (0.200) loss 2.0488 (1.9580) acc 31.2500 (46.2500) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [10/25] time 0.070 (0.171) data 0.001 (0.100) loss 1.8604 (1.8687) acc 53.1250 (48.4375) lr 9.5173e-05 eta 0:00:23
epoch [45/50] batch [15/25] time 0.070 (0.138) data 0.000 (0.067) loss 1.6875 (1.8148) acc 53.1250 (51.2500) lr 9.5173e-05 eta 0:00:18
epoch [45/50] batch [20/25] time 0.070 (0.121) data 0.000 (0.050) loss 1.5225 (1.7867) acc 50.0000 (51.4062) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/25] time 0.070 (0.111) data 0.000 (0.040) loss 1.5928 (1.7685) acc 65.6250 (52.6250) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [5/25] time 0.069 (0.274) data 0.000 (0.204) loss 1.8252 (1.7094) acc 40.6250 (53.7500) lr 7.0224e-05 eta 0:00:32
epoch [46/50] batch [10/25] time 0.070 (0.173) data 0.000 (0.102) loss 1.7861 (1.7583) acc 46.8750 (53.4375) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [15/25] time 0.070 (0.139) data 0.000 (0.068) loss 1.3252 (1.7386) acc 68.7500 (55.6250) lr 7.0224e-05 eta 0:00:15
epoch [46/50] batch [20/25] time 0.070 (0.122) data 0.000 (0.051) loss 1.8594 (1.7269) acc 50.0000 (56.5625) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/25] time 0.070 (0.112) data 0.000 (0.041) loss 1.6875 (1.7554) acc 65.6250 (55.8750) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [5/25] time 0.071 (0.284) data 0.000 (0.212) loss 1.9004 (1.6781) acc 43.7500 (55.0000) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [10/25] time 0.070 (0.178) data 0.000 (0.106) loss 1.5908 (1.6244) acc 59.3750 (60.6250) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.071) loss 1.8857 (1.6523) acc 59.3750 (58.5417) lr 4.8943e-05 eta 0:00:12
epoch [47/50] batch [20/25] time 0.072 (0.124) data 0.000 (0.053) loss 1.6807 (1.6942) acc 62.5000 (57.6562) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.043) loss 1.8486 (1.7436) acc 50.0000 (56.0000) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [5/25] time 0.071 (0.284) data 0.000 (0.212) loss 1.8086 (1.8256) acc 59.3750 (55.0000) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [10/25] time 0.071 (0.177) data 0.000 (0.106) loss 2.0078 (1.7992) acc 50.0000 (53.1250) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [15/25] time 0.070 (0.142) data 0.000 (0.071) loss 1.4697 (1.7415) acc 59.3750 (55.8333) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [20/25] time 0.070 (0.124) data 0.000 (0.053) loss 1.4824 (1.7640) acc 75.0000 (55.4688) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/25] time 0.070 (0.113) data 0.000 (0.043) loss 1.6602 (1.7258) acc 56.2500 (56.1250) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [5/25] time 0.071 (0.265) data 0.000 (0.194) loss 1.7715 (1.8150) acc 43.7500 (51.8750) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [10/25] time 0.072 (0.169) data 0.000 (0.097) loss 1.5205 (1.7971) acc 62.5000 (55.3125) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/25] time 0.072 (0.136) data 0.000 (0.065) loss 1.6514 (1.7421) acc 53.1250 (56.2500) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/25] time 0.072 (0.120) data 0.000 (0.049) loss 1.8730 (1.7783) acc 62.5000 (56.8750) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/25] time 0.071 (0.110) data 0.000 (0.039) loss 2.2422 (1.7985) acc 34.3750 (55.5000) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/25] time 0.072 (0.263) data 0.000 (0.192) loss 1.8203 (1.8504) acc 46.8750 (55.0000) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/25] time 0.071 (0.167) data 0.001 (0.096) loss 1.9336 (1.7634) acc 46.8750 (57.1875) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/25] time 0.074 (0.136) data 0.000 (0.064) loss 1.9609 (1.7804) acc 46.8750 (56.4583) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.070 (0.120) data 0.000 (0.048) loss 1.9648 (1.8390) acc 50.0000 (55.1562) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.072 (0.110) data 0.000 (0.039) loss 1.5693 (1.8090) acc 59.3750 (56.0000) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.30s/it] 50%|█████     | 2/4 [00:10<00:09,  4.58s/it] 75%|███████▌  | 3/4 [00:11<00:02,  2.75s/it]100%|██████████| 4/4 [00:11<00:00,  1.76s/it]100%|██████████| 4/4 [00:11<00:00,  2.96s/it]
=> result
* total: 1,666
* correct: 704
* accuracy: 42.3%
* error: 57.7%
* macro_f1: 40.1%
Elapsed: 0:02:41
Run this job and save the output to output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:23<01:09, 23.08s/it] 50%|█████     | 2/4 [00:23<00:19,  9.84s/it] 75%|███████▌  | 3/4 [00:24<00:05,  5.60s/it]100%|██████████| 4/4 [00:24<00:00,  3.49s/it]100%|██████████| 4/4 [00:24<00:00,  6.14s/it]
=> result
* total: 1,667
* correct: 594
* accuracy: 35.6%
* error: 64.4%
* macro_f1: 32.8%
Run this job and save the output to output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:31, 10.58s/it] 50%|█████     | 2/4 [00:11<00:09,  4.69s/it] 75%|███████▌  | 3/4 [00:11<00:02,  2.81s/it]100%|██████████| 4/4 [00:11<00:00,  1.80s/it]100%|██████████| 4/4 [00:12<00:00,  3.02s/it]
=> result
* total: 1,667
* correct: 553
* accuracy: 33.2%
* error: 66.8%
* macro_f1: 29.6%
Run this job and save the output to output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:32, 10.98s/it] 50%|█████     | 2/4 [00:11<00:09,  4.86s/it] 75%|███████▌  | 3/4 [00:12<00:02,  2.90s/it]100%|██████████| 4/4 [00:12<00:00,  1.85s/it]100%|██████████| 4/4 [00:12<00:00,  3.13s/it]
=> result
* total: 1,667
* correct: 599
* accuracy: 35.9%
* error: 64.1%
* macro_f1: 32.5%
Run this job and save the output to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.073 (0.454) data 0.000 (0.365) loss 2.3242 (2.0529) acc 68.7500 (77.5000) lr 1.0000e-05 eta 0:09:48
epoch [1/50] batch [10/26] time 0.072 (0.299) data 0.000 (0.219) loss 2.0176 (2.0671) acc 78.1250 (76.8750) lr 1.0000e-05 eta 0:06:25
epoch [1/50] batch [15/26] time 0.070 (0.223) data 0.000 (0.146) loss 1.7002 (1.9919) acc 84.3750 (78.1250) lr 1.0000e-05 eta 0:04:45
epoch [1/50] batch [20/26] time 0.069 (0.203) data 0.000 (0.129) loss 1.9023 (1.9899) acc 87.5000 (77.3438) lr 1.0000e-05 eta 0:04:20
epoch [1/50] batch [25/26] time 0.274 (0.185) data 0.205 (0.111) loss 1.4688 (1.9527) acc 90.6250 (77.3750) lr 1.0000e-05 eta 0:03:55
epoch [2/50] batch [5/26] time 0.070 (0.270) data 0.000 (0.183) loss 0.9976 (1.1752) acc 78.1250 (82.5000) lr 2.0000e-03 eta 0:05:42
epoch [2/50] batch [10/26] time 0.071 (0.171) data 0.000 (0.092) loss 0.6377 (1.0355) acc 96.8750 (82.8125) lr 2.0000e-03 eta 0:03:35
epoch [2/50] batch [15/26] time 0.070 (0.137) data 0.000 (0.061) loss 0.9824 (0.9580) acc 71.8750 (82.0833) lr 2.0000e-03 eta 0:02:52
epoch [2/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.046) loss 0.9087 (0.9172) acc 81.2500 (82.9688) lr 2.0000e-03 eta 0:02:30
epoch [2/50] batch [25/26] time 0.073 (0.110) data 0.000 (0.037) loss 0.9634 (0.9065) acc 81.2500 (82.6250) lr 2.0000e-03 eta 0:02:17
epoch [3/50] batch [5/26] time 0.071 (0.380) data 0.000 (0.309) loss 0.5684 (0.7930) acc 87.5000 (81.2500) lr 1.9980e-03 eta 0:07:52
epoch [3/50] batch [10/26] time 0.070 (0.225) data 0.000 (0.155) loss 0.8032 (0.8340) acc 75.0000 (79.6875) lr 1.9980e-03 eta 0:04:38
epoch [3/50] batch [15/26] time 0.070 (0.173) data 0.000 (0.103) loss 0.5635 (0.7910) acc 84.3750 (80.4167) lr 1.9980e-03 eta 0:03:33
epoch [3/50] batch [20/26] time 0.070 (0.147) data 0.000 (0.077) loss 0.5581 (0.7892) acc 87.5000 (80.6250) lr 1.9980e-03 eta 0:03:00
epoch [3/50] batch [25/26] time 0.070 (0.132) data 0.000 (0.062) loss 0.8770 (0.7703) acc 81.2500 (81.3750) lr 1.9980e-03 eta 0:02:41
epoch [4/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.172) loss 1.2158 (0.8269) acc 71.8750 (78.1250) lr 1.9921e-03 eta 0:04:55
epoch [4/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.086) loss 0.6641 (0.7741) acc 81.2500 (80.3125) lr 1.9921e-03 eta 0:03:09
epoch [4/50] batch [15/26] time 0.069 (0.128) data 0.000 (0.058) loss 0.7021 (0.7382) acc 78.1250 (80.0000) lr 1.9921e-03 eta 0:02:33
epoch [4/50] batch [20/26] time 0.069 (0.113) data 0.000 (0.043) loss 0.5718 (0.7340) acc 84.3750 (81.4062) lr 1.9921e-03 eta 0:02:15
epoch [4/50] batch [25/26] time 0.070 (0.105) data 0.001 (0.035) loss 0.5488 (0.6860) acc 84.3750 (82.5000) lr 1.9921e-03 eta 0:02:05
epoch [5/50] batch [5/26] time 0.071 (0.256) data 0.000 (0.184) loss 0.9590 (0.9267) acc 78.1250 (78.1250) lr 1.9823e-03 eta 0:05:04
epoch [5/50] batch [10/26] time 0.070 (0.163) data 0.000 (0.092) loss 0.9155 (0.8068) acc 81.2500 (81.2500) lr 1.9823e-03 eta 0:03:13
epoch [5/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.061) loss 0.4050 (0.7364) acc 93.7500 (81.8750) lr 1.9823e-03 eta 0:02:35
epoch [5/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.046) loss 0.5879 (0.7213) acc 87.5000 (82.6562) lr 1.9823e-03 eta 0:02:16
epoch [5/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.7891 (0.7257) acc 78.1250 (81.7500) lr 1.9823e-03 eta 0:02:05
epoch [6/50] batch [5/26] time 0.069 (0.260) data 0.000 (0.188) loss 0.8618 (0.7542) acc 84.3750 (80.6250) lr 1.9686e-03 eta 0:05:02
epoch [6/50] batch [10/26] time 0.070 (0.165) data 0.000 (0.094) loss 0.5034 (0.7379) acc 87.5000 (80.3125) lr 1.9686e-03 eta 0:03:11
epoch [6/50] batch [15/26] time 0.069 (0.133) data 0.000 (0.063) loss 0.6304 (0.7610) acc 84.3750 (80.4167) lr 1.9686e-03 eta 0:02:33
epoch [6/50] batch [20/26] time 0.069 (0.117) data 0.000 (0.047) loss 0.6899 (0.7863) acc 78.1250 (79.6875) lr 1.9686e-03 eta 0:02:14
epoch [6/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.5078 (0.7431) acc 90.6250 (80.8750) lr 1.9686e-03 eta 0:02:03
epoch [7/50] batch [5/26] time 0.072 (0.260) data 0.000 (0.187) loss 0.3562 (0.6388) acc 93.7500 (81.8750) lr 1.9511e-03 eta 0:04:55
epoch [7/50] batch [10/26] time 0.071 (0.165) data 0.000 (0.094) loss 0.2651 (0.5611) acc 93.7500 (83.4375) lr 1.9511e-03 eta 0:03:07
epoch [7/50] batch [15/26] time 0.069 (0.134) data 0.000 (0.062) loss 0.5215 (0.5864) acc 93.7500 (84.3750) lr 1.9511e-03 eta 0:02:30
epoch [7/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.6064 (0.6226) acc 78.1250 (84.0625) lr 1.9511e-03 eta 0:02:12
epoch [7/50] batch [25/26] time 0.069 (0.108) data 0.000 (0.038) loss 0.3586 (0.6245) acc 90.6250 (83.6250) lr 1.9511e-03 eta 0:02:00
epoch [8/50] batch [5/26] time 0.073 (0.251) data 0.000 (0.177) loss 0.5352 (0.4913) acc 81.2500 (84.3750) lr 1.9298e-03 eta 0:04:39
epoch [8/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.089) loss 0.5752 (0.5541) acc 81.2500 (83.1250) lr 1.9298e-03 eta 0:02:58
epoch [8/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.059) loss 1.0293 (0.6097) acc 75.0000 (82.7083) lr 1.9298e-03 eta 0:02:25
epoch [8/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.6089 (0.5983) acc 84.3750 (82.8125) lr 1.9298e-03 eta 0:02:07
epoch [8/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.6533 (0.6006) acc 84.3750 (83.0000) lr 1.9298e-03 eta 0:01:57
epoch [9/50] batch [5/26] time 0.076 (0.243) data 0.000 (0.167) loss 0.6919 (0.6244) acc 84.3750 (83.7500) lr 1.9048e-03 eta 0:04:24
epoch [9/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.084) loss 0.5898 (0.5987) acc 90.6250 (85.0000) lr 1.9048e-03 eta 0:02:49
epoch [9/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.7861 (0.6054) acc 75.0000 (84.7917) lr 1.9048e-03 eta 0:02:17
epoch [9/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.042) loss 0.7856 (0.6484) acc 78.1250 (83.5938) lr 1.9048e-03 eta 0:02:01
epoch [9/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.7295 (0.6425) acc 75.0000 (83.7500) lr 1.9048e-03 eta 0:01:52
epoch [10/50] batch [5/26] time 0.071 (0.251) data 0.000 (0.180) loss 0.6670 (0.5943) acc 84.3750 (86.8750) lr 1.8763e-03 eta 0:04:26
epoch [10/50] batch [10/26] time 0.070 (0.161) data 0.000 (0.090) loss 0.9175 (0.6820) acc 81.2500 (83.4375) lr 1.8763e-03 eta 0:02:49
epoch [10/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.060) loss 0.5254 (0.6313) acc 90.6250 (83.7500) lr 1.8763e-03 eta 0:02:17
epoch [10/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.5791 (0.6181) acc 84.3750 (83.7500) lr 1.8763e-03 eta 0:02:00
epoch [10/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.036) loss 0.8218 (0.6312) acc 84.3750 (83.7500) lr 1.8763e-03 eta 0:01:50
epoch [11/50] batch [5/26] time 0.069 (0.250) data 0.000 (0.179) loss 1.0059 (0.5908) acc 65.6250 (81.8750) lr 1.8443e-03 eta 0:04:19
epoch [11/50] batch [10/26] time 0.070 (0.160) data 0.000 (0.089) loss 0.4316 (0.6037) acc 87.5000 (82.5000) lr 1.8443e-03 eta 0:02:45
epoch [11/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.060) loss 0.8159 (0.5982) acc 81.2500 (83.3333) lr 1.8443e-03 eta 0:02:13
epoch [11/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.8062 (0.6198) acc 75.0000 (83.2812) lr 1.8443e-03 eta 0:01:57
epoch [11/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.036) loss 0.9937 (0.6302) acc 81.2500 (83.3750) lr 1.8443e-03 eta 0:01:48
epoch [12/50] batch [5/26] time 0.070 (0.311) data 0.000 (0.240) loss 0.9697 (0.6612) acc 68.7500 (82.5000) lr 1.8090e-03 eta 0:05:13
epoch [12/50] batch [10/26] time 0.071 (0.191) data 0.000 (0.120) loss 0.4146 (0.6565) acc 90.6250 (82.5000) lr 1.8090e-03 eta 0:03:11
epoch [12/50] batch [15/26] time 0.071 (0.151) data 0.000 (0.080) loss 0.5469 (0.6395) acc 84.3750 (83.1250) lr 1.8090e-03 eta 0:02:30
epoch [12/50] batch [20/26] time 0.071 (0.131) data 0.000 (0.060) loss 0.4072 (0.6610) acc 90.6250 (82.8125) lr 1.8090e-03 eta 0:02:09
epoch [12/50] batch [25/26] time 0.071 (0.119) data 0.000 (0.048) loss 0.5718 (0.6364) acc 78.1250 (83.2500) lr 1.8090e-03 eta 0:01:57
epoch [13/50] batch [5/26] time 0.073 (0.239) data 0.000 (0.167) loss 0.5630 (0.7574) acc 84.3750 (79.3750) lr 1.7705e-03 eta 0:03:55
epoch [13/50] batch [10/26] time 0.070 (0.155) data 0.000 (0.084) loss 0.7681 (0.6669) acc 78.1250 (82.1875) lr 1.7705e-03 eta 0:02:31
epoch [13/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.7681 (0.6891) acc 81.2500 (82.5000) lr 1.7705e-03 eta 0:02:03
epoch [13/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 1.2500 (0.7142) acc 71.8750 (82.3438) lr 1.7705e-03 eta 0:01:49
epoch [13/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.4084 (0.6653) acc 90.6250 (83.6250) lr 1.7705e-03 eta 0:01:40
epoch [14/50] batch [5/26] time 0.070 (0.242) data 0.000 (0.168) loss 0.3650 (0.5269) acc 84.3750 (86.8750) lr 1.7290e-03 eta 0:03:51
epoch [14/50] batch [10/26] time 0.072 (0.156) data 0.000 (0.084) loss 0.5874 (0.5544) acc 87.5000 (85.9375) lr 1.7290e-03 eta 0:02:28
epoch [14/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.7109 (0.5558) acc 81.2500 (85.6250) lr 1.7290e-03 eta 0:02:00
epoch [14/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5566 (0.5679) acc 84.3750 (85.6250) lr 1.7290e-03 eta 0:01:46
epoch [14/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.6860 (0.5752) acc 81.2500 (85.2500) lr 1.7290e-03 eta 0:01:38
epoch [15/50] batch [5/26] time 0.069 (0.288) data 0.000 (0.216) loss 0.3743 (0.5132) acc 93.7500 (85.0000) lr 1.6845e-03 eta 0:04:28
epoch [15/50] batch [10/26] time 0.070 (0.180) data 0.000 (0.108) loss 0.3828 (0.4974) acc 87.5000 (86.8750) lr 1.6845e-03 eta 0:02:46
epoch [15/50] batch [15/26] time 0.070 (0.143) data 0.000 (0.072) loss 1.0039 (0.5681) acc 71.8750 (85.0000) lr 1.6845e-03 eta 0:02:11
epoch [15/50] batch [20/26] time 0.070 (0.125) data 0.000 (0.054) loss 0.7217 (0.6143) acc 81.2500 (83.5938) lr 1.6845e-03 eta 0:01:54
epoch [15/50] batch [25/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.5532 (0.5967) acc 84.3750 (83.5000) lr 1.6845e-03 eta 0:01:43
epoch [16/50] batch [5/26] time 0.070 (0.244) data 0.000 (0.172) loss 0.5693 (0.6220) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:03:40
epoch [16/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.086) loss 0.7148 (0.6493) acc 81.2500 (83.4375) lr 1.6374e-03 eta 0:02:21
epoch [16/50] batch [15/26] time 0.073 (0.128) data 0.000 (0.057) loss 0.3853 (0.6076) acc 90.6250 (84.3750) lr 1.6374e-03 eta 0:01:54
epoch [16/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.9478 (0.6109) acc 75.0000 (84.3750) lr 1.6374e-03 eta 0:01:41
epoch [16/50] batch [25/26] time 0.072 (0.106) data 0.000 (0.035) loss 0.5254 (0.5889) acc 84.3750 (84.6250) lr 1.6374e-03 eta 0:01:33
epoch [17/50] batch [5/26] time 0.070 (0.254) data 0.000 (0.183) loss 0.7271 (0.5751) acc 84.3750 (88.1250) lr 1.5878e-03 eta 0:03:43
epoch [17/50] batch [10/26] time 0.071 (0.163) data 0.000 (0.092) loss 0.7212 (0.6330) acc 81.2500 (85.6250) lr 1.5878e-03 eta 0:02:22
epoch [17/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.061) loss 0.3933 (0.6670) acc 93.7500 (84.5833) lr 1.5878e-03 eta 0:01:54
epoch [17/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.046) loss 0.3506 (0.6652) acc 93.7500 (84.2188) lr 1.5878e-03 eta 0:01:40
epoch [17/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.5762 (0.6552) acc 90.6250 (84.6250) lr 1.5878e-03 eta 0:01:32
epoch [18/50] batch [5/26] time 0.075 (0.259) data 0.000 (0.188) loss 0.7070 (0.6573) acc 84.3750 (84.3750) lr 1.5358e-03 eta 0:03:41
epoch [18/50] batch [10/26] time 0.070 (0.165) data 0.000 (0.094) loss 0.5547 (0.6350) acc 87.5000 (84.3750) lr 1.5358e-03 eta 0:02:19
epoch [18/50] batch [15/26] time 0.070 (0.133) data 0.000 (0.063) loss 0.4404 (0.5798) acc 84.3750 (85.2083) lr 1.5358e-03 eta 0:01:52
epoch [18/50] batch [20/26] time 0.071 (0.118) data 0.000 (0.047) loss 0.8926 (0.6150) acc 78.1250 (84.2188) lr 1.5358e-03 eta 0:01:38
epoch [18/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.6006 (0.6061) acc 84.3750 (84.5000) lr 1.5358e-03 eta 0:01:30
epoch [19/50] batch [5/26] time 0.070 (0.248) data 0.000 (0.175) loss 0.7681 (0.7046) acc 75.0000 (78.7500) lr 1.4818e-03 eta 0:03:25
epoch [19/50] batch [10/26] time 0.072 (0.160) data 0.000 (0.088) loss 0.7505 (0.6124) acc 84.3750 (83.4375) lr 1.4818e-03 eta 0:02:11
epoch [19/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.059) loss 0.8730 (0.6292) acc 78.1250 (82.5000) lr 1.4818e-03 eta 0:01:46
epoch [19/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.3962 (0.6010) acc 90.6250 (83.2812) lr 1.4818e-03 eta 0:01:33
epoch [19/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.5693 (0.5936) acc 87.5000 (83.8750) lr 1.4818e-03 eta 0:01:25
epoch [20/50] batch [5/26] time 0.071 (0.263) data 0.000 (0.190) loss 0.4114 (0.5578) acc 87.5000 (85.0000) lr 1.4258e-03 eta 0:03:30
epoch [20/50] batch [10/26] time 0.070 (0.167) data 0.000 (0.095) loss 0.9463 (0.6341) acc 84.3750 (84.0625) lr 1.4258e-03 eta 0:02:12
epoch [20/50] batch [15/26] time 0.070 (0.134) data 0.000 (0.064) loss 0.5654 (0.6426) acc 84.3750 (82.7083) lr 1.4258e-03 eta 0:01:46
epoch [20/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.048) loss 0.2590 (0.6044) acc 90.6250 (83.5938) lr 1.4258e-03 eta 0:01:33
epoch [20/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.3499 (0.6152) acc 90.6250 (83.7500) lr 1.4258e-03 eta 0:01:24
epoch [21/50] batch [5/26] time 0.070 (0.228) data 0.000 (0.156) loss 0.4922 (0.6019) acc 87.5000 (80.6250) lr 1.3681e-03 eta 0:02:57
epoch [21/50] batch [10/26] time 0.070 (0.149) data 0.000 (0.078) loss 0.4329 (0.6382) acc 87.5000 (82.5000) lr 1.3681e-03 eta 0:01:54
epoch [21/50] batch [15/26] time 0.069 (0.123) data 0.000 (0.052) loss 0.3413 (0.6469) acc 87.5000 (83.3333) lr 1.3681e-03 eta 0:01:33
epoch [21/50] batch [20/26] time 0.069 (0.109) data 0.000 (0.039) loss 1.0674 (0.6591) acc 78.1250 (82.9688) lr 1.3681e-03 eta 0:01:22
epoch [21/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.031) loss 0.5308 (0.6539) acc 81.2500 (83.0000) lr 1.3681e-03 eta 0:01:16
epoch [22/50] batch [5/26] time 0.070 (0.283) data 0.000 (0.211) loss 0.4436 (0.5201) acc 81.2500 (82.5000) lr 1.3090e-03 eta 0:03:31
epoch [22/50] batch [10/26] time 0.070 (0.178) data 0.000 (0.106) loss 0.5562 (0.5830) acc 84.3750 (82.5000) lr 1.3090e-03 eta 0:02:12
epoch [22/50] batch [15/26] time 0.072 (0.142) data 0.000 (0.071) loss 0.4573 (0.5782) acc 87.5000 (82.9167) lr 1.3090e-03 eta 0:01:45
epoch [22/50] batch [20/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.7021 (0.5665) acc 87.5000 (83.7500) lr 1.3090e-03 eta 0:01:31
epoch [22/50] batch [25/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.4172 (0.5487) acc 90.6250 (84.8750) lr 1.3090e-03 eta 0:01:22
epoch [23/50] batch [5/26] time 0.073 (0.236) data 0.000 (0.161) loss 0.3501 (0.4288) acc 93.7500 (89.3750) lr 1.2487e-03 eta 0:02:50
epoch [23/50] batch [10/26] time 0.071 (0.153) data 0.000 (0.081) loss 0.3157 (0.4931) acc 96.8750 (87.8125) lr 1.2487e-03 eta 0:01:50
epoch [23/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.054) loss 0.5146 (0.4687) acc 87.5000 (88.1250) lr 1.2487e-03 eta 0:01:29
epoch [23/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.4861 (0.4794) acc 84.3750 (87.6562) lr 1.2487e-03 eta 0:01:19
epoch [23/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.7485 (0.5192) acc 81.2500 (87.0000) lr 1.2487e-03 eta 0:01:12
epoch [24/50] batch [5/26] time 0.072 (0.268) data 0.000 (0.195) loss 0.4480 (0.5761) acc 87.5000 (86.8750) lr 1.1874e-03 eta 0:03:07
epoch [24/50] batch [10/26] time 0.072 (0.170) data 0.000 (0.098) loss 0.4417 (0.5518) acc 81.2500 (87.5000) lr 1.1874e-03 eta 0:01:57
epoch [24/50] batch [15/26] time 0.071 (0.137) data 0.000 (0.065) loss 0.7422 (0.5459) acc 75.0000 (86.2500) lr 1.1874e-03 eta 0:01:34
epoch [24/50] batch [20/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.5146 (0.5366) acc 87.5000 (86.4062) lr 1.1874e-03 eta 0:01:22
epoch [24/50] batch [25/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.6709 (0.5701) acc 81.2500 (85.5000) lr 1.1874e-03 eta 0:01:14
epoch [25/50] batch [5/26] time 0.071 (0.246) data 0.000 (0.173) loss 0.5542 (0.5140) acc 81.2500 (86.2500) lr 1.1253e-03 eta 0:02:44
epoch [25/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.087) loss 0.6572 (0.5452) acc 78.1250 (83.7500) lr 1.1253e-03 eta 0:01:45
epoch [25/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.9150 (0.5930) acc 81.2500 (83.1250) lr 1.1253e-03 eta 0:01:25
epoch [25/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.4644 (0.5931) acc 90.6250 (83.5938) lr 1.1253e-03 eta 0:01:15
epoch [25/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.6133 (0.5802) acc 84.3750 (83.6250) lr 1.1253e-03 eta 0:01:08
epoch [26/50] batch [5/26] time 0.071 (0.263) data 0.000 (0.192) loss 0.8965 (0.6127) acc 78.1250 (83.7500) lr 1.0628e-03 eta 0:02:49
epoch [26/50] batch [10/26] time 0.071 (0.167) data 0.000 (0.096) loss 0.6343 (0.6289) acc 84.3750 (83.1250) lr 1.0628e-03 eta 0:01:46
epoch [26/50] batch [15/26] time 0.070 (0.135) data 0.000 (0.064) loss 0.4563 (0.6098) acc 93.7500 (83.3333) lr 1.0628e-03 eta 0:01:25
epoch [26/50] batch [20/26] time 0.070 (0.119) data 0.000 (0.048) loss 0.4470 (0.6070) acc 87.5000 (83.4375) lr 1.0628e-03 eta 0:01:14
epoch [26/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.039) loss 0.5908 (0.6171) acc 81.2500 (83.5000) lr 1.0628e-03 eta 0:01:08
epoch [27/50] batch [5/26] time 0.074 (0.263) data 0.001 (0.191) loss 0.4297 (0.6284) acc 87.5000 (83.7500) lr 1.0000e-03 eta 0:02:43
epoch [27/50] batch [10/26] time 0.071 (0.167) data 0.000 (0.096) loss 0.5811 (0.5915) acc 84.3750 (84.0625) lr 1.0000e-03 eta 0:01:42
epoch [27/50] batch [15/26] time 0.071 (0.135) data 0.000 (0.064) loss 0.4092 (0.5875) acc 90.6250 (84.7917) lr 1.0000e-03 eta 0:01:22
epoch [27/50] batch [20/26] time 0.072 (0.119) data 0.000 (0.048) loss 0.2808 (0.5742) acc 90.6250 (85.7812) lr 1.0000e-03 eta 0:01:11
epoch [27/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.9653 (0.6092) acc 75.0000 (85.2500) lr 1.0000e-03 eta 0:01:05
epoch [28/50] batch [5/26] time 0.070 (0.242) data 0.000 (0.170) loss 0.6519 (0.5250) acc 78.1250 (83.1250) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.085) loss 1.0693 (0.6314) acc 71.8750 (81.8750) lr 9.3721e-04 eta 0:01:31
epoch [28/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.057) loss 1.0693 (0.6314) acc 71.8750 (82.7083) lr 9.3721e-04 eta 0:01:14
epoch [28/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.6772 (0.6177) acc 81.2500 (82.8125) lr 9.3721e-04 eta 0:01:05
epoch [28/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.7051 (0.6055) acc 81.2500 (83.5000) lr 9.3721e-04 eta 0:01:00
epoch [29/50] batch [5/26] time 0.071 (0.246) data 0.000 (0.175) loss 0.4294 (0.5662) acc 90.6250 (83.7500) lr 8.7467e-04 eta 0:02:19
epoch [29/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.088) loss 0.6650 (0.6117) acc 87.5000 (84.6875) lr 8.7467e-04 eta 0:01:29
epoch [29/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.059) loss 0.5137 (0.5665) acc 87.5000 (85.6250) lr 8.7467e-04 eta 0:01:12
epoch [29/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.7598 (0.5974) acc 81.2500 (85.4688) lr 8.7467e-04 eta 0:01:03
epoch [29/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.8511 (0.5907) acc 75.0000 (85.1250) lr 8.7467e-04 eta 0:00:57
epoch [30/50] batch [5/26] time 0.071 (0.284) data 0.000 (0.212) loss 0.4048 (0.6104) acc 90.6250 (85.0000) lr 8.1262e-04 eta 0:02:33
epoch [30/50] batch [10/26] time 0.070 (0.177) data 0.000 (0.106) loss 0.5693 (0.5625) acc 78.1250 (85.6250) lr 8.1262e-04 eta 0:01:35
epoch [30/50] batch [15/26] time 0.070 (0.142) data 0.000 (0.071) loss 1.0898 (0.6226) acc 75.0000 (84.5833) lr 8.1262e-04 eta 0:01:15
epoch [30/50] batch [20/26] time 0.074 (0.124) data 0.000 (0.053) loss 0.6938 (0.5902) acc 84.3750 (85.0000) lr 8.1262e-04 eta 0:01:05
epoch [30/50] batch [25/26] time 0.070 (0.113) data 0.000 (0.043) loss 0.4272 (0.5503) acc 90.6250 (85.8750) lr 8.1262e-04 eta 0:00:59
epoch [31/50] batch [5/26] time 0.073 (0.300) data 0.000 (0.228) loss 0.4150 (0.6455) acc 93.7500 (85.0000) lr 7.5131e-04 eta 0:02:34
epoch [31/50] batch [10/26] time 0.070 (0.185) data 0.000 (0.114) loss 0.4177 (0.5806) acc 90.6250 (85.0000) lr 7.5131e-04 eta 0:01:34
epoch [31/50] batch [15/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.4812 (0.5595) acc 87.5000 (85.2083) lr 7.5131e-04 eta 0:01:14
epoch [31/50] batch [20/26] time 0.071 (0.128) data 0.000 (0.057) loss 0.4016 (0.5726) acc 87.5000 (85.1562) lr 7.5131e-04 eta 0:01:04
epoch [31/50] batch [25/26] time 0.071 (0.117) data 0.000 (0.046) loss 0.8579 (0.5931) acc 78.1250 (84.5000) lr 7.5131e-04 eta 0:00:57
epoch [32/50] batch [5/26] time 0.074 (0.393) data 0.000 (0.320) loss 0.8623 (0.6463) acc 78.1250 (85.6250) lr 6.9098e-04 eta 0:03:12
epoch [32/50] batch [10/26] time 0.070 (0.232) data 0.000 (0.160) loss 0.2776 (0.5385) acc 96.8750 (86.8750) lr 6.9098e-04 eta 0:01:52
epoch [32/50] batch [15/26] time 0.070 (0.178) data 0.000 (0.107) loss 0.4985 (0.5920) acc 87.5000 (85.2083) lr 6.9098e-04 eta 0:01:25
epoch [32/50] batch [20/26] time 0.070 (0.151) data 0.000 (0.080) loss 0.2979 (0.5730) acc 93.7500 (85.7812) lr 6.9098e-04 eta 0:01:11
epoch [32/50] batch [25/26] time 0.070 (0.135) data 0.000 (0.064) loss 0.6240 (0.5663) acc 84.3750 (85.6250) lr 6.9098e-04 eta 0:01:03
epoch [33/50] batch [5/26] time 0.071 (0.281) data 0.000 (0.209) loss 0.9028 (0.5426) acc 78.1250 (86.2500) lr 6.3188e-04 eta 0:02:10
epoch [33/50] batch [10/26] time 0.074 (0.176) data 0.000 (0.105) loss 0.7891 (0.5747) acc 84.3750 (85.6250) lr 6.3188e-04 eta 0:01:20
epoch [33/50] batch [15/26] time 0.070 (0.141) data 0.000 (0.070) loss 0.3198 (0.5412) acc 93.7500 (86.4583) lr 6.3188e-04 eta 0:01:03
epoch [33/50] batch [20/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.4717 (0.5662) acc 90.6250 (86.0938) lr 6.3188e-04 eta 0:00:55
epoch [33/50] batch [25/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5293 (0.5734) acc 90.6250 (86.1250) lr 6.3188e-04 eta 0:00:49
epoch [34/50] batch [5/26] time 0.071 (0.275) data 0.000 (0.200) loss 0.5454 (0.5154) acc 75.0000 (83.7500) lr 5.7422e-04 eta 0:02:00
epoch [34/50] batch [10/26] time 0.072 (0.173) data 0.000 (0.100) loss 0.5234 (0.5370) acc 87.5000 (84.6875) lr 5.7422e-04 eta 0:01:14
epoch [34/50] batch [15/26] time 0.070 (0.139) data 0.000 (0.067) loss 0.3367 (0.4993) acc 93.7500 (86.8750) lr 5.7422e-04 eta 0:00:59
epoch [34/50] batch [20/26] time 0.070 (0.122) data 0.000 (0.050) loss 0.4475 (0.5009) acc 87.5000 (87.0312) lr 5.7422e-04 eta 0:00:51
epoch [34/50] batch [25/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.4988 (0.4998) acc 90.6250 (87.1250) lr 5.7422e-04 eta 0:00:46
epoch [35/50] batch [5/26] time 0.074 (0.310) data 0.000 (0.238) loss 0.3586 (0.5519) acc 90.6250 (84.3750) lr 5.1825e-04 eta 0:02:07
epoch [35/50] batch [10/26] time 0.074 (0.191) data 0.000 (0.119) loss 0.2433 (0.4851) acc 96.8750 (87.5000) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [15/26] time 0.073 (0.152) data 0.000 (0.080) loss 0.7148 (0.5332) acc 71.8750 (84.5833) lr 5.1825e-04 eta 0:01:00
epoch [35/50] batch [20/26] time 0.081 (0.133) data 0.000 (0.060) loss 0.9609 (0.5302) acc 68.7500 (85.1562) lr 5.1825e-04 eta 0:00:52
epoch [35/50] batch [25/26] time 0.074 (0.121) data 0.000 (0.048) loss 0.2220 (0.5085) acc 96.8750 (86.3750) lr 5.1825e-04 eta 0:00:47
epoch [36/50] batch [5/26] time 0.070 (0.648) data 0.000 (0.573) loss 0.5781 (0.5668) acc 87.5000 (86.2500) lr 4.6417e-04 eta 0:04:09
epoch [36/50] batch [10/26] time 0.072 (0.359) data 0.000 (0.287) loss 0.4678 (0.5412) acc 87.5000 (86.2500) lr 4.6417e-04 eta 0:02:16
epoch [36/50] batch [15/26] time 0.072 (0.264) data 0.000 (0.191) loss 0.2108 (0.5192) acc 93.7500 (87.0833) lr 4.6417e-04 eta 0:01:38
epoch [36/50] batch [20/26] time 0.070 (0.215) data 0.000 (0.143) loss 0.7656 (0.5597) acc 84.3750 (86.4062) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [25/26] time 0.070 (0.186) data 0.000 (0.115) loss 0.3977 (0.5450) acc 87.5000 (87.1250) lr 4.6417e-04 eta 0:01:07
epoch [37/50] batch [5/26] time 0.074 (0.278) data 0.000 (0.203) loss 0.4060 (0.4044) acc 87.5000 (90.0000) lr 4.1221e-04 eta 0:01:39
epoch [37/50] batch [10/26] time 0.078 (0.176) data 0.000 (0.102) loss 0.3909 (0.4511) acc 93.7500 (89.3750) lr 4.1221e-04 eta 0:01:02
epoch [37/50] batch [15/26] time 0.071 (0.141) data 0.000 (0.068) loss 0.7832 (0.4977) acc 87.5000 (88.5417) lr 4.1221e-04 eta 0:00:49
epoch [37/50] batch [20/26] time 0.070 (0.123) data 0.000 (0.051) loss 0.5371 (0.5425) acc 84.3750 (87.9688) lr 4.1221e-04 eta 0:00:42
epoch [37/50] batch [25/26] time 0.071 (0.113) data 0.000 (0.041) loss 0.3921 (0.5366) acc 84.3750 (87.3750) lr 4.1221e-04 eta 0:00:38
epoch [38/50] batch [5/26] time 0.070 (0.520) data 0.000 (0.447) loss 0.3730 (0.4546) acc 93.7500 (88.7500) lr 3.6258e-04 eta 0:02:53
epoch [38/50] batch [10/26] time 0.070 (0.295) data 0.000 (0.223) loss 0.3765 (0.5194) acc 90.6250 (88.4375) lr 3.6258e-04 eta 0:01:36
epoch [38/50] batch [15/26] time 0.070 (0.220) data 0.000 (0.149) loss 0.4146 (0.5176) acc 90.6250 (87.9167) lr 3.6258e-04 eta 0:01:11
epoch [38/50] batch [20/26] time 0.070 (0.183) data 0.000 (0.112) loss 0.3191 (0.5538) acc 93.7500 (87.5000) lr 3.6258e-04 eta 0:00:58
epoch [38/50] batch [25/26] time 0.071 (0.160) data 0.000 (0.090) loss 0.5234 (0.5487) acc 84.3750 (87.3750) lr 3.6258e-04 eta 0:00:50
epoch [39/50] batch [5/26] time 0.074 (0.342) data 0.000 (0.268) loss 0.5410 (0.6088) acc 90.6250 (85.0000) lr 3.1545e-04 eta 0:01:45
epoch [39/50] batch [10/26] time 0.074 (0.207) data 0.001 (0.134) loss 0.3877 (0.5528) acc 93.7500 (87.1875) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [15/26] time 0.073 (0.162) data 0.000 (0.090) loss 0.8037 (0.5652) acc 78.1250 (86.8750) lr 3.1545e-04 eta 0:00:48
epoch [39/50] batch [20/26] time 0.071 (0.140) data 0.000 (0.067) loss 0.3647 (0.5694) acc 87.5000 (86.5625) lr 3.1545e-04 eta 0:00:40
epoch [39/50] batch [25/26] time 0.070 (0.126) data 0.000 (0.054) loss 0.4451 (0.5596) acc 84.3750 (86.6250) lr 3.1545e-04 eta 0:00:36
epoch [40/50] batch [5/26] time 0.071 (0.647) data 0.000 (0.573) loss 0.8262 (0.6778) acc 84.3750 (84.3750) lr 2.7103e-04 eta 0:03:01
epoch [40/50] batch [10/26] time 0.070 (0.360) data 0.000 (0.287) loss 0.2915 (0.6431) acc 96.8750 (85.3125) lr 2.7103e-04 eta 0:01:39
epoch [40/50] batch [15/26] time 0.070 (0.263) data 0.000 (0.191) loss 0.4382 (0.6459) acc 87.5000 (85.2083) lr 2.7103e-04 eta 0:01:11
epoch [40/50] batch [20/26] time 0.070 (0.215) data 0.000 (0.143) loss 0.5469 (0.6258) acc 84.3750 (85.1562) lr 2.7103e-04 eta 0:00:57
epoch [40/50] batch [25/26] time 0.070 (0.186) data 0.000 (0.115) loss 0.4629 (0.5779) acc 84.3750 (86.1250) lr 2.7103e-04 eta 0:00:48
epoch [41/50] batch [5/26] time 0.071 (0.308) data 0.000 (0.235) loss 0.7148 (0.5026) acc 81.2500 (87.5000) lr 2.2949e-04 eta 0:01:18
epoch [41/50] batch [10/26] time 0.072 (0.189) data 0.000 (0.118) loss 0.5640 (0.4966) acc 87.5000 (87.5000) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [15/26] time 0.070 (0.150) data 0.000 (0.079) loss 0.9004 (0.5301) acc 81.2500 (86.6667) lr 2.2949e-04 eta 0:00:36
epoch [41/50] batch [20/26] time 0.070 (0.130) data 0.000 (0.059) loss 0.8076 (0.5385) acc 78.1250 (85.7812) lr 2.2949e-04 eta 0:00:31
epoch [41/50] batch [25/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.7905 (0.5511) acc 81.2500 (85.2500) lr 2.2949e-04 eta 0:00:27
epoch [42/50] batch [5/26] time 0.071 (0.594) data 0.000 (0.516) loss 0.5098 (0.5796) acc 87.5000 (85.6250) lr 1.9098e-04 eta 0:02:15
epoch [42/50] batch [10/26] time 0.071 (0.332) data 0.000 (0.258) loss 0.9009 (0.6608) acc 81.2500 (84.0625) lr 1.9098e-04 eta 0:01:14
epoch [42/50] batch [15/26] time 0.071 (0.245) data 0.000 (0.172) loss 0.4773 (0.6607) acc 87.5000 (84.3750) lr 1.9098e-04 eta 0:00:53
epoch [42/50] batch [20/26] time 0.070 (0.201) data 0.000 (0.129) loss 0.8750 (0.6046) acc 75.0000 (85.9375) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [25/26] time 0.070 (0.175) data 0.000 (0.103) loss 0.5322 (0.5936) acc 87.5000 (85.7500) lr 1.9098e-04 eta 0:00:36
epoch [43/50] batch [5/26] time 0.070 (0.273) data 0.000 (0.201) loss 0.5146 (0.3831) acc 90.6250 (93.1250) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [10/26] time 0.071 (0.172) data 0.000 (0.101) loss 0.2422 (0.3830) acc 96.8750 (91.5625) lr 1.5567e-04 eta 0:00:34
epoch [43/50] batch [15/26] time 0.077 (0.139) data 0.000 (0.067) loss 0.4631 (0.4404) acc 90.6250 (89.1667) lr 1.5567e-04 eta 0:00:26
epoch [43/50] batch [20/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.5176 (0.4576) acc 90.6250 (88.7500) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [25/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.5488 (0.4736) acc 87.5000 (89.0000) lr 1.5567e-04 eta 0:00:20
epoch [44/50] batch [5/26] time 0.071 (0.597) data 0.000 (0.523) loss 0.6528 (0.5532) acc 90.6250 (86.2500) lr 1.2369e-04 eta 0:01:45
epoch [44/50] batch [10/26] time 0.071 (0.334) data 0.000 (0.262) loss 0.4580 (0.6175) acc 87.5000 (84.0625) lr 1.2369e-04 eta 0:00:57
epoch [44/50] batch [15/26] time 0.070 (0.246) data 0.000 (0.175) loss 0.5200 (0.5708) acc 87.5000 (85.4167) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [20/26] time 0.070 (0.202) data 0.000 (0.131) loss 0.3862 (0.5549) acc 90.6250 (86.0938) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [25/26] time 0.070 (0.175) data 0.000 (0.105) loss 0.2554 (0.5268) acc 96.8750 (86.7500) lr 1.2369e-04 eta 0:00:27
epoch [45/50] batch [5/26] time 0.071 (0.300) data 0.000 (0.228) loss 0.3896 (0.4321) acc 90.6250 (90.0000) lr 9.5173e-05 eta 0:00:45
epoch [45/50] batch [10/26] time 0.071 (0.186) data 0.000 (0.114) loss 0.5195 (0.4844) acc 84.3750 (89.3750) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [15/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.7427 (0.4908) acc 87.5000 (89.3750) lr 9.5173e-05 eta 0:00:20
epoch [45/50] batch [20/26] time 0.072 (0.128) data 0.000 (0.057) loss 0.4514 (0.4900) acc 93.7500 (89.5312) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [25/26] time 0.070 (0.117) data 0.000 (0.046) loss 0.5400 (0.5056) acc 84.3750 (88.3750) lr 9.5173e-05 eta 0:00:15
epoch [46/50] batch [5/26] time 0.069 (0.541) data 0.000 (0.466) loss 0.3557 (0.5482) acc 87.5000 (85.0000) lr 7.0224e-05 eta 0:01:07
epoch [46/50] batch [10/26] time 0.070 (0.305) data 0.000 (0.233) loss 0.3059 (0.5165) acc 93.7500 (85.9375) lr 7.0224e-05 eta 0:00:36
epoch [46/50] batch [15/26] time 0.070 (0.227) data 0.000 (0.156) loss 0.6567 (0.5075) acc 84.3750 (86.8750) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [20/26] time 0.070 (0.188) data 0.000 (0.117) loss 0.3591 (0.5006) acc 90.6250 (87.8125) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.070 (0.165) data 0.000 (0.093) loss 0.6030 (0.5187) acc 84.3750 (87.5000) lr 7.0224e-05 eta 0:00:17
epoch [47/50] batch [5/26] time 0.073 (0.283) data 0.000 (0.209) loss 0.7388 (0.6772) acc 84.3750 (81.2500) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [10/26] time 0.070 (0.177) data 0.000 (0.105) loss 0.4585 (0.5284) acc 81.2500 (85.0000) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [15/26] time 0.070 (0.141) data 0.000 (0.070) loss 0.4663 (0.5265) acc 87.5000 (85.6250) lr 4.8943e-05 eta 0:00:12
epoch [47/50] batch [20/26] time 0.070 (0.124) data 0.000 (0.052) loss 0.3962 (0.4971) acc 87.5000 (86.8750) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [25/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.4463 (0.4917) acc 87.5000 (87.0000) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.083 (0.683) data 0.005 (0.605) loss 0.7241 (0.6145) acc 84.3750 (87.5000) lr 3.1417e-05 eta 0:00:49
epoch [48/50] batch [10/26] time 0.070 (0.377) data 0.000 (0.303) loss 0.4023 (0.5616) acc 87.5000 (88.1250) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [15/26] time 0.070 (0.275) data 0.000 (0.202) loss 0.6455 (0.5344) acc 81.2500 (87.9167) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [20/26] time 0.070 (0.224) data 0.000 (0.151) loss 0.2400 (0.5053) acc 96.8750 (88.5938) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [25/26] time 0.070 (0.193) data 0.000 (0.121) loss 0.5239 (0.5270) acc 90.6250 (88.2500) lr 3.1417e-05 eta 0:00:10
epoch [49/50] batch [5/26] time 0.070 (0.257) data 0.000 (0.186) loss 0.3533 (0.5196) acc 87.5000 (85.0000) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/26] time 0.070 (0.163) data 0.000 (0.093) loss 0.6406 (0.4918) acc 87.5000 (87.5000) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.062) loss 0.3755 (0.5416) acc 90.6250 (86.2500) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.047) loss 0.4736 (0.5599) acc 93.7500 (86.4062) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.6401 (0.5616) acc 84.3750 (86.1250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.069 (0.718) data 0.000 (0.644) loss 0.3735 (0.4865) acc 93.7500 (92.5000) lr 7.8853e-06 eta 0:00:15
epoch [50/50] batch [10/26] time 0.070 (0.394) data 0.000 (0.322) loss 0.6255 (0.5397) acc 81.2500 (88.1250) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [15/26] time 0.070 (0.286) data 0.000 (0.215) loss 0.4756 (0.5363) acc 84.3750 (87.0833) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [20/26] time 0.070 (0.232) data 0.000 (0.161) loss 0.8457 (0.5518) acc 81.2500 (86.4062) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.070 (0.200) data 0.000 (0.129) loss 0.2896 (0.5701) acc 90.6250 (85.6250) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:19<09:46, 19.54s/it]  6%|▋         | 2/31 [00:21<04:20,  8.97s/it] 10%|▉         | 3/31 [00:21<02:23,  5.13s/it] 13%|█▎        | 4/31 [00:22<01:29,  3.33s/it] 16%|█▌        | 5/31 [00:22<01:00,  2.34s/it] 19%|█▉        | 6/31 [00:23<00:43,  1.74s/it] 23%|██▎       | 7/31 [00:23<00:32,  1.36s/it] 26%|██▌       | 8/31 [00:24<00:25,  1.11s/it] 29%|██▉       | 9/31 [00:36<01:42,  4.65s/it] 32%|███▏      | 10/31 [00:37<01:11,  3.42s/it] 35%|███▌      | 11/31 [00:38<00:50,  2.55s/it] 39%|███▊      | 12/31 [00:38<00:36,  1.95s/it] 42%|████▏     | 13/31 [00:39<00:27,  1.53s/it] 45%|████▌     | 14/31 [00:39<00:21,  1.24s/it] 48%|████▊     | 15/31 [00:40<00:16,  1.04s/it] 52%|█████▏    | 16/31 [00:41<00:13,  1.11it/s] 55%|█████▍    | 17/31 [00:42<00:15,  1.13s/it] 58%|█████▊    | 18/31 [00:43<00:13,  1.03s/it] 61%|██████▏   | 19/31 [00:44<00:10,  1.12it/s] 65%|██████▍   | 20/31 [00:44<00:08,  1.25it/s] 68%|██████▊   | 21/31 [00:45<00:07,  1.37it/s] 71%|███████   | 22/31 [00:45<00:06,  1.46it/s] 74%|███████▍  | 23/31 [00:46<00:05,  1.54it/s] 77%|███████▋  | 24/31 [00:46<00:04,  1.59it/s] 81%|████████  | 25/31 [00:48<00:05,  1.05it/s] 84%|████████▍ | 26/31 [00:50<00:05,  1.16s/it] 87%|████████▋ | 27/31 [00:50<00:03,  1.01it/s] 90%|█████████ | 28/31 [00:51<00:02,  1.16it/s] 94%|█████████▎| 29/31 [00:52<00:01,  1.29it/s] 97%|█████████▋| 30/31 [00:52<00:00,  1.40it/s]100%|██████████| 31/31 [00:53<00:00,  1.61it/s]100%|██████████| 31/31 [00:53<00:00,  1.72s/it]
=> result
* total: 15,300
* correct: 13,855
* accuracy: 90.6%
* error: 9.4%
* macro_f1: 90.6%
Elapsed: 0:03:37
Run this job and save the output to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.070 (0.373) data 0.000 (0.274) loss 1.7930 (1.6604) acc 84.3750 (84.3750) lr 1.0000e-05 eta 0:08:02
epoch [1/50] batch [10/26] time 0.073 (0.229) data 0.000 (0.144) loss 1.5166 (1.7031) acc 87.5000 (83.1250) lr 1.0000e-05 eta 0:04:55
epoch [1/50] batch [15/26] time 0.071 (0.176) data 0.000 (0.096) loss 1.8418 (1.7844) acc 84.3750 (80.4167) lr 1.0000e-05 eta 0:03:46
epoch [1/50] batch [20/26] time 0.070 (0.153) data 0.000 (0.075) loss 1.5811 (1.7450) acc 78.1250 (80.7812) lr 1.0000e-05 eta 0:03:15
epoch [1/50] batch [25/26] time 0.072 (0.137) data 0.000 (0.060) loss 1.6738 (1.7328) acc 84.3750 (80.8750) lr 1.0000e-05 eta 0:02:54
epoch [2/50] batch [5/26] time 0.069 (0.266) data 0.000 (0.189) loss 0.9736 (1.2469) acc 84.3750 (80.6250) lr 2.0000e-03 eta 0:05:37
epoch [2/50] batch [10/26] time 0.071 (0.168) data 0.000 (0.094) loss 0.8086 (1.1032) acc 84.3750 (81.5625) lr 2.0000e-03 eta 0:03:32
epoch [2/50] batch [15/26] time 0.070 (0.135) data 0.000 (0.063) loss 0.7085 (0.9956) acc 84.3750 (82.5000) lr 2.0000e-03 eta 0:02:50
epoch [2/50] batch [20/26] time 0.070 (0.119) data 0.000 (0.047) loss 0.8965 (0.9759) acc 84.3750 (82.6562) lr 2.0000e-03 eta 0:02:29
epoch [2/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.5601 (0.9497) acc 87.5000 (81.5000) lr 2.0000e-03 eta 0:02:16
epoch [3/50] batch [5/26] time 0.070 (0.256) data 0.000 (0.182) loss 0.8853 (0.7516) acc 75.0000 (82.5000) lr 1.9980e-03 eta 0:05:18
epoch [3/50] batch [10/26] time 0.070 (0.163) data 0.000 (0.091) loss 1.0918 (0.7249) acc 71.8750 (82.8125) lr 1.9980e-03 eta 0:03:22
epoch [3/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.061) loss 0.8267 (0.7255) acc 78.1250 (82.7083) lr 1.9980e-03 eta 0:02:43
epoch [3/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.046) loss 1.0566 (0.7485) acc 71.8750 (82.1875) lr 1.9980e-03 eta 0:02:23
epoch [3/50] batch [25/26] time 0.069 (0.107) data 0.000 (0.037) loss 0.7827 (0.7406) acc 75.0000 (82.3750) lr 1.9980e-03 eta 0:02:11
epoch [4/50] batch [5/26] time 0.069 (0.248) data 0.000 (0.174) loss 0.7061 (0.7329) acc 84.3750 (83.1250) lr 1.9921e-03 eta 0:05:01
epoch [4/50] batch [10/26] time 0.070 (0.159) data 0.000 (0.087) loss 0.6792 (0.7377) acc 84.3750 (83.4375) lr 1.9921e-03 eta 0:03:12
epoch [4/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.8633 (0.7466) acc 84.3750 (83.3333) lr 1.9921e-03 eta 0:02:36
epoch [4/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.8555 (0.7322) acc 90.6250 (84.0625) lr 1.9921e-03 eta 0:02:17
epoch [4/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.4761 (0.7270) acc 84.3750 (83.6250) lr 1.9921e-03 eta 0:02:06
epoch [5/50] batch [5/26] time 0.072 (0.239) data 0.000 (0.167) loss 0.7817 (0.7390) acc 81.2500 (80.6250) lr 1.9823e-03 eta 0:04:44
epoch [5/50] batch [10/26] time 0.070 (0.155) data 0.001 (0.084) loss 0.4614 (0.6761) acc 87.5000 (83.1250) lr 1.9823e-03 eta 0:03:03
epoch [5/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.056) loss 0.7349 (0.6544) acc 87.5000 (83.7500) lr 1.9823e-03 eta 0:02:29
epoch [5/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.042) loss 0.2827 (0.6200) acc 96.8750 (85.7812) lr 1.9823e-03 eta 0:02:12
epoch [5/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.6206 (0.6361) acc 90.6250 (85.6250) lr 1.9823e-03 eta 0:02:01
epoch [6/50] batch [5/26] time 0.072 (0.245) data 0.000 (0.172) loss 0.9785 (0.5754) acc 75.0000 (86.8750) lr 1.9686e-03 eta 0:04:45
epoch [6/50] batch [10/26] time 0.071 (0.158) data 0.000 (0.086) loss 0.4658 (0.5744) acc 84.3750 (86.8750) lr 1.9686e-03 eta 0:03:03
epoch [6/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 1.3115 (0.5983) acc 65.6250 (85.2083) lr 1.9686e-03 eta 0:02:28
epoch [6/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.9307 (0.6355) acc 71.8750 (83.4375) lr 1.9686e-03 eta 0:02:11
epoch [6/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.6621 (0.6498) acc 84.3750 (83.6250) lr 1.9686e-03 eta 0:02:00
epoch [7/50] batch [5/26] time 0.074 (0.236) data 0.000 (0.160) loss 0.6875 (0.7393) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:04:29
epoch [7/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.080) loss 0.5708 (0.7359) acc 90.6250 (82.1875) lr 1.9511e-03 eta 0:02:54
epoch [7/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.054) loss 0.5830 (0.6877) acc 90.6250 (83.9583) lr 1.9511e-03 eta 0:02:22
epoch [7/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.8774 (0.6707) acc 81.2500 (84.8438) lr 1.9511e-03 eta 0:02:06
epoch [7/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.6553 (0.6776) acc 75.0000 (83.8750) lr 1.9511e-03 eta 0:01:56
epoch [8/50] batch [5/26] time 0.071 (0.241) data 0.000 (0.169) loss 0.5146 (0.5754) acc 84.3750 (83.7500) lr 1.9298e-03 eta 0:04:28
epoch [8/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.4819 (0.6642) acc 81.2500 (81.8750) lr 1.9298e-03 eta 0:02:53
epoch [8/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.6958 (0.6585) acc 84.3750 (82.9167) lr 1.9298e-03 eta 0:02:21
epoch [8/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.7124 (0.6457) acc 81.2500 (83.5938) lr 1.9298e-03 eta 0:02:04
epoch [8/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.7798 (0.6313) acc 81.2500 (84.1250) lr 1.9298e-03 eta 0:01:54
epoch [9/50] batch [5/26] time 0.073 (0.247) data 0.000 (0.175) loss 0.7773 (0.7777) acc 87.5000 (82.5000) lr 1.9048e-03 eta 0:04:28
epoch [9/50] batch [10/26] time 0.070 (0.159) data 0.000 (0.088) loss 0.6509 (0.7435) acc 90.6250 (83.4375) lr 1.9048e-03 eta 0:02:51
epoch [9/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.059) loss 0.5757 (0.7201) acc 87.5000 (83.5417) lr 1.9048e-03 eta 0:02:19
epoch [9/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.6367 (0.7018) acc 84.3750 (83.5938) lr 1.9048e-03 eta 0:02:02
epoch [9/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.3281 (0.6725) acc 90.6250 (83.5000) lr 1.9048e-03 eta 0:01:52
epoch [10/50] batch [5/26] time 0.073 (0.240) data 0.000 (0.167) loss 0.4812 (0.4761) acc 90.6250 (89.3750) lr 1.8763e-03 eta 0:04:14
epoch [10/50] batch [10/26] time 0.071 (0.155) data 0.000 (0.084) loss 0.4441 (0.5393) acc 87.5000 (86.8750) lr 1.8763e-03 eta 0:02:44
epoch [10/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.056) loss 0.6133 (0.5449) acc 78.1250 (86.4583) lr 1.8763e-03 eta 0:02:13
epoch [10/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.9819 (0.5662) acc 78.1250 (86.4062) lr 1.8763e-03 eta 0:01:58
epoch [10/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.7261 (0.6219) acc 81.2500 (84.2500) lr 1.8763e-03 eta 0:01:48
epoch [11/50] batch [5/26] time 0.069 (0.252) data 0.000 (0.180) loss 0.7227 (0.6482) acc 81.2500 (83.7500) lr 1.8443e-03 eta 0:04:20
epoch [11/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.090) loss 0.6328 (0.6511) acc 87.5000 (84.6875) lr 1.8443e-03 eta 0:02:46
epoch [11/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 0.8428 (0.7085) acc 78.1250 (82.2917) lr 1.8443e-03 eta 0:02:14
epoch [11/50] batch [20/26] time 0.071 (0.116) data 0.000 (0.045) loss 0.5752 (0.6748) acc 81.2500 (82.9688) lr 1.8443e-03 eta 0:01:58
epoch [11/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.036) loss 0.4878 (0.6627) acc 96.8750 (83.6250) lr 1.8443e-03 eta 0:01:48
epoch [12/50] batch [5/26] time 0.073 (0.227) data 0.000 (0.154) loss 0.8916 (0.6809) acc 71.8750 (82.5000) lr 1.8090e-03 eta 0:03:48
epoch [12/50] batch [10/26] time 0.070 (0.149) data 0.000 (0.077) loss 0.4812 (0.6196) acc 90.6250 (85.0000) lr 1.8090e-03 eta 0:02:29
epoch [12/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.052) loss 0.6621 (0.6319) acc 84.3750 (84.7917) lr 1.8090e-03 eta 0:02:02
epoch [12/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.7549 (0.5887) acc 78.1250 (85.6250) lr 1.8090e-03 eta 0:01:49
epoch [12/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.4719 (0.5681) acc 84.3750 (85.6250) lr 1.8090e-03 eta 0:01:40
epoch [13/50] batch [5/26] time 0.070 (0.230) data 0.001 (0.158) loss 0.5127 (0.6148) acc 84.3750 (85.6250) lr 1.7705e-03 eta 0:03:46
epoch [13/50] batch [10/26] time 0.071 (0.150) data 0.001 (0.079) loss 0.5352 (0.6064) acc 84.3750 (84.3750) lr 1.7705e-03 eta 0:02:27
epoch [13/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.6348 (0.6196) acc 87.5000 (84.3750) lr 1.7705e-03 eta 0:02:00
epoch [13/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.040) loss 0.4973 (0.6224) acc 90.6250 (84.8438) lr 1.7705e-03 eta 0:01:46
epoch [13/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.032) loss 0.7202 (0.6222) acc 84.3750 (85.0000) lr 1.7705e-03 eta 0:01:38
epoch [14/50] batch [5/26] time 0.071 (0.245) data 0.000 (0.173) loss 0.3682 (0.5541) acc 93.7500 (85.0000) lr 1.7290e-03 eta 0:03:54
epoch [14/50] batch [10/26] time 0.070 (0.159) data 0.000 (0.088) loss 0.9346 (0.5909) acc 81.2500 (85.0000) lr 1.7290e-03 eta 0:02:31
epoch [14/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.058) loss 0.5664 (0.5742) acc 90.6250 (85.8333) lr 1.7290e-03 eta 0:02:02
epoch [14/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.4500 (0.6136) acc 87.5000 (84.5312) lr 1.7290e-03 eta 0:01:48
epoch [14/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.4485 (0.6132) acc 84.3750 (84.1250) lr 1.7290e-03 eta 0:01:39
epoch [15/50] batch [5/26] time 0.070 (0.241) data 0.000 (0.166) loss 0.5161 (0.4284) acc 90.6250 (88.1250) lr 1.6845e-03 eta 0:03:44
epoch [15/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.083) loss 0.6289 (0.5439) acc 84.3750 (86.2500) lr 1.6845e-03 eta 0:02:24
epoch [15/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.055) loss 0.6924 (0.5598) acc 75.0000 (85.2083) lr 1.6845e-03 eta 0:01:57
epoch [15/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5264 (0.6006) acc 87.5000 (83.9062) lr 1.6845e-03 eta 0:01:43
epoch [15/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.033) loss 0.4365 (0.6290) acc 84.3750 (83.8750) lr 1.6845e-03 eta 0:01:35
epoch [16/50] batch [5/26] time 0.071 (0.267) data 0.000 (0.194) loss 0.4998 (0.5809) acc 87.5000 (85.6250) lr 1.6374e-03 eta 0:04:01
epoch [16/50] batch [10/26] time 0.071 (0.169) data 0.000 (0.097) loss 0.3794 (0.5698) acc 90.6250 (85.6250) lr 1.6374e-03 eta 0:02:32
epoch [16/50] batch [15/26] time 0.070 (0.136) data 0.000 (0.065) loss 0.8647 (0.5847) acc 81.2500 (85.6250) lr 1.6374e-03 eta 0:02:01
epoch [16/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.049) loss 0.3430 (0.5597) acc 93.7500 (86.5625) lr 1.6374e-03 eta 0:01:46
epoch [16/50] batch [25/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.6499 (0.5676) acc 84.3750 (86.6250) lr 1.6374e-03 eta 0:01:37
epoch [17/50] batch [5/26] time 0.071 (0.248) data 0.000 (0.175) loss 0.6138 (0.6764) acc 87.5000 (83.1250) lr 1.5878e-03 eta 0:03:38
epoch [17/50] batch [10/26] time 0.070 (0.160) data 0.000 (0.088) loss 0.4207 (0.6091) acc 84.3750 (85.0000) lr 1.5878e-03 eta 0:02:19
epoch [17/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.058) loss 0.5010 (0.6181) acc 87.5000 (83.9583) lr 1.5878e-03 eta 0:01:52
epoch [17/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.8037 (0.5700) acc 81.2500 (85.7812) lr 1.5878e-03 eta 0:01:39
epoch [17/50] batch [25/26] time 0.075 (0.106) data 0.000 (0.035) loss 0.6167 (0.5688) acc 90.6250 (86.1250) lr 1.5878e-03 eta 0:01:31
epoch [18/50] batch [5/26] time 0.071 (0.246) data 0.000 (0.172) loss 0.8315 (0.6042) acc 75.0000 (84.3750) lr 1.5358e-03 eta 0:03:29
epoch [18/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.086) loss 0.5449 (0.6114) acc 84.3750 (84.6875) lr 1.5358e-03 eta 0:02:14
epoch [18/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.6699 (0.6128) acc 81.2500 (85.8333) lr 1.5358e-03 eta 0:01:48
epoch [18/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.043) loss 0.6826 (0.6779) acc 78.1250 (84.0625) lr 1.5358e-03 eta 0:01:36
epoch [18/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.8008 (0.6394) acc 78.1250 (84.6250) lr 1.5358e-03 eta 0:01:28
epoch [19/50] batch [5/26] time 0.074 (0.245) data 0.000 (0.172) loss 0.9600 (0.6607) acc 78.1250 (83.1250) lr 1.4818e-03 eta 0:03:22
epoch [19/50] batch [10/26] time 0.071 (0.158) data 0.000 (0.086) loss 0.3794 (0.5191) acc 93.7500 (87.1875) lr 1.4818e-03 eta 0:02:09
epoch [19/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.5293 (0.5281) acc 90.6250 (87.2917) lr 1.4818e-03 eta 0:01:45
epoch [19/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.5728 (0.5060) acc 84.3750 (87.6562) lr 1.4818e-03 eta 0:01:32
epoch [19/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.6108 (0.5220) acc 90.6250 (87.7500) lr 1.4818e-03 eta 0:01:25
epoch [20/50] batch [5/26] time 0.077 (0.237) data 0.000 (0.161) loss 0.3081 (0.5609) acc 87.5000 (82.5000) lr 1.4258e-03 eta 0:03:10
epoch [20/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.081) loss 0.6519 (0.5479) acc 84.3750 (85.3125) lr 1.4258e-03 eta 0:02:02
epoch [20/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.054) loss 0.9756 (0.5729) acc 62.5000 (84.3750) lr 1.4258e-03 eta 0:01:39
epoch [20/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.5767 (0.5332) acc 84.3750 (85.7812) lr 1.4258e-03 eta 0:01:28
epoch [20/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.7432 (0.5471) acc 84.3750 (85.8750) lr 1.4258e-03 eta 0:01:21
epoch [21/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.164) loss 0.9814 (0.5575) acc 75.0000 (86.2500) lr 1.3681e-03 eta 0:03:02
epoch [21/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.082) loss 0.5059 (0.5786) acc 84.3750 (84.6875) lr 1.3681e-03 eta 0:01:58
epoch [21/50] batch [15/26] time 0.069 (0.126) data 0.000 (0.055) loss 0.5889 (0.5932) acc 87.5000 (85.0000) lr 1.3681e-03 eta 0:01:36
epoch [21/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.6562 (0.5861) acc 81.2500 (85.3125) lr 1.3681e-03 eta 0:01:25
epoch [21/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.6455 (0.5706) acc 87.5000 (86.1250) lr 1.3681e-03 eta 0:01:18
epoch [22/50] batch [5/26] time 0.072 (0.241) data 0.000 (0.167) loss 0.2451 (0.4520) acc 100.0000 (90.0000) lr 1.3090e-03 eta 0:03:00
epoch [22/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.4958 (0.5500) acc 87.5000 (87.1875) lr 1.3090e-03 eta 0:01:56
epoch [22/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.056) loss 0.2357 (0.5403) acc 96.8750 (87.5000) lr 1.3090e-03 eta 0:01:34
epoch [22/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.042) loss 0.4744 (0.5550) acc 87.5000 (86.7188) lr 1.3090e-03 eta 0:01:23
epoch [22/50] batch [25/26] time 0.074 (0.106) data 0.000 (0.034) loss 0.5532 (0.5738) acc 81.2500 (86.6250) lr 1.3090e-03 eta 0:01:17
epoch [23/50] batch [5/26] time 0.072 (0.233) data 0.000 (0.159) loss 0.4785 (0.4846) acc 87.5000 (88.7500) lr 1.2487e-03 eta 0:02:48
epoch [23/50] batch [10/26] time 0.071 (0.152) data 0.000 (0.080) loss 0.2203 (0.4921) acc 93.7500 (87.1875) lr 1.2487e-03 eta 0:01:49
epoch [23/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.053) loss 0.3101 (0.4762) acc 93.7500 (87.9167) lr 1.2487e-03 eta 0:01:29
epoch [23/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.4658 (0.4995) acc 81.2500 (86.0938) lr 1.2487e-03 eta 0:01:18
epoch [23/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.4927 (0.5272) acc 90.6250 (86.2500) lr 1.2487e-03 eta 0:01:12
epoch [24/50] batch [5/26] time 0.077 (0.257) data 0.000 (0.183) loss 0.5977 (0.5009) acc 81.2500 (88.1250) lr 1.1874e-03 eta 0:02:59
epoch [24/50] batch [10/26] time 0.071 (0.164) data 0.000 (0.092) loss 0.2876 (0.4692) acc 96.8750 (89.0625) lr 1.1874e-03 eta 0:01:53
epoch [24/50] batch [15/26] time 0.071 (0.133) data 0.000 (0.061) loss 0.4966 (0.4857) acc 90.6250 (88.3333) lr 1.1874e-03 eta 0:01:31
epoch [24/50] batch [20/26] time 0.071 (0.118) data 0.000 (0.046) loss 1.0742 (0.5276) acc 71.8750 (87.1875) lr 1.1874e-03 eta 0:01:20
epoch [24/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.037) loss 0.5127 (0.5277) acc 87.5000 (86.8750) lr 1.1874e-03 eta 0:01:13
epoch [25/50] batch [5/26] time 0.071 (0.239) data 0.000 (0.166) loss 0.3284 (0.5245) acc 96.8750 (89.3750) lr 1.1253e-03 eta 0:02:40
epoch [25/50] batch [10/26] time 0.071 (0.155) data 0.000 (0.083) loss 0.7617 (0.5121) acc 81.2500 (89.6875) lr 1.1253e-03 eta 0:01:43
epoch [25/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.055) loss 0.5166 (0.5000) acc 87.5000 (88.7500) lr 1.1253e-03 eta 0:01:23
epoch [25/50] batch [20/26] time 0.074 (0.113) data 0.000 (0.042) loss 0.5293 (0.5086) acc 87.5000 (87.9688) lr 1.1253e-03 eta 0:01:14
epoch [25/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.4001 (0.5001) acc 87.5000 (88.0000) lr 1.1253e-03 eta 0:01:08
epoch [26/50] batch [5/26] time 0.071 (0.229) data 0.000 (0.156) loss 0.5840 (0.6307) acc 78.1250 (82.5000) lr 1.0628e-03 eta 0:02:27
epoch [26/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.078) loss 0.2271 (0.5168) acc 100.0000 (85.9375) lr 1.0628e-03 eta 0:01:35
epoch [26/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.4849 (0.5401) acc 90.6250 (85.8333) lr 1.0628e-03 eta 0:01:18
epoch [26/50] batch [20/26] time 0.072 (0.111) data 0.000 (0.039) loss 0.4746 (0.5246) acc 84.3750 (86.2500) lr 1.0628e-03 eta 0:01:09
epoch [26/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.6436 (0.5497) acc 81.2500 (85.6250) lr 1.0628e-03 eta 0:01:04
epoch [27/50] batch [5/26] time 0.071 (0.210) data 0.000 (0.136) loss 0.4604 (0.4398) acc 90.6250 (92.5000) lr 1.0000e-03 eta 0:02:10
epoch [27/50] batch [10/26] time 0.075 (0.142) data 0.000 (0.068) loss 0.3770 (0.4943) acc 87.5000 (89.3750) lr 1.0000e-03 eta 0:01:26
epoch [27/50] batch [15/26] time 0.071 (0.118) data 0.000 (0.045) loss 0.4417 (0.5010) acc 87.5000 (89.1667) lr 1.0000e-03 eta 0:01:11
epoch [27/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.034) loss 0.3562 (0.5018) acc 90.6250 (88.9062) lr 1.0000e-03 eta 0:01:04
epoch [27/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.027) loss 0.7192 (0.5124) acc 78.1250 (88.6250) lr 1.0000e-03 eta 0:00:59
epoch [28/50] batch [5/26] time 0.075 (0.226) data 0.004 (0.154) loss 0.5601 (0.4439) acc 84.3750 (90.6250) lr 9.3721e-04 eta 0:02:14
epoch [28/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.077) loss 0.2262 (0.4707) acc 96.8750 (89.6875) lr 9.3721e-04 eta 0:01:27
epoch [28/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.051) loss 0.5322 (0.5170) acc 87.5000 (87.9167) lr 9.3721e-04 eta 0:01:11
epoch [28/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.6260 (0.5373) acc 84.3750 (87.6562) lr 9.3721e-04 eta 0:01:03
epoch [28/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5293 (0.5280) acc 87.5000 (87.7500) lr 9.3721e-04 eta 0:00:58
epoch [29/50] batch [5/26] time 0.072 (0.219) data 0.000 (0.145) loss 0.4307 (0.4945) acc 87.5000 (86.2500) lr 8.7467e-04 eta 0:02:03
epoch [29/50] batch [10/26] time 0.072 (0.145) data 0.000 (0.072) loss 0.6265 (0.4929) acc 84.3750 (87.5000) lr 8.7467e-04 eta 0:01:21
epoch [29/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.7744 (0.5236) acc 81.2500 (86.6667) lr 8.7467e-04 eta 0:01:06
epoch [29/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.036) loss 0.4397 (0.5338) acc 93.7500 (87.5000) lr 8.7467e-04 eta 0:00:59
epoch [29/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.3359 (0.5321) acc 93.7500 (87.5000) lr 8.7467e-04 eta 0:00:54
epoch [30/50] batch [5/26] time 0.072 (0.232) data 0.000 (0.160) loss 0.5918 (0.5188) acc 81.2500 (84.3750) lr 8.1262e-04 eta 0:02:05
epoch [30/50] batch [10/26] time 0.071 (0.152) data 0.000 (0.080) loss 0.3364 (0.5642) acc 90.6250 (85.9375) lr 8.1262e-04 eta 0:01:21
epoch [30/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.053) loss 0.7085 (0.6129) acc 84.3750 (85.4167) lr 8.1262e-04 eta 0:01:06
epoch [30/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.8027 (0.5828) acc 81.2500 (86.7188) lr 8.1262e-04 eta 0:00:58
epoch [30/50] batch [25/26] time 0.073 (0.103) data 0.002 (0.032) loss 0.4688 (0.5806) acc 93.7500 (86.8750) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/26] time 0.073 (0.234) data 0.001 (0.160) loss 0.4080 (0.4717) acc 90.6250 (88.1250) lr 7.5131e-04 eta 0:02:00
epoch [31/50] batch [10/26] time 0.073 (0.153) data 0.000 (0.080) loss 0.5522 (0.5127) acc 87.5000 (86.8750) lr 7.5131e-04 eta 0:01:18
epoch [31/50] batch [15/26] time 0.073 (0.126) data 0.000 (0.054) loss 0.5703 (0.5485) acc 84.3750 (86.8750) lr 7.5131e-04 eta 0:01:03
epoch [31/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.040) loss 0.7959 (0.5409) acc 75.0000 (86.2500) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.3359 (0.5385) acc 93.7500 (87.0000) lr 7.5131e-04 eta 0:00:51
epoch [32/50] batch [5/26] time 0.072 (0.241) data 0.000 (0.166) loss 0.7300 (0.5376) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:01:57
epoch [32/50] batch [10/26] time 0.072 (0.156) data 0.000 (0.083) loss 0.4094 (0.4594) acc 90.6250 (89.0625) lr 6.9098e-04 eta 0:01:15
epoch [32/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.055) loss 0.5781 (0.4676) acc 87.5000 (88.9583) lr 6.9098e-04 eta 0:01:00
epoch [32/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.6011 (0.4690) acc 87.5000 (89.0625) lr 6.9098e-04 eta 0:00:53
epoch [32/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.033) loss 0.3955 (0.4773) acc 93.7500 (88.8750) lr 6.9098e-04 eta 0:00:49
epoch [33/50] batch [5/26] time 0.071 (0.253) data 0.000 (0.179) loss 0.6094 (0.4710) acc 84.3750 (89.3750) lr 6.3188e-04 eta 0:01:57
epoch [33/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.090) loss 0.5322 (0.5166) acc 87.5000 (88.4375) lr 6.3188e-04 eta 0:01:14
epoch [33/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 0.1870 (0.5006) acc 100.0000 (89.1667) lr 6.3188e-04 eta 0:00:59
epoch [33/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.4060 (0.5100) acc 90.6250 (88.7500) lr 6.3188e-04 eta 0:00:52
epoch [33/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.3335 (0.4998) acc 93.7500 (88.8750) lr 6.3188e-04 eta 0:00:47
epoch [34/50] batch [5/26] time 0.071 (0.215) data 0.000 (0.141) loss 0.2715 (0.4136) acc 96.8750 (88.7500) lr 5.7422e-04 eta 0:01:33
epoch [34/50] batch [10/26] time 0.071 (0.143) data 0.000 (0.070) loss 0.4556 (0.4115) acc 87.5000 (89.0625) lr 5.7422e-04 eta 0:01:01
epoch [34/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.047) loss 0.4043 (0.4590) acc 90.6250 (88.5417) lr 5.7422e-04 eta 0:00:50
epoch [34/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.035) loss 0.4570 (0.4761) acc 87.5000 (88.1250) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.028) loss 0.7471 (0.4944) acc 78.1250 (87.3750) lr 5.7422e-04 eta 0:00:41
epoch [35/50] batch [5/26] time 0.071 (0.219) data 0.000 (0.147) loss 0.5503 (0.4625) acc 84.3750 (87.5000) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [10/26] time 0.072 (0.145) data 0.000 (0.074) loss 0.3733 (0.4799) acc 90.6250 (88.1250) lr 5.1825e-04 eta 0:00:59
epoch [35/50] batch [15/26] time 0.073 (0.121) data 0.000 (0.049) loss 0.4138 (0.4927) acc 87.5000 (88.1250) lr 5.1825e-04 eta 0:00:48
epoch [35/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.9097 (0.5123) acc 71.8750 (87.1875) lr 5.1825e-04 eta 0:00:42
epoch [35/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.8696 (0.5353) acc 87.5000 (87.1250) lr 5.1825e-04 eta 0:00:39
epoch [36/50] batch [5/26] time 0.073 (0.212) data 0.000 (0.139) loss 0.5078 (0.5346) acc 87.5000 (85.6250) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [10/26] time 0.072 (0.142) data 0.000 (0.070) loss 0.6245 (0.5140) acc 87.5000 (87.8125) lr 4.6417e-04 eta 0:00:53
epoch [36/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.3208 (0.4795) acc 96.8750 (89.1667) lr 4.6417e-04 eta 0:00:44
epoch [36/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.4568 (0.4751) acc 87.5000 (89.2188) lr 4.6417e-04 eta 0:00:39
epoch [36/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.028) loss 0.3191 (0.5012) acc 90.6250 (88.3750) lr 4.6417e-04 eta 0:00:36
epoch [37/50] batch [5/26] time 0.072 (0.216) data 0.000 (0.142) loss 0.6753 (0.4348) acc 87.5000 (91.8750) lr 4.1221e-04 eta 0:01:17
epoch [37/50] batch [10/26] time 0.071 (0.144) data 0.000 (0.071) loss 0.4783 (0.5039) acc 87.5000 (88.4375) lr 4.1221e-04 eta 0:00:50
epoch [37/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.047) loss 0.3611 (0.5457) acc 93.7500 (87.0833) lr 4.1221e-04 eta 0:00:41
epoch [37/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.036) loss 0.3528 (0.5396) acc 96.8750 (87.6562) lr 4.1221e-04 eta 0:00:36
epoch [37/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.2269 (0.5391) acc 93.7500 (87.5000) lr 4.1221e-04 eta 0:00:33
epoch [38/50] batch [5/26] time 0.072 (0.219) data 0.000 (0.146) loss 0.3831 (0.4425) acc 93.7500 (92.5000) lr 3.6258e-04 eta 0:01:12
epoch [38/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.073) loss 0.3215 (0.4536) acc 93.7500 (91.2500) lr 3.6258e-04 eta 0:00:47
epoch [38/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.049) loss 0.4565 (0.4705) acc 93.7500 (91.2500) lr 3.6258e-04 eta 0:00:38
epoch [38/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.4553 (0.4803) acc 87.5000 (90.0000) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [25/26] time 0.074 (0.101) data 0.000 (0.029) loss 0.3342 (0.4536) acc 93.7500 (90.8750) lr 3.6258e-04 eta 0:00:31
epoch [39/50] batch [5/26] time 0.072 (0.219) data 0.000 (0.147) loss 0.5894 (0.4714) acc 87.5000 (87.5000) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [10/26] time 0.075 (0.146) data 0.000 (0.074) loss 0.7866 (0.5076) acc 81.2500 (88.1250) lr 3.1545e-04 eta 0:00:43
epoch [39/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.2874 (0.5187) acc 93.7500 (87.7083) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.3257 (0.5370) acc 93.7500 (87.6562) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.4348 (0.5065) acc 87.5000 (88.6250) lr 3.1545e-04 eta 0:00:28
epoch [40/50] batch [5/26] time 0.072 (0.213) data 0.001 (0.141) loss 0.4250 (0.4460) acc 87.5000 (90.0000) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.071) loss 0.4819 (0.4141) acc 84.3750 (89.3750) lr 2.7103e-04 eta 0:00:39
epoch [40/50] batch [15/26] time 0.071 (0.118) data 0.000 (0.047) loss 1.0820 (0.4570) acc 81.2500 (89.5833) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.1711 (0.4790) acc 96.8750 (89.8438) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.028) loss 0.5205 (0.4591) acc 90.6250 (90.3750) lr 2.7103e-04 eta 0:00:25
epoch [41/50] batch [5/26] time 0.071 (0.230) data 0.000 (0.159) loss 0.3474 (0.5728) acc 90.6250 (88.7500) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.079) loss 0.4224 (0.5554) acc 93.7500 (88.4375) lr 2.2949e-04 eta 0:00:37
epoch [41/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.053) loss 0.3755 (0.5374) acc 93.7500 (88.9583) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [20/26] time 0.073 (0.111) data 0.000 (0.040) loss 0.2927 (0.4821) acc 96.8750 (90.7812) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.7402 (0.4826) acc 75.0000 (90.1250) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/26] time 0.072 (0.241) data 0.000 (0.169) loss 0.5132 (0.5749) acc 90.6250 (86.2500) lr 1.9098e-04 eta 0:00:55
epoch [42/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.3770 (0.5510) acc 93.7500 (87.1875) lr 1.9098e-04 eta 0:00:34
epoch [42/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.9585 (0.5611) acc 75.0000 (86.6667) lr 1.9098e-04 eta 0:00:27
epoch [42/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5444 (0.5437) acc 84.3750 (87.1875) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.4907 (0.5303) acc 84.3750 (87.6250) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.074 (0.216) data 0.000 (0.145) loss 0.2729 (0.3894) acc 90.6250 (93.1250) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [10/26] time 0.071 (0.144) data 0.000 (0.072) loss 0.6885 (0.4237) acc 81.2500 (90.9375) lr 1.5567e-04 eta 0:00:28
epoch [43/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.048) loss 0.3301 (0.4350) acc 90.6250 (90.8333) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.4321 (0.4400) acc 90.6250 (90.9375) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.5728 (0.4621) acc 84.3750 (90.1250) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.070 (0.235) data 0.000 (0.162) loss 0.9004 (0.6563) acc 78.1250 (84.3750) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [10/26] time 0.071 (0.153) data 0.000 (0.081) loss 0.3286 (0.5766) acc 93.7500 (86.8750) lr 1.2369e-04 eta 0:00:26
epoch [44/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.054) loss 0.3298 (0.5279) acc 93.7500 (88.3333) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.069 (0.112) data 0.000 (0.041) loss 0.2554 (0.4978) acc 93.7500 (89.0625) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.033) loss 0.2812 (0.4917) acc 96.8750 (88.8750) lr 1.2369e-04 eta 0:00:16
epoch [45/50] batch [5/26] time 0.071 (0.234) data 0.000 (0.161) loss 0.7461 (0.5407) acc 84.3750 (88.7500) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.081) loss 0.4077 (0.5592) acc 93.7500 (88.1250) lr 9.5173e-05 eta 0:00:22
epoch [45/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.054) loss 0.3625 (0.5515) acc 90.6250 (88.3333) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [20/26] time 0.074 (0.112) data 0.000 (0.040) loss 0.2778 (0.5156) acc 96.8750 (88.9062) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.3391 (0.4967) acc 90.6250 (89.2500) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.071 (0.218) data 0.000 (0.145) loss 0.4607 (0.5392) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.073) loss 0.3586 (0.4884) acc 93.7500 (88.4375) lr 7.0224e-05 eta 0:00:17
epoch [46/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.048) loss 0.2900 (0.4568) acc 96.8750 (89.7917) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/26] time 0.072 (0.108) data 0.000 (0.036) loss 0.6152 (0.4829) acc 90.6250 (89.3750) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.029) loss 0.3623 (0.4793) acc 90.6250 (89.3750) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.077 (0.242) data 0.000 (0.169) loss 0.3923 (0.3562) acc 93.7500 (94.3750) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.085) loss 0.4470 (0.3953) acc 87.5000 (93.1250) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.5610 (0.4232) acc 87.5000 (91.4583) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.3289 (0.4235) acc 93.7500 (91.5625) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.4141 (0.4653) acc 90.6250 (90.1250) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.076 (0.230) data 0.000 (0.156) loss 0.2646 (0.4384) acc 96.8750 (89.3750) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.078) loss 0.2808 (0.4348) acc 96.8750 (90.0000) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.052) loss 0.4226 (0.4428) acc 87.5000 (89.7917) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.2330 (0.4322) acc 96.8750 (90.1562) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.2637 (0.4346) acc 96.8750 (89.8750) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.071 (0.225) data 0.000 (0.151) loss 0.2886 (0.4356) acc 90.6250 (89.3750) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.076) loss 0.2449 (0.4262) acc 96.8750 (90.3125) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.050) loss 0.5020 (0.4180) acc 87.5000 (91.2500) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3035 (0.4092) acc 93.7500 (91.0938) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.030) loss 0.4150 (0.4353) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.075 (0.219) data 0.001 (0.144) loss 0.2056 (0.4573) acc 100.0000 (90.6250) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.072 (0.146) data 0.000 (0.072) loss 0.4778 (0.4157) acc 84.3750 (91.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.048) loss 0.5459 (0.4333) acc 90.6250 (91.4583) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.036) loss 0.4756 (0.4695) acc 87.5000 (90.0000) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.029) loss 0.2253 (0.4778) acc 100.0000 (89.7500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:04<02:18,  4.62s/it]  6%|▋         | 2/31 [00:05<01:04,  2.24s/it] 10%|▉         | 3/31 [00:05<00:41,  1.48s/it] 13%|█▎        | 4/31 [00:06<00:30,  1.12s/it] 16%|█▌        | 5/31 [00:06<00:24,  1.07it/s] 19%|█▉        | 6/31 [00:07<00:20,  1.23it/s] 23%|██▎       | 7/31 [00:08<00:17,  1.36it/s] 26%|██▌       | 8/31 [00:08<00:15,  1.46it/s] 29%|██▉       | 9/31 [00:09<00:14,  1.54it/s] 32%|███▏      | 10/31 [00:09<00:13,  1.58it/s] 35%|███▌      | 11/31 [00:10<00:12,  1.63it/s] 39%|███▊      | 12/31 [00:10<00:11,  1.66it/s] 42%|████▏     | 13/31 [00:11<00:10,  1.68it/s] 45%|████▌     | 14/31 [00:12<00:10,  1.70it/s] 48%|████▊     | 15/31 [00:12<00:09,  1.71it/s] 52%|█████▏    | 16/31 [00:13<00:08,  1.71it/s] 55%|█████▍    | 17/31 [00:13<00:08,  1.68it/s] 58%|█████▊    | 18/31 [00:14<00:07,  1.70it/s] 61%|██████▏   | 19/31 [00:15<00:07,  1.71it/s] 65%|██████▍   | 20/31 [00:15<00:06,  1.72it/s] 68%|██████▊   | 21/31 [00:16<00:05,  1.72it/s] 71%|███████   | 22/31 [00:16<00:05,  1.73it/s] 74%|███████▍  | 23/31 [00:17<00:04,  1.73it/s] 77%|███████▋  | 24/31 [00:17<00:04,  1.73it/s] 81%|████████  | 25/31 [00:18<00:03,  1.73it/s] 84%|████████▍ | 26/31 [00:19<00:02,  1.73it/s] 87%|████████▋ | 27/31 [00:19<00:02,  1.73it/s] 90%|█████████ | 28/31 [00:20<00:01,  1.74it/s] 94%|█████████▎| 29/31 [00:20<00:01,  1.73it/s] 97%|█████████▋| 30/31 [00:21<00:00,  1.73it/s]100%|██████████| 31/31 [00:21<00:00,  1.90it/s]100%|██████████| 31/31 [00:21<00:00,  1.41it/s]
=> result
* total: 15,300
* correct: 13,855
* accuracy: 90.6%
* error: 9.4%
* macro_f1: 90.6%
Elapsed: 0:02:41
Run this job and save the output to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.070 (0.359) data 0.000 (0.264) loss 2.8320 (2.2092) acc 53.1250 (75.6250) lr 1.0000e-05 eta 0:07:44
epoch [1/50] batch [10/26] time 0.093 (0.217) data 0.022 (0.134) loss 2.5215 (2.0914) acc 78.1250 (79.0625) lr 1.0000e-05 eta 0:04:39
epoch [1/50] batch [15/26] time 0.071 (0.168) data 0.000 (0.090) loss 1.9385 (2.0145) acc 81.2500 (80.4167) lr 1.0000e-05 eta 0:03:35
epoch [1/50] batch [20/26] time 0.071 (0.146) data 0.000 (0.070) loss 1.9258 (1.9925) acc 75.0000 (80.3125) lr 1.0000e-05 eta 0:03:07
epoch [1/50] batch [25/26] time 0.071 (0.131) data 0.000 (0.056) loss 1.7109 (1.9538) acc 90.6250 (81.6250) lr 1.0000e-05 eta 0:02:47
epoch [2/50] batch [5/26] time 0.070 (0.269) data 0.000 (0.186) loss 1.0352 (1.3762) acc 81.2500 (83.7500) lr 2.0000e-03 eta 0:05:41
epoch [2/50] batch [10/26] time 0.070 (0.170) data 0.000 (0.093) loss 0.5537 (1.0718) acc 96.8750 (86.2500) lr 2.0000e-03 eta 0:03:35
epoch [2/50] batch [15/26] time 0.070 (0.137) data 0.000 (0.062) loss 1.4932 (1.0303) acc 56.2500 (83.7500) lr 2.0000e-03 eta 0:02:52
epoch [2/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.047) loss 0.9004 (1.0091) acc 78.1250 (82.5000) lr 2.0000e-03 eta 0:02:30
epoch [2/50] batch [25/26] time 0.070 (0.110) data 0.000 (0.037) loss 0.7705 (0.9470) acc 87.5000 (83.3750) lr 2.0000e-03 eta 0:02:17
epoch [3/50] batch [5/26] time 0.071 (0.245) data 0.000 (0.171) loss 0.8037 (0.7211) acc 84.3750 (84.3750) lr 1.9980e-03 eta 0:05:04
epoch [3/50] batch [10/26] time 0.071 (0.158) data 0.000 (0.085) loss 0.6074 (0.7251) acc 87.5000 (82.1875) lr 1.9980e-03 eta 0:03:15
epoch [3/50] batch [15/26] time 0.071 (0.130) data 0.000 (0.057) loss 0.8003 (0.7158) acc 84.3750 (83.1250) lr 1.9980e-03 eta 0:02:39
epoch [3/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.5352 (0.6774) acc 84.3750 (84.3750) lr 1.9980e-03 eta 0:02:21
epoch [3/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.034) loss 0.5942 (0.6699) acc 87.5000 (84.2500) lr 1.9980e-03 eta 0:02:09
epoch [4/50] batch [5/26] time 0.071 (0.247) data 0.000 (0.174) loss 0.6597 (0.5175) acc 78.1250 (88.1250) lr 1.9921e-03 eta 0:05:00
epoch [4/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.5225 (0.6117) acc 87.5000 (85.0000) lr 1.9921e-03 eta 0:03:12
epoch [4/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.7437 (0.6384) acc 78.1250 (83.9583) lr 1.9921e-03 eta 0:02:35
epoch [4/50] batch [20/26] time 0.074 (0.115) data 0.000 (0.044) loss 0.7598 (0.6457) acc 81.2500 (83.5938) lr 1.9921e-03 eta 0:02:18
epoch [4/50] batch [25/26] time 0.074 (0.107) data 0.000 (0.035) loss 0.8618 (0.6698) acc 75.0000 (83.2500) lr 1.9921e-03 eta 0:02:08
epoch [5/50] batch [5/26] time 0.072 (0.258) data 0.000 (0.185) loss 0.3804 (0.5815) acc 93.7500 (85.6250) lr 1.9823e-03 eta 0:05:07
epoch [5/50] batch [10/26] time 0.071 (0.164) data 0.000 (0.093) loss 0.5234 (0.7247) acc 87.5000 (82.1875) lr 1.9823e-03 eta 0:03:14
epoch [5/50] batch [15/26] time 0.071 (0.133) data 0.000 (0.062) loss 0.5776 (0.6894) acc 90.6250 (83.7500) lr 1.9823e-03 eta 0:02:36
epoch [5/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.047) loss 0.6963 (0.7027) acc 81.2500 (83.2812) lr 1.9823e-03 eta 0:02:17
epoch [5/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.9229 (0.6891) acc 78.1250 (83.8750) lr 1.9823e-03 eta 0:02:06
epoch [6/50] batch [5/26] time 0.070 (0.271) data 0.000 (0.198) loss 1.5156 (0.8660) acc 68.7500 (78.7500) lr 1.9686e-03 eta 0:05:15
epoch [6/50] batch [10/26] time 0.070 (0.171) data 0.000 (0.099) loss 0.7393 (0.6926) acc 84.3750 (83.4375) lr 1.9686e-03 eta 0:03:18
epoch [6/50] batch [15/26] time 0.070 (0.137) data 0.000 (0.066) loss 0.5386 (0.6411) acc 87.5000 (84.3750) lr 1.9686e-03 eta 0:02:38
epoch [6/50] batch [20/26] time 0.071 (0.121) data 0.000 (0.050) loss 0.8706 (0.6861) acc 81.2500 (83.4375) lr 1.9686e-03 eta 0:02:18
epoch [6/50] batch [25/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.3342 (0.6436) acc 93.7500 (84.5000) lr 1.9686e-03 eta 0:02:06
epoch [7/50] batch [5/26] time 0.074 (0.253) data 0.000 (0.181) loss 0.8535 (0.6545) acc 81.2500 (83.1250) lr 1.9511e-03 eta 0:04:48
epoch [7/50] batch [10/26] time 0.070 (0.162) data 0.000 (0.091) loss 0.7749 (0.7279) acc 81.2500 (80.9375) lr 1.9511e-03 eta 0:03:03
epoch [7/50] batch [15/26] time 0.071 (0.131) data 0.000 (0.061) loss 0.5713 (0.6906) acc 84.3750 (83.1250) lr 1.9511e-03 eta 0:02:28
epoch [7/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.6362 (0.6888) acc 81.2500 (82.5000) lr 1.9511e-03 eta 0:02:10
epoch [7/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.3799 (0.6411) acc 87.5000 (83.5000) lr 1.9511e-03 eta 0:01:59
epoch [8/50] batch [5/26] time 0.070 (0.260) data 0.000 (0.188) loss 0.8013 (0.6978) acc 75.0000 (82.5000) lr 1.9298e-03 eta 0:04:49
epoch [8/50] batch [10/26] time 0.070 (0.165) data 0.000 (0.094) loss 0.9268 (0.7125) acc 71.8750 (81.8750) lr 1.9298e-03 eta 0:03:03
epoch [8/50] batch [15/26] time 0.071 (0.134) data 0.000 (0.063) loss 0.6582 (0.6613) acc 81.2500 (83.1250) lr 1.9298e-03 eta 0:02:27
epoch [8/50] batch [20/26] time 0.071 (0.118) data 0.000 (0.047) loss 0.2510 (0.6318) acc 96.8750 (83.4375) lr 1.9298e-03 eta 0:02:09
epoch [8/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3467 (0.6156) acc 90.6250 (83.6250) lr 1.9298e-03 eta 0:01:58
epoch [9/50] batch [5/26] time 0.071 (0.243) data 0.000 (0.168) loss 0.4216 (0.4824) acc 84.3750 (88.1250) lr 1.9048e-03 eta 0:04:24
epoch [9/50] batch [10/26] time 0.071 (0.157) data 0.001 (0.084) loss 0.6123 (0.5204) acc 84.3750 (87.5000) lr 1.9048e-03 eta 0:02:50
epoch [9/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 1.1191 (0.5736) acc 75.0000 (85.6250) lr 1.9048e-03 eta 0:02:18
epoch [9/50] batch [20/26] time 0.072 (0.114) data 0.000 (0.042) loss 0.5635 (0.5877) acc 93.7500 (85.4688) lr 1.9048e-03 eta 0:02:02
epoch [9/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.6890 (0.5971) acc 81.2500 (85.2500) lr 1.9048e-03 eta 0:01:52
epoch [10/50] batch [5/26] time 0.072 (0.258) data 0.000 (0.183) loss 0.3342 (0.5042) acc 90.6250 (86.8750) lr 1.8763e-03 eta 0:04:33
epoch [10/50] batch [10/26] time 0.071 (0.165) data 0.000 (0.092) loss 0.4050 (0.5481) acc 87.5000 (86.5625) lr 1.8763e-03 eta 0:02:54
epoch [10/50] batch [15/26] time 0.070 (0.133) data 0.000 (0.061) loss 0.5166 (0.5764) acc 81.2500 (85.6250) lr 1.8763e-03 eta 0:02:20
epoch [10/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.046) loss 0.5830 (0.5703) acc 84.3750 (85.6250) lr 1.8763e-03 eta 0:02:03
epoch [10/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.5396 (0.5687) acc 81.2500 (85.5000) lr 1.8763e-03 eta 0:01:52
epoch [11/50] batch [5/26] time 0.071 (0.252) data 0.000 (0.179) loss 0.6602 (0.7130) acc 84.3750 (82.5000) lr 1.8443e-03 eta 0:04:20
epoch [11/50] batch [10/26] time 0.071 (0.162) data 0.001 (0.090) loss 0.5850 (0.5897) acc 78.1250 (84.3750) lr 1.8443e-03 eta 0:02:46
epoch [11/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 0.3928 (0.5520) acc 90.6250 (85.4167) lr 1.8443e-03 eta 0:02:15
epoch [11/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.045) loss 0.7188 (0.5899) acc 81.2500 (84.5312) lr 1.8443e-03 eta 0:01:58
epoch [11/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.036) loss 0.6274 (0.5801) acc 84.3750 (85.0000) lr 1.8443e-03 eta 0:01:49
epoch [12/50] batch [5/26] time 0.071 (0.246) data 0.000 (0.174) loss 0.9014 (0.6886) acc 75.0000 (80.6250) lr 1.8090e-03 eta 0:04:07
epoch [12/50] batch [10/26] time 0.072 (0.158) data 0.000 (0.087) loss 0.3481 (0.6116) acc 87.5000 (82.1875) lr 1.8090e-03 eta 0:02:39
epoch [12/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.058) loss 0.4783 (0.5703) acc 81.2500 (82.9167) lr 1.8090e-03 eta 0:02:09
epoch [12/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.5410 (0.5813) acc 90.6250 (84.0625) lr 1.8090e-03 eta 0:01:53
epoch [12/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.4592 (0.5929) acc 87.5000 (83.8750) lr 1.8090e-03 eta 0:01:44
epoch [13/50] batch [5/26] time 0.070 (0.234) data 0.000 (0.161) loss 0.4099 (0.5446) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:03:50
epoch [13/50] batch [10/26] time 0.071 (0.152) data 0.000 (0.080) loss 0.7070 (0.5132) acc 81.2500 (86.8750) lr 1.7705e-03 eta 0:02:29
epoch [13/50] batch [15/26] time 0.072 (0.125) data 0.000 (0.054) loss 0.5654 (0.5216) acc 87.5000 (86.4583) lr 1.7705e-03 eta 0:02:01
epoch [13/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.5254 (0.5046) acc 90.6250 (87.0312) lr 1.7705e-03 eta 0:01:47
epoch [13/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.3184 (0.4860) acc 93.7500 (87.5000) lr 1.7705e-03 eta 0:01:39
epoch [14/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.170) loss 0.4551 (0.5825) acc 90.6250 (86.8750) lr 1.7290e-03 eta 0:03:52
epoch [14/50] batch [10/26] time 0.071 (0.157) data 0.001 (0.085) loss 0.3831 (0.5141) acc 90.6250 (87.1875) lr 1.7290e-03 eta 0:02:29
epoch [14/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.5913 (0.4973) acc 87.5000 (88.3333) lr 1.7290e-03 eta 0:02:01
epoch [14/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.7583 (0.5288) acc 87.5000 (87.6562) lr 1.7290e-03 eta 0:01:46
epoch [14/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.8350 (0.5342) acc 78.1250 (87.3750) lr 1.7290e-03 eta 0:01:38
epoch [15/50] batch [5/26] time 0.074 (0.241) data 0.000 (0.166) loss 0.6636 (0.6607) acc 84.3750 (85.0000) lr 1.6845e-03 eta 0:03:44
epoch [15/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.5005 (0.5824) acc 87.5000 (86.2500) lr 1.6845e-03 eta 0:02:24
epoch [15/50] batch [15/26] time 0.073 (0.129) data 0.000 (0.056) loss 0.4417 (0.5383) acc 84.3750 (86.6667) lr 1.6845e-03 eta 0:01:58
epoch [15/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.042) loss 0.5669 (0.5435) acc 87.5000 (85.9375) lr 1.6845e-03 eta 0:01:45
epoch [15/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.034) loss 0.2365 (0.5417) acc 93.7500 (85.7500) lr 1.6845e-03 eta 0:01:36
epoch [16/50] batch [5/26] time 0.070 (0.244) data 0.000 (0.171) loss 0.2150 (0.4577) acc 93.7500 (88.7500) lr 1.6374e-03 eta 0:03:40
epoch [16/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.086) loss 0.2258 (0.5512) acc 93.7500 (85.3125) lr 1.6374e-03 eta 0:02:21
epoch [16/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.057) loss 0.5327 (0.5717) acc 78.1250 (84.7917) lr 1.6374e-03 eta 0:01:54
epoch [16/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.7383 (0.5892) acc 84.3750 (84.2188) lr 1.6374e-03 eta 0:01:41
epoch [16/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.034) loss 0.8740 (0.6023) acc 78.1250 (83.7500) lr 1.6374e-03 eta 0:01:33
epoch [17/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.171) loss 0.6348 (0.5856) acc 81.2500 (81.8750) lr 1.5878e-03 eta 0:03:33
epoch [17/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.086) loss 0.7485 (0.5836) acc 81.2500 (83.7500) lr 1.5878e-03 eta 0:02:16
epoch [17/50] batch [15/26] time 0.072 (0.128) data 0.000 (0.057) loss 0.6641 (0.5613) acc 81.2500 (85.0000) lr 1.5878e-03 eta 0:01:51
epoch [17/50] batch [20/26] time 0.072 (0.114) data 0.000 (0.043) loss 0.4595 (0.5439) acc 84.3750 (85.0000) lr 1.5878e-03 eta 0:01:38
epoch [17/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.7104 (0.5181) acc 87.5000 (85.8750) lr 1.5878e-03 eta 0:01:30
epoch [18/50] batch [5/26] time 0.070 (0.246) data 0.000 (0.173) loss 0.5078 (0.4432) acc 90.6250 (88.7500) lr 1.5358e-03 eta 0:03:29
epoch [18/50] batch [10/26] time 0.074 (0.159) data 0.000 (0.087) loss 0.6094 (0.5480) acc 84.3750 (85.3125) lr 1.5358e-03 eta 0:02:14
epoch [18/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.058) loss 0.4617 (0.5265) acc 90.6250 (86.2500) lr 1.5358e-03 eta 0:01:49
epoch [18/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.7539 (0.5692) acc 78.1250 (85.7812) lr 1.5358e-03 eta 0:01:36
epoch [18/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.7300 (0.5723) acc 81.2500 (85.5000) lr 1.5358e-03 eta 0:01:28
epoch [19/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.168) loss 0.6079 (0.6483) acc 87.5000 (84.3750) lr 1.4818e-03 eta 0:03:20
epoch [19/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.084) loss 0.4548 (0.6060) acc 87.5000 (85.0000) lr 1.4818e-03 eta 0:02:08
epoch [19/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.4656 (0.6032) acc 87.5000 (84.7917) lr 1.4818e-03 eta 0:01:44
epoch [19/50] batch [20/26] time 0.072 (0.114) data 0.000 (0.042) loss 0.4976 (0.5937) acc 93.7500 (85.3125) lr 1.4818e-03 eta 0:01:32
epoch [19/50] batch [25/26] time 0.075 (0.106) data 0.001 (0.034) loss 0.2817 (0.5811) acc 93.7500 (86.0000) lr 1.4818e-03 eta 0:01:25
epoch [20/50] batch [5/26] time 0.073 (0.247) data 0.000 (0.174) loss 0.3005 (0.5439) acc 93.7500 (85.6250) lr 1.4258e-03 eta 0:03:17
epoch [20/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.087) loss 0.4231 (0.5848) acc 90.6250 (85.6250) lr 1.4258e-03 eta 0:02:06
epoch [20/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.7842 (0.5571) acc 78.1250 (86.0417) lr 1.4258e-03 eta 0:01:42
epoch [20/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.8032 (0.5607) acc 71.8750 (85.0000) lr 1.4258e-03 eta 0:01:30
epoch [20/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.4214 (0.5698) acc 90.6250 (85.2500) lr 1.4258e-03 eta 0:01:22
epoch [21/50] batch [5/26] time 0.073 (0.264) data 0.000 (0.191) loss 0.6191 (0.5138) acc 84.3750 (88.7500) lr 1.3681e-03 eta 0:03:24
epoch [21/50] batch [10/26] time 0.072 (0.168) data 0.001 (0.096) loss 0.8159 (0.5443) acc 81.2500 (87.5000) lr 1.3681e-03 eta 0:02:09
epoch [21/50] batch [15/26] time 0.072 (0.136) data 0.000 (0.064) loss 1.1436 (0.5357) acc 65.6250 (87.0833) lr 1.3681e-03 eta 0:01:43
epoch [21/50] batch [20/26] time 0.072 (0.119) data 0.000 (0.048) loss 0.7119 (0.5203) acc 81.2500 (87.8125) lr 1.3681e-03 eta 0:01:30
epoch [21/50] batch [25/26] time 0.071 (0.110) data 0.000 (0.038) loss 0.5488 (0.5134) acc 87.5000 (87.8750) lr 1.3681e-03 eta 0:01:22
epoch [22/50] batch [5/26] time 0.070 (0.249) data 0.000 (0.173) loss 0.7393 (0.6376) acc 75.0000 (82.5000) lr 1.3090e-03 eta 0:03:06
epoch [22/50] batch [10/26] time 0.071 (0.160) data 0.001 (0.087) loss 0.7100 (0.5851) acc 78.1250 (84.0625) lr 1.3090e-03 eta 0:01:58
epoch [22/50] batch [15/26] time 0.071 (0.130) data 0.000 (0.058) loss 0.3931 (0.5399) acc 87.5000 (85.8333) lr 1.3090e-03 eta 0:01:36
epoch [22/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.2308 (0.4999) acc 96.8750 (87.3438) lr 1.3090e-03 eta 0:01:24
epoch [22/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.5117 (0.5178) acc 90.6250 (86.7500) lr 1.3090e-03 eta 0:01:17
epoch [23/50] batch [5/26] time 0.071 (0.253) data 0.000 (0.180) loss 0.2991 (0.4232) acc 90.6250 (90.0000) lr 1.2487e-03 eta 0:03:03
epoch [23/50] batch [10/26] time 0.073 (0.163) data 0.001 (0.090) loss 0.6924 (0.4804) acc 87.5000 (88.4375) lr 1.2487e-03 eta 0:01:57
epoch [23/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 0.5103 (0.4994) acc 84.3750 (87.5000) lr 1.2487e-03 eta 0:01:34
epoch [23/50] batch [20/26] time 0.071 (0.117) data 0.000 (0.045) loss 0.3545 (0.5243) acc 93.7500 (87.0312) lr 1.2487e-03 eta 0:01:22
epoch [23/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.036) loss 0.4314 (0.5162) acc 93.7500 (87.2500) lr 1.2487e-03 eta 0:01:15
epoch [24/50] batch [5/26] time 0.077 (0.250) data 0.000 (0.174) loss 0.3618 (0.4785) acc 93.7500 (89.3750) lr 1.1874e-03 eta 0:02:54
epoch [24/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.087) loss 0.3896 (0.5471) acc 93.7500 (87.8125) lr 1.1874e-03 eta 0:01:51
epoch [24/50] batch [15/26] time 0.071 (0.131) data 0.000 (0.058) loss 0.7251 (0.5715) acc 81.2500 (86.2500) lr 1.1874e-03 eta 0:01:30
epoch [24/50] batch [20/26] time 0.071 (0.116) data 0.000 (0.044) loss 0.5664 (0.5294) acc 81.2500 (87.1875) lr 1.1874e-03 eta 0:01:19
epoch [24/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.035) loss 0.6191 (0.5206) acc 84.3750 (86.7500) lr 1.1874e-03 eta 0:01:12
epoch [25/50] batch [5/26] time 0.070 (0.287) data 0.000 (0.216) loss 0.8003 (0.4144) acc 71.8750 (88.1250) lr 1.1253e-03 eta 0:03:12
epoch [25/50] batch [10/26] time 0.071 (0.179) data 0.000 (0.108) loss 0.3267 (0.4745) acc 90.6250 (86.8750) lr 1.1253e-03 eta 0:01:59
epoch [25/50] batch [15/26] time 0.071 (0.144) data 0.000 (0.072) loss 0.4343 (0.4819) acc 93.7500 (87.5000) lr 1.1253e-03 eta 0:01:34
epoch [25/50] batch [20/26] time 0.072 (0.126) data 0.000 (0.054) loss 0.5420 (0.5274) acc 81.2500 (86.0938) lr 1.1253e-03 eta 0:01:22
epoch [25/50] batch [25/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.9609 (0.5424) acc 84.3750 (86.6250) lr 1.1253e-03 eta 0:01:14
epoch [26/50] batch [5/26] time 0.074 (0.246) data 0.001 (0.172) loss 0.2783 (0.3983) acc 90.6250 (86.8750) lr 1.0628e-03 eta 0:02:38
epoch [26/50] batch [10/26] time 0.072 (0.158) data 0.000 (0.086) loss 0.4241 (0.4144) acc 90.6250 (88.7500) lr 1.0628e-03 eta 0:01:41
epoch [26/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.3735 (0.4247) acc 90.6250 (88.9583) lr 1.0628e-03 eta 0:01:22
epoch [26/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.7002 (0.4657) acc 84.3750 (88.4375) lr 1.0628e-03 eta 0:01:12
epoch [26/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.3738 (0.4599) acc 90.6250 (88.2500) lr 1.0628e-03 eta 0:01:06
epoch [27/50] batch [5/26] time 0.073 (0.227) data 0.000 (0.151) loss 0.4036 (0.4081) acc 93.7500 (91.8750) lr 1.0000e-03 eta 0:02:20
epoch [27/50] batch [10/26] time 0.072 (0.150) data 0.000 (0.076) loss 0.4978 (0.4480) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:01:31
epoch [27/50] batch [15/26] time 0.072 (0.123) data 0.000 (0.051) loss 0.8882 (0.5070) acc 75.0000 (88.3333) lr 1.0000e-03 eta 0:01:15
epoch [27/50] batch [20/26] time 0.072 (0.111) data 0.000 (0.038) loss 0.4863 (0.5219) acc 90.6250 (88.1250) lr 1.0000e-03 eta 0:01:06
epoch [27/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.5112 (0.5131) acc 84.3750 (87.8750) lr 1.0000e-03 eta 0:01:01
epoch [28/50] batch [5/26] time 0.070 (0.240) data 0.000 (0.167) loss 0.2185 (0.5714) acc 96.8750 (86.8750) lr 9.3721e-04 eta 0:02:22
epoch [28/50] batch [10/26] time 0.071 (0.155) data 0.000 (0.084) loss 0.6099 (0.5540) acc 81.2500 (85.3125) lr 9.3721e-04 eta 0:01:31
epoch [28/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.4832 (0.5463) acc 87.5000 (85.2083) lr 9.3721e-04 eta 0:01:14
epoch [28/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.8086 (0.5649) acc 87.5000 (85.1562) lr 9.3721e-04 eta 0:01:05
epoch [28/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.5063 (0.5622) acc 87.5000 (85.2500) lr 9.3721e-04 eta 0:00:59
epoch [29/50] batch [5/26] time 0.071 (0.228) data 0.000 (0.155) loss 0.2690 (0.5674) acc 93.7500 (86.8750) lr 8.7467e-04 eta 0:02:09
epoch [29/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.078) loss 0.2111 (0.4912) acc 96.8750 (89.0625) lr 8.7467e-04 eta 0:01:23
epoch [29/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.4165 (0.5153) acc 90.6250 (87.5000) lr 8.7467e-04 eta 0:01:08
epoch [29/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.5664 (0.5282) acc 81.2500 (86.4062) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.8662 (0.5152) acc 75.0000 (87.0000) lr 8.7467e-04 eta 0:00:55
epoch [30/50] batch [5/26] time 0.071 (0.230) data 0.000 (0.157) loss 0.8779 (0.7900) acc 81.2500 (80.6250) lr 8.1262e-04 eta 0:02:04
epoch [30/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.079) loss 0.6118 (0.6424) acc 78.1250 (83.7500) lr 8.1262e-04 eta 0:01:20
epoch [30/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.2275 (0.5856) acc 96.8750 (85.0000) lr 8.1262e-04 eta 0:01:05
epoch [30/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.039) loss 0.3276 (0.5758) acc 90.6250 (84.8438) lr 8.1262e-04 eta 0:00:58
epoch [30/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.5063 (0.5446) acc 90.6250 (86.0000) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.149) loss 0.7529 (0.5781) acc 78.1250 (83.7500) lr 7.5131e-04 eta 0:01:53
epoch [31/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.075) loss 0.6621 (0.5933) acc 87.5000 (85.3125) lr 7.5131e-04 eta 0:01:14
epoch [31/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.050) loss 0.5645 (0.5475) acc 78.1250 (85.4167) lr 7.5131e-04 eta 0:01:01
epoch [31/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.7437 (0.5551) acc 78.1250 (85.1562) lr 7.5131e-04 eta 0:00:54
epoch [31/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.3345 (0.5332) acc 93.7500 (86.2500) lr 7.5131e-04 eta 0:00:49
epoch [32/50] batch [5/26] time 0.071 (0.223) data 0.000 (0.150) loss 0.2793 (0.4593) acc 93.7500 (90.0000) lr 6.9098e-04 eta 0:01:49
epoch [32/50] batch [10/26] time 0.071 (0.147) data 0.001 (0.075) loss 0.2852 (0.5168) acc 90.6250 (88.7500) lr 6.9098e-04 eta 0:01:11
epoch [32/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.050) loss 0.9424 (0.5600) acc 75.0000 (87.2917) lr 6.9098e-04 eta 0:00:58
epoch [32/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.4744 (0.5703) acc 87.5000 (86.8750) lr 6.9098e-04 eta 0:00:51
epoch [32/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.9331 (0.5591) acc 75.0000 (87.5000) lr 6.9098e-04 eta 0:00:47
epoch [33/50] batch [5/26] time 0.072 (0.224) data 0.000 (0.146) loss 0.7964 (0.5367) acc 87.5000 (86.8750) lr 6.3188e-04 eta 0:01:43
epoch [33/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.073) loss 0.3794 (0.4721) acc 93.7500 (88.7500) lr 6.3188e-04 eta 0:01:07
epoch [33/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.049) loss 0.2891 (0.4933) acc 93.7500 (88.7500) lr 6.3188e-04 eta 0:00:55
epoch [33/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.037) loss 0.4800 (0.5101) acc 90.6250 (88.2812) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.029) loss 0.8125 (0.5292) acc 84.3750 (87.8750) lr 6.3188e-04 eta 0:00:44
epoch [34/50] batch [5/26] time 0.073 (0.231) data 0.000 (0.156) loss 0.4080 (0.5616) acc 87.5000 (85.0000) lr 5.7422e-04 eta 0:01:40
epoch [34/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.078) loss 0.5483 (0.5513) acc 90.6250 (86.2500) lr 5.7422e-04 eta 0:01:05
epoch [34/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.4343 (0.5144) acc 90.6250 (86.6667) lr 5.7422e-04 eta 0:00:53
epoch [34/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.4275 (0.4971) acc 87.5000 (87.6562) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.031) loss 0.6362 (0.5115) acc 90.6250 (87.8750) lr 5.7422e-04 eta 0:00:42
epoch [35/50] batch [5/26] time 0.076 (0.228) data 0.005 (0.155) loss 0.4373 (0.4821) acc 90.6250 (90.0000) lr 5.1825e-04 eta 0:01:33
epoch [35/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.078) loss 0.4824 (0.4561) acc 81.2500 (90.3125) lr 5.1825e-04 eta 0:01:00
epoch [35/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.052) loss 0.7480 (0.4627) acc 81.2500 (89.3750) lr 5.1825e-04 eta 0:00:49
epoch [35/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.4084 (0.4726) acc 93.7500 (89.2188) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.4192 (0.4715) acc 90.6250 (88.7500) lr 5.1825e-04 eta 0:00:40
epoch [36/50] batch [5/26] time 0.071 (0.226) data 0.000 (0.154) loss 0.4639 (0.4885) acc 90.6250 (88.7500) lr 4.6417e-04 eta 0:01:26
epoch [36/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.077) loss 0.6636 (0.5039) acc 87.5000 (88.7500) lr 4.6417e-04 eta 0:00:56
epoch [36/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.052) loss 0.4258 (0.4870) acc 90.6250 (88.3333) lr 4.6417e-04 eta 0:00:46
epoch [36/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.3818 (0.4586) acc 90.6250 (88.5938) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [25/26] time 0.072 (0.102) data 0.000 (0.031) loss 0.4897 (0.4528) acc 90.6250 (89.0000) lr 4.6417e-04 eta 0:00:37
epoch [37/50] batch [5/26] time 0.071 (0.234) data 0.000 (0.160) loss 0.7271 (0.4568) acc 87.5000 (89.3750) lr 4.1221e-04 eta 0:01:24
epoch [37/50] batch [10/26] time 0.071 (0.152) data 0.000 (0.080) loss 0.4092 (0.4904) acc 90.6250 (88.1250) lr 4.1221e-04 eta 0:00:53
epoch [37/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.054) loss 0.5972 (0.5387) acc 87.5000 (87.0833) lr 4.1221e-04 eta 0:00:43
epoch [37/50] batch [20/26] time 0.072 (0.112) data 0.000 (0.040) loss 0.4700 (0.5146) acc 81.2500 (87.0312) lr 4.1221e-04 eta 0:00:38
epoch [37/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.4912 (0.5337) acc 90.6250 (86.5000) lr 4.1221e-04 eta 0:00:35
epoch [38/50] batch [5/26] time 0.070 (0.229) data 0.000 (0.156) loss 0.1655 (0.4511) acc 100.0000 (89.3750) lr 3.6258e-04 eta 0:01:16
epoch [38/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.078) loss 0.4683 (0.4783) acc 84.3750 (87.8125) lr 3.6258e-04 eta 0:00:49
epoch [38/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.7344 (0.4914) acc 81.2500 (88.3333) lr 3.6258e-04 eta 0:00:39
epoch [38/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.5020 (0.4974) acc 81.2500 (87.5000) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.5439 (0.4967) acc 90.6250 (87.0000) lr 3.6258e-04 eta 0:00:31
epoch [39/50] batch [5/26] time 0.071 (0.233) data 0.000 (0.159) loss 0.5537 (0.3837) acc 87.5000 (91.2500) lr 3.1545e-04 eta 0:01:11
epoch [39/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.080) loss 0.2544 (0.4125) acc 93.7500 (90.3125) lr 3.1545e-04 eta 0:00:46
epoch [39/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.053) loss 0.7583 (0.4656) acc 81.2500 (89.3750) lr 3.1545e-04 eta 0:00:37
epoch [39/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.1996 (0.4958) acc 96.8750 (88.5938) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.4346 (0.4797) acc 90.6250 (89.0000) lr 3.1545e-04 eta 0:00:29
epoch [40/50] batch [5/26] time 0.071 (0.225) data 0.000 (0.153) loss 0.6055 (0.4244) acc 84.3750 (90.6250) lr 2.7103e-04 eta 0:01:03
epoch [40/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.077) loss 0.6523 (0.4735) acc 87.5000 (89.3750) lr 2.7103e-04 eta 0:00:40
epoch [40/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.4111 (0.4717) acc 93.7500 (90.0000) lr 2.7103e-04 eta 0:00:33
epoch [40/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3945 (0.4653) acc 87.5000 (89.3750) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [25/26] time 0.072 (0.102) data 0.000 (0.031) loss 0.6216 (0.4900) acc 84.3750 (88.8750) lr 2.7103e-04 eta 0:00:26
epoch [41/50] batch [5/26] time 0.070 (0.225) data 0.000 (0.152) loss 0.3103 (0.4307) acc 96.8750 (89.3750) lr 2.2949e-04 eta 0:00:57
epoch [41/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.076) loss 0.3726 (0.4397) acc 87.5000 (89.3750) lr 2.2949e-04 eta 0:00:37
epoch [41/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.8198 (0.4444) acc 87.5000 (90.4167) lr 2.2949e-04 eta 0:00:29
epoch [41/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4321 (0.4524) acc 93.7500 (90.1562) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [25/26] time 0.073 (0.102) data 0.000 (0.031) loss 0.7705 (0.4656) acc 81.2500 (89.5000) lr 2.2949e-04 eta 0:00:23
epoch [42/50] batch [5/26] time 0.070 (0.222) data 0.000 (0.147) loss 0.4580 (0.4865) acc 87.5000 (88.1250) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.074) loss 0.4841 (0.5020) acc 87.5000 (87.8125) lr 1.9098e-04 eta 0:00:32
epoch [42/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.049) loss 0.2333 (0.4716) acc 93.7500 (87.9167) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.037) loss 0.5781 (0.4558) acc 84.3750 (88.4375) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.5269 (0.4475) acc 90.6250 (88.6250) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.071 (0.228) data 0.000 (0.155) loss 0.7085 (0.4560) acc 78.1250 (89.3750) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.077) loss 1.0586 (0.4776) acc 81.2500 (89.3750) lr 1.5567e-04 eta 0:00:29
epoch [43/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.3057 (0.4617) acc 93.7500 (89.1667) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.2925 (0.4518) acc 93.7500 (89.5312) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5737 (0.4689) acc 87.5000 (88.8750) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.157) loss 0.6016 (0.5180) acc 87.5000 (89.3750) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.079) loss 0.4163 (0.4712) acc 93.7500 (90.0000) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.5146 (0.4576) acc 84.3750 (89.7917) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.3662 (0.4509) acc 90.6250 (90.0000) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.7314 (0.4369) acc 75.0000 (90.3750) lr 1.2369e-04 eta 0:00:16
epoch [45/50] batch [5/26] time 0.071 (0.238) data 0.000 (0.166) loss 0.8145 (0.4815) acc 81.2500 (89.3750) lr 9.5173e-05 eta 0:00:36
epoch [45/50] batch [10/26] time 0.071 (0.155) data 0.000 (0.083) loss 0.6938 (0.4851) acc 87.5000 (89.3750) lr 9.5173e-05 eta 0:00:22
epoch [45/50] batch [15/26] time 0.072 (0.127) data 0.000 (0.055) loss 1.3330 (0.5119) acc 71.8750 (88.7500) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5024 (0.5094) acc 87.5000 (88.4375) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.2839 (0.4762) acc 93.7500 (89.5000) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.071 (0.252) data 0.000 (0.178) loss 0.8213 (0.4932) acc 78.1250 (87.5000) lr 7.0224e-05 eta 0:00:31
epoch [46/50] batch [10/26] time 0.071 (0.161) data 0.000 (0.089) loss 0.3530 (0.4780) acc 90.6250 (88.1250) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.059) loss 0.5488 (0.4885) acc 90.6250 (87.7083) lr 7.0224e-05 eta 0:00:15
epoch [46/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.5576 (0.4906) acc 87.5000 (87.0312) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.036) loss 0.4397 (0.4870) acc 93.7500 (87.2500) lr 7.0224e-05 eta 0:00:11
epoch [47/50] batch [5/26] time 0.074 (0.224) data 0.000 (0.147) loss 0.3599 (0.4913) acc 93.7500 (89.3750) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [10/26] time 0.070 (0.149) data 0.000 (0.074) loss 0.2969 (0.4229) acc 93.7500 (90.6250) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.049) loss 0.4529 (0.4729) acc 90.6250 (88.7500) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.037) loss 0.4399 (0.4663) acc 90.6250 (89.3750) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.030) loss 0.4153 (0.4549) acc 87.5000 (89.6250) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.071 (0.252) data 0.000 (0.180) loss 0.5054 (0.5228) acc 87.5000 (89.3750) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.090) loss 0.5229 (0.5301) acc 87.5000 (87.8125) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 0.4058 (0.4925) acc 90.6250 (87.9167) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.5850 (0.4948) acc 84.3750 (88.1250) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.3062 (0.4851) acc 93.7500 (88.1250) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.073 (0.228) data 0.000 (0.154) loss 0.6113 (0.4435) acc 81.2500 (88.1250) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.098 (0.152) data 0.000 (0.077) loss 0.4509 (0.4398) acc 87.5000 (88.1250) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.052) loss 0.1837 (0.4289) acc 96.8750 (88.5417) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.5737 (0.4552) acc 84.3750 (87.9688) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.5254 (0.4629) acc 84.3750 (87.7500) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.071 (0.223) data 0.000 (0.152) loss 1.0645 (0.8059) acc 78.1250 (84.3750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.4375 (0.6603) acc 93.7500 (87.1875) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.2739 (0.6103) acc 93.7500 (87.0833) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.074 (0.109) data 0.000 (0.038) loss 0.4612 (0.5820) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.074 (0.102) data 0.000 (0.031) loss 0.1699 (0.5508) acc 96.8750 (88.1250) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:04<02:27,  4.91s/it]  6%|▋         | 2/31 [00:05<01:10,  2.43s/it] 10%|▉         | 3/31 [00:06<00:44,  1.58s/it] 13%|█▎        | 4/31 [00:06<00:32,  1.19s/it] 16%|█▌        | 5/31 [00:07<00:25,  1.01it/s] 19%|█▉        | 6/31 [00:07<00:21,  1.18it/s] 23%|██▎       | 7/31 [00:08<00:18,  1.32it/s] 26%|██▌       | 8/31 [00:09<00:16,  1.43it/s] 29%|██▉       | 9/31 [00:09<00:14,  1.47it/s] 32%|███▏      | 10/31 [00:10<00:13,  1.51it/s] 35%|███▌      | 11/31 [00:10<00:12,  1.58it/s] 39%|███▊      | 12/31 [00:11<00:11,  1.62it/s] 42%|████▏     | 13/31 [00:12<00:10,  1.66it/s] 45%|████▌     | 14/31 [00:12<00:10,  1.68it/s] 48%|████▊     | 15/31 [00:13<00:09,  1.70it/s] 52%|█████▏    | 16/31 [00:13<00:08,  1.71it/s] 55%|█████▍    | 17/31 [00:14<00:08,  1.65it/s] 58%|█████▊    | 18/31 [00:15<00:07,  1.67it/s] 61%|██████▏   | 19/31 [00:15<00:07,  1.69it/s] 65%|██████▍   | 20/31 [00:16<00:06,  1.71it/s] 68%|██████▊   | 21/31 [00:16<00:05,  1.72it/s] 71%|███████   | 22/31 [00:17<00:05,  1.72it/s] 74%|███████▍  | 23/31 [00:17<00:04,  1.73it/s] 77%|███████▋  | 24/31 [00:18<00:04,  1.73it/s] 81%|████████  | 25/31 [00:19<00:03,  1.73it/s] 84%|████████▍ | 26/31 [00:19<00:02,  1.73it/s] 87%|████████▋ | 27/31 [00:20<00:02,  1.74it/s] 90%|█████████ | 28/31 [00:20<00:01,  1.74it/s] 94%|█████████▎| 29/31 [00:21<00:01,  1.74it/s] 97%|█████████▋| 30/31 [00:21<00:00,  1.74it/s]100%|██████████| 31/31 [00:22<00:00,  1.90it/s]100%|██████████| 31/31 [00:22<00:00,  1.38it/s]
=> result
* total: 15,300
* correct: 13,842
* accuracy: 90.5%
* error: 9.5%
* macro_f1: 90.5%
Elapsed: 0:02:43
Run this job and save the output to output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:09<04:42,  9.73s/it]  7%|▋         | 2/30 [00:10<02:01,  4.34s/it] 10%|█         | 3/30 [00:10<01:10,  2.62s/it] 13%|█▎        | 4/30 [00:11<00:47,  1.81s/it] 17%|█▋        | 5/30 [00:12<00:34,  1.36s/it] 20%|██        | 6/30 [00:12<00:26,  1.09s/it] 23%|██▎       | 7/30 [00:13<00:21,  1.08it/s] 27%|██▋       | 8/30 [00:13<00:17,  1.23it/s] 30%|███       | 9/30 [00:14<00:18,  1.14it/s] 33%|███▎      | 10/30 [00:15<00:15,  1.27it/s] 37%|███▋      | 11/30 [00:15<00:13,  1.39it/s] 40%|████      | 12/30 [00:16<00:12,  1.48it/s] 43%|████▎     | 13/30 [00:17<00:10,  1.55it/s] 47%|████▋     | 14/30 [00:17<00:09,  1.61it/s] 50%|█████     | 15/30 [00:18<00:09,  1.65it/s] 53%|█████▎    | 16/30 [00:18<00:08,  1.68it/s] 57%|█████▋    | 17/30 [00:21<00:16,  1.26s/it] 60%|██████    | 18/30 [00:22<00:12,  1.05s/it] 63%|██████▎   | 19/30 [00:22<00:10,  1.09it/s] 67%|██████▋   | 20/30 [00:23<00:08,  1.23it/s] 70%|███████   | 21/30 [00:23<00:06,  1.35it/s] 73%|███████▎  | 22/30 [00:24<00:05,  1.45it/s] 77%|███████▋  | 23/30 [00:25<00:04,  1.53it/s] 80%|████████  | 24/30 [00:25<00:03,  1.59it/s] 83%|████████▎ | 25/30 [00:27<00:05,  1.12s/it] 87%|████████▋ | 26/30 [00:28<00:03,  1.04it/s] 90%|█████████ | 27/30 [00:29<00:02,  1.18it/s] 93%|█████████▎| 28/30 [00:29<00:01,  1.31it/s] 97%|█████████▋| 29/30 [00:30<00:00,  1.42it/s]100%|██████████| 30/30 [00:30<00:00,  1.50it/s]100%|██████████| 30/30 [00:30<00:00,  1.03s/it]
=> result
* total: 15,000
* correct: 13,679
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 91.2%
Run this job and save the output to output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:05<02:52,  5.95s/it]  7%|▋         | 2/30 [00:06<01:18,  2.79s/it] 10%|█         | 3/30 [00:07<00:47,  1.78s/it] 13%|█▎        | 4/30 [00:07<00:33,  1.30s/it] 17%|█▋        | 5/30 [00:08<00:25,  1.04s/it] 20%|██        | 6/30 [00:08<00:21,  1.14it/s] 23%|██▎       | 7/30 [00:09<00:17,  1.29it/s] 27%|██▋       | 8/30 [00:09<00:15,  1.40it/s] 30%|███       | 9/30 [00:10<00:14,  1.49it/s] 33%|███▎      | 10/30 [00:11<00:12,  1.56it/s] 37%|███▋      | 11/30 [00:11<00:12,  1.53it/s] 40%|████      | 12/30 [00:12<00:11,  1.57it/s] 43%|████▎     | 13/30 [00:12<00:10,  1.62it/s] 47%|████▋     | 14/30 [00:13<00:09,  1.66it/s] 50%|█████     | 15/30 [00:14<00:08,  1.68it/s] 53%|█████▎    | 16/30 [00:14<00:08,  1.70it/s] 57%|█████▋    | 17/30 [00:15<00:07,  1.72it/s] 60%|██████    | 18/30 [00:15<00:06,  1.72it/s] 63%|██████▎   | 19/30 [00:16<00:06,  1.73it/s] 67%|██████▋   | 20/30 [00:16<00:05,  1.73it/s] 70%|███████   | 21/30 [00:17<00:05,  1.73it/s] 73%|███████▎  | 22/30 [00:18<00:04,  1.74it/s] 77%|███████▋  | 23/30 [00:18<00:04,  1.74it/s] 80%|████████  | 24/30 [00:19<00:03,  1.74it/s] 83%|████████▎ | 25/30 [00:19<00:02,  1.74it/s] 87%|████████▋ | 26/30 [00:20<00:02,  1.74it/s] 90%|█████████ | 27/30 [00:20<00:01,  1.74it/s] 93%|█████████▎| 28/30 [00:21<00:01,  1.74it/s] 97%|█████████▋| 29/30 [00:22<00:00,  1.74it/s]100%|██████████| 30/30 [00:22<00:00,  1.74it/s]100%|██████████| 30/30 [00:22<00:00,  1.31it/s]
=> result
* total: 15,000
* correct: 13,696
* accuracy: 91.3%
* error: 8.7%
* macro_f1: 91.3%
Run this job and save the output to output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:06<02:56,  6.08s/it]  7%|▋         | 2/30 [00:06<01:19,  2.83s/it] 10%|█         | 3/30 [00:07<00:48,  1.80s/it] 13%|█▎        | 4/30 [00:07<00:35,  1.37s/it] 17%|█▋        | 5/30 [00:08<00:26,  1.08s/it] 20%|██        | 6/30 [00:09<00:21,  1.11it/s] 23%|██▎       | 7/30 [00:09<00:18,  1.25it/s] 27%|██▋       | 8/30 [00:10<00:15,  1.38it/s] 30%|███       | 9/30 [00:10<00:14,  1.48it/s] 33%|███▎      | 10/30 [00:11<00:12,  1.55it/s] 37%|███▋      | 11/30 [00:11<00:11,  1.61it/s] 40%|████      | 12/30 [00:12<00:10,  1.65it/s] 43%|████▎     | 13/30 [00:13<00:10,  1.68it/s] 47%|████▋     | 14/30 [00:13<00:09,  1.69it/s] 50%|█████     | 15/30 [00:14<00:08,  1.71it/s] 53%|█████▎    | 16/30 [00:14<00:08,  1.72it/s] 57%|█████▋    | 17/30 [00:15<00:07,  1.70it/s] 60%|██████    | 18/30 [00:15<00:06,  1.72it/s] 63%|██████▎   | 19/30 [00:16<00:06,  1.73it/s] 67%|██████▋   | 20/30 [00:17<00:05,  1.73it/s] 70%|███████   | 21/30 [00:17<00:05,  1.74it/s] 73%|███████▎  | 22/30 [00:18<00:04,  1.74it/s] 77%|███████▋  | 23/30 [00:18<00:04,  1.74it/s] 80%|████████  | 24/30 [00:19<00:03,  1.74it/s] 83%|████████▎ | 25/30 [00:19<00:02,  1.74it/s] 87%|████████▋ | 26/30 [00:20<00:02,  1.75it/s] 90%|█████████ | 27/30 [00:21<00:01,  1.75it/s] 93%|█████████▎| 28/30 [00:21<00:01,  1.75it/s] 97%|█████████▋| 29/30 [00:22<00:00,  1.74it/s]100%|██████████| 30/30 [00:22<00:00,  1.74it/s]100%|██████████| 30/30 [00:22<00:00,  1.30it/s]
=> result
* total: 15,000
* correct: 13,742
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.6%
Run this job and save the output to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.071 (0.367) data 0.000 (0.275) loss 2.4805 (2.7898) acc 68.7500 (63.1250) lr 1.0000e-05 eta 0:07:54
epoch [1/50] batch [10/26] time 0.084 (0.224) data 0.001 (0.138) loss 2.7695 (2.9035) acc 71.8750 (60.6250) lr 1.0000e-05 eta 0:04:48
epoch [1/50] batch [15/26] time 0.070 (0.174) data 0.000 (0.092) loss 2.7695 (2.9214) acc 53.1250 (59.1667) lr 1.0000e-05 eta 0:03:43
epoch [1/50] batch [20/26] time 0.070 (0.148) data 0.000 (0.069) loss 3.1328 (2.9461) acc 46.8750 (57.8125) lr 1.0000e-05 eta 0:03:08
epoch [1/50] batch [25/26] time 0.070 (0.132) data 0.000 (0.055) loss 2.6875 (2.9679) acc 59.3750 (57.3750) lr 1.0000e-05 eta 0:02:48
epoch [2/50] batch [5/26] time 0.070 (0.238) data 0.000 (0.166) loss 1.7607 (1.9428) acc 75.0000 (65.6250) lr 2.0000e-03 eta 0:05:02
epoch [2/50] batch [10/26] time 0.070 (0.154) data 0.000 (0.083) loss 1.5088 (1.7646) acc 71.8750 (66.2500) lr 2.0000e-03 eta 0:03:14
epoch [2/50] batch [15/26] time 0.072 (0.127) data 0.000 (0.055) loss 1.9648 (1.7473) acc 59.3750 (65.6250) lr 2.0000e-03 eta 0:02:39
epoch [2/50] batch [20/26] time 0.069 (0.113) data 0.000 (0.042) loss 1.4521 (1.6651) acc 62.5000 (65.3125) lr 2.0000e-03 eta 0:02:21
epoch [2/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 1.1602 (1.6309) acc 68.7500 (64.3750) lr 2.0000e-03 eta 0:02:09
epoch [3/50] batch [5/26] time 0.070 (0.286) data 0.000 (0.210) loss 1.3027 (1.4285) acc 71.8750 (66.8750) lr 1.9980e-03 eta 0:05:55
epoch [3/50] batch [10/26] time 0.070 (0.178) data 0.000 (0.105) loss 1.1660 (1.3822) acc 75.0000 (66.5625) lr 1.9980e-03 eta 0:03:40
epoch [3/50] batch [15/26] time 0.069 (0.142) data 0.000 (0.070) loss 1.2568 (1.2748) acc 78.1250 (71.2500) lr 1.9980e-03 eta 0:02:55
epoch [3/50] batch [20/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.7144 (1.2138) acc 84.3750 (72.0312) lr 1.9980e-03 eta 0:02:32
epoch [3/50] batch [25/26] time 0.070 (0.113) data 0.000 (0.042) loss 1.3965 (1.2037) acc 65.6250 (71.5000) lr 1.9980e-03 eta 0:02:18
epoch [4/50] batch [5/26] time 0.069 (0.246) data 0.000 (0.174) loss 0.9204 (1.0446) acc 87.5000 (77.5000) lr 1.9921e-03 eta 0:04:58
epoch [4/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.8281 (0.9591) acc 93.7500 (80.3125) lr 1.9921e-03 eta 0:03:11
epoch [4/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 1.2402 (0.9952) acc 62.5000 (78.3333) lr 1.9921e-03 eta 0:02:35
epoch [4/50] batch [20/26] time 0.075 (0.114) data 0.000 (0.044) loss 0.8613 (0.9610) acc 75.0000 (78.7500) lr 1.9921e-03 eta 0:02:17
epoch [4/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.8818 (0.9585) acc 87.5000 (79.1250) lr 1.9921e-03 eta 0:02:06
epoch [5/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.163) loss 0.9243 (0.8566) acc 78.1250 (85.6250) lr 1.9823e-03 eta 0:04:40
epoch [5/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.6978 (0.8371) acc 84.3750 (84.3750) lr 1.9823e-03 eta 0:03:01
epoch [5/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.8950 (0.8492) acc 84.3750 (84.7917) lr 1.9823e-03 eta 0:02:28
epoch [5/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.6709 (0.8301) acc 90.6250 (85.1562) lr 1.9823e-03 eta 0:02:11
epoch [5/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 1.2559 (0.8513) acc 78.1250 (84.6250) lr 1.9823e-03 eta 0:02:00
epoch [6/50] batch [5/26] time 0.071 (0.239) data 0.000 (0.167) loss 1.1025 (0.9094) acc 75.0000 (82.5000) lr 1.9686e-03 eta 0:04:38
epoch [6/50] batch [10/26] time 0.070 (0.155) data 0.000 (0.083) loss 0.8013 (0.8418) acc 87.5000 (85.0000) lr 1.9686e-03 eta 0:02:59
epoch [6/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.8335 (0.8494) acc 84.3750 (85.0000) lr 1.9686e-03 eta 0:02:26
epoch [6/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.6226 (0.8149) acc 87.5000 (85.7812) lr 1.9686e-03 eta 0:02:09
epoch [6/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.6309 (0.8014) acc 90.6250 (86.0000) lr 1.9686e-03 eta 0:01:59
epoch [7/50] batch [5/26] time 0.071 (0.235) data 0.000 (0.163) loss 0.6401 (0.7161) acc 90.6250 (90.6250) lr 1.9511e-03 eta 0:04:28
epoch [7/50] batch [10/26] time 0.070 (0.154) data 0.000 (0.082) loss 0.7041 (0.6974) acc 87.5000 (89.3750) lr 1.9511e-03 eta 0:02:54
epoch [7/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.8071 (0.6966) acc 87.5000 (89.3750) lr 1.9511e-03 eta 0:02:22
epoch [7/50] batch [20/26] time 0.069 (0.112) data 0.000 (0.041) loss 0.7646 (0.7260) acc 84.3750 (88.1250) lr 1.9511e-03 eta 0:02:05
epoch [7/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.5322 (0.7018) acc 93.7500 (88.6250) lr 1.9511e-03 eta 0:01:55
epoch [8/50] batch [5/26] time 0.071 (0.239) data 0.000 (0.167) loss 0.7368 (0.6596) acc 87.5000 (88.7500) lr 1.9298e-03 eta 0:04:26
epoch [8/50] batch [10/26] time 0.070 (0.155) data 0.000 (0.083) loss 0.6187 (0.6746) acc 90.6250 (88.7500) lr 1.9298e-03 eta 0:02:52
epoch [8/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.5825 (0.6721) acc 93.7500 (88.7500) lr 1.9298e-03 eta 0:02:20
epoch [8/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.7656 (0.6785) acc 87.5000 (88.5938) lr 1.9298e-03 eta 0:02:04
epoch [8/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.7900 (0.6710) acc 87.5000 (88.8750) lr 1.9298e-03 eta 0:01:54
epoch [9/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.164) loss 0.5586 (0.6933) acc 90.6250 (88.1250) lr 1.9048e-03 eta 0:04:16
epoch [9/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.4919 (0.6646) acc 96.8750 (88.7500) lr 1.9048e-03 eta 0:02:45
epoch [9/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.5356 (0.6452) acc 87.5000 (89.1667) lr 1.9048e-03 eta 0:02:15
epoch [9/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.6709 (0.6274) acc 87.5000 (89.5312) lr 1.9048e-03 eta 0:01:59
epoch [9/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.7803 (0.6445) acc 78.1250 (89.0000) lr 1.9048e-03 eta 0:01:50
epoch [10/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.164) loss 0.4126 (0.5493) acc 100.0000 (93.1250) lr 1.8763e-03 eta 0:04:09
epoch [10/50] batch [10/26] time 0.072 (0.153) data 0.001 (0.082) loss 0.8418 (0.5858) acc 84.3750 (90.9375) lr 1.8763e-03 eta 0:02:41
epoch [10/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.6064 (0.5996) acc 93.7500 (90.8333) lr 1.8763e-03 eta 0:02:12
epoch [10/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.041) loss 0.8530 (0.6289) acc 87.5000 (90.0000) lr 1.8763e-03 eta 0:01:57
epoch [10/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.5732 (0.6233) acc 90.6250 (90.1250) lr 1.8763e-03 eta 0:01:47
epoch [11/50] batch [5/26] time 0.072 (0.238) data 0.000 (0.163) loss 0.6406 (0.5755) acc 87.5000 (91.2500) lr 1.8443e-03 eta 0:04:06
epoch [11/50] batch [10/26] time 0.073 (0.155) data 0.000 (0.082) loss 0.5151 (0.5817) acc 96.8750 (90.3125) lr 1.8443e-03 eta 0:02:39
epoch [11/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.055) loss 0.6777 (0.6228) acc 90.6250 (89.5833) lr 1.8443e-03 eta 0:02:09
epoch [11/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.041) loss 0.5728 (0.5969) acc 93.7500 (90.9375) lr 1.8443e-03 eta 0:01:55
epoch [11/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.8574 (0.5959) acc 84.3750 (91.0000) lr 1.8443e-03 eta 0:01:45
epoch [12/50] batch [5/26] time 0.072 (0.237) data 0.000 (0.165) loss 0.6299 (0.6000) acc 90.6250 (90.0000) lr 1.8090e-03 eta 0:03:59
epoch [12/50] batch [10/26] time 0.070 (0.154) data 0.000 (0.082) loss 0.5454 (0.5760) acc 93.7500 (91.2500) lr 1.8090e-03 eta 0:02:34
epoch [12/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.6387 (0.5682) acc 87.5000 (91.2500) lr 1.8090e-03 eta 0:02:05
epoch [12/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.5210 (0.5602) acc 96.8750 (91.5625) lr 1.8090e-03 eta 0:01:51
epoch [12/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.6367 (0.5691) acc 87.5000 (91.1250) lr 1.8090e-03 eta 0:01:42
epoch [13/50] batch [5/26] time 0.071 (0.241) data 0.000 (0.168) loss 0.4724 (0.6469) acc 93.7500 (89.3750) lr 1.7705e-03 eta 0:03:57
epoch [13/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.8037 (0.5821) acc 81.2500 (90.3125) lr 1.7705e-03 eta 0:02:32
epoch [13/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.4316 (0.5557) acc 96.8750 (91.6667) lr 1.7705e-03 eta 0:02:04
epoch [13/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.042) loss 0.5605 (0.5365) acc 90.6250 (92.3438) lr 1.7705e-03 eta 0:01:49
epoch [13/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4893 (0.5307) acc 93.7500 (92.1250) lr 1.7705e-03 eta 0:01:41
epoch [14/50] batch [5/26] time 0.072 (0.286) data 0.000 (0.214) loss 0.5034 (0.5212) acc 93.7500 (91.2500) lr 1.7290e-03 eta 0:04:34
epoch [14/50] batch [10/26] time 0.071 (0.179) data 0.000 (0.107) loss 0.4697 (0.4961) acc 93.7500 (93.4375) lr 1.7290e-03 eta 0:02:49
epoch [14/50] batch [15/26] time 0.071 (0.143) data 0.000 (0.072) loss 1.0020 (0.5381) acc 75.0000 (91.6667) lr 1.7290e-03 eta 0:02:15
epoch [14/50] batch [20/26] time 0.071 (0.125) data 0.000 (0.054) loss 0.4346 (0.5531) acc 93.7500 (90.4688) lr 1.7290e-03 eta 0:01:57
epoch [14/50] batch [25/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.7671 (0.5507) acc 81.2500 (90.7500) lr 1.7290e-03 eta 0:01:46
epoch [15/50] batch [5/26] time 0.088 (0.270) data 0.000 (0.194) loss 0.5352 (0.5008) acc 90.6250 (91.2500) lr 1.6845e-03 eta 0:04:11
epoch [15/50] batch [10/26] time 0.072 (0.171) data 0.000 (0.097) loss 0.5205 (0.5103) acc 90.6250 (91.8750) lr 1.6845e-03 eta 0:02:38
epoch [15/50] batch [15/26] time 0.070 (0.137) data 0.000 (0.065) loss 0.4092 (0.4732) acc 96.8750 (93.3333) lr 1.6845e-03 eta 0:02:06
epoch [15/50] batch [20/26] time 0.070 (0.121) data 0.000 (0.049) loss 0.4121 (0.4716) acc 96.8750 (93.4375) lr 1.6845e-03 eta 0:01:50
epoch [15/50] batch [25/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.4294 (0.4641) acc 96.8750 (93.7500) lr 1.6845e-03 eta 0:01:40
epoch [16/50] batch [5/26] time 0.073 (0.254) data 0.000 (0.180) loss 0.3398 (0.4485) acc 96.8750 (94.3750) lr 1.6374e-03 eta 0:03:49
epoch [16/50] batch [10/26] time 0.073 (0.163) data 0.000 (0.090) loss 0.4165 (0.4526) acc 96.8750 (93.4375) lr 1.6374e-03 eta 0:02:26
epoch [16/50] batch [15/26] time 0.074 (0.133) data 0.000 (0.060) loss 0.4712 (0.4880) acc 93.7500 (93.1250) lr 1.6374e-03 eta 0:01:59
epoch [16/50] batch [20/26] time 0.073 (0.118) data 0.000 (0.045) loss 0.7002 (0.5162) acc 87.5000 (92.8125) lr 1.6374e-03 eta 0:01:45
epoch [16/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.036) loss 0.5000 (0.5108) acc 87.5000 (92.7500) lr 1.6374e-03 eta 0:01:36
epoch [17/50] batch [5/26] time 0.073 (0.240) data 0.000 (0.167) loss 0.5142 (0.5462) acc 93.7500 (91.8750) lr 1.5878e-03 eta 0:03:30
epoch [17/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.084) loss 0.4312 (0.5422) acc 96.8750 (92.5000) lr 1.5878e-03 eta 0:02:16
epoch [17/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.6255 (0.5382) acc 81.2500 (91.8750) lr 1.5878e-03 eta 0:01:51
epoch [17/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.4961 (0.5264) acc 96.8750 (92.3438) lr 1.5878e-03 eta 0:01:37
epoch [17/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.3572 (0.5210) acc 96.8750 (92.6250) lr 1.5878e-03 eta 0:01:29
epoch [18/50] batch [5/26] time 0.074 (0.244) data 0.000 (0.171) loss 0.5381 (0.5204) acc 90.6250 (93.1250) lr 1.5358e-03 eta 0:03:27
epoch [18/50] batch [10/26] time 0.072 (0.157) data 0.000 (0.086) loss 0.3923 (0.4539) acc 96.8750 (95.3125) lr 1.5358e-03 eta 0:02:13
epoch [18/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.7632 (0.4897) acc 81.2500 (93.7500) lr 1.5358e-03 eta 0:01:48
epoch [18/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.4802 (0.5114) acc 93.7500 (92.8125) lr 1.5358e-03 eta 0:01:35
epoch [18/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4817 (0.5013) acc 93.7500 (93.0000) lr 1.5358e-03 eta 0:01:27
epoch [19/50] batch [5/26] time 0.076 (0.242) data 0.000 (0.170) loss 0.5210 (0.4587) acc 93.7500 (95.6250) lr 1.4818e-03 eta 0:03:20
epoch [19/50] batch [10/26] time 0.072 (0.157) data 0.000 (0.085) loss 0.4646 (0.4870) acc 93.7500 (93.1250) lr 1.4818e-03 eta 0:02:09
epoch [19/50] batch [15/26] time 0.072 (0.129) data 0.000 (0.057) loss 0.7451 (0.5073) acc 87.5000 (92.7083) lr 1.4818e-03 eta 0:01:45
epoch [19/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.5527 (0.4990) acc 87.5000 (92.6562) lr 1.4818e-03 eta 0:01:32
epoch [19/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.034) loss 0.4465 (0.5022) acc 93.7500 (92.8750) lr 1.4818e-03 eta 0:01:25
epoch [20/50] batch [5/26] time 0.072 (0.247) data 0.000 (0.173) loss 0.3931 (0.4643) acc 96.8750 (93.1250) lr 1.4258e-03 eta 0:03:17
epoch [20/50] batch [10/26] time 0.071 (0.160) data 0.000 (0.087) loss 0.4673 (0.4833) acc 96.8750 (94.0625) lr 1.4258e-03 eta 0:02:07
epoch [20/50] batch [15/26] time 0.071 (0.130) data 0.000 (0.058) loss 0.5068 (0.4874) acc 93.7500 (93.1250) lr 1.4258e-03 eta 0:01:42
epoch [20/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.043) loss 0.3394 (0.4848) acc 100.0000 (92.9688) lr 1.4258e-03 eta 0:01:30
epoch [20/50] batch [25/26] time 0.072 (0.106) data 0.000 (0.035) loss 0.6504 (0.4969) acc 90.6250 (92.8750) lr 1.4258e-03 eta 0:01:23
epoch [21/50] batch [5/26] time 0.076 (0.244) data 0.000 (0.169) loss 0.3394 (0.4354) acc 96.8750 (96.2500) lr 1.3681e-03 eta 0:03:08
epoch [21/50] batch [10/26] time 0.072 (0.158) data 0.001 (0.085) loss 0.5928 (0.4724) acc 90.6250 (94.0625) lr 1.3681e-03 eta 0:02:02
epoch [21/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.057) loss 0.4849 (0.4869) acc 93.7500 (93.9583) lr 1.3681e-03 eta 0:01:39
epoch [21/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.4958 (0.5004) acc 87.5000 (92.6562) lr 1.3681e-03 eta 0:01:27
epoch [21/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.034) loss 0.5000 (0.4882) acc 90.6250 (92.6250) lr 1.3681e-03 eta 0:01:19
epoch [22/50] batch [5/26] time 0.071 (0.266) data 0.000 (0.194) loss 0.4023 (0.4488) acc 96.8750 (96.2500) lr 1.3090e-03 eta 0:03:19
epoch [22/50] batch [10/26] time 0.071 (0.169) data 0.000 (0.097) loss 0.3931 (0.4509) acc 96.8750 (95.6250) lr 1.3090e-03 eta 0:02:05
epoch [22/50] batch [15/26] time 0.073 (0.136) data 0.000 (0.065) loss 0.4392 (0.4662) acc 96.8750 (94.7917) lr 1.3090e-03 eta 0:01:40
epoch [22/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.049) loss 0.4849 (0.4539) acc 90.6250 (94.8438) lr 1.3090e-03 eta 0:01:27
epoch [22/50] batch [25/26] time 0.072 (0.110) data 0.000 (0.039) loss 0.4355 (0.4664) acc 93.7500 (93.8750) lr 1.3090e-03 eta 0:01:20
epoch [23/50] batch [5/26] time 0.072 (0.246) data 0.000 (0.175) loss 0.5449 (0.5060) acc 90.6250 (94.3750) lr 1.2487e-03 eta 0:02:57
epoch [23/50] batch [10/26] time 0.072 (0.159) data 0.000 (0.088) loss 0.4258 (0.4566) acc 96.8750 (95.9375) lr 1.2487e-03 eta 0:01:53
epoch [23/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.3230 (0.4386) acc 93.7500 (95.4167) lr 1.2487e-03 eta 0:01:32
epoch [23/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.4573 (0.4494) acc 93.7500 (94.5312) lr 1.2487e-03 eta 0:01:21
epoch [23/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.3787 (0.4654) acc 93.7500 (93.3750) lr 1.2487e-03 eta 0:01:14
epoch [24/50] batch [5/26] time 0.072 (0.277) data 0.000 (0.204) loss 0.5488 (0.4681) acc 90.6250 (93.1250) lr 1.1874e-03 eta 0:03:12
epoch [24/50] batch [10/26] time 0.070 (0.174) data 0.000 (0.102) loss 0.4341 (0.4429) acc 93.7500 (94.3750) lr 1.1874e-03 eta 0:02:00
epoch [24/50] batch [15/26] time 0.071 (0.140) data 0.000 (0.068) loss 0.4153 (0.4263) acc 93.7500 (94.5833) lr 1.1874e-03 eta 0:01:35
epoch [24/50] batch [20/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.3345 (0.4252) acc 100.0000 (94.6875) lr 1.1874e-03 eta 0:01:23
epoch [24/50] batch [25/26] time 0.072 (0.112) data 0.000 (0.041) loss 0.5469 (0.4530) acc 87.5000 (93.5000) lr 1.1874e-03 eta 0:01:15
epoch [25/50] batch [5/26] time 0.071 (0.275) data 0.001 (0.201) loss 0.4978 (0.4157) acc 90.6250 (95.6250) lr 1.1253e-03 eta 0:03:04
epoch [25/50] batch [10/26] time 0.072 (0.174) data 0.000 (0.101) loss 0.4739 (0.4655) acc 93.7500 (93.4375) lr 1.1253e-03 eta 0:01:55
epoch [25/50] batch [15/26] time 0.070 (0.139) data 0.000 (0.067) loss 0.4165 (0.4608) acc 96.8750 (94.1667) lr 1.1253e-03 eta 0:01:32
epoch [25/50] batch [20/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.5840 (0.4678) acc 87.5000 (93.5938) lr 1.1253e-03 eta 0:01:20
epoch [25/50] batch [25/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.3206 (0.4707) acc 96.8750 (93.3750) lr 1.1253e-03 eta 0:01:12
epoch [26/50] batch [5/26] time 0.070 (0.280) data 0.000 (0.208) loss 0.3730 (0.4382) acc 93.7500 (95.6250) lr 1.0628e-03 eta 0:03:00
epoch [26/50] batch [10/26] time 0.072 (0.176) data 0.000 (0.104) loss 0.4182 (0.4145) acc 100.0000 (95.9375) lr 1.0628e-03 eta 0:01:52
epoch [26/50] batch [15/26] time 0.072 (0.141) data 0.000 (0.070) loss 0.5400 (0.4253) acc 90.6250 (95.4167) lr 1.0628e-03 eta 0:01:29
epoch [26/50] batch [20/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.3950 (0.4439) acc 96.8750 (93.7500) lr 1.0628e-03 eta 0:01:18
epoch [26/50] batch [25/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.5410 (0.4359) acc 90.6250 (94.5000) lr 1.0628e-03 eta 0:01:10
epoch [27/50] batch [5/26] time 0.071 (0.254) data 0.001 (0.182) loss 0.4919 (0.4295) acc 93.7500 (95.6250) lr 1.0000e-03 eta 0:02:37
epoch [27/50] batch [10/26] time 0.070 (0.162) data 0.000 (0.091) loss 0.2861 (0.3958) acc 100.0000 (96.2500) lr 1.0000e-03 eta 0:01:39
epoch [27/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.061) loss 0.5049 (0.4326) acc 96.8750 (95.2083) lr 1.0000e-03 eta 0:01:20
epoch [27/50] batch [20/26] time 0.071 (0.116) data 0.000 (0.046) loss 0.3674 (0.4347) acc 100.0000 (94.8438) lr 1.0000e-03 eta 0:01:10
epoch [27/50] batch [25/26] time 0.072 (0.107) data 0.000 (0.037) loss 0.4558 (0.4289) acc 93.7500 (94.8750) lr 1.0000e-03 eta 0:01:04
epoch [28/50] batch [5/26] time 0.071 (0.218) data 0.000 (0.145) loss 0.3884 (0.4103) acc 96.8750 (95.0000) lr 9.3721e-04 eta 0:02:09
epoch [28/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.073) loss 0.4292 (0.4415) acc 93.7500 (94.6875) lr 9.3721e-04 eta 0:01:24
epoch [28/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.049) loss 0.6367 (0.4572) acc 87.5000 (94.3750) lr 9.3721e-04 eta 0:01:09
epoch [28/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.4607 (0.4507) acc 93.7500 (94.6875) lr 9.3721e-04 eta 0:01:02
epoch [28/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.3621 (0.4410) acc 93.7500 (94.7500) lr 9.3721e-04 eta 0:00:57
epoch [29/50] batch [5/26] time 0.072 (0.230) data 0.000 (0.158) loss 0.2808 (0.4076) acc 100.0000 (92.5000) lr 8.7467e-04 eta 0:02:10
epoch [29/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.079) loss 0.4827 (0.3854) acc 96.8750 (95.0000) lr 8.7467e-04 eta 0:01:24
epoch [29/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.053) loss 0.3638 (0.4503) acc 96.8750 (94.5833) lr 8.7467e-04 eta 0:01:09
epoch [29/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.040) loss 0.3804 (0.4372) acc 96.8750 (94.6875) lr 8.7467e-04 eta 0:01:01
epoch [29/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.7778 (0.4668) acc 84.3750 (93.6250) lr 8.7467e-04 eta 0:00:56
epoch [30/50] batch [5/26] time 0.072 (0.220) data 0.000 (0.147) loss 0.5596 (0.4652) acc 90.6250 (93.7500) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.074) loss 0.2754 (0.4354) acc 100.0000 (94.0625) lr 8.1262e-04 eta 0:01:18
epoch [30/50] batch [15/26] time 0.074 (0.122) data 0.000 (0.049) loss 0.4448 (0.4494) acc 96.8750 (93.3333) lr 8.1262e-04 eta 0:01:04
epoch [30/50] batch [20/26] time 0.072 (0.110) data 0.000 (0.037) loss 0.4089 (0.4374) acc 96.8750 (93.5938) lr 8.1262e-04 eta 0:00:57
epoch [30/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.030) loss 0.3926 (0.4378) acc 93.7500 (93.1250) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/26] time 0.071 (0.260) data 0.000 (0.187) loss 0.3691 (0.3989) acc 96.8750 (95.0000) lr 7.5131e-04 eta 0:02:13
epoch [31/50] batch [10/26] time 0.071 (0.165) data 0.000 (0.093) loss 0.2983 (0.4222) acc 100.0000 (95.3125) lr 7.5131e-04 eta 0:01:24
epoch [31/50] batch [15/26] time 0.070 (0.134) data 0.000 (0.062) loss 0.6006 (0.4253) acc 90.6250 (95.2083) lr 7.5131e-04 eta 0:01:07
epoch [31/50] batch [20/26] time 0.071 (0.118) data 0.000 (0.047) loss 0.4285 (0.4328) acc 93.7500 (95.0000) lr 7.5131e-04 eta 0:00:59
epoch [31/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4307 (0.4368) acc 90.6250 (94.5000) lr 7.5131e-04 eta 0:00:53
epoch [32/50] batch [5/26] time 0.072 (0.237) data 0.000 (0.165) loss 0.4070 (0.4004) acc 96.8750 (96.2500) lr 6.9098e-04 eta 0:01:55
epoch [32/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.083) loss 0.4092 (0.3898) acc 93.7500 (96.2500) lr 6.9098e-04 eta 0:01:14
epoch [32/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.055) loss 0.4802 (0.4009) acc 90.6250 (95.4167) lr 6.9098e-04 eta 0:01:00
epoch [32/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.041) loss 0.3750 (0.4006) acc 96.8750 (95.4688) lr 6.9098e-04 eta 0:00:53
epoch [32/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.5273 (0.4067) acc 90.6250 (95.0000) lr 6.9098e-04 eta 0:00:48
epoch [33/50] batch [5/26] time 0.073 (0.240) data 0.000 (0.167) loss 0.2915 (0.4376) acc 100.0000 (95.0000) lr 6.3188e-04 eta 0:01:50
epoch [33/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.2671 (0.4140) acc 100.0000 (95.9375) lr 6.3188e-04 eta 0:01:11
epoch [33/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.056) loss 0.5088 (0.3980) acc 90.6250 (95.6250) lr 6.3188e-04 eta 0:00:57
epoch [33/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.5664 (0.3983) acc 90.6250 (95.7812) lr 6.3188e-04 eta 0:00:50
epoch [33/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4490 (0.4113) acc 96.8750 (95.2500) lr 6.3188e-04 eta 0:00:46
epoch [34/50] batch [5/26] time 0.072 (0.269) data 0.000 (0.197) loss 0.4915 (0.4237) acc 90.6250 (93.7500) lr 5.7422e-04 eta 0:01:57
epoch [34/50] batch [10/26] time 0.074 (0.171) data 0.000 (0.100) loss 0.4441 (0.4066) acc 96.8750 (95.0000) lr 5.7422e-04 eta 0:01:13
epoch [34/50] batch [15/26] time 0.070 (0.138) data 0.000 (0.066) loss 0.3799 (0.4089) acc 93.7500 (94.7917) lr 5.7422e-04 eta 0:00:58
epoch [34/50] batch [20/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.4939 (0.4053) acc 90.6250 (95.0000) lr 5.7422e-04 eta 0:00:50
epoch [34/50] batch [25/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.4438 (0.4096) acc 96.8750 (95.1250) lr 5.7422e-04 eta 0:00:46
epoch [35/50] batch [5/26] time 0.073 (0.255) data 0.000 (0.181) loss 0.2527 (0.3894) acc 100.0000 (97.5000) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [10/26] time 0.071 (0.163) data 0.000 (0.091) loss 0.4412 (0.4204) acc 90.6250 (95.3125) lr 5.1825e-04 eta 0:01:06
epoch [35/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.060) loss 0.4268 (0.4537) acc 93.7500 (93.9583) lr 5.1825e-04 eta 0:00:53
epoch [35/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.045) loss 0.3306 (0.4547) acc 100.0000 (93.5938) lr 5.1825e-04 eta 0:00:46
epoch [35/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.036) loss 0.3105 (0.4420) acc 100.0000 (94.1250) lr 5.1825e-04 eta 0:00:42
epoch [36/50] batch [5/26] time 0.072 (0.236) data 0.000 (0.165) loss 0.5107 (0.3498) acc 93.7500 (96.8750) lr 4.6417e-04 eta 0:01:30
epoch [36/50] batch [10/26] time 0.072 (0.154) data 0.000 (0.083) loss 0.5361 (0.4142) acc 93.7500 (95.0000) lr 4.6417e-04 eta 0:00:58
epoch [36/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.055) loss 0.3184 (0.3957) acc 100.0000 (96.0417) lr 4.6417e-04 eta 0:00:47
epoch [36/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.041) loss 0.5073 (0.3872) acc 90.6250 (96.2500) lr 4.6417e-04 eta 0:00:41
epoch [36/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.4417 (0.3830) acc 93.7500 (96.5000) lr 4.6417e-04 eta 0:00:37
epoch [37/50] batch [5/26] time 0.074 (0.227) data 0.000 (0.153) loss 0.6104 (0.4451) acc 90.6250 (94.3750) lr 4.1221e-04 eta 0:01:21
epoch [37/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.077) loss 0.3433 (0.4115) acc 100.0000 (95.3125) lr 4.1221e-04 eta 0:00:52
epoch [37/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.051) loss 0.5400 (0.4170) acc 90.6250 (95.0000) lr 4.1221e-04 eta 0:00:42
epoch [37/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.4355 (0.4283) acc 96.8750 (94.8438) lr 4.1221e-04 eta 0:00:37
epoch [37/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5610 (0.4490) acc 90.6250 (94.0000) lr 4.1221e-04 eta 0:00:34
epoch [38/50] batch [5/26] time 0.071 (0.225) data 0.000 (0.153) loss 0.3555 (0.4332) acc 96.8750 (94.3750) lr 3.6258e-04 eta 0:01:14
epoch [38/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.077) loss 0.4731 (0.4283) acc 96.8750 (95.0000) lr 3.6258e-04 eta 0:00:48
epoch [38/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.6250 (0.4277) acc 90.6250 (95.0000) lr 3.6258e-04 eta 0:00:39
epoch [38/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3833 (0.4252) acc 96.8750 (95.4688) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.3770 (0.4110) acc 96.8750 (95.7500) lr 3.6258e-04 eta 0:00:31
epoch [39/50] batch [5/26] time 0.071 (0.220) data 0.000 (0.145) loss 0.2878 (0.3589) acc 100.0000 (98.1250) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [10/26] time 0.072 (0.146) data 0.000 (0.073) loss 0.4353 (0.3798) acc 93.7500 (96.8750) lr 3.1545e-04 eta 0:00:44
epoch [39/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.049) loss 0.3406 (0.3774) acc 96.8750 (96.8750) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.037) loss 0.5483 (0.4089) acc 84.3750 (95.4688) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/26] time 0.071 (0.101) data 0.001 (0.029) loss 0.3389 (0.3999) acc 96.8750 (95.5000) lr 3.1545e-04 eta 0:00:29
epoch [40/50] batch [5/26] time 0.073 (0.217) data 0.000 (0.145) loss 0.6602 (0.4937) acc 87.5000 (93.7500) lr 2.7103e-04 eta 0:01:00
epoch [40/50] batch [10/26] time 0.072 (0.145) data 0.000 (0.072) loss 0.3679 (0.4211) acc 93.7500 (94.6875) lr 2.7103e-04 eta 0:00:39
epoch [40/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.5181 (0.4361) acc 90.6250 (94.3750) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.036) loss 0.4121 (0.4176) acc 93.7500 (94.5312) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.3218 (0.4080) acc 96.8750 (94.7500) lr 2.7103e-04 eta 0:00:26
epoch [41/50] batch [5/26] time 0.072 (0.245) data 0.000 (0.174) loss 0.4993 (0.3822) acc 93.7500 (95.6250) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.087) loss 0.3237 (0.3859) acc 100.0000 (95.0000) lr 2.2949e-04 eta 0:00:39
epoch [41/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.058) loss 0.2268 (0.3829) acc 100.0000 (95.4167) lr 2.2949e-04 eta 0:00:31
epoch [41/50] batch [20/26] time 0.072 (0.115) data 0.000 (0.044) loss 0.6367 (0.4085) acc 90.6250 (94.6875) lr 2.2949e-04 eta 0:00:27
epoch [41/50] batch [25/26] time 0.072 (0.106) data 0.000 (0.035) loss 0.2822 (0.4171) acc 96.8750 (94.6250) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/26] time 0.076 (0.226) data 0.000 (0.152) loss 0.2881 (0.3908) acc 93.7500 (94.3750) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.076) loss 0.4365 (0.3770) acc 96.8750 (95.3125) lr 1.9098e-04 eta 0:00:33
epoch [42/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.051) loss 0.3486 (0.3937) acc 96.8750 (95.4167) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.038) loss 0.2759 (0.4071) acc 100.0000 (95.0000) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.4307 (0.4099) acc 93.7500 (94.7500) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.071 (0.255) data 0.000 (0.185) loss 0.3372 (0.4132) acc 96.8750 (94.3750) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [10/26] time 0.070 (0.163) data 0.000 (0.092) loss 0.3394 (0.3763) acc 100.0000 (95.3125) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.062) loss 0.4888 (0.3929) acc 96.8750 (94.7917) lr 1.5567e-04 eta 0:00:25
epoch [43/50] batch [20/26] time 0.071 (0.117) data 0.000 (0.046) loss 0.4980 (0.4041) acc 87.5000 (94.8438) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.3750 (0.3951) acc 96.8750 (95.1250) lr 1.5567e-04 eta 0:00:19
epoch [44/50] batch [5/26] time 0.072 (0.256) data 0.000 (0.183) loss 0.4629 (0.4309) acc 96.8750 (95.6250) lr 1.2369e-04 eta 0:00:45
epoch [44/50] batch [10/26] time 0.071 (0.164) data 0.000 (0.092) loss 0.4854 (0.4425) acc 93.7500 (94.3750) lr 1.2369e-04 eta 0:00:28
epoch [44/50] batch [15/26] time 0.070 (0.133) data 0.000 (0.061) loss 0.5088 (0.4271) acc 93.7500 (94.7917) lr 1.2369e-04 eta 0:00:22
epoch [44/50] batch [20/26] time 0.072 (0.117) data 0.000 (0.046) loss 0.4409 (0.4158) acc 93.7500 (95.0000) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.2625 (0.4039) acc 100.0000 (95.3750) lr 1.2369e-04 eta 0:00:16
epoch [45/50] batch [5/26] time 0.073 (0.259) data 0.001 (0.187) loss 0.4937 (0.4107) acc 90.6250 (93.1250) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [10/26] time 0.071 (0.166) data 0.000 (0.094) loss 0.2336 (0.3725) acc 100.0000 (95.3125) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [15/26] time 0.071 (0.134) data 0.000 (0.062) loss 0.4580 (0.3887) acc 93.7500 (95.0000) lr 9.5173e-05 eta 0:00:18
epoch [45/50] batch [20/26] time 0.071 (0.118) data 0.000 (0.047) loss 0.4121 (0.3849) acc 96.8750 (95.1562) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3767 (0.3833) acc 93.7500 (95.2500) lr 9.5173e-05 eta 0:00:14
epoch [46/50] batch [5/26] time 0.070 (0.232) data 0.000 (0.162) loss 0.3904 (0.3615) acc 96.8750 (97.5000) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.081) loss 0.4045 (0.3818) acc 93.7500 (96.2500) lr 7.0224e-05 eta 0:00:18
epoch [46/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.054) loss 0.4187 (0.3773) acc 96.8750 (96.2500) lr 7.0224e-05 eta 0:00:14
epoch [46/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.041) loss 0.2900 (0.3823) acc 96.8750 (96.0938) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.3027 (0.3709) acc 100.0000 (96.3750) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.071 (0.228) data 0.000 (0.156) loss 0.2822 (0.3531) acc 100.0000 (96.2500) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.078) loss 0.4255 (0.3690) acc 96.8750 (96.2500) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.3911 (0.3720) acc 93.7500 (96.0417) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.3477 (0.3714) acc 96.8750 (95.9375) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5381 (0.3833) acc 90.6250 (95.8750) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.071 (0.229) data 0.000 (0.157) loss 0.4797 (0.4007) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.078) loss 0.2920 (0.3971) acc 96.8750 (93.7500) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.052) loss 0.2490 (0.4047) acc 100.0000 (93.9583) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.3032 (0.3965) acc 100.0000 (94.3750) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.2351 (0.3911) acc 100.0000 (94.8750) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.075 (0.227) data 0.000 (0.153) loss 0.4846 (0.5146) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.077) loss 0.4639 (0.4782) acc 84.3750 (90.6250) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.051) loss 0.4180 (0.4879) acc 96.8750 (91.4583) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.4624 (0.4663) acc 90.6250 (92.3438) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.4763 (0.4444) acc 96.8750 (93.2500) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.073 (0.229) data 0.000 (0.158) loss 0.3682 (0.3902) acc 96.8750 (96.2500) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.079) loss 0.3562 (0.4026) acc 93.7500 (95.9375) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.053) loss 0.2949 (0.3816) acc 96.8750 (96.0417) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.040) loss 0.3857 (0.3909) acc 93.7500 (95.9375) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.4849 (0.4038) acc 93.7500 (95.7500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:07<00:14,  7.08s/it] 67%|██████▋   | 2/3 [00:07<00:03,  3.25s/it]100%|██████████| 3/3 [00:07<00:00,  1.82s/it]100%|██████████| 3/3 [00:07<00:00,  2.63s/it]
=> result
* total: 1,053
* correct: 1,031
* accuracy: 97.9%
* error: 2.1%
* macro_f1: 97.9%
Elapsed: 0:02:29
Run this job and save the output to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.070 (0.298) data 0.000 (0.195) loss 2.6758 (2.8055) acc 56.2500 (57.5000) lr 1.0000e-05 eta 0:06:25
epoch [1/50] batch [10/26] time 0.071 (0.185) data 0.000 (0.098) loss 2.7617 (2.8490) acc 65.6250 (56.2500) lr 1.0000e-05 eta 0:03:58
epoch [1/50] batch [15/26] time 0.070 (0.147) data 0.000 (0.065) loss 2.9297 (2.9036) acc 46.8750 (55.0000) lr 1.0000e-05 eta 0:03:08
epoch [1/50] batch [20/26] time 0.070 (0.127) data 0.000 (0.049) loss 2.7852 (2.8191) acc 59.3750 (56.4062) lr 1.0000e-05 eta 0:02:43
epoch [1/50] batch [25/26] time 0.070 (0.116) data 0.000 (0.039) loss 3.3613 (2.8643) acc 40.6250 (55.6250) lr 1.0000e-05 eta 0:02:27
epoch [2/50] batch [5/26] time 0.070 (0.232) data 0.000 (0.161) loss 1.7500 (2.2469) acc 65.6250 (56.8750) lr 2.0000e-03 eta 0:04:54
epoch [2/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.081) loss 1.5273 (1.9403) acc 71.8750 (61.2500) lr 2.0000e-03 eta 0:03:10
epoch [2/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.054) loss 1.8428 (1.7747) acc 56.2500 (63.3333) lr 2.0000e-03 eta 0:02:36
epoch [2/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.041) loss 0.9761 (1.6795) acc 71.8750 (62.8125) lr 2.0000e-03 eta 0:02:18
epoch [2/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 1.0059 (1.6038) acc 75.0000 (63.1250) lr 2.0000e-03 eta 0:02:08
epoch [3/50] batch [5/26] time 0.070 (0.289) data 0.000 (0.218) loss 1.0537 (1.3158) acc 84.3750 (70.0000) lr 1.9980e-03 eta 0:05:59
epoch [3/50] batch [10/26] time 0.071 (0.181) data 0.000 (0.109) loss 1.4043 (1.2238) acc 68.7500 (73.1250) lr 1.9980e-03 eta 0:03:43
epoch [3/50] batch [15/26] time 0.070 (0.144) data 0.000 (0.073) loss 1.1289 (1.1745) acc 81.2500 (74.1667) lr 1.9980e-03 eta 0:02:57
epoch [3/50] batch [20/26] time 0.070 (0.126) data 0.000 (0.055) loss 1.4551 (1.1432) acc 59.3750 (73.7500) lr 1.9980e-03 eta 0:02:34
epoch [3/50] batch [25/26] time 0.069 (0.114) data 0.000 (0.044) loss 0.9297 (1.1199) acc 75.0000 (74.3750) lr 1.9980e-03 eta 0:02:20
epoch [4/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.170) loss 1.1875 (0.9911) acc 81.2500 (81.2500) lr 1.9921e-03 eta 0:04:55
epoch [4/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.085) loss 0.9404 (0.9899) acc 81.2500 (78.7500) lr 1.9921e-03 eta 0:03:09
epoch [4/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.7158 (0.9275) acc 90.6250 (80.6250) lr 1.9921e-03 eta 0:02:34
epoch [4/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.043) loss 1.2627 (0.9165) acc 62.5000 (81.2500) lr 1.9921e-03 eta 0:02:16
epoch [4/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 1.0723 (0.9322) acc 71.8750 (79.6250) lr 1.9921e-03 eta 0:02:05
epoch [5/50] batch [5/26] time 0.073 (0.246) data 0.000 (0.174) loss 0.7939 (0.7849) acc 81.2500 (85.0000) lr 1.9823e-03 eta 0:04:53
epoch [5/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.7622 (0.7613) acc 81.2500 (86.2500) lr 1.9823e-03 eta 0:03:07
epoch [5/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.6870 (0.7394) acc 93.7500 (87.5000) lr 1.9823e-03 eta 0:02:32
epoch [5/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.044) loss 0.6333 (0.7389) acc 87.5000 (86.5625) lr 1.9823e-03 eta 0:02:14
epoch [5/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.8735 (0.7420) acc 84.3750 (86.5000) lr 1.9823e-03 eta 0:02:03
epoch [6/50] batch [5/26] time 0.073 (0.230) data 0.000 (0.158) loss 0.6738 (0.6384) acc 87.5000 (90.0000) lr 1.9686e-03 eta 0:04:28
epoch [6/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.079) loss 0.6538 (0.6694) acc 84.3750 (88.7500) lr 1.9686e-03 eta 0:02:54
epoch [6/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.9375 (0.7005) acc 75.0000 (86.2500) lr 1.9686e-03 eta 0:02:22
epoch [6/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.040) loss 0.5752 (0.6787) acc 93.7500 (87.5000) lr 1.9686e-03 eta 0:02:06
epoch [6/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.5684 (0.6931) acc 90.6250 (87.2500) lr 1.9686e-03 eta 0:01:57
epoch [7/50] batch [5/26] time 0.070 (0.246) data 0.000 (0.174) loss 0.6768 (0.6558) acc 87.5000 (90.0000) lr 1.9511e-03 eta 0:04:40
epoch [7/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.6143 (0.6567) acc 90.6250 (90.9375) lr 1.9511e-03 eta 0:02:59
epoch [7/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.5767 (0.6723) acc 87.5000 (88.5417) lr 1.9511e-03 eta 0:02:25
epoch [7/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.044) loss 0.8193 (0.6797) acc 87.5000 (88.2812) lr 1.9511e-03 eta 0:02:08
epoch [7/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.6230 (0.6705) acc 90.6250 (88.5000) lr 1.9511e-03 eta 0:01:57
epoch [8/50] batch [5/26] time 0.070 (0.229) data 0.000 (0.158) loss 0.5669 (0.6399) acc 93.7500 (89.3750) lr 1.9298e-03 eta 0:04:15
epoch [8/50] batch [10/26] time 0.069 (0.151) data 0.000 (0.079) loss 0.6406 (0.6376) acc 90.6250 (89.0625) lr 1.9298e-03 eta 0:02:47
epoch [8/50] batch [15/26] time 0.069 (0.124) data 0.000 (0.053) loss 0.5488 (0.6553) acc 90.6250 (89.1667) lr 1.9298e-03 eta 0:02:16
epoch [8/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.5146 (0.6292) acc 96.8750 (89.3750) lr 1.9298e-03 eta 0:02:01
epoch [8/50] batch [25/26] time 0.072 (0.103) data 0.000 (0.032) loss 0.5674 (0.6215) acc 90.6250 (90.0000) lr 1.9298e-03 eta 0:01:52
epoch [9/50] batch [5/26] time 0.071 (0.256) data 0.000 (0.185) loss 0.4424 (0.5096) acc 96.8750 (93.1250) lr 1.9048e-03 eta 0:04:37
epoch [9/50] batch [10/26] time 0.074 (0.163) data 0.000 (0.093) loss 0.6025 (0.5358) acc 93.7500 (92.1875) lr 1.9048e-03 eta 0:02:56
epoch [9/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.062) loss 0.6572 (0.5633) acc 93.7500 (91.8750) lr 1.9048e-03 eta 0:02:22
epoch [9/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.047) loss 0.5298 (0.5554) acc 96.8750 (92.5000) lr 1.9048e-03 eta 0:02:05
epoch [9/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.037) loss 0.5117 (0.5764) acc 93.7500 (91.5000) lr 1.9048e-03 eta 0:01:54
epoch [10/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.165) loss 0.4443 (0.5412) acc 96.8750 (95.0000) lr 1.8763e-03 eta 0:04:09
epoch [10/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.5850 (0.5504) acc 90.6250 (93.4375) lr 1.8763e-03 eta 0:02:41
epoch [10/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.5068 (0.5399) acc 93.7500 (93.3333) lr 1.8763e-03 eta 0:02:11
epoch [10/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.041) loss 0.3901 (0.5504) acc 100.0000 (92.3438) lr 1.8763e-03 eta 0:01:56
epoch [10/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.5088 (0.5636) acc 93.7500 (92.0000) lr 1.8763e-03 eta 0:01:47
epoch [11/50] batch [5/26] time 0.072 (0.236) data 0.000 (0.164) loss 0.4092 (0.4670) acc 96.8750 (94.3750) lr 1.8443e-03 eta 0:04:04
epoch [11/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.5684 (0.5346) acc 93.7500 (91.8750) lr 1.8443e-03 eta 0:02:37
epoch [11/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.4453 (0.5163) acc 93.7500 (92.0833) lr 1.8443e-03 eta 0:02:08
epoch [11/50] batch [20/26] time 0.076 (0.112) data 0.000 (0.041) loss 0.5439 (0.5384) acc 93.7500 (91.5625) lr 1.8443e-03 eta 0:01:54
epoch [11/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.5879 (0.5400) acc 87.5000 (91.2500) lr 1.8443e-03 eta 0:01:45
epoch [12/50] batch [5/26] time 0.070 (0.235) data 0.000 (0.164) loss 0.5537 (0.5621) acc 90.6250 (91.8750) lr 1.8090e-03 eta 0:03:56
epoch [12/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.5659 (0.5176) acc 87.5000 (92.5000) lr 1.8090e-03 eta 0:02:33
epoch [12/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.4395 (0.5323) acc 96.8750 (92.0833) lr 1.8090e-03 eta 0:02:04
epoch [12/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.041) loss 0.6494 (0.5332) acc 87.5000 (92.1875) lr 1.8090e-03 eta 0:01:50
epoch [12/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.3882 (0.5223) acc 96.8750 (92.6250) lr 1.8090e-03 eta 0:01:41
epoch [13/50] batch [5/26] time 0.072 (0.241) data 0.000 (0.169) loss 0.8569 (0.5282) acc 81.2500 (92.5000) lr 1.7705e-03 eta 0:03:56
epoch [13/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.085) loss 0.5352 (0.4642) acc 90.6250 (94.3750) lr 1.7705e-03 eta 0:02:32
epoch [13/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.057) loss 0.5342 (0.4712) acc 87.5000 (93.3333) lr 1.7705e-03 eta 0:02:03
epoch [13/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.4893 (0.4824) acc 96.8750 (92.9688) lr 1.7705e-03 eta 0:01:49
epoch [13/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.034) loss 0.7812 (0.4980) acc 87.5000 (92.8750) lr 1.7705e-03 eta 0:01:40
epoch [14/50] batch [5/26] time 0.070 (0.249) data 0.000 (0.177) loss 0.4160 (0.4913) acc 93.7500 (95.6250) lr 1.7290e-03 eta 0:03:58
epoch [14/50] batch [10/26] time 0.074 (0.161) data 0.000 (0.089) loss 0.5957 (0.5454) acc 87.5000 (92.1875) lr 1.7290e-03 eta 0:02:33
epoch [14/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.059) loss 0.5854 (0.5394) acc 93.7500 (92.5000) lr 1.7290e-03 eta 0:02:03
epoch [14/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.6504 (0.5227) acc 90.6250 (92.8125) lr 1.7290e-03 eta 0:01:49
epoch [14/50] batch [25/26] time 0.072 (0.107) data 0.000 (0.036) loss 0.6831 (0.5265) acc 87.5000 (92.5000) lr 1.7290e-03 eta 0:01:40
epoch [15/50] batch [5/26] time 0.073 (0.256) data 0.000 (0.182) loss 0.3618 (0.5491) acc 100.0000 (91.8750) lr 1.6845e-03 eta 0:03:57
epoch [15/50] batch [10/26] time 0.071 (0.163) data 0.000 (0.091) loss 0.5449 (0.5611) acc 93.7500 (90.9375) lr 1.6845e-03 eta 0:02:31
epoch [15/50] batch [15/26] time 0.070 (0.133) data 0.000 (0.061) loss 0.5732 (0.5397) acc 96.8750 (92.0833) lr 1.6845e-03 eta 0:02:02
epoch [15/50] batch [20/26] time 0.071 (0.117) data 0.000 (0.046) loss 0.5801 (0.5118) acc 87.5000 (92.6562) lr 1.6845e-03 eta 0:01:47
epoch [15/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.7446 (0.5426) acc 84.3750 (91.2500) lr 1.6845e-03 eta 0:01:38
epoch [16/50] batch [5/26] time 0.072 (0.243) data 0.000 (0.172) loss 0.3560 (0.4402) acc 96.8750 (96.2500) lr 1.6374e-03 eta 0:03:39
epoch [16/50] batch [10/26] time 0.073 (0.157) data 0.000 (0.086) loss 0.5586 (0.4425) acc 90.6250 (96.2500) lr 1.6374e-03 eta 0:02:21
epoch [16/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.4951 (0.4811) acc 90.6250 (93.5417) lr 1.6374e-03 eta 0:01:54
epoch [16/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.6753 (0.4780) acc 81.2500 (93.4375) lr 1.6374e-03 eta 0:01:41
epoch [16/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.5205 (0.4762) acc 93.7500 (93.7500) lr 1.6374e-03 eta 0:01:32
epoch [17/50] batch [5/26] time 0.070 (0.254) data 0.000 (0.182) loss 0.6860 (0.4880) acc 90.6250 (93.7500) lr 1.5878e-03 eta 0:03:43
epoch [17/50] batch [10/26] time 0.071 (0.162) data 0.000 (0.091) loss 0.6377 (0.4662) acc 90.6250 (94.0625) lr 1.5878e-03 eta 0:02:21
epoch [17/50] batch [15/26] time 0.071 (0.131) data 0.000 (0.061) loss 0.3572 (0.4930) acc 100.0000 (93.9583) lr 1.5878e-03 eta 0:01:54
epoch [17/50] batch [20/26] time 0.072 (0.117) data 0.000 (0.046) loss 0.5371 (0.4817) acc 90.6250 (94.0625) lr 1.5878e-03 eta 0:01:40
epoch [17/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.4731 (0.4862) acc 93.7500 (93.2500) lr 1.5878e-03 eta 0:01:32
epoch [18/50] batch [5/26] time 0.075 (0.249) data 0.000 (0.174) loss 0.4954 (0.4659) acc 90.6250 (94.3750) lr 1.5358e-03 eta 0:03:32
epoch [18/50] batch [10/26] time 0.071 (0.160) data 0.000 (0.087) loss 0.3730 (0.4747) acc 96.8750 (93.7500) lr 1.5358e-03 eta 0:02:15
epoch [18/50] batch [15/26] time 0.071 (0.130) data 0.000 (0.058) loss 0.4685 (0.4562) acc 93.7500 (95.0000) lr 1.5358e-03 eta 0:01:49
epoch [18/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.5527 (0.4584) acc 90.6250 (95.0000) lr 1.5358e-03 eta 0:01:36
epoch [18/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.5596 (0.4685) acc 90.6250 (94.0000) lr 1.5358e-03 eta 0:01:28
epoch [19/50] batch [5/26] time 0.072 (0.249) data 0.000 (0.177) loss 0.6250 (0.4596) acc 87.5000 (93.1250) lr 1.4818e-03 eta 0:03:25
epoch [19/50] batch [10/26] time 0.071 (0.160) data 0.000 (0.089) loss 0.5449 (0.4733) acc 90.6250 (92.8125) lr 1.4818e-03 eta 0:02:11
epoch [19/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.059) loss 0.4580 (0.4806) acc 93.7500 (93.5417) lr 1.4818e-03 eta 0:01:46
epoch [19/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.3015 (0.4675) acc 100.0000 (94.3750) lr 1.4818e-03 eta 0:01:33
epoch [19/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.036) loss 0.4763 (0.4684) acc 90.6250 (93.7500) lr 1.4818e-03 eta 0:01:25
epoch [20/50] batch [5/26] time 0.071 (0.242) data 0.000 (0.171) loss 0.3149 (0.4072) acc 100.0000 (96.8750) lr 1.4258e-03 eta 0:03:14
epoch [20/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.086) loss 0.3457 (0.4102) acc 96.8750 (96.2500) lr 1.4258e-03 eta 0:02:04
epoch [20/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.4136 (0.4111) acc 93.7500 (95.4167) lr 1.4258e-03 eta 0:01:41
epoch [20/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.043) loss 0.6318 (0.4404) acc 84.3750 (94.3750) lr 1.4258e-03 eta 0:01:29
epoch [20/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.4082 (0.4461) acc 96.8750 (94.3750) lr 1.4258e-03 eta 0:01:21
epoch [21/50] batch [5/26] time 0.072 (0.235) data 0.000 (0.163) loss 0.7500 (0.4903) acc 84.3750 (94.3750) lr 1.3681e-03 eta 0:03:02
epoch [21/50] batch [10/26] time 0.071 (0.154) data 0.001 (0.082) loss 0.5469 (0.4942) acc 87.5000 (92.8125) lr 1.3681e-03 eta 0:01:58
epoch [21/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.4927 (0.4985) acc 93.7500 (92.9167) lr 1.3681e-03 eta 0:01:36
epoch [21/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.4343 (0.5025) acc 90.6250 (91.8750) lr 1.3681e-03 eta 0:01:25
epoch [21/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.5024 (0.5005) acc 93.7500 (91.7500) lr 1.3681e-03 eta 0:01:18
epoch [22/50] batch [5/26] time 0.071 (0.243) data 0.000 (0.171) loss 0.5063 (0.3932) acc 93.7500 (96.8750) lr 1.3090e-03 eta 0:03:02
epoch [22/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.086) loss 0.6172 (0.4433) acc 87.5000 (95.3125) lr 1.3090e-03 eta 0:01:57
epoch [22/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.4165 (0.4430) acc 96.8750 (95.4167) lr 1.3090e-03 eta 0:01:34
epoch [22/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.4353 (0.4474) acc 93.7500 (95.0000) lr 1.3090e-03 eta 0:01:23
epoch [22/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.4827 (0.4500) acc 93.7500 (94.8750) lr 1.3090e-03 eta 0:01:16
epoch [23/50] batch [5/26] time 0.071 (0.243) data 0.000 (0.169) loss 0.4180 (0.3742) acc 93.7500 (95.6250) lr 1.2487e-03 eta 0:02:55
epoch [23/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.085) loss 0.4011 (0.4409) acc 96.8750 (94.3750) lr 1.2487e-03 eta 0:01:52
epoch [23/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.057) loss 0.5859 (0.4413) acc 90.6250 (94.5833) lr 1.2487e-03 eta 0:01:31
epoch [23/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.3833 (0.4494) acc 93.7500 (94.5312) lr 1.2487e-03 eta 0:01:20
epoch [23/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.5547 (0.4534) acc 87.5000 (94.0000) lr 1.2487e-03 eta 0:01:14
epoch [24/50] batch [5/26] time 0.071 (0.245) data 0.000 (0.173) loss 0.7354 (0.5856) acc 81.2500 (88.1250) lr 1.1874e-03 eta 0:02:50
epoch [24/50] batch [10/26] time 0.074 (0.158) data 0.000 (0.087) loss 0.4094 (0.5168) acc 93.7500 (90.0000) lr 1.1874e-03 eta 0:01:49
epoch [24/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.5171 (0.5083) acc 93.7500 (91.0417) lr 1.1874e-03 eta 0:01:28
epoch [24/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.7749 (0.5264) acc 87.5000 (90.9375) lr 1.1874e-03 eta 0:01:18
epoch [24/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.4241 (0.5070) acc 93.7500 (91.5000) lr 1.1874e-03 eta 0:01:11
epoch [25/50] batch [5/26] time 0.072 (0.240) data 0.000 (0.167) loss 0.4646 (0.4750) acc 90.6250 (93.1250) lr 1.1253e-03 eta 0:02:40
epoch [25/50] batch [10/26] time 0.072 (0.156) data 0.000 (0.084) loss 0.6396 (0.4634) acc 84.3750 (93.7500) lr 1.1253e-03 eta 0:01:43
epoch [25/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.056) loss 0.5737 (0.4786) acc 87.5000 (92.9167) lr 1.1253e-03 eta 0:01:24
epoch [25/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.4375 (0.4649) acc 93.7500 (93.1250) lr 1.1253e-03 eta 0:01:14
epoch [25/50] batch [25/26] time 0.073 (0.105) data 0.000 (0.034) loss 0.4946 (0.4586) acc 93.7500 (93.2500) lr 1.1253e-03 eta 0:01:08
epoch [26/50] batch [5/26] time 0.072 (0.291) data 0.000 (0.219) loss 0.5605 (0.4265) acc 87.5000 (94.3750) lr 1.0628e-03 eta 0:03:07
epoch [26/50] batch [10/26] time 0.071 (0.182) data 0.000 (0.109) loss 0.3970 (0.4064) acc 93.7500 (95.9375) lr 1.0628e-03 eta 0:01:56
epoch [26/50] batch [15/26] time 0.074 (0.145) data 0.000 (0.073) loss 0.3667 (0.4172) acc 96.8750 (95.4167) lr 1.0628e-03 eta 0:01:32
epoch [26/50] batch [20/26] time 0.071 (0.127) data 0.000 (0.055) loss 0.4702 (0.4210) acc 90.6250 (95.0000) lr 1.0628e-03 eta 0:01:20
epoch [26/50] batch [25/26] time 0.070 (0.116) data 0.000 (0.044) loss 0.5322 (0.4204) acc 87.5000 (95.0000) lr 1.0628e-03 eta 0:01:12
epoch [27/50] batch [5/26] time 0.075 (0.230) data 0.000 (0.154) loss 0.5059 (0.4538) acc 90.6250 (92.5000) lr 1.0000e-03 eta 0:02:22
epoch [27/50] batch [10/26] time 0.074 (0.151) data 0.000 (0.077) loss 0.4775 (0.4540) acc 90.6250 (92.5000) lr 1.0000e-03 eta 0:01:32
epoch [27/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.7720 (0.4501) acc 81.2500 (92.5000) lr 1.0000e-03 eta 0:01:15
epoch [27/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.4006 (0.4540) acc 96.8750 (92.6562) lr 1.0000e-03 eta 0:01:06
epoch [27/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.4258 (0.4508) acc 96.8750 (93.2500) lr 1.0000e-03 eta 0:01:01
epoch [28/50] batch [5/26] time 0.071 (0.243) data 0.000 (0.171) loss 0.3428 (0.3740) acc 96.8750 (97.5000) lr 9.3721e-04 eta 0:02:24
epoch [28/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.086) loss 0.4292 (0.4175) acc 96.8750 (95.3125) lr 9.3721e-04 eta 0:01:32
epoch [28/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.057) loss 0.4277 (0.4440) acc 96.8750 (93.7500) lr 9.3721e-04 eta 0:01:15
epoch [28/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.6587 (0.4492) acc 90.6250 (93.9062) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.034) loss 0.6064 (0.4435) acc 84.3750 (93.7500) lr 9.3721e-04 eta 0:01:00
epoch [29/50] batch [5/26] time 0.071 (0.222) data 0.000 (0.151) loss 0.2605 (0.4147) acc 100.0000 (95.0000) lr 8.7467e-04 eta 0:02:06
epoch [29/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.076) loss 0.7236 (0.4486) acc 90.6250 (94.6875) lr 8.7467e-04 eta 0:01:22
epoch [29/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.050) loss 0.6968 (0.4510) acc 81.2500 (93.7500) lr 8.7467e-04 eta 0:01:07
epoch [29/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.4465 (0.4308) acc 90.6250 (94.5312) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.3857 (0.4258) acc 93.7500 (94.5000) lr 8.7467e-04 eta 0:00:55
epoch [30/50] batch [5/26] time 0.074 (0.227) data 0.000 (0.150) loss 0.4287 (0.4259) acc 96.8750 (94.3750) lr 8.1262e-04 eta 0:02:02
epoch [30/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.075) loss 0.3262 (0.4144) acc 96.8750 (95.0000) lr 8.1262e-04 eta 0:01:19
epoch [30/50] batch [15/26] time 0.072 (0.123) data 0.000 (0.050) loss 0.4712 (0.4289) acc 90.6250 (93.5417) lr 8.1262e-04 eta 0:01:05
epoch [30/50] batch [20/26] time 0.072 (0.110) data 0.000 (0.038) loss 0.6211 (0.4456) acc 90.6250 (93.4375) lr 8.1262e-04 eta 0:00:57
epoch [30/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.030) loss 0.6328 (0.4479) acc 90.6250 (93.5000) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/26] time 0.074 (0.221) data 0.000 (0.146) loss 0.2732 (0.3576) acc 100.0000 (97.5000) lr 7.5131e-04 eta 0:01:53
epoch [31/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.073) loss 0.2466 (0.3627) acc 100.0000 (97.1875) lr 7.5131e-04 eta 0:01:14
epoch [31/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.3389 (0.3759) acc 96.8750 (96.4583) lr 7.5131e-04 eta 0:01:01
epoch [31/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.2937 (0.3743) acc 100.0000 (96.2500) lr 7.5131e-04 eta 0:00:54
epoch [31/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.029) loss 0.4629 (0.3887) acc 96.8750 (96.0000) lr 7.5131e-04 eta 0:00:49
epoch [32/50] batch [5/26] time 0.075 (0.224) data 0.000 (0.151) loss 0.5151 (0.4198) acc 90.6250 (94.3750) lr 6.9098e-04 eta 0:01:49
epoch [32/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.075) loss 0.6758 (0.4386) acc 87.5000 (92.8125) lr 6.9098e-04 eta 0:01:11
epoch [32/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.050) loss 0.3330 (0.4365) acc 100.0000 (92.9167) lr 6.9098e-04 eta 0:00:58
epoch [32/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.038) loss 0.3428 (0.4177) acc 96.8750 (93.7500) lr 6.9098e-04 eta 0:00:51
epoch [32/50] batch [25/26] time 0.072 (0.102) data 0.000 (0.030) loss 0.7178 (0.4368) acc 84.3750 (93.3750) lr 6.9098e-04 eta 0:00:47
epoch [33/50] batch [5/26] time 0.071 (0.223) data 0.000 (0.151) loss 0.2959 (0.4096) acc 96.8750 (94.3750) lr 6.3188e-04 eta 0:01:43
epoch [33/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.3831 (0.3993) acc 96.8750 (95.6250) lr 6.3188e-04 eta 0:01:07
epoch [33/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.3379 (0.4064) acc 100.0000 (95.4167) lr 6.3188e-04 eta 0:00:55
epoch [33/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.6162 (0.4307) acc 90.6250 (94.3750) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.3120 (0.4198) acc 96.8750 (94.8750) lr 6.3188e-04 eta 0:00:44
epoch [34/50] batch [5/26] time 0.076 (0.232) data 0.000 (0.160) loss 0.4109 (0.4769) acc 93.7500 (92.5000) lr 5.7422e-04 eta 0:01:41
epoch [34/50] batch [10/26] time 0.071 (0.152) data 0.000 (0.080) loss 0.3103 (0.4356) acc 96.8750 (92.5000) lr 5.7422e-04 eta 0:01:05
epoch [34/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.053) loss 0.5410 (0.4304) acc 90.6250 (92.7083) lr 5.7422e-04 eta 0:00:53
epoch [34/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.4512 (0.4315) acc 93.7500 (93.2812) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.4001 (0.4303) acc 96.8750 (93.5000) lr 5.7422e-04 eta 0:00:42
epoch [35/50] batch [5/26] time 0.073 (0.231) data 0.000 (0.156) loss 0.3848 (0.3828) acc 93.7500 (97.5000) lr 5.1825e-04 eta 0:01:34
epoch [35/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.078) loss 0.5146 (0.3915) acc 90.6250 (95.9375) lr 5.1825e-04 eta 0:01:01
epoch [35/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.3555 (0.4037) acc 93.7500 (95.4167) lr 5.1825e-04 eta 0:00:49
epoch [35/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.4785 (0.4085) acc 96.8750 (95.6250) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.3997 (0.4154) acc 100.0000 (95.5000) lr 5.1825e-04 eta 0:00:40
epoch [36/50] batch [5/26] time 0.070 (0.220) data 0.000 (0.148) loss 0.3579 (0.3764) acc 93.7500 (95.0000) lr 4.6417e-04 eta 0:01:24
epoch [36/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.074) loss 0.4287 (0.3915) acc 90.6250 (95.3125) lr 4.6417e-04 eta 0:00:55
epoch [36/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.5107 (0.4106) acc 90.6250 (94.5833) lr 4.6417e-04 eta 0:00:45
epoch [36/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.3508 (0.4165) acc 96.8750 (94.2188) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.3308 (0.4237) acc 100.0000 (94.2500) lr 4.6417e-04 eta 0:00:36
epoch [37/50] batch [5/26] time 0.072 (0.226) data 0.000 (0.150) loss 0.4041 (0.3609) acc 96.8750 (97.5000) lr 4.1221e-04 eta 0:01:21
epoch [37/50] batch [10/26] time 0.072 (0.149) data 0.001 (0.075) loss 0.3862 (0.3930) acc 96.8750 (95.9375) lr 4.1221e-04 eta 0:00:52
epoch [37/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.050) loss 0.3823 (0.3985) acc 93.7500 (95.6250) lr 4.1221e-04 eta 0:00:43
epoch [37/50] batch [20/26] time 0.072 (0.111) data 0.000 (0.038) loss 0.3726 (0.4112) acc 96.8750 (94.8438) lr 4.1221e-04 eta 0:00:38
epoch [37/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.030) loss 0.4192 (0.4038) acc 93.7500 (95.1250) lr 4.1221e-04 eta 0:00:34
epoch [38/50] batch [5/26] time 0.073 (0.247) data 0.000 (0.174) loss 0.4749 (0.3700) acc 87.5000 (93.1250) lr 3.6258e-04 eta 0:01:22
epoch [38/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.087) loss 0.3799 (0.3756) acc 93.7500 (94.3750) lr 3.6258e-04 eta 0:00:52
epoch [38/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.058) loss 0.3564 (0.3705) acc 90.6250 (94.5833) lr 3.6258e-04 eta 0:00:41
epoch [38/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.2773 (0.3671) acc 100.0000 (95.0000) lr 3.6258e-04 eta 0:00:36
epoch [38/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.2637 (0.3785) acc 100.0000 (94.6250) lr 3.6258e-04 eta 0:00:33
epoch [39/50] batch [5/26] time 0.073 (0.242) data 0.000 (0.169) loss 0.3276 (0.3227) acc 96.8750 (97.5000) lr 3.1545e-04 eta 0:01:14
epoch [39/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.4351 (0.3412) acc 93.7500 (96.2500) lr 3.1545e-04 eta 0:00:47
epoch [39/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.057) loss 0.4207 (0.3541) acc 96.8750 (96.2500) lr 3.1545e-04 eta 0:00:37
epoch [39/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.043) loss 0.5469 (0.3680) acc 90.6250 (95.9375) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4214 (0.3754) acc 96.8750 (95.8750) lr 3.1545e-04 eta 0:00:30
epoch [40/50] batch [5/26] time 0.071 (0.236) data 0.000 (0.164) loss 0.2910 (0.3607) acc 100.0000 (96.2500) lr 2.7103e-04 eta 0:01:06
epoch [40/50] batch [10/26] time 0.075 (0.154) data 0.001 (0.082) loss 0.4561 (0.3943) acc 93.7500 (95.0000) lr 2.7103e-04 eta 0:00:42
epoch [40/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.055) loss 0.4319 (0.4020) acc 96.8750 (95.2083) lr 2.7103e-04 eta 0:00:34
epoch [40/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.041) loss 0.3826 (0.3846) acc 96.8750 (95.7812) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [25/26] time 0.072 (0.105) data 0.000 (0.033) loss 0.3167 (0.3856) acc 100.0000 (95.5000) lr 2.7103e-04 eta 0:00:27
epoch [41/50] batch [5/26] time 0.071 (0.242) data 0.000 (0.170) loss 0.2764 (0.4415) acc 100.0000 (93.1250) lr 2.2949e-04 eta 0:01:01
epoch [41/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.085) loss 0.5571 (0.4519) acc 93.7500 (93.4375) lr 2.2949e-04 eta 0:00:39
epoch [41/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.3125 (0.4248) acc 96.8750 (94.5833) lr 2.2949e-04 eta 0:00:31
epoch [41/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.4004 (0.4193) acc 93.7500 (94.6875) lr 2.2949e-04 eta 0:00:27
epoch [41/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.3447 (0.4106) acc 100.0000 (95.2500) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/26] time 0.070 (0.225) data 0.000 (0.152) loss 0.3735 (0.3471) acc 93.7500 (96.8750) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [10/26] time 0.072 (0.148) data 0.000 (0.076) loss 0.3386 (0.3759) acc 96.8750 (95.9375) lr 1.9098e-04 eta 0:00:33
epoch [42/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.4338 (0.3663) acc 81.2500 (95.8333) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4827 (0.3896) acc 87.5000 (95.3125) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.4048 (0.3980) acc 93.7500 (95.1250) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.071 (0.224) data 0.000 (0.152) loss 0.4229 (0.3738) acc 96.8750 (96.2500) lr 1.5567e-04 eta 0:00:45
epoch [43/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.3008 (0.3632) acc 96.8750 (96.5625) lr 1.5567e-04 eta 0:00:29
epoch [43/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.3340 (0.3691) acc 96.8750 (95.8333) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [20/26] time 0.072 (0.109) data 0.000 (0.038) loss 0.4258 (0.3741) acc 100.0000 (96.2500) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.031) loss 0.4705 (0.3840) acc 93.7500 (95.8750) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.072 (0.225) data 0.000 (0.153) loss 0.2910 (0.3729) acc 100.0000 (96.8750) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.077) loss 0.2866 (0.3782) acc 100.0000 (96.2500) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.4377 (0.3864) acc 96.8750 (96.0417) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4912 (0.3897) acc 93.7500 (96.2500) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.031) loss 0.3333 (0.3941) acc 100.0000 (96.2500) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/26] time 0.070 (0.233) data 0.001 (0.159) loss 0.3682 (0.3778) acc 100.0000 (96.8750) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [10/26] time 0.070 (0.153) data 0.000 (0.079) loss 0.3159 (0.3523) acc 100.0000 (97.8125) lr 9.5173e-05 eta 0:00:22
epoch [45/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.053) loss 0.3081 (0.3507) acc 96.8750 (97.2917) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.3623 (0.3571) acc 96.8750 (96.5625) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.2974 (0.3720) acc 100.0000 (96.1250) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.072 (0.225) data 0.000 (0.149) loss 0.3882 (0.4094) acc 96.8750 (95.0000) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.077) loss 0.3369 (0.3734) acc 100.0000 (96.5625) lr 7.0224e-05 eta 0:00:18
epoch [46/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.051) loss 0.3335 (0.3681) acc 96.8750 (96.6667) lr 7.0224e-05 eta 0:00:14
epoch [46/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.4111 (0.3718) acc 96.8750 (96.4062) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.3140 (0.3691) acc 96.8750 (96.3750) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.076 (0.228) data 0.000 (0.154) loss 0.3728 (0.3308) acc 96.8750 (97.5000) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.077) loss 0.2937 (0.3282) acc 96.8750 (96.5625) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.5381 (0.3601) acc 90.6250 (95.2083) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.4597 (0.3655) acc 96.8750 (95.3125) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.4009 (0.3694) acc 93.7500 (95.1250) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.149) loss 0.4072 (0.3417) acc 93.7500 (96.8750) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.072 (0.147) data 0.000 (0.075) loss 0.2832 (0.3696) acc 100.0000 (96.2500) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.050) loss 0.4233 (0.3816) acc 93.7500 (95.2083) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.037) loss 0.5391 (0.3879) acc 93.7500 (95.1562) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.6504 (0.3999) acc 90.6250 (95.0000) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.073 (0.232) data 0.000 (0.156) loss 0.3677 (0.3568) acc 93.7500 (96.2500) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.072 (0.152) data 0.001 (0.078) loss 0.2661 (0.3297) acc 100.0000 (97.5000) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.052) loss 0.4612 (0.3539) acc 93.7500 (96.6667) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.039) loss 0.2771 (0.3568) acc 100.0000 (96.5625) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.031) loss 0.5137 (0.3706) acc 93.7500 (96.1250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.075 (0.237) data 0.000 (0.164) loss 0.4685 (0.3967) acc 93.7500 (96.2500) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.073 (0.154) data 0.000 (0.082) loss 0.4624 (0.3829) acc 93.7500 (96.5625) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.3042 (0.3859) acc 96.8750 (96.4583) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.3857 (0.3886) acc 96.8750 (96.2500) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.3545 (0.3802) acc 96.8750 (96.5000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:10,  5.50s/it] 67%|██████▋   | 2/3 [00:06<00:02,  2.60s/it]100%|██████████| 3/3 [00:06<00:00,  1.46s/it]100%|██████████| 3/3 [00:06<00:00,  2.10s/it]
=> result
* total: 1,053
* correct: 1,028
* accuracy: 97.6%
* error: 2.4%
* macro_f1: 97.7%
Elapsed: 0:02:26
Run this job and save the output to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.073 (0.411) data 0.000 (0.321) loss 3.0898 (2.9859) acc 62.5000 (66.8750) lr 1.0000e-05 eta 0:08:52
epoch [1/50] batch [10/26] time 0.070 (0.241) data 0.001 (0.161) loss 3.4355 (3.0873) acc 37.5000 (59.6875) lr 1.0000e-05 eta 0:05:11
epoch [1/50] batch [15/26] time 0.257 (0.197) data 0.187 (0.120) loss 2.8477 (3.0879) acc 65.6250 (58.3333) lr 1.0000e-05 eta 0:04:13
epoch [1/50] batch [20/26] time 0.069 (0.274) data 0.000 (0.198) loss 2.8047 (3.0485) acc 62.5000 (58.2812) lr 1.0000e-05 eta 0:05:50
epoch [1/50] batch [25/26] time 2.506 (0.349) data 2.432 (0.274) loss 3.1543 (3.0330) acc 62.5000 (58.1250) lr 1.0000e-05 eta 0:07:25
epoch [2/50] batch [5/26] time 0.072 (0.334) data 0.001 (0.252) loss 1.7295 (2.1355) acc 53.1250 (62.5000) lr 2.0000e-03 eta 0:07:03
epoch [2/50] batch [10/26] time 0.070 (0.202) data 0.000 (0.126) loss 1.4355 (1.9210) acc 62.5000 (62.8125) lr 2.0000e-03 eta 0:04:15
epoch [2/50] batch [15/26] time 0.069 (0.158) data 0.000 (0.084) loss 1.4883 (1.8340) acc 65.6250 (62.2917) lr 2.0000e-03 eta 0:03:19
epoch [2/50] batch [20/26] time 0.070 (0.136) data 0.000 (0.063) loss 1.7725 (1.7491) acc 56.2500 (63.4375) lr 2.0000e-03 eta 0:02:50
epoch [2/50] batch [25/26] time 0.070 (0.123) data 0.000 (0.051) loss 1.8066 (1.6759) acc 53.1250 (64.5000) lr 2.0000e-03 eta 0:02:33
epoch [3/50] batch [5/26] time 0.072 (0.300) data 0.000 (0.229) loss 1.2148 (1.1909) acc 71.8750 (69.3750) lr 1.9980e-03 eta 0:06:12
epoch [3/50] batch [10/26] time 0.070 (0.185) data 0.000 (0.115) loss 1.5195 (1.2123) acc 65.6250 (70.3125) lr 1.9980e-03 eta 0:03:48
epoch [3/50] batch [15/26] time 0.070 (0.147) data 0.000 (0.077) loss 0.8774 (1.1743) acc 78.1250 (71.4583) lr 1.9980e-03 eta 0:03:01
epoch [3/50] batch [20/26] time 0.070 (0.128) data 0.000 (0.057) loss 1.3242 (1.1882) acc 78.1250 (71.5625) lr 1.9980e-03 eta 0:02:36
epoch [3/50] batch [25/26] time 0.070 (0.116) data 0.000 (0.046) loss 0.7070 (1.1767) acc 93.7500 (72.0000) lr 1.9980e-03 eta 0:02:22
epoch [4/50] batch [5/26] time 0.073 (0.319) data 0.000 (0.247) loss 0.7773 (1.0018) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:06:27
epoch [4/50] batch [10/26] time 0.070 (0.195) data 0.000 (0.124) loss 0.7930 (0.9836) acc 87.5000 (77.1875) lr 1.9921e-03 eta 0:03:55
epoch [4/50] batch [15/26] time 0.070 (0.153) data 0.000 (0.082) loss 0.8911 (0.9313) acc 84.3750 (80.2083) lr 1.9921e-03 eta 0:03:04
epoch [4/50] batch [20/26] time 0.070 (0.132) data 0.000 (0.062) loss 0.9077 (0.9004) acc 78.1250 (81.0938) lr 1.9921e-03 eta 0:02:38
epoch [4/50] batch [25/26] time 0.070 (0.120) data 0.000 (0.050) loss 0.7783 (0.9149) acc 90.6250 (81.6250) lr 1.9921e-03 eta 0:02:23
epoch [5/50] batch [5/26] time 0.070 (0.331) data 0.000 (0.259) loss 1.0391 (0.9114) acc 75.0000 (84.3750) lr 1.9823e-03 eta 0:06:34
epoch [5/50] batch [10/26] time 0.071 (0.201) data 0.000 (0.130) loss 1.1074 (0.9456) acc 75.0000 (82.5000) lr 1.9823e-03 eta 0:03:58
epoch [5/50] batch [15/26] time 0.070 (0.158) data 0.000 (0.086) loss 0.9961 (0.9281) acc 75.0000 (82.2917) lr 1.9823e-03 eta 0:03:06
epoch [5/50] batch [20/26] time 0.071 (0.136) data 0.000 (0.065) loss 0.6533 (0.8819) acc 81.2500 (83.1250) lr 1.9823e-03 eta 0:02:39
epoch [5/50] batch [25/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.8677 (0.8469) acc 90.6250 (85.0000) lr 1.9823e-03 eta 0:02:23
epoch [6/50] batch [5/26] time 0.074 (0.593) data 0.000 (0.518) loss 0.9873 (0.8322) acc 75.0000 (83.7500) lr 1.9686e-03 eta 0:11:30
epoch [6/50] batch [10/26] time 0.070 (0.332) data 0.000 (0.259) loss 1.0332 (0.8136) acc 78.1250 (85.3125) lr 1.9686e-03 eta 0:06:25
epoch [6/50] batch [15/26] time 0.070 (0.245) data 0.000 (0.173) loss 0.5986 (0.7859) acc 96.8750 (86.6667) lr 1.9686e-03 eta 0:04:42
epoch [6/50] batch [20/26] time 0.070 (0.201) data 0.000 (0.130) loss 0.8477 (0.7648) acc 81.2500 (87.0312) lr 1.9686e-03 eta 0:03:51
epoch [6/50] batch [25/26] time 0.070 (0.175) data 0.000 (0.104) loss 0.6533 (0.7627) acc 93.7500 (86.8750) lr 1.9686e-03 eta 0:03:20
epoch [7/50] batch [5/26] time 0.071 (0.345) data 0.000 (0.273) loss 0.7119 (0.6938) acc 84.3750 (89.3750) lr 1.9511e-03 eta 0:06:32
epoch [7/50] batch [10/26] time 0.071 (0.209) data 0.000 (0.136) loss 0.7705 (0.6901) acc 93.7500 (89.6875) lr 1.9511e-03 eta 0:03:56
epoch [7/50] batch [15/26] time 0.071 (0.163) data 0.000 (0.091) loss 0.6733 (0.7065) acc 93.7500 (88.5417) lr 1.9511e-03 eta 0:03:03
epoch [7/50] batch [20/26] time 0.071 (0.140) data 0.000 (0.068) loss 0.6211 (0.6990) acc 93.7500 (88.2812) lr 1.9511e-03 eta 0:02:37
epoch [7/50] batch [25/26] time 0.071 (0.126) data 0.000 (0.055) loss 0.6929 (0.6931) acc 87.5000 (87.8750) lr 1.9511e-03 eta 0:02:20
epoch [8/50] batch [5/26] time 0.070 (0.815) data 0.000 (0.739) loss 0.6060 (0.6026) acc 87.5000 (91.2500) lr 1.9298e-03 eta 0:15:07
epoch [8/50] batch [10/26] time 0.075 (0.444) data 0.000 (0.370) loss 0.8013 (0.6581) acc 84.3750 (88.4375) lr 1.9298e-03 eta 0:08:11
epoch [8/50] batch [15/26] time 0.070 (0.319) data 0.000 (0.246) loss 0.6455 (0.6512) acc 90.6250 (88.5417) lr 1.9298e-03 eta 0:05:52
epoch [8/50] batch [20/26] time 0.070 (0.257) data 0.000 (0.185) loss 0.4648 (0.6386) acc 96.8750 (89.5312) lr 1.9298e-03 eta 0:04:42
epoch [8/50] batch [25/26] time 0.070 (0.220) data 0.000 (0.148) loss 0.6938 (0.6397) acc 90.6250 (89.7500) lr 1.9298e-03 eta 0:04:00
epoch [9/50] batch [5/26] time 0.072 (0.290) data 0.000 (0.218) loss 0.4844 (0.6099) acc 93.7500 (91.2500) lr 1.9048e-03 eta 0:05:15
epoch [9/50] batch [10/26] time 0.070 (0.181) data 0.000 (0.109) loss 0.7173 (0.6191) acc 84.3750 (91.2500) lr 1.9048e-03 eta 0:03:15
epoch [9/50] batch [15/26] time 0.070 (0.144) data 0.000 (0.073) loss 0.7305 (0.6423) acc 84.3750 (89.7917) lr 1.9048e-03 eta 0:02:34
epoch [9/50] batch [20/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.6426 (0.6283) acc 93.7500 (90.1562) lr 1.9048e-03 eta 0:02:14
epoch [9/50] batch [25/26] time 0.071 (0.114) data 0.000 (0.044) loss 0.6064 (0.6259) acc 87.5000 (90.5000) lr 1.9048e-03 eta 0:02:02
epoch [10/50] batch [5/26] time 0.072 (0.468) data 0.000 (0.394) loss 0.6123 (0.6754) acc 93.7500 (88.1250) lr 1.8763e-03 eta 0:08:16
epoch [10/50] batch [10/26] time 0.071 (0.270) data 0.000 (0.197) loss 0.7065 (0.6413) acc 87.5000 (89.6875) lr 1.8763e-03 eta 0:04:45
epoch [10/50] batch [15/26] time 0.070 (0.203) data 0.000 (0.132) loss 0.6431 (0.6516) acc 93.7500 (90.2083) lr 1.8763e-03 eta 0:03:33
epoch [10/50] batch [20/26] time 0.071 (0.170) data 0.000 (0.099) loss 0.5059 (0.6303) acc 87.5000 (89.2188) lr 1.8763e-03 eta 0:02:57
epoch [10/50] batch [25/26] time 0.070 (0.150) data 0.000 (0.079) loss 0.6123 (0.6445) acc 90.6250 (89.3750) lr 1.8763e-03 eta 0:02:36
epoch [11/50] batch [5/26] time 0.073 (0.267) data 0.000 (0.192) loss 0.6631 (0.7145) acc 90.6250 (87.5000) lr 1.8443e-03 eta 0:04:35
epoch [11/50] batch [10/26] time 0.071 (0.169) data 0.000 (0.096) loss 0.7749 (0.6727) acc 81.2500 (88.1250) lr 1.8443e-03 eta 0:02:54
epoch [11/50] batch [15/26] time 0.070 (0.136) data 0.000 (0.064) loss 0.5586 (0.6066) acc 93.7500 (90.8333) lr 1.8443e-03 eta 0:02:19
epoch [11/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.5107 (0.5980) acc 93.7500 (91.0938) lr 1.8443e-03 eta 0:02:02
epoch [11/50] batch [25/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.5347 (0.5760) acc 93.7500 (91.6250) lr 1.8443e-03 eta 0:01:51
epoch [12/50] batch [5/26] time 0.075 (0.575) data 0.000 (0.499) loss 0.5068 (0.5381) acc 100.0000 (93.7500) lr 1.8090e-03 eta 0:09:39
epoch [12/50] batch [10/26] time 0.070 (0.322) data 0.000 (0.250) loss 0.4753 (0.5274) acc 96.8750 (93.7500) lr 1.8090e-03 eta 0:05:23
epoch [12/50] batch [15/26] time 0.070 (0.238) data 0.000 (0.167) loss 0.7095 (0.5679) acc 84.3750 (92.0833) lr 1.8090e-03 eta 0:03:58
epoch [12/50] batch [20/26] time 0.070 (0.196) data 0.000 (0.125) loss 0.5264 (0.5812) acc 93.7500 (91.8750) lr 1.8090e-03 eta 0:03:15
epoch [12/50] batch [25/26] time 0.070 (0.171) data 0.000 (0.100) loss 0.7759 (0.5939) acc 81.2500 (91.1250) lr 1.8090e-03 eta 0:02:49
epoch [13/50] batch [5/26] time 0.071 (0.254) data 0.000 (0.182) loss 0.4280 (0.5700) acc 96.8750 (91.8750) lr 1.7705e-03 eta 0:04:09
epoch [13/50] batch [10/26] time 0.070 (0.162) data 0.000 (0.091) loss 0.6602 (0.5618) acc 87.5000 (90.9375) lr 1.7705e-03 eta 0:02:38
epoch [13/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.061) loss 0.5000 (0.5633) acc 93.7500 (90.8333) lr 1.7705e-03 eta 0:02:07
epoch [13/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.046) loss 0.7275 (0.5701) acc 90.6250 (91.0938) lr 1.7705e-03 eta 0:01:52
epoch [13/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.7559 (0.5643) acc 78.1250 (91.2500) lr 1.7705e-03 eta 0:01:43
epoch [14/50] batch [5/26] time 0.073 (0.567) data 0.000 (0.493) loss 0.5444 (0.5767) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:09:02
epoch [14/50] batch [10/26] time 0.070 (0.318) data 0.000 (0.247) loss 0.5923 (0.5582) acc 87.5000 (90.6250) lr 1.7290e-03 eta 0:05:03
epoch [14/50] batch [15/26] time 0.070 (0.236) data 0.000 (0.165) loss 0.5771 (0.5226) acc 90.6250 (91.8750) lr 1.7290e-03 eta 0:03:43
epoch [14/50] batch [20/26] time 0.070 (0.194) data 0.000 (0.124) loss 0.5840 (0.5319) acc 90.6250 (91.8750) lr 1.7290e-03 eta 0:03:02
epoch [14/50] batch [25/26] time 0.070 (0.169) data 0.000 (0.099) loss 0.4983 (0.5533) acc 90.6250 (91.2500) lr 1.7290e-03 eta 0:02:38
epoch [15/50] batch [5/26] time 0.072 (0.282) data 0.000 (0.210) loss 0.6372 (0.5292) acc 87.5000 (93.1250) lr 1.6845e-03 eta 0:04:22
epoch [15/50] batch [10/26] time 0.070 (0.176) data 0.000 (0.105) loss 0.5000 (0.5270) acc 90.6250 (92.5000) lr 1.6845e-03 eta 0:02:42
epoch [15/50] batch [15/26] time 0.070 (0.141) data 0.000 (0.070) loss 0.4353 (0.5345) acc 100.0000 (92.7083) lr 1.6845e-03 eta 0:02:09
epoch [15/50] batch [20/26] time 0.070 (0.123) data 0.000 (0.053) loss 0.4924 (0.5431) acc 93.7500 (92.8125) lr 1.6845e-03 eta 0:01:52
epoch [15/50] batch [25/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.6626 (0.5581) acc 84.3750 (92.1250) lr 1.6845e-03 eta 0:01:42
epoch [16/50] batch [5/26] time 0.072 (0.318) data 0.000 (0.246) loss 0.5332 (0.5175) acc 96.8750 (93.7500) lr 1.6374e-03 eta 0:04:48
epoch [16/50] batch [10/26] time 0.071 (0.195) data 0.000 (0.123) loss 0.4160 (0.4947) acc 93.7500 (93.7500) lr 1.6374e-03 eta 0:02:55
epoch [16/50] batch [15/26] time 0.071 (0.153) data 0.000 (0.082) loss 0.3896 (0.4804) acc 96.8750 (94.1667) lr 1.6374e-03 eta 0:02:17
epoch [16/50] batch [20/26] time 0.070 (0.133) data 0.000 (0.062) loss 0.6196 (0.4992) acc 87.5000 (93.2812) lr 1.6374e-03 eta 0:01:58
epoch [16/50] batch [25/26] time 0.074 (0.120) data 0.000 (0.049) loss 0.3357 (0.4953) acc 96.8750 (93.5000) lr 1.6374e-03 eta 0:01:46
epoch [17/50] batch [5/26] time 0.072 (0.276) data 0.000 (0.204) loss 0.5786 (0.5073) acc 87.5000 (95.0000) lr 1.5878e-03 eta 0:04:02
epoch [17/50] batch [10/26] time 0.070 (0.173) data 0.000 (0.102) loss 0.4077 (0.5072) acc 93.7500 (93.7500) lr 1.5878e-03 eta 0:02:31
epoch [17/50] batch [15/26] time 0.070 (0.139) data 0.000 (0.068) loss 0.3794 (0.4861) acc 93.7500 (94.5833) lr 1.5878e-03 eta 0:02:00
epoch [17/50] batch [20/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.5986 (0.4959) acc 84.3750 (93.5938) lr 1.5878e-03 eta 0:01:45
epoch [17/50] batch [25/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.6929 (0.5007) acc 87.5000 (93.3750) lr 1.5878e-03 eta 0:01:35
epoch [18/50] batch [5/26] time 0.070 (0.662) data 0.000 (0.587) loss 0.4355 (0.5114) acc 93.7500 (90.0000) lr 1.5358e-03 eta 0:09:24
epoch [18/50] batch [10/26] time 0.071 (0.367) data 0.000 (0.294) loss 0.5688 (0.4931) acc 93.7500 (92.8125) lr 1.5358e-03 eta 0:05:11
epoch [18/50] batch [15/26] time 0.071 (0.268) data 0.000 (0.196) loss 0.3545 (0.4525) acc 96.8750 (93.9583) lr 1.5358e-03 eta 0:03:46
epoch [18/50] batch [20/26] time 0.071 (0.219) data 0.000 (0.147) loss 0.6211 (0.4952) acc 90.6250 (93.1250) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [25/26] time 0.070 (0.189) data 0.000 (0.118) loss 0.5796 (0.4984) acc 87.5000 (93.0000) lr 1.5358e-03 eta 0:02:37
epoch [19/50] batch [5/26] time 0.071 (0.333) data 0.000 (0.261) loss 0.5068 (0.4097) acc 93.7500 (96.2500) lr 1.4818e-03 eta 0:04:35
epoch [19/50] batch [10/26] time 0.070 (0.202) data 0.000 (0.131) loss 0.4192 (0.4678) acc 96.8750 (93.7500) lr 1.4818e-03 eta 0:02:46
epoch [19/50] batch [15/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.4873 (0.4604) acc 96.8750 (93.7500) lr 1.4818e-03 eta 0:02:09
epoch [19/50] batch [20/26] time 0.069 (0.136) data 0.000 (0.065) loss 0.4946 (0.4691) acc 90.6250 (93.4375) lr 1.4818e-03 eta 0:01:50
epoch [19/50] batch [25/26] time 0.072 (0.123) data 0.000 (0.052) loss 0.4634 (0.4601) acc 96.8750 (93.8750) lr 1.4818e-03 eta 0:01:39
epoch [20/50] batch [5/26] time 0.071 (0.685) data 0.000 (0.609) loss 0.5449 (0.5194) acc 90.6250 (92.5000) lr 1.4258e-03 eta 0:09:08
epoch [20/50] batch [10/26] time 0.074 (0.378) data 0.000 (0.305) loss 0.3877 (0.4754) acc 96.8750 (94.0625) lr 1.4258e-03 eta 0:05:01
epoch [20/50] batch [15/26] time 0.070 (0.276) data 0.000 (0.203) loss 0.5215 (0.4676) acc 93.7500 (93.9583) lr 1.4258e-03 eta 0:03:37
epoch [20/50] batch [20/26] time 0.070 (0.224) data 0.000 (0.152) loss 0.3950 (0.4583) acc 93.7500 (94.2188) lr 1.4258e-03 eta 0:02:56
epoch [20/50] batch [25/26] time 0.070 (0.193) data 0.000 (0.122) loss 0.4802 (0.4587) acc 93.7500 (94.0000) lr 1.4258e-03 eta 0:02:31
epoch [21/50] batch [5/26] time 0.070 (0.309) data 0.000 (0.236) loss 0.4636 (0.4474) acc 90.6250 (93.1250) lr 1.3681e-03 eta 0:03:59
epoch [21/50] batch [10/26] time 0.072 (0.190) data 0.000 (0.118) loss 0.5488 (0.4768) acc 93.7500 (92.8125) lr 1.3681e-03 eta 0:02:26
epoch [21/50] batch [15/26] time 0.070 (0.151) data 0.000 (0.079) loss 0.4448 (0.4946) acc 96.8750 (92.9167) lr 1.3681e-03 eta 0:01:55
epoch [21/50] batch [20/26] time 0.071 (0.131) data 0.000 (0.059) loss 0.5210 (0.4881) acc 90.6250 (93.2812) lr 1.3681e-03 eta 0:01:39
epoch [21/50] batch [25/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.5186 (0.4830) acc 90.6250 (93.2500) lr 1.3681e-03 eta 0:01:29
epoch [22/50] batch [5/26] time 0.070 (0.514) data 0.000 (0.439) loss 0.4282 (0.4258) acc 93.7500 (95.0000) lr 1.3090e-03 eta 0:06:25
epoch [22/50] batch [10/26] time 0.070 (0.292) data 0.000 (0.220) loss 0.6147 (0.4827) acc 87.5000 (93.1250) lr 1.3090e-03 eta 0:03:37
epoch [22/50] batch [15/26] time 0.070 (0.218) data 0.000 (0.147) loss 0.5127 (0.4713) acc 96.8750 (94.3750) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [20/26] time 0.070 (0.181) data 0.000 (0.110) loss 0.4062 (0.4715) acc 96.8750 (94.5312) lr 1.3090e-03 eta 0:02:12
epoch [22/50] batch [25/26] time 0.070 (0.159) data 0.000 (0.088) loss 0.3286 (0.4691) acc 96.8750 (94.3750) lr 1.3090e-03 eta 0:01:55
epoch [23/50] batch [5/26] time 0.070 (0.327) data 0.000 (0.255) loss 0.6553 (0.4945) acc 93.7500 (93.1250) lr 1.2487e-03 eta 0:03:56
epoch [23/50] batch [10/26] time 0.070 (0.199) data 0.000 (0.128) loss 0.4395 (0.4521) acc 90.6250 (93.4375) lr 1.2487e-03 eta 0:02:22
epoch [23/50] batch [15/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.4255 (0.4437) acc 93.7500 (93.7500) lr 1.2487e-03 eta 0:01:51
epoch [23/50] batch [20/26] time 0.070 (0.134) data 0.000 (0.064) loss 0.3203 (0.4352) acc 100.0000 (93.4375) lr 1.2487e-03 eta 0:01:35
epoch [23/50] batch [25/26] time 0.071 (0.121) data 0.000 (0.051) loss 0.5146 (0.4445) acc 90.6250 (93.2500) lr 1.2487e-03 eta 0:01:25
epoch [24/50] batch [5/26] time 0.071 (0.557) data 0.000 (0.482) loss 0.4092 (0.4636) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:06:28
epoch [24/50] batch [10/26] time 0.070 (0.314) data 0.000 (0.241) loss 0.3621 (0.4328) acc 100.0000 (95.0000) lr 1.1874e-03 eta 0:03:37
epoch [24/50] batch [15/26] time 0.070 (0.233) data 0.000 (0.161) loss 0.4426 (0.4332) acc 96.8750 (95.6250) lr 1.1874e-03 eta 0:02:39
epoch [24/50] batch [20/26] time 0.070 (0.192) data 0.000 (0.121) loss 0.4194 (0.4498) acc 96.8750 (95.1562) lr 1.1874e-03 eta 0:02:10
epoch [24/50] batch [25/26] time 0.071 (0.168) data 0.000 (0.097) loss 0.6582 (0.4626) acc 87.5000 (94.7500) lr 1.1874e-03 eta 0:01:53
epoch [25/50] batch [5/26] time 0.071 (0.279) data 0.000 (0.205) loss 0.5010 (0.5167) acc 90.6250 (91.2500) lr 1.1253e-03 eta 0:03:07
epoch [25/50] batch [10/26] time 0.071 (0.176) data 0.000 (0.103) loss 0.4053 (0.5030) acc 93.7500 (92.5000) lr 1.1253e-03 eta 0:01:57
epoch [25/50] batch [15/26] time 0.071 (0.141) data 0.000 (0.069) loss 0.5381 (0.4953) acc 90.6250 (93.1250) lr 1.1253e-03 eta 0:01:33
epoch [25/50] batch [20/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.5977 (0.4927) acc 84.3750 (92.6562) lr 1.1253e-03 eta 0:01:20
epoch [25/50] batch [25/26] time 0.071 (0.113) data 0.000 (0.041) loss 0.5537 (0.5205) acc 87.5000 (91.6250) lr 1.1253e-03 eta 0:01:13
epoch [26/50] batch [5/26] time 0.073 (0.444) data 0.000 (0.370) loss 0.4565 (0.4383) acc 96.8750 (95.6250) lr 1.0628e-03 eta 0:04:46
epoch [26/50] batch [10/26] time 0.071 (0.257) data 0.001 (0.185) loss 0.3184 (0.4152) acc 100.0000 (96.2500) lr 1.0628e-03 eta 0:02:44
epoch [26/50] batch [15/26] time 0.070 (0.195) data 0.000 (0.124) loss 0.4363 (0.4356) acc 96.8750 (95.2083) lr 1.0628e-03 eta 0:02:03
epoch [26/50] batch [20/26] time 0.070 (0.164) data 0.000 (0.093) loss 0.4482 (0.4550) acc 96.8750 (94.6875) lr 1.0628e-03 eta 0:01:43
epoch [26/50] batch [25/26] time 0.070 (0.145) data 0.000 (0.074) loss 0.5000 (0.4654) acc 93.7500 (94.0000) lr 1.0628e-03 eta 0:01:30
epoch [27/50] batch [5/26] time 0.071 (0.246) data 0.000 (0.174) loss 0.4570 (0.3937) acc 93.7500 (95.6250) lr 1.0000e-03 eta 0:02:32
epoch [27/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.4951 (0.4279) acc 93.7500 (94.6875) lr 1.0000e-03 eta 0:01:37
epoch [27/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.058) loss 0.4749 (0.4217) acc 90.6250 (94.3750) lr 1.0000e-03 eta 0:01:18
epoch [27/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.044) loss 0.4141 (0.4365) acc 93.7500 (94.2188) lr 1.0000e-03 eta 0:01:09
epoch [27/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.7002 (0.4487) acc 84.3750 (93.7500) lr 1.0000e-03 eta 0:01:03
epoch [28/50] batch [5/26] time 0.071 (0.265) data 0.001 (0.192) loss 0.3843 (0.3842) acc 96.8750 (97.5000) lr 9.3721e-04 eta 0:02:37
epoch [28/50] batch [10/26] time 0.076 (0.169) data 0.000 (0.096) loss 0.6123 (0.4676) acc 87.5000 (93.4375) lr 9.3721e-04 eta 0:01:39
epoch [28/50] batch [15/26] time 0.073 (0.136) data 0.000 (0.064) loss 0.5659 (0.4938) acc 93.7500 (93.3333) lr 9.3721e-04 eta 0:01:19
epoch [28/50] batch [20/26] time 0.071 (0.120) data 0.000 (0.048) loss 0.2949 (0.4880) acc 100.0000 (93.9062) lr 9.3721e-04 eta 0:01:09
epoch [28/50] batch [25/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.5762 (0.4773) acc 87.5000 (93.8750) lr 9.3721e-04 eta 0:01:03
epoch [29/50] batch [5/26] time 0.072 (0.247) data 0.000 (0.175) loss 0.4485 (0.4398) acc 90.6250 (94.3750) lr 8.7467e-04 eta 0:02:20
epoch [29/50] batch [10/26] time 0.071 (0.159) data 0.000 (0.088) loss 0.5664 (0.4523) acc 90.6250 (94.6875) lr 8.7467e-04 eta 0:01:29
epoch [29/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.4185 (0.4350) acc 100.0000 (95.4167) lr 8.7467e-04 eta 0:01:12
epoch [29/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.5415 (0.4527) acc 90.6250 (94.2188) lr 8.7467e-04 eta 0:01:03
epoch [29/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.4429 (0.4487) acc 93.7500 (94.1250) lr 8.7467e-04 eta 0:00:57
epoch [30/50] batch [5/26] time 0.072 (0.256) data 0.000 (0.181) loss 0.3848 (0.3916) acc 96.8750 (96.2500) lr 8.1262e-04 eta 0:02:18
epoch [30/50] batch [10/26] time 0.071 (0.163) data 0.000 (0.091) loss 0.5269 (0.3972) acc 90.6250 (96.8750) lr 8.1262e-04 eta 0:01:27
epoch [30/50] batch [15/26] time 0.076 (0.133) data 0.000 (0.061) loss 0.4954 (0.4116) acc 96.8750 (96.2500) lr 8.1262e-04 eta 0:01:10
epoch [30/50] batch [20/26] time 0.072 (0.118) data 0.000 (0.045) loss 0.5732 (0.4278) acc 90.6250 (94.8438) lr 8.1262e-04 eta 0:01:01
epoch [30/50] batch [25/26] time 0.071 (0.108) data 0.000 (0.036) loss 0.4690 (0.4352) acc 93.7500 (94.6250) lr 8.1262e-04 eta 0:00:56
epoch [31/50] batch [5/26] time 0.076 (0.234) data 0.000 (0.158) loss 0.4771 (0.4130) acc 87.5000 (93.7500) lr 7.5131e-04 eta 0:02:00
epoch [31/50] batch [10/26] time 0.072 (0.153) data 0.000 (0.079) loss 0.4053 (0.4067) acc 90.6250 (95.0000) lr 7.5131e-04 eta 0:01:17
epoch [31/50] batch [15/26] time 0.072 (0.126) data 0.000 (0.053) loss 0.6489 (0.4362) acc 87.5000 (93.7500) lr 7.5131e-04 eta 0:01:03
epoch [31/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 0.4285 (0.4464) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:56
epoch [31/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.5610 (0.4451) acc 96.8750 (94.2500) lr 7.5131e-04 eta 0:00:51
epoch [32/50] batch [5/26] time 0.073 (0.229) data 0.001 (0.155) loss 0.4788 (0.3953) acc 93.7500 (95.6250) lr 6.9098e-04 eta 0:01:51
epoch [32/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.078) loss 0.8574 (0.4562) acc 71.8750 (92.1875) lr 6.9098e-04 eta 0:01:12
epoch [32/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.052) loss 0.3374 (0.4396) acc 100.0000 (93.3333) lr 6.9098e-04 eta 0:00:59
epoch [32/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.3032 (0.4406) acc 100.0000 (93.7500) lr 6.9098e-04 eta 0:00:52
epoch [32/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.4170 (0.4480) acc 90.6250 (93.2500) lr 6.9098e-04 eta 0:00:47
epoch [33/50] batch [5/26] time 0.072 (0.236) data 0.001 (0.162) loss 0.2805 (0.4366) acc 100.0000 (93.7500) lr 6.3188e-04 eta 0:01:49
epoch [33/50] batch [10/26] time 0.073 (0.154) data 0.000 (0.081) loss 0.5107 (0.4497) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:01:10
epoch [33/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.054) loss 0.4290 (0.4322) acc 93.7500 (94.1667) lr 6.3188e-04 eta 0:00:57
epoch [33/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.041) loss 0.4124 (0.4426) acc 93.7500 (93.9062) lr 6.3188e-04 eta 0:00:50
epoch [33/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.033) loss 0.4136 (0.4415) acc 93.7500 (93.8750) lr 6.3188e-04 eta 0:00:46
epoch [34/50] batch [5/26] time 0.075 (0.234) data 0.000 (0.157) loss 0.3179 (0.3910) acc 96.8750 (95.6250) lr 5.7422e-04 eta 0:01:42
epoch [34/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.079) loss 0.5449 (0.3887) acc 93.7500 (95.6250) lr 5.7422e-04 eta 0:01:05
epoch [34/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.053) loss 0.5596 (0.4117) acc 87.5000 (94.7917) lr 5.7422e-04 eta 0:00:53
epoch [34/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.039) loss 0.4268 (0.4223) acc 93.7500 (94.6875) lr 5.7422e-04 eta 0:00:47
epoch [34/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.3203 (0.4291) acc 100.0000 (94.5000) lr 5.7422e-04 eta 0:00:43
epoch [35/50] batch [5/26] time 0.078 (0.243) data 0.000 (0.169) loss 0.2585 (0.4196) acc 100.0000 (94.3750) lr 5.1825e-04 eta 0:01:39
epoch [35/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.085) loss 0.4314 (0.4163) acc 93.7500 (95.0000) lr 5.1825e-04 eta 0:01:03
epoch [35/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.057) loss 0.5430 (0.4163) acc 93.7500 (95.0000) lr 5.1825e-04 eta 0:00:51
epoch [35/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.3467 (0.4036) acc 96.8750 (95.6250) lr 5.1825e-04 eta 0:00:45
epoch [35/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4312 (0.4056) acc 96.8750 (95.6250) lr 5.1825e-04 eta 0:00:41
epoch [36/50] batch [5/26] time 0.070 (0.225) data 0.000 (0.153) loss 0.4053 (0.3938) acc 96.8750 (96.2500) lr 4.6417e-04 eta 0:01:26
epoch [36/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.077) loss 0.3677 (0.4085) acc 100.0000 (95.6250) lr 4.6417e-04 eta 0:00:56
epoch [36/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.3342 (0.4085) acc 96.8750 (95.2083) lr 4.6417e-04 eta 0:00:45
epoch [36/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.038) loss 0.4185 (0.4118) acc 93.7500 (95.0000) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.6519 (0.4280) acc 93.7500 (94.7500) lr 4.6417e-04 eta 0:00:37
epoch [37/50] batch [5/26] time 0.075 (0.234) data 0.000 (0.161) loss 0.3562 (0.3794) acc 100.0000 (96.8750) lr 4.1221e-04 eta 0:01:23
epoch [37/50] batch [10/26] time 0.071 (0.153) data 0.000 (0.081) loss 0.3574 (0.4071) acc 100.0000 (95.3125) lr 4.1221e-04 eta 0:00:54
epoch [37/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.054) loss 0.3892 (0.4027) acc 93.7500 (95.6250) lr 4.1221e-04 eta 0:00:43
epoch [37/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.040) loss 0.3711 (0.4003) acc 96.8750 (95.4688) lr 4.1221e-04 eta 0:00:38
epoch [37/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.3760 (0.4031) acc 93.7500 (95.2500) lr 4.1221e-04 eta 0:00:35
epoch [38/50] batch [5/26] time 0.072 (0.245) data 0.000 (0.172) loss 0.4661 (0.4309) acc 93.7500 (96.2500) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.086) loss 0.3123 (0.4206) acc 96.8750 (95.0000) lr 3.6258e-04 eta 0:00:51
epoch [38/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.057) loss 0.3738 (0.4351) acc 93.7500 (94.5833) lr 3.6258e-04 eta 0:00:41
epoch [38/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.5869 (0.4344) acc 90.6250 (94.8438) lr 3.6258e-04 eta 0:00:36
epoch [38/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.035) loss 0.2839 (0.4334) acc 100.0000 (95.1250) lr 3.6258e-04 eta 0:00:32
epoch [39/50] batch [5/26] time 0.078 (0.225) data 0.000 (0.149) loss 0.3638 (0.3135) acc 93.7500 (98.1250) lr 3.1545e-04 eta 0:01:09
epoch [39/50] batch [10/26] time 0.072 (0.148) data 0.000 (0.075) loss 0.4578 (0.3581) acc 93.7500 (97.1875) lr 3.1545e-04 eta 0:00:44
epoch [39/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.050) loss 0.9155 (0.3890) acc 81.2500 (95.8333) lr 3.1545e-04 eta 0:00:36
epoch [39/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.038) loss 0.3853 (0.4075) acc 93.7500 (95.0000) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.030) loss 0.4033 (0.4111) acc 96.8750 (95.0000) lr 3.1545e-04 eta 0:00:29
epoch [40/50] batch [5/26] time 0.074 (0.232) data 0.000 (0.160) loss 0.2559 (0.4304) acc 100.0000 (95.0000) lr 2.7103e-04 eta 0:01:05
epoch [40/50] batch [10/26] time 0.070 (0.152) data 0.000 (0.080) loss 0.3450 (0.4087) acc 96.8750 (95.6250) lr 2.7103e-04 eta 0:00:41
epoch [40/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.4053 (0.4445) acc 100.0000 (94.5833) lr 2.7103e-04 eta 0:00:33
epoch [40/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.3413 (0.4368) acc 96.8750 (94.5312) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.4541 (0.4216) acc 93.7500 (94.8750) lr 2.7103e-04 eta 0:00:26
epoch [41/50] batch [5/26] time 0.072 (0.231) data 0.000 (0.156) loss 0.3042 (0.3921) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:59
epoch [41/50] batch [10/26] time 0.070 (0.152) data 0.000 (0.078) loss 0.3738 (0.3633) acc 100.0000 (96.8750) lr 2.2949e-04 eta 0:00:37
epoch [41/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.052) loss 0.4246 (0.3773) acc 96.8750 (96.4583) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.5947 (0.3967) acc 90.6250 (96.0938) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [25/26] time 0.073 (0.103) data 0.000 (0.031) loss 0.4678 (0.4079) acc 90.6250 (95.6250) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/26] time 0.076 (0.239) data 0.000 (0.166) loss 0.3472 (0.3402) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:54
epoch [42/50] batch [10/26] time 0.070 (0.155) data 0.000 (0.083) loss 0.3672 (0.3484) acc 93.7500 (96.2500) lr 1.9098e-04 eta 0:00:34
epoch [42/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.3110 (0.3360) acc 100.0000 (96.6667) lr 1.9098e-04 eta 0:00:27
epoch [42/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5947 (0.3599) acc 90.6250 (96.0938) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [25/26] time 0.072 (0.104) data 0.000 (0.033) loss 0.4639 (0.3689) acc 93.7500 (95.7500) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.071 (0.225) data 0.000 (0.153) loss 0.3193 (0.3671) acc 96.8750 (96.2500) lr 1.5567e-04 eta 0:00:45
epoch [43/50] batch [10/26] time 0.072 (0.148) data 0.000 (0.077) loss 0.3804 (0.3844) acc 96.8750 (95.0000) lr 1.5567e-04 eta 0:00:29
epoch [43/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.4331 (0.4011) acc 87.5000 (94.3750) lr 1.5567e-04 eta 0:00:23
epoch [43/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.039) loss 0.3931 (0.4132) acc 93.7500 (94.3750) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5430 (0.4190) acc 93.7500 (94.3750) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.073 (0.225) data 0.000 (0.153) loss 0.4365 (0.4510) acc 87.5000 (91.8750) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.077) loss 0.2656 (0.4410) acc 96.8750 (93.1250) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.3989 (0.4177) acc 93.7500 (93.3333) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.038) loss 0.3347 (0.4187) acc 100.0000 (94.0625) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.3853 (0.4111) acc 96.8750 (94.6250) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/26] time 0.074 (0.240) data 0.000 (0.162) loss 0.4192 (0.3688) acc 93.7500 (96.2500) lr 9.5173e-05 eta 0:00:36
epoch [45/50] batch [10/26] time 0.071 (0.155) data 0.000 (0.081) loss 0.2800 (0.3898) acc 96.8750 (94.6875) lr 9.5173e-05 eta 0:00:22
epoch [45/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.054) loss 0.3809 (0.3959) acc 96.8750 (95.2083) lr 9.5173e-05 eta 0:00:17
epoch [45/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.041) loss 0.3950 (0.3897) acc 93.7500 (95.7812) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.033) loss 0.3896 (0.3850) acc 96.8750 (96.2500) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.073 (0.244) data 0.000 (0.171) loss 0.4709 (0.4264) acc 90.6250 (93.1250) lr 7.0224e-05 eta 0:00:30
epoch [46/50] batch [10/26] time 0.070 (0.157) data 0.000 (0.086) loss 0.2683 (0.3845) acc 100.0000 (95.3125) lr 7.0224e-05 eta 0:00:18
epoch [46/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.4856 (0.4164) acc 93.7500 (93.5417) lr 7.0224e-05 eta 0:00:14
epoch [46/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.4336 (0.4122) acc 96.8750 (94.3750) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.2983 (0.4034) acc 100.0000 (94.7500) lr 7.0224e-05 eta 0:00:11
epoch [47/50] batch [5/26] time 0.071 (0.235) data 0.000 (0.160) loss 0.4341 (0.4175) acc 96.8750 (93.1250) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.080) loss 0.4419 (0.4327) acc 93.7500 (93.1250) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.054) loss 0.3242 (0.4176) acc 96.8750 (93.7500) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.040) loss 0.3301 (0.4145) acc 100.0000 (94.0625) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.4954 (0.4321) acc 90.6250 (93.3750) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/26] time 0.071 (0.226) data 0.000 (0.153) loss 0.4243 (0.3683) acc 96.8750 (98.1250) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.072 (0.149) data 0.000 (0.077) loss 0.4915 (0.3892) acc 90.6250 (96.8750) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.051) loss 0.7231 (0.4307) acc 84.3750 (95.2083) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.3843 (0.4207) acc 93.7500 (95.4688) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.071 (0.102) data 0.000 (0.031) loss 0.3997 (0.4188) acc 93.7500 (95.1250) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.072 (0.233) data 0.000 (0.157) loss 0.4456 (0.4437) acc 93.7500 (93.1250) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.072 (0.153) data 0.001 (0.079) loss 0.4714 (0.4430) acc 90.6250 (93.7500) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.053) loss 0.4824 (0.4378) acc 93.7500 (94.1667) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.040) loss 0.2668 (0.4095) acc 100.0000 (95.1562) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.3330 (0.4004) acc 96.8750 (95.2500) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.071 (0.239) data 0.000 (0.166) loss 0.3130 (0.4249) acc 100.0000 (95.0000) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.073 (0.155) data 0.000 (0.083) loss 0.4019 (0.4016) acc 90.6250 (95.3125) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.127) data 0.000 (0.056) loss 0.4131 (0.4005) acc 93.7500 (95.4167) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.072 (0.113) data 0.000 (0.042) loss 0.4417 (0.4003) acc 90.6250 (95.3125) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.033) loss 0.4819 (0.4052) acc 87.5000 (94.7500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:10,  5.33s/it] 67%|██████▋   | 2/3 [00:05<00:02,  2.53s/it]100%|██████████| 3/3 [00:06<00:00,  1.43s/it]100%|██████████| 3/3 [00:06<00:00,  2.05s/it]
=> result
* total: 1,053
* correct: 1,029
* accuracy: 97.7%
* error: 2.3%
* macro_f1: 97.7%
Elapsed: 0:02:55
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:09<00:19,  9.77s/it] 67%|██████▋   | 2/3 [00:10<00:04,  4.36s/it]100%|██████████| 3/3 [00:10<00:00,  2.62s/it]100%|██████████| 3/3 [00:11<00:00,  3.68s/it]
=> result
* total: 1,410
* correct: 1,065
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 70.6%
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:11,  5.57s/it] 67%|██████▋   | 2/3 [00:06<00:02,  2.63s/it]100%|██████████| 3/3 [00:06<00:00,  1.67s/it]100%|██████████| 3/3 [00:06<00:00,  2.26s/it]
=> result
* total: 1,410
* correct: 1,070
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 70.6%
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:10,  5.39s/it] 67%|██████▋   | 2/3 [00:05<00:02,  2.55s/it]100%|██████████| 3/3 [00:06<00:00,  1.63s/it]100%|██████████| 3/3 [00:06<00:00,  2.21s/it]
=> result
* total: 1,410
* correct: 1,062
* accuracy: 75.3%
* error: 24.7%
* macro_f1: 70.3%
Run this job and save the output to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/10] time 0.063 (0.394) data 0.000 (0.304) loss 2.3359 (2.5141) acc 78.1250 (72.5000) lr 1.0000e-05 eta 0:03:15
epoch [1/50] batch [10/10] time 0.072 (0.229) data 0.000 (0.152) loss 2.2520 (2.5475) acc 75.0000 (70.0000) lr 2.0000e-03 eta 0:01:52
epoch [2/50] batch [5/10] time 0.061 (0.233) data 0.000 (0.169) loss 1.1699 (1.3875) acc 71.8750 (76.2500) lr 2.0000e-03 eta 0:01:53
epoch [2/50] batch [10/10] time 0.041 (0.145) data 0.000 (0.084) loss 1.0430 (1.1675) acc 68.7500 (78.1250) lr 1.9980e-03 eta 0:01:09
epoch [3/50] batch [5/10] time 0.062 (0.222) data 0.000 (0.159) loss 0.9521 (0.8253) acc 78.1250 (80.0000) lr 1.9980e-03 eta 0:01:45
epoch [3/50] batch [10/10] time 0.045 (0.140) data 0.000 (0.079) loss 0.7690 (0.7667) acc 81.2500 (81.5625) lr 1.9921e-03 eta 0:01:05
epoch [4/50] batch [5/10] time 0.063 (0.233) data 0.000 (0.168) loss 0.5703 (0.7103) acc 87.5000 (85.0000) lr 1.9921e-03 eta 0:01:48
epoch [4/50] batch [10/10] time 0.044 (0.146) data 0.000 (0.084) loss 0.7285 (0.6723) acc 81.2500 (85.3125) lr 1.9823e-03 eta 0:01:07
epoch [5/50] batch [5/10] time 0.064 (0.221) data 0.000 (0.157) loss 0.6812 (0.6935) acc 81.2500 (78.1250) lr 1.9823e-03 eta 0:01:40
epoch [5/50] batch [10/10] time 0.044 (0.140) data 0.000 (0.078) loss 0.5781 (0.7008) acc 81.2500 (79.0625) lr 1.9686e-03 eta 0:01:02
epoch [6/50] batch [5/10] time 0.061 (0.245) data 0.000 (0.181) loss 0.4558 (0.7045) acc 90.6250 (82.5000) lr 1.9686e-03 eta 0:01:49
epoch [6/50] batch [10/10] time 0.045 (0.152) data 0.000 (0.090) loss 0.4119 (0.6075) acc 87.5000 (84.6875) lr 1.9511e-03 eta 0:01:07
epoch [7/50] batch [5/10] time 0.063 (0.232) data 0.000 (0.167) loss 0.5332 (0.5081) acc 78.1250 (85.0000) lr 1.9511e-03 eta 0:01:40
epoch [7/50] batch [10/10] time 0.040 (0.144) data 0.000 (0.084) loss 0.9507 (0.5377) acc 68.7500 (85.3125) lr 1.9298e-03 eta 0:01:01
epoch [8/50] batch [5/10] time 0.064 (0.218) data 0.000 (0.153) loss 0.6934 (0.4677) acc 87.5000 (90.0000) lr 1.9298e-03 eta 0:01:32
epoch [8/50] batch [10/10] time 0.045 (0.138) data 0.000 (0.077) loss 0.3052 (0.5025) acc 93.7500 (87.8125) lr 1.9048e-03 eta 0:00:58
epoch [9/50] batch [5/10] time 0.064 (0.219) data 0.000 (0.156) loss 0.6714 (0.4800) acc 78.1250 (87.5000) lr 1.9048e-03 eta 0:01:30
epoch [9/50] batch [10/10] time 0.046 (0.140) data 0.000 (0.078) loss 0.5210 (0.5370) acc 81.2500 (85.0000) lr 1.8763e-03 eta 0:00:57
epoch [10/50] batch [5/10] time 0.057 (0.238) data 0.000 (0.175) loss 0.6675 (0.5667) acc 81.2500 (86.8750) lr 1.8763e-03 eta 0:01:36
epoch [10/50] batch [10/10] time 0.042 (0.148) data 0.000 (0.088) loss 0.2969 (0.4903) acc 87.5000 (88.4375) lr 1.8443e-03 eta 0:00:59
epoch [11/50] batch [5/10] time 0.064 (0.227) data 0.000 (0.164) loss 0.6270 (0.6029) acc 81.2500 (84.3750) lr 1.8443e-03 eta 0:01:29
epoch [11/50] batch [10/10] time 0.044 (0.143) data 0.000 (0.082) loss 0.7402 (0.5805) acc 75.0000 (84.6875) lr 1.8090e-03 eta 0:00:55
epoch [12/50] batch [5/10] time 0.063 (0.238) data 0.000 (0.175) loss 0.4961 (0.4596) acc 84.3750 (85.6250) lr 1.8090e-03 eta 0:01:31
epoch [12/50] batch [10/10] time 0.041 (0.148) data 0.000 (0.087) loss 0.7061 (0.4576) acc 93.7500 (88.1250) lr 1.7705e-03 eta 0:00:56
epoch [13/50] batch [5/10] time 0.062 (0.222) data 0.000 (0.159) loss 0.3787 (0.3603) acc 90.6250 (93.7500) lr 1.7705e-03 eta 0:01:23
epoch [13/50] batch [10/10] time 0.051 (0.141) data 0.000 (0.079) loss 0.6299 (0.4495) acc 81.2500 (89.3750) lr 1.7290e-03 eta 0:00:52
epoch [14/50] batch [5/10] time 0.064 (0.233) data 0.000 (0.168) loss 0.3750 (0.4374) acc 93.7500 (88.7500) lr 1.7290e-03 eta 0:01:25
epoch [14/50] batch [10/10] time 0.053 (0.147) data 0.000 (0.084) loss 0.3826 (0.4636) acc 87.5000 (88.7500) lr 1.6845e-03 eta 0:00:52
epoch [15/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.155) loss 0.5068 (0.4156) acc 90.6250 (89.3750) lr 1.6845e-03 eta 0:01:17
epoch [15/50] batch [10/10] time 0.058 (0.139) data 0.000 (0.078) loss 0.3916 (0.4572) acc 93.7500 (90.0000) lr 1.6374e-03 eta 0:00:48
epoch [16/50] batch [5/10] time 0.063 (0.227) data 0.000 (0.162) loss 0.3442 (0.3283) acc 93.7500 (94.3750) lr 1.6374e-03 eta 0:01:18
epoch [16/50] batch [10/10] time 0.046 (0.143) data 0.000 (0.081) loss 0.7959 (0.4227) acc 87.5000 (91.5625) lr 1.5878e-03 eta 0:00:48
epoch [17/50] batch [5/10] time 0.061 (0.226) data 0.000 (0.163) loss 0.5894 (0.4656) acc 87.5000 (88.1250) lr 1.5878e-03 eta 0:01:15
epoch [17/50] batch [10/10] time 0.043 (0.142) data 0.000 (0.082) loss 0.5679 (0.4394) acc 81.2500 (87.8125) lr 1.5358e-03 eta 0:00:46
epoch [18/50] batch [5/10] time 0.062 (0.235) data 0.000 (0.173) loss 0.4727 (0.4275) acc 87.5000 (90.6250) lr 1.5358e-03 eta 0:01:16
epoch [18/50] batch [10/10] time 0.044 (0.147) data 0.000 (0.086) loss 0.4990 (0.4098) acc 75.0000 (89.0625) lr 1.4818e-03 eta 0:00:47
epoch [19/50] batch [5/10] time 0.062 (0.222) data 0.000 (0.158) loss 0.4900 (0.4287) acc 84.3750 (90.6250) lr 1.4818e-03 eta 0:01:09
epoch [19/50] batch [10/10] time 0.046 (0.141) data 0.000 (0.079) loss 0.3960 (0.3643) acc 93.7500 (92.1875) lr 1.4258e-03 eta 0:00:43
epoch [20/50] batch [5/10] time 0.061 (0.240) data 0.000 (0.178) loss 0.5410 (0.3829) acc 90.6250 (91.8750) lr 1.4258e-03 eta 0:01:13
epoch [20/50] batch [10/10] time 0.045 (0.150) data 0.000 (0.089) loss 0.1731 (0.4082) acc 100.0000 (90.6250) lr 1.3681e-03 eta 0:00:44
epoch [21/50] batch [5/10] time 0.063 (0.231) data 0.000 (0.165) loss 0.5010 (0.4018) acc 87.5000 (89.3750) lr 1.3681e-03 eta 0:01:08
epoch [21/50] batch [10/10] time 0.051 (0.146) data 0.000 (0.083) loss 0.3293 (0.4116) acc 87.5000 (89.3750) lr 1.3090e-03 eta 0:00:42
epoch [22/50] batch [5/10] time 0.061 (0.230) data 0.000 (0.169) loss 0.5332 (0.3832) acc 84.3750 (91.8750) lr 1.3090e-03 eta 0:01:05
epoch [22/50] batch [10/10] time 0.049 (0.145) data 0.000 (0.084) loss 0.6396 (0.4607) acc 75.0000 (87.5000) lr 1.2487e-03 eta 0:00:40
epoch [23/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.155) loss 0.4189 (0.5252) acc 90.6250 (86.2500) lr 1.2487e-03 eta 0:00:59
epoch [23/50] batch [10/10] time 0.046 (0.138) data 0.000 (0.078) loss 0.2866 (0.4667) acc 87.5000 (87.1875) lr 1.1874e-03 eta 0:00:37
epoch [24/50] batch [5/10] time 0.062 (0.225) data 0.000 (0.164) loss 0.4961 (0.3967) acc 81.2500 (91.2500) lr 1.1874e-03 eta 0:00:59
epoch [24/50] batch [10/10] time 0.043 (0.142) data 0.000 (0.082) loss 0.2695 (0.4219) acc 93.7500 (89.3750) lr 1.1253e-03 eta 0:00:36
epoch [25/50] batch [5/10] time 0.061 (0.222) data 0.000 (0.155) loss 0.4424 (0.4543) acc 93.7500 (89.3750) lr 1.1253e-03 eta 0:00:56
epoch [25/50] batch [10/10] time 0.044 (0.140) data 0.000 (0.078) loss 0.7964 (0.4484) acc 75.0000 (88.4375) lr 1.0628e-03 eta 0:00:35
epoch [26/50] batch [5/10] time 0.059 (0.220) data 0.000 (0.157) loss 0.5381 (0.5179) acc 81.2500 (86.8750) lr 1.0628e-03 eta 0:00:54
epoch [26/50] batch [10/10] time 0.044 (0.139) data 0.000 (0.079) loss 0.2404 (0.4010) acc 100.0000 (91.8750) lr 1.0000e-03 eta 0:00:33
epoch [27/50] batch [5/10] time 0.061 (0.236) data 0.000 (0.171) loss 0.6978 (0.4189) acc 78.1250 (88.7500) lr 1.0000e-03 eta 0:00:55
epoch [27/50] batch [10/10] time 0.044 (0.148) data 0.000 (0.085) loss 0.3362 (0.3701) acc 93.7500 (91.8750) lr 9.3721e-04 eta 0:00:33
epoch [28/50] batch [5/10] time 0.062 (0.242) data 0.000 (0.179) loss 0.3625 (0.3832) acc 90.6250 (88.7500) lr 9.3721e-04 eta 0:00:54
epoch [28/50] batch [10/10] time 0.046 (0.150) data 0.000 (0.090) loss 0.1108 (0.3488) acc 100.0000 (91.2500) lr 8.7467e-04 eta 0:00:33
epoch [29/50] batch [5/10] time 0.061 (0.245) data 0.000 (0.183) loss 0.4583 (0.4261) acc 90.6250 (87.5000) lr 8.7467e-04 eta 0:00:52
epoch [29/50] batch [10/10] time 0.044 (0.152) data 0.000 (0.091) loss 0.2076 (0.4087) acc 93.7500 (87.8125) lr 8.1262e-04 eta 0:00:31
epoch [30/50] batch [5/10] time 0.060 (0.229) data 0.000 (0.168) loss 0.1587 (0.3732) acc 100.0000 (91.8750) lr 8.1262e-04 eta 0:00:46
epoch [30/50] batch [10/10] time 0.043 (0.143) data 0.000 (0.084) loss 0.4070 (0.3875) acc 93.7500 (91.2500) lr 7.5131e-04 eta 0:00:28
epoch [31/50] batch [5/10] time 0.059 (0.218) data 0.000 (0.155) loss 0.3940 (0.3504) acc 90.6250 (94.3750) lr 7.5131e-04 eta 0:00:42
epoch [31/50] batch [10/10] time 0.044 (0.138) data 0.000 (0.077) loss 0.7061 (0.3823) acc 75.0000 (91.2500) lr 6.9098e-04 eta 0:00:26
epoch [32/50] batch [5/10] time 0.061 (0.226) data 0.000 (0.165) loss 0.3904 (0.4045) acc 90.6250 (91.2500) lr 6.9098e-04 eta 0:00:41
epoch [32/50] batch [10/10] time 0.043 (0.142) data 0.000 (0.083) loss 0.2883 (0.3408) acc 100.0000 (93.7500) lr 6.3188e-04 eta 0:00:25
epoch [33/50] batch [5/10] time 0.061 (0.219) data 0.000 (0.157) loss 0.5186 (0.4356) acc 90.6250 (90.0000) lr 6.3188e-04 eta 0:00:38
epoch [33/50] batch [10/10] time 0.044 (0.139) data 0.000 (0.078) loss 0.1287 (0.3635) acc 100.0000 (91.2500) lr 5.7422e-04 eta 0:00:23
epoch [34/50] batch [5/10] time 0.064 (0.232) data 0.000 (0.163) loss 0.4565 (0.4067) acc 90.6250 (91.2500) lr 5.7422e-04 eta 0:00:38
epoch [34/50] batch [10/10] time 0.046 (0.144) data 0.000 (0.082) loss 0.3911 (0.3804) acc 87.5000 (91.2500) lr 5.1825e-04 eta 0:00:23
epoch [35/50] batch [5/10] time 0.068 (0.228) data 0.000 (0.161) loss 0.2593 (0.3647) acc 96.8750 (92.5000) lr 5.1825e-04 eta 0:00:35
epoch [35/50] batch [10/10] time 0.050 (0.145) data 0.000 (0.081) loss 0.3376 (0.3361) acc 93.7500 (93.4375) lr 4.6417e-04 eta 0:00:21
epoch [36/50] batch [5/10] time 0.061 (0.230) data 0.000 (0.168) loss 0.3540 (0.3711) acc 93.7500 (90.6250) lr 4.6417e-04 eta 0:00:33
epoch [36/50] batch [10/10] time 0.046 (0.144) data 0.000 (0.084) loss 0.1937 (0.4076) acc 93.7500 (90.6250) lr 4.1221e-04 eta 0:00:20
epoch [37/50] batch [5/10] time 0.064 (0.223) data 0.000 (0.155) loss 0.2812 (0.3529) acc 90.6250 (89.3750) lr 4.1221e-04 eta 0:00:30
epoch [37/50] batch [10/10] time 0.046 (0.141) data 0.000 (0.078) loss 0.5381 (0.3634) acc 81.2500 (90.3125) lr 3.6258e-04 eta 0:00:18
epoch [38/50] batch [5/10] time 0.062 (0.244) data 0.000 (0.181) loss 0.3604 (0.3944) acc 90.6250 (88.1250) lr 3.6258e-04 eta 0:00:30
epoch [38/50] batch [10/10] time 0.045 (0.151) data 0.000 (0.091) loss 0.2637 (0.3909) acc 93.7500 (89.0625) lr 3.1545e-04 eta 0:00:18
epoch [39/50] batch [5/10] time 0.063 (0.244) data 0.000 (0.178) loss 0.3081 (0.3657) acc 96.8750 (92.5000) lr 3.1545e-04 eta 0:00:28
epoch [39/50] batch [10/10] time 0.045 (0.152) data 0.000 (0.089) loss 0.8701 (0.4142) acc 75.0000 (90.3125) lr 2.7103e-04 eta 0:00:16
epoch [40/50] batch [5/10] time 0.060 (0.227) data 0.000 (0.160) loss 0.2957 (0.3811) acc 96.8750 (91.2500) lr 2.7103e-04 eta 0:00:23
epoch [40/50] batch [10/10] time 0.044 (0.143) data 0.000 (0.080) loss 0.3359 (0.3603) acc 93.7500 (90.9375) lr 2.2949e-04 eta 0:00:14
epoch [41/50] batch [5/10] time 0.062 (0.228) data 0.000 (0.164) loss 0.3325 (0.2972) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:21
epoch [41/50] batch [10/10] time 0.045 (0.143) data 0.000 (0.082) loss 0.4417 (0.3557) acc 81.2500 (92.1875) lr 1.9098e-04 eta 0:00:12
epoch [42/50] batch [5/10] time 0.063 (0.231) data 0.000 (0.169) loss 0.1605 (0.3553) acc 100.0000 (92.5000) lr 1.9098e-04 eta 0:00:19
epoch [42/50] batch [10/10] time 0.045 (0.145) data 0.000 (0.085) loss 0.2900 (0.3541) acc 93.7500 (93.4375) lr 1.5567e-04 eta 0:00:11
epoch [43/50] batch [5/10] time 0.063 (0.262) data 0.000 (0.199) loss 0.2900 (0.4269) acc 90.6250 (88.1250) lr 1.5567e-04 eta 0:00:19
epoch [43/50] batch [10/10] time 0.046 (0.161) data 0.000 (0.099) loss 0.2610 (0.3791) acc 93.7500 (90.3125) lr 1.2369e-04 eta 0:00:11
epoch [44/50] batch [5/10] time 0.062 (0.230) data 0.000 (0.166) loss 0.3342 (0.3974) acc 90.6250 (88.7500) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [10/10] time 0.041 (0.144) data 0.000 (0.083) loss 0.5947 (0.3799) acc 87.5000 (90.0000) lr 9.5173e-05 eta 0:00:08
epoch [45/50] batch [5/10] time 0.062 (0.234) data 0.000 (0.171) loss 0.3352 (0.3049) acc 90.6250 (91.8750) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [10/10] time 0.048 (0.147) data 0.000 (0.086) loss 0.2380 (0.3097) acc 100.0000 (92.5000) lr 7.0224e-05 eta 0:00:07
epoch [46/50] batch [5/10] time 0.063 (0.262) data 0.000 (0.198) loss 0.4919 (0.4006) acc 90.6250 (91.8750) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [10/10] time 0.053 (0.162) data 0.000 (0.099) loss 0.3032 (0.3681) acc 100.0000 (93.4375) lr 4.8943e-05 eta 0:00:06
epoch [47/50] batch [5/10] time 0.061 (0.233) data 0.000 (0.170) loss 0.2695 (0.3978) acc 96.8750 (91.2500) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [10/10] time 0.045 (0.146) data 0.000 (0.085) loss 0.2455 (0.3890) acc 93.7500 (90.6250) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/10] time 0.063 (0.220) data 0.000 (0.156) loss 0.3975 (0.3155) acc 84.3750 (91.2500) lr 3.1417e-05 eta 0:00:05
epoch [48/50] batch [10/10] time 0.047 (0.139) data 0.000 (0.078) loss 0.3152 (0.3251) acc 93.7500 (92.1875) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [5/10] time 0.061 (0.237) data 0.000 (0.175) loss 0.4292 (0.3646) acc 84.3750 (88.7500) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [10/10] time 0.045 (0.148) data 0.000 (0.088) loss 0.2744 (0.3610) acc 93.7500 (90.9375) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/10] time 0.062 (0.263) data 0.000 (0.198) loss 0.3997 (0.3117) acc 90.6250 (91.8750) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.049 (0.161) data 0.000 (0.099) loss 0.1461 (0.3079) acc 100.0000 (92.8125) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:07<00:23,  7.83s/it] 50%|█████     | 2/4 [00:08<00:07,  3.56s/it] 75%|███████▌  | 3/4 [00:08<00:02,  2.19s/it]100%|██████████| 4/4 [00:09<00:00,  1.52s/it]100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
=> result
* total: 1,881
* correct: 1,779
* accuracy: 94.6%
* error: 5.4%
* macro_f1: 94.6%
Elapsed: 0:01:29
Run this job and save the output to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/10] time 0.061 (0.315) data 0.000 (0.243) loss 2.1055 (1.8916) acc 71.8750 (78.1250) lr 1.0000e-05 eta 0:02:36
epoch [1/50] batch [10/10] time 0.063 (0.188) data 0.000 (0.121) loss 1.6387 (1.8432) acc 81.2500 (77.8125) lr 2.0000e-03 eta 0:01:32
epoch [2/50] batch [5/10] time 0.061 (0.251) data 0.000 (0.188) loss 1.3887 (1.2796) acc 68.7500 (81.2500) lr 2.0000e-03 eta 0:02:01
epoch [2/50] batch [10/10] time 0.043 (0.155) data 0.000 (0.094) loss 1.0742 (1.0500) acc 75.0000 (83.4375) lr 1.9980e-03 eta 0:01:14
epoch [3/50] batch [5/10] time 0.062 (0.250) data 0.000 (0.187) loss 0.7261 (0.9165) acc 87.5000 (80.6250) lr 1.9980e-03 eta 0:01:58
epoch [3/50] batch [10/10] time 0.043 (0.154) data 0.000 (0.094) loss 0.5151 (0.7663) acc 93.7500 (83.7500) lr 1.9921e-03 eta 0:01:12
epoch [4/50] batch [5/10] time 0.061 (0.244) data 0.000 (0.183) loss 0.8462 (0.6864) acc 75.0000 (83.7500) lr 1.9921e-03 eta 0:01:53
epoch [4/50] batch [10/10] time 0.043 (0.151) data 0.000 (0.091) loss 0.7637 (0.6583) acc 81.2500 (83.4375) lr 1.9823e-03 eta 0:01:09
epoch [5/50] batch [5/10] time 0.061 (0.253) data 0.000 (0.191) loss 0.5166 (0.5708) acc 87.5000 (84.3750) lr 1.9823e-03 eta 0:01:55
epoch [5/50] batch [10/10] time 0.049 (0.155) data 0.000 (0.096) loss 0.6992 (0.5787) acc 87.5000 (85.3125) lr 1.9686e-03 eta 0:01:09
epoch [6/50] batch [5/10] time 0.061 (0.247) data 0.000 (0.187) loss 0.5020 (0.5127) acc 87.5000 (88.1250) lr 1.9686e-03 eta 0:01:49
epoch [6/50] batch [10/10] time 0.044 (0.152) data 0.000 (0.093) loss 0.5942 (0.5212) acc 81.2500 (85.6250) lr 1.9511e-03 eta 0:01:06
epoch [7/50] batch [5/10] time 0.062 (0.244) data 0.000 (0.182) loss 0.5234 (0.5215) acc 87.5000 (88.1250) lr 1.9511e-03 eta 0:01:46
epoch [7/50] batch [10/10] time 0.043 (0.151) data 0.000 (0.091) loss 0.2930 (0.4847) acc 93.7500 (87.8125) lr 1.9298e-03 eta 0:01:04
epoch [8/50] batch [5/10] time 0.062 (0.251) data 0.000 (0.189) loss 0.8164 (0.5984) acc 78.1250 (83.1250) lr 1.9298e-03 eta 0:01:46
epoch [8/50] batch [10/10] time 0.042 (0.154) data 0.000 (0.094) loss 0.2725 (0.4949) acc 93.7500 (86.5625) lr 1.9048e-03 eta 0:01:04
epoch [9/50] batch [5/10] time 0.061 (0.253) data 0.000 (0.191) loss 0.7119 (0.5251) acc 78.1250 (87.5000) lr 1.9048e-03 eta 0:01:44
epoch [9/50] batch [10/10] time 0.039 (0.154) data 0.000 (0.096) loss 0.6274 (0.5435) acc 87.5000 (85.9375) lr 1.8763e-03 eta 0:01:03
epoch [10/50] batch [5/10] time 0.061 (0.254) data 0.000 (0.190) loss 0.5264 (0.4357) acc 87.5000 (91.2500) lr 1.8763e-03 eta 0:01:42
epoch [10/50] batch [10/10] time 0.043 (0.156) data 0.000 (0.095) loss 1.1611 (0.4721) acc 75.0000 (89.6875) lr 1.8443e-03 eta 0:01:02
epoch [11/50] batch [5/10] time 0.063 (0.249) data 0.000 (0.186) loss 0.4316 (0.5054) acc 90.6250 (85.0000) lr 1.8443e-03 eta 0:01:38
epoch [11/50] batch [10/10] time 0.044 (0.153) data 0.000 (0.093) loss 0.1956 (0.4735) acc 100.0000 (86.8750) lr 1.8090e-03 eta 0:00:59
epoch [12/50] batch [5/10] time 0.061 (0.247) data 0.000 (0.185) loss 0.7056 (0.5033) acc 84.3750 (89.3750) lr 1.8090e-03 eta 0:01:35
epoch [12/50] batch [10/10] time 0.043 (0.153) data 0.000 (0.093) loss 0.2406 (0.4851) acc 100.0000 (87.8125) lr 1.7705e-03 eta 0:00:57
epoch [13/50] batch [5/10] time 0.061 (0.253) data 0.000 (0.187) loss 0.2798 (0.3640) acc 93.7500 (90.0000) lr 1.7705e-03 eta 0:01:34
epoch [13/50] batch [10/10] time 0.043 (0.155) data 0.000 (0.094) loss 0.4014 (0.4440) acc 87.5000 (88.1250) lr 1.7290e-03 eta 0:00:57
epoch [14/50] batch [5/10] time 0.060 (0.250) data 0.000 (0.185) loss 0.3359 (0.4786) acc 96.8750 (88.1250) lr 1.7290e-03 eta 0:01:31
epoch [14/50] batch [10/10] time 0.043 (0.154) data 0.000 (0.093) loss 0.2037 (0.4015) acc 93.7500 (90.3125) lr 1.6845e-03 eta 0:00:55
epoch [15/50] batch [5/10] time 0.060 (0.260) data 0.000 (0.199) loss 0.4128 (0.5044) acc 84.3750 (85.6250) lr 1.6845e-03 eta 0:01:32
epoch [15/50] batch [10/10] time 0.047 (0.159) data 0.000 (0.099) loss 0.6191 (0.4719) acc 81.2500 (86.2500) lr 1.6374e-03 eta 0:00:55
epoch [16/50] batch [5/10] time 0.061 (0.258) data 0.000 (0.194) loss 0.3845 (0.3668) acc 93.7500 (91.2500) lr 1.6374e-03 eta 0:01:28
epoch [16/50] batch [10/10] time 0.044 (0.158) data 0.000 (0.097) loss 0.7280 (0.4367) acc 75.0000 (88.7500) lr 1.5878e-03 eta 0:00:53
epoch [17/50] batch [5/10] time 0.062 (0.248) data 0.000 (0.187) loss 0.2983 (0.3922) acc 90.6250 (89.3750) lr 1.5878e-03 eta 0:01:23
epoch [17/50] batch [10/10] time 0.044 (0.153) data 0.000 (0.094) loss 0.3647 (0.3951) acc 87.5000 (89.6875) lr 1.5358e-03 eta 0:00:50
epoch [18/50] batch [5/10] time 0.062 (0.250) data 0.000 (0.184) loss 0.5059 (0.5058) acc 90.6250 (88.1250) lr 1.5358e-03 eta 0:01:21
epoch [18/50] batch [10/10] time 0.044 (0.154) data 0.000 (0.092) loss 0.2155 (0.4391) acc 93.7500 (90.0000) lr 1.4818e-03 eta 0:00:49
epoch [19/50] batch [5/10] time 0.061 (0.252) data 0.000 (0.190) loss 0.4365 (0.4237) acc 90.6250 (91.2500) lr 1.4818e-03 eta 0:01:19
epoch [19/50] batch [10/10] time 0.043 (0.155) data 0.000 (0.095) loss 0.4373 (0.3811) acc 87.5000 (91.5625) lr 1.4258e-03 eta 0:00:48
epoch [20/50] batch [5/10] time 0.059 (0.260) data 0.000 (0.197) loss 0.4910 (0.4011) acc 87.5000 (91.8750) lr 1.4258e-03 eta 0:01:19
epoch [20/50] batch [10/10] time 0.048 (0.158) data 0.000 (0.099) loss 0.3640 (0.3935) acc 87.5000 (92.5000) lr 1.3681e-03 eta 0:00:47
epoch [21/50] batch [5/10] time 0.063 (0.248) data 0.000 (0.185) loss 0.3484 (0.3677) acc 93.7500 (92.5000) lr 1.3681e-03 eta 0:01:13
epoch [21/50] batch [10/10] time 0.049 (0.155) data 0.000 (0.092) loss 0.8438 (0.4429) acc 81.2500 (90.3125) lr 1.3090e-03 eta 0:00:44
epoch [22/50] batch [5/10] time 0.061 (0.234) data 0.000 (0.168) loss 0.3669 (0.3999) acc 93.7500 (89.3750) lr 1.3090e-03 eta 0:01:06
epoch [22/50] batch [10/10] time 0.043 (0.146) data 0.000 (0.084) loss 0.5039 (0.4196) acc 87.5000 (89.6875) lr 1.2487e-03 eta 0:00:40
epoch [23/50] batch [5/10] time 0.061 (0.240) data 0.000 (0.176) loss 0.6089 (0.4487) acc 84.3750 (88.1250) lr 1.2487e-03 eta 0:01:06
epoch [23/50] batch [10/10] time 0.045 (0.149) data 0.000 (0.088) loss 0.6494 (0.4571) acc 75.0000 (87.1875) lr 1.1874e-03 eta 0:00:40
epoch [24/50] batch [5/10] time 0.062 (0.245) data 0.000 (0.180) loss 0.4531 (0.4043) acc 84.3750 (89.3750) lr 1.1874e-03 eta 0:01:04
epoch [24/50] batch [10/10] time 0.045 (0.151) data 0.000 (0.090) loss 0.3955 (0.3886) acc 87.5000 (89.6875) lr 1.1253e-03 eta 0:00:39
epoch [25/50] batch [5/10] time 0.060 (0.256) data 0.000 (0.192) loss 0.1565 (0.3815) acc 100.0000 (91.8750) lr 1.1253e-03 eta 0:01:05
epoch [25/50] batch [10/10] time 0.045 (0.157) data 0.000 (0.096) loss 0.2482 (0.3588) acc 100.0000 (92.1875) lr 1.0628e-03 eta 0:00:39
epoch [26/50] batch [5/10] time 0.061 (0.248) data 0.000 (0.187) loss 0.5244 (0.3520) acc 84.3750 (91.8750) lr 1.0628e-03 eta 0:01:00
epoch [26/50] batch [10/10] time 0.043 (0.153) data 0.000 (0.094) loss 0.2308 (0.2937) acc 93.7500 (94.0625) lr 1.0000e-03 eta 0:00:36
epoch [27/50] batch [5/10] time 0.064 (0.256) data 0.000 (0.194) loss 0.4434 (0.3303) acc 90.6250 (91.2500) lr 1.0000e-03 eta 0:01:00
epoch [27/50] batch [10/10] time 0.049 (0.158) data 0.000 (0.097) loss 0.1846 (0.3055) acc 93.7500 (92.5000) lr 9.3721e-04 eta 0:00:36
epoch [28/50] batch [5/10] time 0.061 (0.238) data 0.000 (0.175) loss 0.4919 (0.4851) acc 81.2500 (85.6250) lr 9.3721e-04 eta 0:00:53
epoch [28/50] batch [10/10] time 0.046 (0.148) data 0.000 (0.088) loss 0.3237 (0.4099) acc 93.7500 (89.0625) lr 8.7467e-04 eta 0:00:32
epoch [29/50] batch [5/10] time 0.061 (0.269) data 0.000 (0.207) loss 0.2310 (0.2883) acc 96.8750 (93.7500) lr 8.7467e-04 eta 0:00:57
epoch [29/50] batch [10/10] time 0.043 (0.163) data 0.000 (0.104) loss 0.7793 (0.3737) acc 62.5000 (89.0625) lr 8.1262e-04 eta 0:00:34
epoch [30/50] batch [5/10] time 0.060 (0.248) data 0.000 (0.187) loss 0.3374 (0.4127) acc 93.7500 (88.1250) lr 8.1262e-04 eta 0:00:50
epoch [30/50] batch [10/10] time 0.044 (0.153) data 0.000 (0.094) loss 0.2524 (0.3623) acc 100.0000 (91.8750) lr 7.5131e-04 eta 0:00:30
epoch [31/50] batch [5/10] time 0.061 (0.256) data 0.000 (0.193) loss 0.2607 (0.3456) acc 93.7500 (93.1250) lr 7.5131e-04 eta 0:00:49
epoch [31/50] batch [10/10] time 0.043 (0.157) data 0.000 (0.097) loss 0.4795 (0.3113) acc 81.2500 (93.4375) lr 6.9098e-04 eta 0:00:29
epoch [32/50] batch [5/10] time 0.061 (0.254) data 0.000 (0.193) loss 0.2986 (0.2704) acc 96.8750 (95.0000) lr 6.9098e-04 eta 0:00:46
epoch [32/50] batch [10/10] time 0.044 (0.156) data 0.000 (0.096) loss 0.3333 (0.3056) acc 93.7500 (93.4375) lr 6.3188e-04 eta 0:00:28
epoch [33/50] batch [5/10] time 0.062 (0.239) data 0.000 (0.178) loss 0.3625 (0.2899) acc 93.7500 (94.3750) lr 6.3188e-04 eta 0:00:41
epoch [33/50] batch [10/10] time 0.044 (0.149) data 0.000 (0.089) loss 0.7720 (0.3812) acc 75.0000 (89.3750) lr 5.7422e-04 eta 0:00:25
epoch [34/50] batch [5/10] time 0.057 (0.246) data 0.000 (0.184) loss 0.4189 (0.4275) acc 90.6250 (91.2500) lr 5.7422e-04 eta 0:00:40
epoch [34/50] batch [10/10] time 0.047 (0.152) data 0.000 (0.092) loss 0.2478 (0.3653) acc 100.0000 (93.1250) lr 5.1825e-04 eta 0:00:24
epoch [35/50] batch [5/10] time 0.062 (0.259) data 0.000 (0.196) loss 0.2957 (0.3374) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:00:40
epoch [35/50] batch [10/10] time 0.044 (0.158) data 0.000 (0.098) loss 0.2886 (0.3407) acc 93.7500 (92.8125) lr 4.6417e-04 eta 0:00:23
epoch [36/50] batch [5/10] time 0.061 (0.250) data 0.000 (0.189) loss 0.4910 (0.3189) acc 90.6250 (92.5000) lr 4.6417e-04 eta 0:00:36
epoch [36/50] batch [10/10] time 0.044 (0.154) data 0.000 (0.095) loss 0.1796 (0.3668) acc 93.7500 (90.6250) lr 4.1221e-04 eta 0:00:21
epoch [37/50] batch [5/10] time 0.059 (0.249) data 0.000 (0.188) loss 0.3728 (0.3878) acc 90.6250 (89.3750) lr 4.1221e-04 eta 0:00:33
epoch [37/50] batch [10/10] time 0.043 (0.153) data 0.000 (0.094) loss 0.3281 (0.3561) acc 93.7500 (91.5625) lr 3.6258e-04 eta 0:00:19
epoch [38/50] batch [5/10] time 0.081 (0.245) data 0.000 (0.180) loss 0.3809 (0.3877) acc 90.6250 (91.2500) lr 3.6258e-04 eta 0:00:30
epoch [38/50] batch [10/10] time 0.041 (0.151) data 0.000 (0.090) loss 0.1895 (0.3685) acc 93.7500 (91.2500) lr 3.1545e-04 eta 0:00:18
epoch [39/50] batch [5/10] time 0.061 (0.261) data 0.000 (0.199) loss 0.3223 (0.4158) acc 93.7500 (88.1250) lr 3.1545e-04 eta 0:00:29
epoch [39/50] batch [10/10] time 0.045 (0.159) data 0.000 (0.100) loss 0.2612 (0.3569) acc 100.0000 (92.1875) lr 2.7103e-04 eta 0:00:17
epoch [40/50] batch [5/10] time 0.061 (0.250) data 0.000 (0.189) loss 0.3130 (0.3077) acc 93.7500 (93.1250) lr 2.7103e-04 eta 0:00:26
epoch [40/50] batch [10/10] time 0.046 (0.154) data 0.000 (0.095) loss 0.2047 (0.3838) acc 100.0000 (90.9375) lr 2.2949e-04 eta 0:00:15
epoch [41/50] batch [5/10] time 0.060 (0.245) data 0.000 (0.181) loss 0.3284 (0.3416) acc 93.7500 (92.5000) lr 2.2949e-04 eta 0:00:23
epoch [41/50] batch [10/10] time 0.043 (0.152) data 0.000 (0.091) loss 0.2588 (0.3607) acc 93.7500 (91.5625) lr 1.9098e-04 eta 0:00:13
epoch [42/50] batch [5/10] time 0.061 (0.238) data 0.000 (0.176) loss 0.2651 (0.3019) acc 93.7500 (93.1250) lr 1.9098e-04 eta 0:00:20
epoch [42/50] batch [10/10] time 0.044 (0.147) data 0.000 (0.088) loss 0.4783 (0.3119) acc 93.7500 (92.1875) lr 1.5567e-04 eta 0:00:11
epoch [43/50] batch [5/10] time 0.062 (0.239) data 0.000 (0.178) loss 0.4253 (0.3610) acc 87.5000 (90.6250) lr 1.5567e-04 eta 0:00:17
epoch [43/50] batch [10/10] time 0.046 (0.148) data 0.000 (0.089) loss 0.3130 (0.3557) acc 93.7500 (91.5625) lr 1.2369e-04 eta 0:00:10
epoch [44/50] batch [5/10] time 0.056 (0.255) data 0.000 (0.196) loss 0.2673 (0.3399) acc 96.8750 (92.5000) lr 1.2369e-04 eta 0:00:16
epoch [44/50] batch [10/10] time 0.047 (0.156) data 0.000 (0.098) loss 0.1730 (0.3679) acc 100.0000 (92.1875) lr 9.5173e-05 eta 0:00:09
epoch [45/50] batch [5/10] time 0.063 (0.249) data 0.000 (0.186) loss 0.3303 (0.4042) acc 90.6250 (89.3750) lr 9.5173e-05 eta 0:00:13
epoch [45/50] batch [10/10] time 0.045 (0.153) data 0.000 (0.093) loss 0.1846 (0.3490) acc 100.0000 (91.2500) lr 7.0224e-05 eta 0:00:07
epoch [46/50] batch [5/10] time 0.061 (0.247) data 0.000 (0.185) loss 0.1494 (0.2790) acc 100.0000 (95.0000) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [10/10] time 0.045 (0.153) data 0.000 (0.093) loss 0.1943 (0.2686) acc 100.0000 (95.3125) lr 4.8943e-05 eta 0:00:06
epoch [47/50] batch [5/10] time 0.062 (0.237) data 0.000 (0.175) loss 0.3000 (0.3224) acc 93.7500 (92.5000) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [10/10] time 0.045 (0.147) data 0.000 (0.087) loss 0.4685 (0.3328) acc 87.5000 (92.5000) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/10] time 0.063 (0.253) data 0.000 (0.188) loss 0.2294 (0.3301) acc 100.0000 (91.8750) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [10/10] time 0.045 (0.156) data 0.000 (0.094) loss 0.4368 (0.3156) acc 87.5000 (92.8125) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [5/10] time 0.061 (0.252) data 0.000 (0.190) loss 0.3020 (0.2787) acc 90.6250 (95.6250) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [10/10] time 0.045 (0.155) data 0.000 (0.095) loss 0.2079 (0.2849) acc 100.0000 (95.3125) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/10] time 0.058 (0.247) data 0.000 (0.184) loss 0.3706 (0.3299) acc 90.6250 (91.8750) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.044 (0.153) data 0.000 (0.092) loss 0.0962 (0.3291) acc 100.0000 (92.5000) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.23s/it] 50%|█████     | 2/4 [00:05<00:04,  2.48s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.61s/it]100%|██████████| 4/4 [00:06<00:00,  1.17s/it]100%|██████████| 4/4 [00:06<00:00,  1.75s/it]
=> result
* total: 1,881
* correct: 1,780
* accuracy: 94.6%
* error: 5.4%
* macro_f1: 94.6%
Elapsed: 0:01:30
Run this job and save the output to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/10] time 0.063 (0.273) data 0.000 (0.200) loss 2.0137 (1.9242) acc 71.8750 (74.3750) lr 1.0000e-05 eta 0:02:14
epoch [1/50] batch [10/10] time 0.067 (0.167) data 0.000 (0.100) loss 2.0547 (1.9116) acc 62.5000 (74.3750) lr 2.0000e-03 eta 0:01:22
epoch [2/50] batch [5/10] time 0.058 (0.212) data 0.000 (0.148) loss 1.1270 (1.2043) acc 87.5000 (84.3750) lr 2.0000e-03 eta 0:01:42
epoch [2/50] batch [10/10] time 0.042 (0.135) data 0.000 (0.074) loss 0.6904 (1.0162) acc 87.5000 (82.5000) lr 1.9980e-03 eta 0:01:04
epoch [3/50] batch [5/10] time 0.063 (0.221) data 0.000 (0.160) loss 0.7593 (0.7929) acc 87.5000 (83.1250) lr 1.9980e-03 eta 0:01:45
epoch [3/50] batch [10/10] time 0.044 (0.140) data 0.000 (0.080) loss 1.0459 (0.7799) acc 81.2500 (83.7500) lr 1.9921e-03 eta 0:01:05
epoch [4/50] batch [5/10] time 0.061 (0.208) data 0.000 (0.146) loss 0.4612 (0.6401) acc 93.7500 (84.3750) lr 1.9921e-03 eta 0:01:36
epoch [4/50] batch [10/10] time 0.043 (0.132) data 0.000 (0.073) loss 0.8350 (0.6613) acc 75.0000 (82.8125) lr 1.9823e-03 eta 0:01:00
epoch [5/50] batch [5/10] time 0.061 (0.214) data 0.000 (0.149) loss 0.4280 (0.4692) acc 90.6250 (88.7500) lr 1.9823e-03 eta 0:01:37
epoch [5/50] batch [10/10] time 0.046 (0.136) data 0.000 (0.074) loss 0.3037 (0.5111) acc 93.7500 (85.9375) lr 1.9686e-03 eta 0:01:01
epoch [6/50] batch [5/10] time 0.061 (0.216) data 0.000 (0.150) loss 0.3818 (0.5771) acc 90.6250 (83.7500) lr 1.9686e-03 eta 0:01:36
epoch [6/50] batch [10/10] time 0.046 (0.137) data 0.000 (0.075) loss 0.6855 (0.5175) acc 75.0000 (85.3125) lr 1.9511e-03 eta 0:01:00
epoch [7/50] batch [5/10] time 0.060 (0.221) data 0.000 (0.159) loss 0.5171 (0.6045) acc 93.7500 (81.2500) lr 1.9511e-03 eta 0:01:36
epoch [7/50] batch [10/10] time 0.043 (0.139) data 0.000 (0.080) loss 0.8174 (0.5979) acc 75.0000 (83.1250) lr 1.9298e-03 eta 0:00:59
epoch [8/50] batch [5/10] time 0.055 (0.205) data 0.000 (0.146) loss 0.4258 (0.4444) acc 87.5000 (90.0000) lr 1.9298e-03 eta 0:01:27
epoch [8/50] batch [10/10] time 0.039 (0.129) data 0.000 (0.073) loss 0.3247 (0.4562) acc 93.7500 (89.3750) lr 1.9048e-03 eta 0:00:54
epoch [9/50] batch [5/10] time 0.063 (0.224) data 0.000 (0.160) loss 0.5117 (0.5169) acc 84.3750 (83.1250) lr 1.9048e-03 eta 0:01:32
epoch [9/50] batch [10/10] time 0.044 (0.141) data 0.000 (0.080) loss 0.4043 (0.4530) acc 87.5000 (86.2500) lr 1.8763e-03 eta 0:00:57
epoch [10/50] batch [5/10] time 0.055 (0.209) data 0.000 (0.149) loss 0.6567 (0.5070) acc 81.2500 (87.5000) lr 1.8763e-03 eta 0:01:24
epoch [10/50] batch [10/10] time 0.038 (0.130) data 0.000 (0.075) loss 0.5215 (0.4867) acc 93.7500 (88.1250) lr 1.8443e-03 eta 0:00:52
epoch [11/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.153) loss 0.3069 (0.3715) acc 93.7500 (91.2500) lr 1.8443e-03 eta 0:01:25
epoch [11/50] batch [10/10] time 0.044 (0.137) data 0.000 (0.077) loss 0.5562 (0.4557) acc 87.5000 (89.0625) lr 1.8090e-03 eta 0:00:53
epoch [12/50] batch [5/10] time 0.061 (0.215) data 0.000 (0.152) loss 0.6846 (0.5390) acc 87.5000 (86.8750) lr 1.8090e-03 eta 0:01:22
epoch [12/50] batch [10/10] time 0.039 (0.136) data 0.000 (0.076) loss 0.6265 (0.5238) acc 75.0000 (85.0000) lr 1.7705e-03 eta 0:00:51
epoch [13/50] batch [5/10] time 0.061 (0.226) data 0.000 (0.165) loss 0.3667 (0.4642) acc 90.6250 (88.1250) lr 1.7705e-03 eta 0:01:24
epoch [13/50] batch [10/10] time 0.042 (0.142) data 0.000 (0.083) loss 0.2974 (0.4440) acc 93.7500 (86.8750) lr 1.7290e-03 eta 0:00:52
epoch [14/50] batch [5/10] time 0.060 (0.206) data 0.000 (0.144) loss 0.5264 (0.4536) acc 81.2500 (88.7500) lr 1.7290e-03 eta 0:01:15
epoch [14/50] batch [10/10] time 0.044 (0.131) data 0.000 (0.072) loss 0.3735 (0.4368) acc 87.5000 (90.0000) lr 1.6845e-03 eta 0:00:47
epoch [15/50] batch [5/10] time 0.061 (0.237) data 0.000 (0.176) loss 0.3638 (0.4540) acc 90.6250 (88.7500) lr 1.6845e-03 eta 0:01:24
epoch [15/50] batch [10/10] time 0.042 (0.148) data 0.000 (0.088) loss 0.3130 (0.4125) acc 93.7500 (90.3125) lr 1.6374e-03 eta 0:00:51
epoch [16/50] batch [5/10] time 0.061 (0.220) data 0.000 (0.157) loss 0.3635 (0.3362) acc 90.6250 (91.8750) lr 1.6374e-03 eta 0:01:15
epoch [16/50] batch [10/10] time 0.044 (0.139) data 0.000 (0.079) loss 0.3555 (0.4002) acc 81.2500 (88.1250) lr 1.5878e-03 eta 0:00:47
epoch [17/50] batch [5/10] time 0.060 (0.208) data 0.000 (0.144) loss 0.6157 (0.4826) acc 78.1250 (87.5000) lr 1.5878e-03 eta 0:01:09
epoch [17/50] batch [10/10] time 0.044 (0.133) data 0.000 (0.072) loss 0.4956 (0.4562) acc 81.2500 (88.1250) lr 1.5358e-03 eta 0:00:43
epoch [18/50] batch [5/10] time 0.061 (0.207) data 0.000 (0.143) loss 0.4585 (0.4250) acc 90.6250 (91.8750) lr 1.5358e-03 eta 0:01:07
epoch [18/50] batch [10/10] time 0.044 (0.133) data 0.000 (0.072) loss 0.3201 (0.4476) acc 93.7500 (89.3750) lr 1.4818e-03 eta 0:00:42
epoch [19/50] batch [5/10] time 0.060 (0.215) data 0.000 (0.153) loss 0.3511 (0.4354) acc 93.7500 (88.1250) lr 1.4818e-03 eta 0:01:07
epoch [19/50] batch [10/10] time 0.041 (0.136) data 0.000 (0.077) loss 0.9995 (0.5430) acc 81.2500 (86.2500) lr 1.4258e-03 eta 0:00:42
epoch [20/50] batch [5/10] time 0.060 (0.216) data 0.000 (0.155) loss 0.3584 (0.4635) acc 81.2500 (87.5000) lr 1.4258e-03 eta 0:01:05
epoch [20/50] batch [10/10] time 0.043 (0.137) data 0.000 (0.077) loss 0.8472 (0.4683) acc 81.2500 (88.1250) lr 1.3681e-03 eta 0:00:41
epoch [21/50] batch [5/10] time 0.063 (0.223) data 0.000 (0.161) loss 0.5308 (0.4404) acc 84.3750 (90.6250) lr 1.3681e-03 eta 0:01:05
epoch [21/50] batch [10/10] time 0.046 (0.141) data 0.000 (0.080) loss 0.5029 (0.3897) acc 81.2500 (90.9375) lr 1.3090e-03 eta 0:00:41
epoch [22/50] batch [5/10] time 0.061 (0.243) data 0.000 (0.177) loss 0.6211 (0.4685) acc 78.1250 (88.1250) lr 1.3090e-03 eta 0:01:09
epoch [22/50] batch [10/10] time 0.044 (0.150) data 0.000 (0.088) loss 0.2150 (0.4531) acc 93.7500 (89.3750) lr 1.2487e-03 eta 0:00:42
epoch [23/50] batch [5/10] time 0.060 (0.222) data 0.000 (0.160) loss 0.7402 (0.4161) acc 81.2500 (89.3750) lr 1.2487e-03 eta 0:01:00
epoch [23/50] batch [10/10] time 0.042 (0.140) data 0.000 (0.080) loss 0.1486 (0.4097) acc 100.0000 (89.0625) lr 1.1874e-03 eta 0:00:37
epoch [24/50] batch [5/10] time 0.060 (0.218) data 0.000 (0.151) loss 0.5225 (0.4302) acc 81.2500 (88.7500) lr 1.1874e-03 eta 0:00:57
epoch [24/50] batch [10/10] time 0.044 (0.137) data 0.000 (0.076) loss 0.2306 (0.4072) acc 100.0000 (90.0000) lr 1.1253e-03 eta 0:00:35
epoch [25/50] batch [5/10] time 0.062 (0.215) data 0.000 (0.151) loss 0.3706 (0.3518) acc 87.5000 (89.3750) lr 1.1253e-03 eta 0:00:54
epoch [25/50] batch [10/10] time 0.044 (0.136) data 0.000 (0.076) loss 0.7231 (0.4038) acc 75.0000 (89.0625) lr 1.0628e-03 eta 0:00:33
epoch [26/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.154) loss 0.2194 (0.3405) acc 96.8750 (93.1250) lr 1.0628e-03 eta 0:00:53
epoch [26/50] batch [10/10] time 0.044 (0.137) data 0.000 (0.077) loss 0.3894 (0.3507) acc 87.5000 (92.1875) lr 1.0000e-03 eta 0:00:32
epoch [27/50] batch [5/10] time 0.061 (0.208) data 0.000 (0.145) loss 0.4094 (0.3675) acc 90.6250 (91.2500) lr 1.0000e-03 eta 0:00:48
epoch [27/50] batch [10/10] time 0.043 (0.132) data 0.000 (0.072) loss 0.6440 (0.4508) acc 87.5000 (89.3750) lr 9.3721e-04 eta 0:00:30
epoch [28/50] batch [5/10] time 0.061 (0.243) data 0.000 (0.182) loss 0.2429 (0.3838) acc 96.8750 (93.1250) lr 9.3721e-04 eta 0:00:54
epoch [28/50] batch [10/10] time 0.051 (0.151) data 0.000 (0.091) loss 0.5225 (0.4136) acc 81.2500 (90.3125) lr 8.7467e-04 eta 0:00:33
epoch [29/50] batch [5/10] time 0.061 (0.207) data 0.000 (0.143) loss 0.5542 (0.3810) acc 87.5000 (93.1250) lr 8.7467e-04 eta 0:00:44
epoch [29/50] batch [10/10] time 0.043 (0.132) data 0.000 (0.072) loss 0.6050 (0.4192) acc 87.5000 (90.9375) lr 8.1262e-04 eta 0:00:27
epoch [30/50] batch [5/10] time 0.061 (0.214) data 0.000 (0.151) loss 0.3088 (0.4917) acc 93.7500 (88.1250) lr 8.1262e-04 eta 0:00:43
epoch [30/50] batch [10/10] time 0.040 (0.135) data 0.000 (0.075) loss 0.1333 (0.4081) acc 100.0000 (90.0000) lr 7.5131e-04 eta 0:00:26
epoch [31/50] batch [5/10] time 0.061 (0.247) data 0.000 (0.183) loss 0.4067 (0.2924) acc 90.6250 (95.6250) lr 7.5131e-04 eta 0:00:48
epoch [31/50] batch [10/10] time 0.045 (0.153) data 0.000 (0.092) loss 0.3269 (0.4032) acc 93.7500 (92.1875) lr 6.9098e-04 eta 0:00:29
epoch [32/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.156) loss 0.2415 (0.3078) acc 96.8750 (93.7500) lr 6.9098e-04 eta 0:00:40
epoch [32/50] batch [10/10] time 0.044 (0.138) data 0.000 (0.078) loss 0.4365 (0.3672) acc 93.7500 (92.8125) lr 6.3188e-04 eta 0:00:24
epoch [33/50] batch [5/10] time 0.063 (0.245) data 0.000 (0.182) loss 0.4382 (0.4244) acc 87.5000 (88.1250) lr 6.3188e-04 eta 0:00:42
epoch [33/50] batch [10/10] time 0.044 (0.152) data 0.000 (0.091) loss 0.1973 (0.3537) acc 100.0000 (91.5625) lr 5.7422e-04 eta 0:00:25
epoch [34/50] batch [5/10] time 0.059 (0.216) data 0.000 (0.155) loss 0.4167 (0.3290) acc 90.6250 (93.7500) lr 5.7422e-04 eta 0:00:35
epoch [34/50] batch [10/10] time 0.043 (0.136) data 0.000 (0.077) loss 0.5059 (0.3784) acc 75.0000 (90.0000) lr 5.1825e-04 eta 0:00:21
epoch [35/50] batch [5/10] time 0.063 (0.216) data 0.000 (0.151) loss 0.3079 (0.3720) acc 93.7500 (89.3750) lr 5.1825e-04 eta 0:00:33
epoch [35/50] batch [10/10] time 0.044 (0.137) data 0.000 (0.076) loss 0.5884 (0.4148) acc 81.2500 (88.4375) lr 4.6417e-04 eta 0:00:20
epoch [36/50] batch [5/10] time 0.061 (0.214) data 0.000 (0.152) loss 0.5342 (0.3697) acc 87.5000 (92.5000) lr 4.6417e-04 eta 0:00:30
epoch [36/50] batch [10/10] time 0.043 (0.136) data 0.000 (0.076) loss 0.2410 (0.3097) acc 100.0000 (95.0000) lr 4.1221e-04 eta 0:00:18
epoch [37/50] batch [5/10] time 0.062 (0.221) data 0.000 (0.158) loss 0.3118 (0.3506) acc 87.5000 (91.8750) lr 4.1221e-04 eta 0:00:29
epoch [37/50] batch [10/10] time 0.043 (0.139) data 0.000 (0.079) loss 0.7305 (0.4398) acc 81.2500 (90.0000) lr 3.6258e-04 eta 0:00:18
epoch [38/50] batch [5/10] time 0.060 (0.205) data 0.000 (0.144) loss 0.3882 (0.3556) acc 87.5000 (91.8750) lr 3.6258e-04 eta 0:00:25
epoch [38/50] batch [10/10] time 0.044 (0.131) data 0.000 (0.072) loss 0.4873 (0.3995) acc 87.5000 (89.6875) lr 3.1545e-04 eta 0:00:15
epoch [39/50] batch [5/10] time 0.061 (0.242) data 0.000 (0.178) loss 0.5420 (0.4248) acc 81.2500 (88.7500) lr 3.1545e-04 eta 0:00:27
epoch [39/50] batch [10/10] time 0.044 (0.149) data 0.000 (0.089) loss 0.4456 (0.3863) acc 87.5000 (90.6250) lr 2.7103e-04 eta 0:00:16
epoch [40/50] batch [5/10] time 0.061 (0.212) data 0.000 (0.147) loss 0.2069 (0.2761) acc 96.8750 (95.6250) lr 2.7103e-04 eta 0:00:22
epoch [40/50] batch [10/10] time 0.044 (0.135) data 0.000 (0.074) loss 0.5342 (0.3464) acc 87.5000 (92.8125) lr 2.2949e-04 eta 0:00:13
epoch [41/50] batch [5/10] time 0.061 (0.212) data 0.000 (0.149) loss 0.3105 (0.2796) acc 96.8750 (95.0000) lr 2.2949e-04 eta 0:00:20
epoch [41/50] batch [10/10] time 0.043 (0.134) data 0.000 (0.075) loss 0.3511 (0.2900) acc 93.7500 (94.3750) lr 1.9098e-04 eta 0:00:12
epoch [42/50] batch [5/10] time 0.061 (0.216) data 0.000 (0.155) loss 0.3345 (0.2923) acc 90.6250 (93.7500) lr 1.9098e-04 eta 0:00:18
epoch [42/50] batch [10/10] time 0.044 (0.137) data 0.000 (0.078) loss 0.3469 (0.3243) acc 93.7500 (94.0625) lr 1.5567e-04 eta 0:00:10
epoch [43/50] batch [5/10] time 0.061 (0.223) data 0.000 (0.160) loss 0.5039 (0.4072) acc 84.3750 (89.3750) lr 1.5567e-04 eta 0:00:16
epoch [43/50] batch [10/10] time 0.045 (0.140) data 0.000 (0.080) loss 0.2847 (0.4530) acc 93.7500 (87.1875) lr 1.2369e-04 eta 0:00:09
epoch [44/50] batch [5/10] time 0.060 (0.217) data 0.000 (0.155) loss 0.2467 (0.4005) acc 93.7500 (89.3750) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [10/10] time 0.045 (0.137) data 0.000 (0.078) loss 0.2759 (0.3966) acc 100.0000 (90.9375) lr 9.5173e-05 eta 0:00:08
epoch [45/50] batch [5/10] time 0.061 (0.212) data 0.000 (0.147) loss 0.3450 (0.4261) acc 87.5000 (91.2500) lr 9.5173e-05 eta 0:00:11
epoch [45/50] batch [10/10] time 0.044 (0.135) data 0.000 (0.074) loss 0.3171 (0.3620) acc 93.7500 (92.5000) lr 7.0224e-05 eta 0:00:06
epoch [46/50] batch [5/10] time 0.060 (0.221) data 0.000 (0.161) loss 0.2373 (0.3979) acc 93.7500 (89.3750) lr 7.0224e-05 eta 0:00:09
epoch [46/50] batch [10/10] time 0.043 (0.139) data 0.000 (0.080) loss 0.5537 (0.4179) acc 87.5000 (88.4375) lr 4.8943e-05 eta 0:00:05
epoch [47/50] batch [5/10] time 0.062 (0.227) data 0.000 (0.163) loss 0.5034 (0.4349) acc 84.3750 (88.7500) lr 4.8943e-05 eta 0:00:07
epoch [47/50] batch [10/10] time 0.045 (0.143) data 0.000 (0.082) loss 0.6255 (0.4406) acc 81.2500 (89.0625) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/10] time 0.060 (0.226) data 0.000 (0.165) loss 0.1475 (0.2382) acc 100.0000 (96.2500) lr 3.1417e-05 eta 0:00:05
epoch [48/50] batch [10/10] time 0.043 (0.141) data 0.000 (0.083) loss 0.3594 (0.3077) acc 87.5000 (93.4375) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [5/10] time 0.061 (0.217) data 0.000 (0.155) loss 0.4402 (0.3834) acc 84.3750 (88.7500) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [10/10] time 0.043 (0.137) data 0.000 (0.078) loss 0.4395 (0.3618) acc 93.7500 (90.6250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/10] time 0.062 (0.215) data 0.000 (0.154) loss 0.7266 (0.3887) acc 78.1250 (90.0000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.044 (0.136) data 0.000 (0.077) loss 0.3792 (0.3881) acc 93.7500 (90.3125) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:04<00:14,  4.99s/it] 50%|█████     | 2/4 [00:05<00:04,  2.39s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.55s/it]100%|██████████| 4/4 [00:06<00:00,  1.14s/it]100%|██████████| 4/4 [00:06<00:00,  1.69s/it]
=> result
* total: 1,881
* correct: 1,780
* accuracy: 94.6%
* error: 5.4%
* macro_f1: 94.6%
Elapsed: 0:01:22
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:08<00:24,  8.06s/it] 50%|█████     | 2/4 [00:08<00:07,  3.65s/it] 75%|███████▌  | 3/4 [00:09<00:02,  2.24s/it]100%|██████████| 4/4 [00:09<00:00,  1.51s/it]100%|██████████| 4/4 [00:09<00:00,  2.42s/it]
=> result
* total: 1,788
* correct: 1,735
* accuracy: 97.0%
* error: 3.0%
* macro_f1: 97.1%
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.05s/it] 50%|█████     | 2/4 [00:05<00:04,  2.41s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.57s/it]100%|██████████| 4/4 [00:06<00:00,  1.10s/it]100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
=> result
* total: 1,788
* correct: 1,739
* accuracy: 97.3%
* error: 2.7%
* macro_f1: 97.3%
Run this job and save the output to output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:04<00:14,  4.97s/it] 50%|█████     | 2/4 [00:05<00:04,  2.38s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.55s/it]100%|██████████| 4/4 [00:06<00:00,  1.09s/it]100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
=> result
* total: 1,788
* correct: 1,740
* accuracy: 97.3%
* error: 2.7%
* macro_f1: 97.3%
Run this job and save the output to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/49] time 0.092 (0.438) data 0.000 (0.325) loss 3.2656 (3.0996) acc 43.7500 (51.2500) lr 1.0000e-05 eta 0:17:51
epoch [1/50] batch [10/49] time 0.092 (0.265) data 0.000 (0.163) loss 3.2031 (3.1354) acc 46.8750 (49.6875) lr 1.0000e-05 eta 0:10:46
epoch [1/50] batch [15/49] time 0.095 (0.208) data 0.000 (0.109) loss 3.2969 (3.1660) acc 37.5000 (47.0833) lr 1.0000e-05 eta 0:08:26
epoch [1/50] batch [20/49] time 0.092 (0.179) data 0.000 (0.082) loss 3.0703 (3.1454) acc 50.0000 (46.5625) lr 1.0000e-05 eta 0:07:14
epoch [1/50] batch [25/49] time 0.092 (0.162) data 0.000 (0.065) loss 3.0938 (3.1329) acc 50.0000 (46.2500) lr 1.0000e-05 eta 0:06:32
epoch [1/50] batch [30/49] time 0.105 (0.151) data 0.000 (0.055) loss 3.2344 (3.1183) acc 37.5000 (45.9375) lr 1.0000e-05 eta 0:06:05
epoch [1/50] batch [35/49] time 0.092 (0.143) data 0.000 (0.047) loss 3.0020 (3.1017) acc 43.7500 (45.0893) lr 1.0000e-05 eta 0:05:44
epoch [1/50] batch [40/49] time 0.092 (0.137) data 0.000 (0.041) loss 2.7461 (3.0456) acc 46.8750 (46.4062) lr 1.0000e-05 eta 0:05:29
epoch [1/50] batch [45/49] time 0.092 (0.132) data 0.000 (0.036) loss 2.9199 (3.0188) acc 53.1250 (46.8056) lr 1.0000e-05 eta 0:05:16
epoch [2/50] batch [5/49] time 0.092 (0.301) data 0.000 (0.208) loss 2.0547 (2.0338) acc 53.1250 (55.0000) lr 2.0000e-03 eta 0:12:00
epoch [2/50] batch [10/49] time 0.092 (0.198) data 0.000 (0.104) loss 1.8096 (1.9140) acc 46.8750 (55.6250) lr 2.0000e-03 eta 0:07:54
epoch [2/50] batch [15/49] time 0.092 (0.163) data 0.000 (0.070) loss 1.9326 (1.9581) acc 43.7500 (53.5417) lr 2.0000e-03 eta 0:06:29
epoch [2/50] batch [20/49] time 0.092 (0.145) data 0.000 (0.052) loss 1.5635 (1.8721) acc 59.3750 (55.3125) lr 2.0000e-03 eta 0:05:46
epoch [2/50] batch [25/49] time 0.092 (0.135) data 0.000 (0.042) loss 1.4951 (1.8476) acc 59.3750 (54.7500) lr 2.0000e-03 eta 0:05:20
epoch [2/50] batch [30/49] time 0.093 (0.128) data 0.001 (0.035) loss 2.0137 (1.8316) acc 53.1250 (54.2708) lr 2.0000e-03 eta 0:05:03
epoch [2/50] batch [35/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.1777 (1.7953) acc 62.5000 (53.6607) lr 2.0000e-03 eta 0:04:50
epoch [2/50] batch [40/49] time 0.091 (0.119) data 0.000 (0.026) loss 1.3672 (1.7800) acc 56.2500 (53.2031) lr 2.0000e-03 eta 0:04:40
epoch [2/50] batch [45/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.5049 (1.7637) acc 68.7500 (52.9167) lr 2.0000e-03 eta 0:04:33
epoch [3/50] batch [5/49] time 0.092 (0.298) data 0.000 (0.205) loss 1.8203 (1.5475) acc 50.0000 (56.8750) lr 1.9980e-03 eta 0:11:39
epoch [3/50] batch [10/49] time 0.093 (0.195) data 0.000 (0.103) loss 2.1758 (1.6023) acc 21.8750 (52.1875) lr 1.9980e-03 eta 0:07:37
epoch [3/50] batch [15/49] time 0.093 (0.161) data 0.001 (0.069) loss 1.4355 (1.6053) acc 56.2500 (51.8750) lr 1.9980e-03 eta 0:06:16
epoch [3/50] batch [20/49] time 0.092 (0.144) data 0.000 (0.052) loss 1.5791 (1.6016) acc 59.3750 (52.1875) lr 1.9980e-03 eta 0:05:35
epoch [3/50] batch [25/49] time 0.092 (0.134) data 0.000 (0.041) loss 1.3496 (1.5681) acc 53.1250 (53.3750) lr 1.9980e-03 eta 0:05:11
epoch [3/50] batch [30/49] time 0.092 (0.127) data 0.000 (0.034) loss 1.5586 (1.5698) acc 53.1250 (53.1250) lr 1.9980e-03 eta 0:04:54
epoch [3/50] batch [35/49] time 0.092 (0.122) data 0.000 (0.030) loss 1.5273 (1.5573) acc 56.2500 (53.4821) lr 1.9980e-03 eta 0:04:42
epoch [3/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.026) loss 1.8223 (1.5444) acc 50.0000 (54.0625) lr 1.9980e-03 eta 0:04:32
epoch [3/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.023) loss 1.8555 (1.5324) acc 43.7500 (54.0278) lr 1.9980e-03 eta 0:04:25
epoch [4/50] batch [5/49] time 0.092 (0.270) data 0.000 (0.178) loss 1.5771 (1.4854) acc 46.8750 (55.0000) lr 1.9921e-03 eta 0:10:21
epoch [4/50] batch [10/49] time 0.092 (0.181) data 0.000 (0.089) loss 1.5986 (1.4279) acc 59.3750 (57.1875) lr 1.9921e-03 eta 0:06:55
epoch [4/50] batch [15/49] time 0.092 (0.152) data 0.000 (0.060) loss 1.8750 (1.4804) acc 53.1250 (55.2083) lr 1.9921e-03 eta 0:05:47
epoch [4/50] batch [20/49] time 0.092 (0.137) data 0.000 (0.045) loss 1.7607 (1.4866) acc 53.1250 (55.6250) lr 1.9921e-03 eta 0:05:13
epoch [4/50] batch [25/49] time 0.092 (0.128) data 0.000 (0.036) loss 1.3789 (1.4919) acc 59.3750 (55.2500) lr 1.9921e-03 eta 0:04:52
epoch [4/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.030) loss 1.5859 (1.5060) acc 53.1250 (54.8958) lr 1.9921e-03 eta 0:04:37
epoch [4/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.026) loss 1.4023 (1.5258) acc 50.0000 (54.4643) lr 1.9921e-03 eta 0:04:27
epoch [4/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.023) loss 1.2744 (1.5193) acc 59.3750 (53.5938) lr 1.9921e-03 eta 0:04:19
epoch [4/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.020) loss 1.2432 (1.5123) acc 59.3750 (53.6806) lr 1.9921e-03 eta 0:04:13
epoch [5/50] batch [5/49] time 0.093 (0.284) data 0.000 (0.191) loss 1.6787 (1.5760) acc 40.6250 (51.2500) lr 1.9823e-03 eta 0:10:37
epoch [5/50] batch [10/49] time 0.093 (0.188) data 0.001 (0.096) loss 1.5146 (1.5170) acc 46.8750 (52.5000) lr 1.9823e-03 eta 0:07:02
epoch [5/50] batch [15/49] time 0.092 (0.156) data 0.000 (0.064) loss 1.3936 (1.5108) acc 50.0000 (53.1250) lr 1.9823e-03 eta 0:05:50
epoch [5/50] batch [20/49] time 0.092 (0.140) data 0.000 (0.048) loss 1.6436 (1.5337) acc 50.0000 (52.1875) lr 1.9823e-03 eta 0:05:13
epoch [5/50] batch [25/49] time 0.092 (0.131) data 0.000 (0.038) loss 1.2100 (1.4922) acc 62.5000 (54.2500) lr 1.9823e-03 eta 0:04:51
epoch [5/50] batch [30/49] time 0.092 (0.124) data 0.000 (0.032) loss 1.8770 (1.5211) acc 43.7500 (52.9167) lr 1.9823e-03 eta 0:04:36
epoch [5/50] batch [35/49] time 0.092 (0.120) data 0.000 (0.028) loss 1.7461 (1.5273) acc 46.8750 (52.6786) lr 1.9823e-03 eta 0:04:26
epoch [5/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.024) loss 1.4336 (1.5299) acc 59.3750 (52.7344) lr 1.9823e-03 eta 0:04:17
epoch [5/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.6572 (1.5364) acc 43.7500 (52.7083) lr 1.9823e-03 eta 0:04:11
epoch [6/50] batch [5/49] time 0.093 (0.295) data 0.000 (0.201) loss 1.2266 (1.4682) acc 62.5000 (55.6250) lr 1.9686e-03 eta 0:10:48
epoch [6/50] batch [10/49] time 0.092 (0.194) data 0.000 (0.101) loss 1.4248 (1.4079) acc 46.8750 (55.3125) lr 1.9686e-03 eta 0:07:05
epoch [6/50] batch [15/49] time 0.093 (0.160) data 0.000 (0.067) loss 1.7305 (1.4661) acc 53.1250 (53.9583) lr 1.9686e-03 eta 0:05:50
epoch [6/50] batch [20/49] time 0.092 (0.143) data 0.000 (0.050) loss 1.5566 (1.4479) acc 43.7500 (54.3750) lr 1.9686e-03 eta 0:05:12
epoch [6/50] batch [25/49] time 0.092 (0.133) data 0.000 (0.040) loss 1.5137 (1.4356) acc 53.1250 (54.6250) lr 1.9686e-03 eta 0:04:49
epoch [6/50] batch [30/49] time 0.095 (0.126) data 0.000 (0.034) loss 1.7617 (1.4642) acc 50.0000 (53.7500) lr 1.9686e-03 eta 0:04:34
epoch [6/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.029) loss 1.6543 (1.4682) acc 50.0000 (53.8393) lr 1.9686e-03 eta 0:04:23
epoch [6/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.5801 (1.4682) acc 43.7500 (53.9062) lr 1.9686e-03 eta 0:04:15
epoch [6/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.023) loss 1.3486 (1.4612) acc 50.0000 (54.5139) lr 1.9686e-03 eta 0:04:08
epoch [7/50] batch [5/49] time 0.096 (0.282) data 0.000 (0.187) loss 1.3096 (1.4402) acc 59.3750 (57.5000) lr 1.9511e-03 eta 0:10:06
epoch [7/50] batch [10/49] time 0.093 (0.187) data 0.000 (0.094) loss 1.3076 (1.4388) acc 59.3750 (56.8750) lr 1.9511e-03 eta 0:06:42
epoch [7/50] batch [15/49] time 0.097 (0.157) data 0.000 (0.063) loss 1.3652 (1.4697) acc 56.2500 (54.5833) lr 1.9511e-03 eta 0:05:35
epoch [7/50] batch [20/49] time 0.093 (0.141) data 0.000 (0.047) loss 1.8047 (1.4984) acc 46.8750 (53.1250) lr 1.9511e-03 eta 0:05:00
epoch [7/50] batch [25/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.4648 (1.4839) acc 62.5000 (55.3750) lr 1.9511e-03 eta 0:04:39
epoch [7/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.4902 (1.4640) acc 43.7500 (55.2083) lr 1.9511e-03 eta 0:04:25
epoch [7/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2158 (1.4698) acc 59.3750 (55.0000) lr 1.9511e-03 eta 0:04:14
epoch [7/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.0566 (1.4698) acc 68.7500 (55.0781) lr 1.9511e-03 eta 0:04:07
epoch [7/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.6719 (1.4746) acc 50.0000 (55.0000) lr 1.9511e-03 eta 0:04:01
epoch [8/50] batch [5/49] time 0.098 (0.290) data 0.000 (0.196) loss 1.7461 (1.4709) acc 65.6250 (58.7500) lr 1.9298e-03 eta 0:10:09
epoch [8/50] batch [10/49] time 0.093 (0.192) data 0.000 (0.098) loss 1.4551 (1.5680) acc 50.0000 (54.6875) lr 1.9298e-03 eta 0:06:41
epoch [8/50] batch [15/49] time 0.092 (0.159) data 0.000 (0.066) loss 0.9614 (1.4806) acc 78.1250 (57.7083) lr 1.9298e-03 eta 0:05:32
epoch [8/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.049) loss 1.4492 (1.4526) acc 56.2500 (57.6562) lr 1.9298e-03 eta 0:04:57
epoch [8/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.0820 (1.4717) acc 68.7500 (57.3750) lr 1.9298e-03 eta 0:04:36
epoch [8/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.4434 (1.4749) acc 62.5000 (56.5625) lr 1.9298e-03 eta 0:04:21
epoch [8/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.3535 (1.4653) acc 59.3750 (57.2321) lr 1.9298e-03 eta 0:04:11
epoch [8/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4580 (1.4626) acc 56.2500 (57.0312) lr 1.9298e-03 eta 0:04:03
epoch [8/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.5693 (1.4687) acc 56.2500 (56.2500) lr 1.9298e-03 eta 0:03:57
epoch [9/50] batch [5/49] time 0.093 (0.315) data 0.000 (0.222) loss 1.5410 (1.4756) acc 56.2500 (55.0000) lr 1.9048e-03 eta 0:10:47
epoch [9/50] batch [10/49] time 0.093 (0.204) data 0.000 (0.111) loss 1.0615 (1.4025) acc 68.7500 (56.8750) lr 1.9048e-03 eta 0:06:58
epoch [9/50] batch [15/49] time 0.092 (0.167) data 0.000 (0.074) loss 1.0508 (1.4069) acc 65.6250 (56.6667) lr 1.9048e-03 eta 0:05:41
epoch [9/50] batch [20/49] time 0.095 (0.149) data 0.000 (0.056) loss 1.4561 (1.4096) acc 56.2500 (58.4375) lr 1.9048e-03 eta 0:05:03
epoch [9/50] batch [25/49] time 0.092 (0.138) data 0.000 (0.045) loss 1.6445 (1.4472) acc 53.1250 (57.2500) lr 1.9048e-03 eta 0:04:39
epoch [9/50] batch [30/49] time 0.093 (0.130) data 0.000 (0.037) loss 1.1113 (1.4515) acc 65.6250 (56.2500) lr 1.9048e-03 eta 0:04:23
epoch [9/50] batch [35/49] time 0.092 (0.125) data 0.000 (0.032) loss 2.0312 (1.4682) acc 46.8750 (55.8929) lr 1.9048e-03 eta 0:04:12
epoch [9/50] batch [40/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.5566 (1.4793) acc 62.5000 (55.6250) lr 1.9048e-03 eta 0:04:03
epoch [9/50] batch [45/49] time 0.092 (0.117) data 0.000 (0.025) loss 1.5010 (1.4738) acc 53.1250 (55.9722) lr 1.9048e-03 eta 0:03:56
epoch [10/50] batch [5/49] time 0.094 (0.283) data 0.000 (0.191) loss 1.6719 (1.5131) acc 50.0000 (56.8750) lr 1.8763e-03 eta 0:09:27
epoch [10/50] batch [10/49] time 0.092 (0.188) data 0.000 (0.096) loss 1.3818 (1.3943) acc 59.3750 (60.0000) lr 1.8763e-03 eta 0:06:15
epoch [10/50] batch [15/49] time 0.094 (0.156) data 0.000 (0.064) loss 1.1123 (1.4620) acc 68.7500 (57.2917) lr 1.8763e-03 eta 0:05:11
epoch [10/50] batch [20/49] time 0.094 (0.141) data 0.000 (0.048) loss 1.9521 (1.4522) acc 46.8750 (58.5938) lr 1.8763e-03 eta 0:04:39
epoch [10/50] batch [25/49] time 0.095 (0.132) data 0.000 (0.038) loss 1.2314 (1.4391) acc 71.8750 (58.7500) lr 1.8763e-03 eta 0:04:21
epoch [10/50] batch [30/49] time 0.093 (0.125) data 0.001 (0.032) loss 1.4248 (1.3967) acc 56.2500 (59.7917) lr 1.8763e-03 eta 0:04:08
epoch [10/50] batch [35/49] time 0.094 (0.121) data 0.000 (0.028) loss 1.4668 (1.4031) acc 53.1250 (59.3750) lr 1.8763e-03 eta 0:03:58
epoch [10/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.6738 (1.3895) acc 62.5000 (60.7812) lr 1.8763e-03 eta 0:03:50
epoch [10/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.022) loss 1.3730 (1.3989) acc 65.6250 (60.0694) lr 1.8763e-03 eta 0:03:44
epoch [11/50] batch [5/49] time 0.098 (0.260) data 0.000 (0.166) loss 1.2148 (1.2623) acc 68.7500 (63.1250) lr 1.8443e-03 eta 0:08:29
epoch [11/50] batch [10/49] time 0.093 (0.177) data 0.000 (0.083) loss 1.5781 (1.3748) acc 56.2500 (59.3750) lr 1.8443e-03 eta 0:05:44
epoch [11/50] batch [15/49] time 0.094 (0.149) data 0.000 (0.056) loss 1.4189 (1.3739) acc 53.1250 (59.5833) lr 1.8443e-03 eta 0:04:49
epoch [11/50] batch [20/49] time 0.094 (0.135) data 0.000 (0.042) loss 1.4707 (1.3720) acc 56.2500 (58.9062) lr 1.8443e-03 eta 0:04:21
epoch [11/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.0938 (1.3841) acc 75.0000 (59.1250) lr 1.8443e-03 eta 0:04:04
epoch [11/50] batch [30/49] time 0.095 (0.121) data 0.000 (0.028) loss 1.1523 (1.3817) acc 56.2500 (58.8542) lr 1.8443e-03 eta 0:03:53
epoch [11/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.7031 (1.3741) acc 53.1250 (59.4643) lr 1.8443e-03 eta 0:03:44
epoch [11/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.4600 (1.3688) acc 65.6250 (59.8438) lr 1.8443e-03 eta 0:03:38
epoch [11/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.5811 (1.3869) acc 50.0000 (59.4444) lr 1.8443e-03 eta 0:03:33
epoch [12/50] batch [5/49] time 0.093 (0.304) data 0.000 (0.212) loss 1.3848 (1.4627) acc 56.2500 (56.8750) lr 1.8090e-03 eta 0:09:40
epoch [12/50] batch [10/49] time 0.093 (0.199) data 0.001 (0.106) loss 1.3330 (1.4938) acc 53.1250 (56.2500) lr 1.8090e-03 eta 0:06:18
epoch [12/50] batch [15/49] time 0.092 (0.164) data 0.000 (0.071) loss 1.2588 (1.4419) acc 53.1250 (56.8750) lr 1.8090e-03 eta 0:05:10
epoch [12/50] batch [20/49] time 0.093 (0.146) data 0.000 (0.053) loss 1.4395 (1.4077) acc 53.1250 (59.5312) lr 1.8090e-03 eta 0:04:36
epoch [12/50] batch [25/49] time 0.093 (0.136) data 0.000 (0.043) loss 1.3115 (1.4154) acc 59.3750 (59.0000) lr 1.8090e-03 eta 0:04:16
epoch [12/50] batch [30/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.3887 (1.3850) acc 53.1250 (59.2708) lr 1.8090e-03 eta 0:04:02
epoch [12/50] batch [35/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.4922 (1.3802) acc 56.2500 (59.3750) lr 1.8090e-03 eta 0:03:52
epoch [12/50] batch [40/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.6709 (1.3851) acc 56.2500 (59.3750) lr 1.8090e-03 eta 0:03:44
epoch [12/50] batch [45/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.2373 (1.3896) acc 59.3750 (59.1667) lr 1.8090e-03 eta 0:03:37
epoch [13/50] batch [5/49] time 0.093 (0.298) data 0.000 (0.205) loss 1.1553 (1.1186) acc 68.7500 (63.7500) lr 1.7705e-03 eta 0:09:13
epoch [13/50] batch [10/49] time 0.092 (0.196) data 0.000 (0.103) loss 1.1748 (1.2122) acc 68.7500 (61.5625) lr 1.7705e-03 eta 0:06:03
epoch [13/50] batch [15/49] time 0.093 (0.162) data 0.000 (0.069) loss 1.2217 (1.2546) acc 62.5000 (60.4167) lr 1.7705e-03 eta 0:04:59
epoch [13/50] batch [20/49] time 0.093 (0.145) data 0.000 (0.052) loss 1.2910 (1.2786) acc 62.5000 (60.4688) lr 1.7705e-03 eta 0:04:26
epoch [13/50] batch [25/49] time 0.092 (0.135) data 0.000 (0.041) loss 1.6523 (1.3190) acc 59.3750 (60.5000) lr 1.7705e-03 eta 0:04:07
epoch [13/50] batch [30/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.3555 (1.3051) acc 65.6250 (61.4583) lr 1.7705e-03 eta 0:03:53
epoch [13/50] batch [35/49] time 0.092 (0.123) data 0.000 (0.030) loss 1.3184 (1.3269) acc 59.3750 (61.2500) lr 1.7705e-03 eta 0:03:44
epoch [13/50] batch [40/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.6279 (1.3317) acc 53.1250 (60.9375) lr 1.7705e-03 eta 0:03:36
epoch [13/50] batch [45/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.2949 (1.3512) acc 53.1250 (60.2778) lr 1.7705e-03 eta 0:03:30
epoch [14/50] batch [5/49] time 0.099 (0.299) data 0.000 (0.204) loss 1.2412 (1.4248) acc 56.2500 (58.1250) lr 1.7290e-03 eta 0:08:59
epoch [14/50] batch [10/49] time 0.094 (0.196) data 0.000 (0.102) loss 1.2988 (1.2896) acc 62.5000 (64.0625) lr 1.7290e-03 eta 0:05:53
epoch [14/50] batch [15/49] time 0.093 (0.162) data 0.000 (0.068) loss 1.5352 (1.3090) acc 50.0000 (62.5000) lr 1.7290e-03 eta 0:04:51
epoch [14/50] batch [20/49] time 0.093 (0.145) data 0.000 (0.051) loss 1.7783 (1.2776) acc 43.7500 (62.6562) lr 1.7290e-03 eta 0:04:19
epoch [14/50] batch [25/49] time 0.095 (0.135) data 0.001 (0.041) loss 1.2393 (1.2859) acc 62.5000 (61.6250) lr 1.7290e-03 eta 0:04:00
epoch [14/50] batch [30/49] time 0.095 (0.128) data 0.000 (0.034) loss 1.1162 (1.2832) acc 65.6250 (61.5625) lr 1.7290e-03 eta 0:03:47
epoch [14/50] batch [35/49] time 0.093 (0.123) data 0.000 (0.030) loss 0.8638 (1.2753) acc 81.2500 (61.8750) lr 1.7290e-03 eta 0:03:38
epoch [14/50] batch [40/49] time 0.092 (0.119) data 0.000 (0.026) loss 1.7090 (1.2918) acc 50.0000 (62.2656) lr 1.7290e-03 eta 0:03:30
epoch [14/50] batch [45/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.3486 (1.3235) acc 62.5000 (61.3194) lr 1.7290e-03 eta 0:03:25
epoch [15/50] batch [5/49] time 0.092 (0.272) data 0.000 (0.179) loss 1.2422 (1.2070) acc 59.3750 (59.3750) lr 1.6845e-03 eta 0:07:58
epoch [15/50] batch [10/49] time 0.094 (0.183) data 0.000 (0.090) loss 1.4385 (1.3036) acc 65.6250 (59.0625) lr 1.6845e-03 eta 0:05:20
epoch [15/50] batch [15/49] time 0.093 (0.153) data 0.000 (0.060) loss 1.1260 (1.2999) acc 68.7500 (60.2083) lr 1.6845e-03 eta 0:04:27
epoch [15/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.045) loss 1.3408 (1.2976) acc 59.3750 (61.0938) lr 1.6845e-03 eta 0:04:00
epoch [15/50] batch [25/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.2471 (1.3058) acc 56.2500 (60.3750) lr 1.6845e-03 eta 0:03:44
epoch [15/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.4492 (1.3127) acc 59.3750 (60.2083) lr 1.6845e-03 eta 0:03:34
epoch [15/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.3643 (1.3360) acc 62.5000 (59.9107) lr 1.6845e-03 eta 0:03:26
epoch [15/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.0059 (1.3305) acc 75.0000 (60.3125) lr 1.6845e-03 eta 0:03:19
epoch [15/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.9541 (1.3288) acc 71.8750 (60.5556) lr 1.6845e-03 eta 0:03:15
epoch [16/50] batch [5/49] time 0.094 (0.266) data 0.001 (0.172) loss 1.8389 (1.4474) acc 46.8750 (63.7500) lr 1.6374e-03 eta 0:07:34
epoch [16/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.086) loss 1.3398 (1.3790) acc 40.6250 (59.6875) lr 1.6374e-03 eta 0:05:06
epoch [16/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.058) loss 1.2070 (1.3542) acc 71.8750 (61.0417) lr 1.6374e-03 eta 0:04:17
epoch [16/50] batch [20/49] time 0.096 (0.137) data 0.000 (0.043) loss 1.3076 (1.3536) acc 56.2500 (60.7812) lr 1.6374e-03 eta 0:03:52
epoch [16/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.4189 (1.3440) acc 53.1250 (60.5000) lr 1.6374e-03 eta 0:03:36
epoch [16/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 1.1953 (1.3202) acc 59.3750 (61.8750) lr 1.6374e-03 eta 0:03:26
epoch [16/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.5068 (1.3414) acc 65.6250 (61.4286) lr 1.6374e-03 eta 0:03:18
epoch [16/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.2803 (1.3376) acc 62.5000 (61.6406) lr 1.6374e-03 eta 0:03:12
epoch [16/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.019) loss 1.6211 (1.3484) acc 53.1250 (61.4583) lr 1.6374e-03 eta 0:03:08
epoch [17/50] batch [5/49] time 0.092 (0.256) data 0.000 (0.163) loss 1.3506 (1.1678) acc 65.6250 (71.2500) lr 1.5878e-03 eta 0:07:04
epoch [17/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.082) loss 1.3955 (1.1798) acc 50.0000 (65.9375) lr 1.5878e-03 eta 0:04:49
epoch [17/50] batch [15/49] time 0.092 (0.147) data 0.000 (0.055) loss 1.2734 (1.1854) acc 62.5000 (66.2500) lr 1.5878e-03 eta 0:04:03
epoch [17/50] batch [20/49] time 0.094 (0.134) data 0.000 (0.041) loss 1.1553 (1.1916) acc 65.6250 (66.7188) lr 1.5878e-03 eta 0:03:39
epoch [17/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.3984 (1.2184) acc 62.5000 (67.1250) lr 1.5878e-03 eta 0:03:25
epoch [17/50] batch [30/49] time 0.092 (0.120) data 0.000 (0.028) loss 1.1904 (1.2197) acc 71.8750 (67.3958) lr 1.5878e-03 eta 0:03:16
epoch [17/50] batch [35/49] time 0.092 (0.116) data 0.000 (0.024) loss 1.5361 (1.2346) acc 59.3750 (66.7857) lr 1.5878e-03 eta 0:03:09
epoch [17/50] batch [40/49] time 0.092 (0.113) data 0.000 (0.021) loss 1.3721 (1.2454) acc 56.2500 (66.4844) lr 1.5878e-03 eta 0:03:04
epoch [17/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.3398 (1.2534) acc 59.3750 (65.7639) lr 1.5878e-03 eta 0:02:59
epoch [18/50] batch [5/49] time 0.093 (0.253) data 0.000 (0.160) loss 1.2383 (1.1647) acc 65.6250 (68.1250) lr 1.5358e-03 eta 0:06:48
epoch [18/50] batch [10/49] time 0.093 (0.173) data 0.000 (0.080) loss 1.0742 (1.2033) acc 78.1250 (67.8125) lr 1.5358e-03 eta 0:04:38
epoch [18/50] batch [15/49] time 0.094 (0.146) data 0.000 (0.054) loss 1.1992 (1.2198) acc 65.6250 (66.2500) lr 1.5358e-03 eta 0:03:54
epoch [18/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.1416 (1.2141) acc 68.7500 (66.2500) lr 1.5358e-03 eta 0:03:32
epoch [18/50] batch [25/49] time 0.092 (0.125) data 0.000 (0.032) loss 1.3320 (1.2162) acc 65.6250 (66.1250) lr 1.5358e-03 eta 0:03:19
epoch [18/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2412 (1.2251) acc 68.7500 (65.7292) lr 1.5358e-03 eta 0:03:10
epoch [18/50] batch [35/49] time 0.095 (0.116) data 0.002 (0.023) loss 1.5156 (1.2279) acc 62.5000 (65.2679) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.9946 (1.2381) acc 62.5000 (64.8438) lr 1.5358e-03 eta 0:02:58
epoch [18/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.2480 (1.2526) acc 59.3750 (64.2361) lr 1.5358e-03 eta 0:02:54
epoch [19/50] batch [5/49] time 0.092 (0.242) data 0.000 (0.149) loss 1.5693 (1.1949) acc 59.3750 (68.1250) lr 1.4818e-03 eta 0:06:18
epoch [19/50] batch [10/49] time 0.092 (0.167) data 0.000 (0.074) loss 1.1191 (1.2562) acc 62.5000 (63.7500) lr 1.4818e-03 eta 0:04:20
epoch [19/50] batch [15/49] time 0.092 (0.142) data 0.000 (0.050) loss 1.1816 (1.2492) acc 59.3750 (64.3750) lr 1.4818e-03 eta 0:03:40
epoch [19/50] batch [20/49] time 0.092 (0.130) data 0.000 (0.037) loss 1.1172 (1.2148) acc 65.6250 (65.9375) lr 1.4818e-03 eta 0:03:21
epoch [19/50] batch [25/49] time 0.092 (0.122) data 0.000 (0.030) loss 1.3174 (1.2453) acc 71.8750 (64.7500) lr 1.4818e-03 eta 0:03:08
epoch [19/50] batch [30/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.2070 (1.2546) acc 65.6250 (63.8542) lr 1.4818e-03 eta 0:03:01
epoch [19/50] batch [35/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.5107 (1.2407) acc 53.1250 (64.1964) lr 1.4818e-03 eta 0:02:55
epoch [19/50] batch [40/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.0439 (1.2511) acc 75.0000 (63.6719) lr 1.4818e-03 eta 0:02:50
epoch [19/50] batch [45/49] time 0.092 (0.109) data 0.000 (0.017) loss 1.3486 (1.2485) acc 75.0000 (64.3056) lr 1.4818e-03 eta 0:02:46
epoch [20/50] batch [5/49] time 0.092 (0.242) data 0.000 (0.147) loss 0.9282 (1.1130) acc 75.0000 (69.3750) lr 1.4258e-03 eta 0:06:06
epoch [20/50] batch [10/49] time 0.092 (0.167) data 0.000 (0.074) loss 1.3018 (1.1845) acc 68.7500 (68.1250) lr 1.4258e-03 eta 0:04:12
epoch [20/50] batch [15/49] time 0.093 (0.142) data 0.000 (0.049) loss 1.3857 (1.2143) acc 68.7500 (68.1250) lr 1.4258e-03 eta 0:03:33
epoch [20/50] batch [20/49] time 0.093 (0.130) data 0.000 (0.037) loss 1.3545 (1.2014) acc 59.3750 (67.9688) lr 1.4258e-03 eta 0:03:14
epoch [20/50] batch [25/49] time 0.095 (0.123) data 0.000 (0.030) loss 1.4014 (1.2127) acc 68.7500 (67.5000) lr 1.4258e-03 eta 0:03:03
epoch [20/50] batch [30/49] time 0.096 (0.118) data 0.000 (0.025) loss 1.0020 (1.2293) acc 78.1250 (67.6042) lr 1.4258e-03 eta 0:02:55
epoch [20/50] batch [35/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.6904 (1.2481) acc 46.8750 (67.0536) lr 1.4258e-03 eta 0:02:49
epoch [20/50] batch [40/49] time 0.093 (0.111) data 0.000 (0.019) loss 1.1123 (1.2332) acc 59.3750 (67.0312) lr 1.4258e-03 eta 0:02:44
epoch [20/50] batch [45/49] time 0.093 (0.109) data 0.000 (0.017) loss 1.2031 (1.2460) acc 59.3750 (66.3194) lr 1.4258e-03 eta 0:02:41
epoch [21/50] batch [5/49] time 0.093 (0.249) data 0.000 (0.154) loss 1.0938 (1.1924) acc 75.0000 (68.7500) lr 1.3681e-03 eta 0:06:04
epoch [21/50] batch [10/49] time 0.093 (0.172) data 0.000 (0.077) loss 1.4463 (1.1942) acc 62.5000 (68.4375) lr 1.3681e-03 eta 0:04:10
epoch [21/50] batch [15/49] time 0.093 (0.146) data 0.000 (0.052) loss 1.1260 (1.1958) acc 56.2500 (67.5000) lr 1.3681e-03 eta 0:03:31
epoch [21/50] batch [20/49] time 0.093 (0.132) data 0.000 (0.039) loss 1.3984 (1.2158) acc 53.1250 (67.1875) lr 1.3681e-03 eta 0:03:11
epoch [21/50] batch [25/49] time 0.092 (0.124) data 0.000 (0.031) loss 1.5625 (1.2095) acc 65.6250 (67.5000) lr 1.3681e-03 eta 0:02:59
epoch [21/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.2549 (1.2255) acc 62.5000 (66.8750) lr 1.3681e-03 eta 0:02:51
epoch [21/50] batch [35/49] time 0.093 (0.115) data 0.001 (0.022) loss 1.6289 (1.2351) acc 65.6250 (67.5893) lr 1.3681e-03 eta 0:02:45
epoch [21/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.2295 (1.2361) acc 65.6250 (67.1094) lr 1.3681e-03 eta 0:02:41
epoch [21/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 1.0254 (1.2279) acc 75.0000 (67.2222) lr 1.3681e-03 eta 0:02:37
epoch [22/50] batch [5/49] time 0.093 (0.257) data 0.000 (0.164) loss 1.1875 (1.1973) acc 56.2500 (68.7500) lr 1.3090e-03 eta 0:06:04
epoch [22/50] batch [10/49] time 0.096 (0.176) data 0.000 (0.082) loss 1.0918 (1.1617) acc 62.5000 (68.1250) lr 1.3090e-03 eta 0:04:08
epoch [22/50] batch [15/49] time 0.092 (0.148) data 0.000 (0.055) loss 1.1445 (1.1730) acc 71.8750 (67.5000) lr 1.3090e-03 eta 0:03:28
epoch [22/50] batch [20/49] time 0.096 (0.134) data 0.000 (0.041) loss 1.0879 (1.1547) acc 68.7500 (67.6562) lr 1.3090e-03 eta 0:03:08
epoch [22/50] batch [25/49] time 0.094 (0.126) data 0.000 (0.033) loss 1.6045 (1.1980) acc 50.0000 (66.6250) lr 1.3090e-03 eta 0:02:56
epoch [22/50] batch [30/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.0000 (1.1968) acc 68.7500 (66.5625) lr 1.3090e-03 eta 0:02:48
epoch [22/50] batch [35/49] time 0.093 (0.117) data 0.001 (0.024) loss 1.6250 (1.2058) acc 56.2500 (66.0714) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.0684 (1.1976) acc 75.0000 (66.3281) lr 1.3090e-03 eta 0:02:37
epoch [22/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0791 (1.2096) acc 71.8750 (66.0417) lr 1.3090e-03 eta 0:02:33
epoch [23/50] batch [5/49] time 0.093 (0.264) data 0.000 (0.172) loss 1.2939 (1.0973) acc 68.7500 (72.5000) lr 1.2487e-03 eta 0:06:00
epoch [23/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.087) loss 1.0146 (1.0866) acc 71.8750 (70.0000) lr 1.2487e-03 eta 0:04:05
epoch [23/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.058) loss 1.3027 (1.1926) acc 68.7500 (67.7083) lr 1.2487e-03 eta 0:03:25
epoch [23/50] batch [20/49] time 0.092 (0.137) data 0.000 (0.044) loss 1.6260 (1.2060) acc 56.2500 (66.8750) lr 1.2487e-03 eta 0:03:04
epoch [23/50] batch [25/49] time 0.094 (0.128) data 0.000 (0.035) loss 1.5645 (1.2120) acc 65.6250 (67.2500) lr 1.2487e-03 eta 0:02:52
epoch [23/50] batch [30/49] time 0.093 (0.122) data 0.001 (0.029) loss 1.3018 (1.2227) acc 56.2500 (67.1875) lr 1.2487e-03 eta 0:02:44
epoch [23/50] batch [35/49] time 0.094 (0.118) data 0.000 (0.025) loss 1.1680 (1.2171) acc 68.7500 (67.4107) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 0.8926 (1.2160) acc 71.8750 (66.7188) lr 1.2487e-03 eta 0:02:33
epoch [23/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.2910 (1.2165) acc 65.6250 (66.6667) lr 1.2487e-03 eta 0:02:29
epoch [24/50] batch [5/49] time 0.096 (0.266) data 0.000 (0.171) loss 0.8267 (1.1566) acc 84.3750 (68.7500) lr 1.1874e-03 eta 0:05:50
epoch [24/50] batch [10/49] time 0.093 (0.179) data 0.000 (0.086) loss 1.2061 (1.1951) acc 59.3750 (69.0625) lr 1.1874e-03 eta 0:03:55
epoch [24/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.057) loss 1.1436 (1.1869) acc 59.3750 (67.9167) lr 1.1874e-03 eta 0:03:16
epoch [24/50] batch [20/49] time 0.092 (0.136) data 0.000 (0.043) loss 1.0449 (1.1914) acc 68.7500 (68.1250) lr 1.1874e-03 eta 0:02:57
epoch [24/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.4326 (1.1977) acc 53.1250 (68.5000) lr 1.1874e-03 eta 0:02:45
epoch [24/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.1484 (1.1787) acc 59.3750 (68.0208) lr 1.1874e-03 eta 0:02:37
epoch [24/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4062 (1.1901) acc 62.5000 (67.7679) lr 1.1874e-03 eta 0:02:31
epoch [24/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.3818 (1.2043) acc 62.5000 (67.1875) lr 1.1874e-03 eta 0:02:26
epoch [24/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.0195 (1.1975) acc 59.3750 (66.8750) lr 1.1874e-03 eta 0:02:23
epoch [25/50] batch [5/49] time 0.093 (0.250) data 0.000 (0.156) loss 1.2217 (1.3176) acc 65.6250 (61.8750) lr 1.1253e-03 eta 0:05:17
epoch [25/50] batch [10/49] time 0.092 (0.172) data 0.000 (0.078) loss 0.7358 (1.2857) acc 68.7500 (64.3750) lr 1.1253e-03 eta 0:03:37
epoch [25/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.052) loss 1.2520 (1.2340) acc 71.8750 (67.0833) lr 1.1253e-03 eta 0:03:03
epoch [25/50] batch [20/49] time 0.095 (0.133) data 0.000 (0.039) loss 1.0781 (1.2231) acc 75.0000 (67.0312) lr 1.1253e-03 eta 0:02:46
epoch [25/50] batch [25/49] time 0.092 (0.125) data 0.000 (0.032) loss 1.0957 (1.2441) acc 68.7500 (66.8750) lr 1.1253e-03 eta 0:02:36
epoch [25/50] batch [30/49] time 0.094 (0.120) data 0.000 (0.026) loss 1.0820 (1.1999) acc 68.7500 (68.2292) lr 1.1253e-03 eta 0:02:28
epoch [25/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.0430 (1.1940) acc 71.8750 (68.2143) lr 1.1253e-03 eta 0:02:23
epoch [25/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.2461 (1.2123) acc 75.0000 (67.7344) lr 1.1253e-03 eta 0:02:19
epoch [25/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.3516 (1.2402) acc 65.6250 (66.7361) lr 1.1253e-03 eta 0:02:16
epoch [26/50] batch [5/49] time 0.097 (0.246) data 0.004 (0.153) loss 1.3389 (1.1033) acc 75.0000 (74.3750) lr 1.0628e-03 eta 0:05:00
epoch [26/50] batch [10/49] time 0.093 (0.170) data 0.000 (0.077) loss 1.1758 (1.1725) acc 71.8750 (72.5000) lr 1.0628e-03 eta 0:03:26
epoch [26/50] batch [15/49] time 0.096 (0.145) data 0.000 (0.051) loss 1.2324 (1.1382) acc 62.5000 (73.7500) lr 1.0628e-03 eta 0:02:54
epoch [26/50] batch [20/49] time 0.095 (0.132) data 0.000 (0.038) loss 1.1338 (1.1222) acc 75.0000 (73.4375) lr 1.0628e-03 eta 0:02:39
epoch [26/50] batch [25/49] time 0.094 (0.124) data 0.000 (0.031) loss 1.1729 (1.1316) acc 71.8750 (72.0000) lr 1.0628e-03 eta 0:02:29
epoch [26/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 0.8213 (1.1258) acc 78.1250 (71.3542) lr 1.0628e-03 eta 0:02:22
epoch [26/50] batch [35/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.1465 (1.1446) acc 68.7500 (70.6250) lr 1.0628e-03 eta 0:02:17
epoch [26/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.019) loss 1.0830 (1.1473) acc 68.7500 (69.6875) lr 1.0628e-03 eta 0:02:13
epoch [26/50] batch [45/49] time 0.092 (0.110) data 0.000 (0.017) loss 0.8882 (1.1361) acc 78.1250 (70.0694) lr 1.0628e-03 eta 0:02:10
epoch [27/50] batch [5/49] time 0.093 (0.255) data 0.000 (0.162) loss 1.2266 (1.2095) acc 65.6250 (69.3750) lr 1.0000e-03 eta 0:04:58
epoch [27/50] batch [10/49] time 0.093 (0.174) data 0.000 (0.081) loss 1.0430 (1.1722) acc 81.2500 (72.1875) lr 1.0000e-03 eta 0:03:22
epoch [27/50] batch [15/49] time 0.092 (0.147) data 0.000 (0.054) loss 1.3369 (1.1464) acc 71.8750 (72.9167) lr 1.0000e-03 eta 0:02:50
epoch [27/50] batch [20/49] time 0.094 (0.133) data 0.000 (0.041) loss 1.1006 (1.1571) acc 68.7500 (71.8750) lr 1.0000e-03 eta 0:02:34
epoch [27/50] batch [25/49] time 0.097 (0.126) data 0.000 (0.033) loss 1.1143 (1.1777) acc 65.6250 (71.7500) lr 1.0000e-03 eta 0:02:24
epoch [27/50] batch [30/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.2334 (1.1746) acc 68.7500 (71.3542) lr 1.0000e-03 eta 0:02:17
epoch [27/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.5586 (1.1730) acc 59.3750 (71.0714) lr 1.0000e-03 eta 0:02:12
epoch [27/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.2666 (1.1672) acc 62.5000 (70.8594) lr 1.0000e-03 eta 0:02:08
epoch [27/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.2715 (1.1860) acc 59.3750 (70.6944) lr 1.0000e-03 eta 0:02:05
epoch [28/50] batch [5/49] time 0.097 (0.276) data 0.000 (0.181) loss 0.7812 (1.1477) acc 78.1250 (67.5000) lr 9.3721e-04 eta 0:05:09
epoch [28/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.091) loss 0.9854 (1.1479) acc 78.1250 (67.8125) lr 9.3721e-04 eta 0:03:26
epoch [28/50] batch [15/49] time 0.094 (0.154) data 0.000 (0.061) loss 1.3379 (1.1399) acc 65.6250 (69.3750) lr 9.3721e-04 eta 0:02:51
epoch [28/50] batch [20/49] time 0.094 (0.140) data 0.001 (0.046) loss 1.3193 (1.1615) acc 65.6250 (69.0625) lr 9.3721e-04 eta 0:02:34
epoch [28/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.037) loss 0.7104 (1.1530) acc 78.1250 (69.2500) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [30/49] time 0.098 (0.124) data 0.000 (0.031) loss 1.3984 (1.1737) acc 62.5000 (68.7500) lr 9.3721e-04 eta 0:02:16
epoch [28/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.026) loss 1.2998 (1.1627) acc 71.8750 (69.4643) lr 9.3721e-04 eta 0:02:10
epoch [28/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.0664 (1.1702) acc 68.7500 (69.2188) lr 9.3721e-04 eta 0:02:06
epoch [28/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.020) loss 1.1807 (1.1671) acc 71.8750 (69.6528) lr 9.3721e-04 eta 0:02:03
epoch [29/50] batch [5/49] time 0.092 (0.250) data 0.000 (0.158) loss 1.1836 (1.1992) acc 75.0000 (65.0000) lr 8.7467e-04 eta 0:04:28
epoch [29/50] batch [10/49] time 0.092 (0.172) data 0.000 (0.079) loss 1.0820 (1.1922) acc 65.6250 (66.5625) lr 8.7467e-04 eta 0:03:03
epoch [29/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.053) loss 1.2861 (1.2326) acc 71.8750 (67.2917) lr 8.7467e-04 eta 0:02:34
epoch [29/50] batch [20/49] time 0.092 (0.132) data 0.000 (0.040) loss 1.0879 (1.1943) acc 75.0000 (67.8125) lr 8.7467e-04 eta 0:02:20
epoch [29/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.5020 (1.1978) acc 53.1250 (67.1250) lr 8.7467e-04 eta 0:02:11
epoch [29/50] batch [30/49] time 0.094 (0.120) data 0.000 (0.027) loss 1.3330 (1.1948) acc 62.5000 (67.5000) lr 8.7467e-04 eta 0:02:05
epoch [29/50] batch [35/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.3223 (1.2054) acc 71.8750 (67.5893) lr 8.7467e-04 eta 0:02:00
epoch [29/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.1807 (1.2021) acc 59.3750 (67.5781) lr 8.7467e-04 eta 0:01:57
epoch [29/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.1533 (1.2066) acc 78.1250 (67.8472) lr 8.7467e-04 eta 0:01:54
epoch [30/50] batch [5/49] time 0.093 (0.257) data 0.000 (0.164) loss 1.1455 (1.0825) acc 75.0000 (73.7500) lr 8.1262e-04 eta 0:04:23
epoch [30/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.082) loss 1.1699 (1.1183) acc 71.8750 (71.5625) lr 8.1262e-04 eta 0:02:58
epoch [30/50] batch [15/49] time 0.093 (0.148) data 0.000 (0.055) loss 1.2588 (1.1498) acc 65.6250 (69.3750) lr 8.1262e-04 eta 0:02:29
epoch [30/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.041) loss 1.3525 (1.1528) acc 65.6250 (69.6875) lr 8.1262e-04 eta 0:02:15
epoch [30/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.1553 (1.1555) acc 68.7500 (69.8750) lr 8.1262e-04 eta 0:02:06
epoch [30/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.028) loss 0.9111 (1.1362) acc 75.0000 (70.1042) lr 8.1262e-04 eta 0:02:00
epoch [30/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.024) loss 1.3389 (1.1462) acc 62.5000 (69.6429) lr 8.1262e-04 eta 0:01:55
epoch [30/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.021) loss 1.3818 (1.1573) acc 65.6250 (69.7656) lr 8.1262e-04 eta 0:01:52
epoch [30/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.2979 (1.1613) acc 65.6250 (69.8611) lr 8.1262e-04 eta 0:01:49
epoch [31/50] batch [5/49] time 0.092 (0.246) data 0.000 (0.152) loss 1.0010 (1.0525) acc 78.1250 (75.6250) lr 7.5131e-04 eta 0:03:59
epoch [31/50] batch [10/49] time 0.093 (0.169) data 0.000 (0.076) loss 0.9629 (1.0322) acc 75.0000 (74.6875) lr 7.5131e-04 eta 0:02:44
epoch [31/50] batch [15/49] time 0.093 (0.144) data 0.000 (0.051) loss 1.5771 (1.1631) acc 59.3750 (72.7083) lr 7.5131e-04 eta 0:02:18
epoch [31/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 0.8203 (1.1300) acc 84.3750 (73.9062) lr 7.5131e-04 eta 0:02:05
epoch [31/50] batch [25/49] time 0.092 (0.124) data 0.000 (0.031) loss 1.1553 (1.1189) acc 78.1250 (74.7500) lr 7.5131e-04 eta 0:01:57
epoch [31/50] batch [30/49] time 0.092 (0.118) data 0.000 (0.026) loss 1.0137 (1.0909) acc 65.6250 (74.2708) lr 7.5131e-04 eta 0:01:52
epoch [31/50] batch [35/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.1035 (1.1040) acc 68.7500 (73.7500) lr 7.5131e-04 eta 0:01:48
epoch [31/50] batch [40/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.4053 (1.1239) acc 46.8750 (72.5781) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 1.4678 (1.1293) acc 59.3750 (72.2917) lr 7.5131e-04 eta 0:01:42
epoch [32/50] batch [5/49] time 0.094 (0.251) data 0.000 (0.157) loss 1.0693 (1.1268) acc 71.8750 (73.1250) lr 6.9098e-04 eta 0:03:52
epoch [32/50] batch [10/49] time 0.093 (0.172) data 0.001 (0.079) loss 0.9463 (1.1113) acc 75.0000 (73.1250) lr 6.9098e-04 eta 0:02:38
epoch [32/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.053) loss 1.3691 (1.0743) acc 59.3750 (72.9167) lr 6.9098e-04 eta 0:02:13
epoch [32/50] batch [20/49] time 0.093 (0.133) data 0.001 (0.040) loss 1.1406 (1.0743) acc 71.8750 (72.3438) lr 6.9098e-04 eta 0:02:00
epoch [32/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.3789 (1.1070) acc 62.5000 (71.3750) lr 6.9098e-04 eta 0:01:53
epoch [32/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 0.9233 (1.0990) acc 75.0000 (71.3542) lr 6.9098e-04 eta 0:01:47
epoch [32/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9365 (1.1002) acc 78.1250 (71.4286) lr 6.9098e-04 eta 0:01:44
epoch [32/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.1719 (1.1091) acc 71.8750 (71.0938) lr 6.9098e-04 eta 0:01:40
epoch [32/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.2344 (1.1339) acc 65.6250 (70.5556) lr 6.9098e-04 eta 0:01:38
epoch [33/50] batch [5/49] time 0.093 (0.256) data 0.000 (0.163) loss 0.8550 (0.9948) acc 81.2500 (76.8750) lr 6.3188e-04 eta 0:03:44
epoch [33/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.082) loss 1.2295 (1.0755) acc 68.7500 (73.1250) lr 6.3188e-04 eta 0:02:32
epoch [33/50] batch [15/49] time 0.093 (0.148) data 0.000 (0.055) loss 1.2373 (1.1300) acc 65.6250 (72.2917) lr 6.3188e-04 eta 0:02:08
epoch [33/50] batch [20/49] time 0.092 (0.134) data 0.000 (0.041) loss 0.6777 (1.1084) acc 84.3750 (73.5938) lr 6.3188e-04 eta 0:01:55
epoch [33/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.2480 (1.1052) acc 56.2500 (73.0000) lr 6.3188e-04 eta 0:01:47
epoch [33/50] batch [30/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.4111 (1.1250) acc 71.8750 (72.1875) lr 6.3188e-04 eta 0:01:42
epoch [33/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.024) loss 0.7793 (1.1278) acc 90.6250 (72.5893) lr 6.3188e-04 eta 0:01:38
epoch [33/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.021) loss 1.1992 (1.1360) acc 68.7500 (71.6406) lr 6.3188e-04 eta 0:01:35
epoch [33/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 0.7876 (1.1211) acc 81.2500 (72.2222) lr 6.3188e-04 eta 0:01:33
epoch [34/50] batch [5/49] time 0.093 (0.276) data 0.000 (0.183) loss 1.5020 (1.3328) acc 56.2500 (65.6250) lr 5.7422e-04 eta 0:03:48
epoch [34/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.091) loss 0.9048 (1.2358) acc 71.8750 (67.1875) lr 5.7422e-04 eta 0:02:31
epoch [34/50] batch [15/49] time 0.092 (0.154) data 0.000 (0.061) loss 1.3594 (1.1911) acc 65.6250 (69.7917) lr 5.7422e-04 eta 0:02:05
epoch [34/50] batch [20/49] time 0.093 (0.139) data 0.000 (0.046) loss 1.0586 (1.1575) acc 65.6250 (71.5625) lr 5.7422e-04 eta 0:01:52
epoch [34/50] batch [25/49] time 0.094 (0.129) data 0.000 (0.037) loss 1.1279 (1.1323) acc 75.0000 (72.0000) lr 5.7422e-04 eta 0:01:44
epoch [34/50] batch [30/49] time 0.096 (0.123) data 0.000 (0.031) loss 1.1865 (1.1426) acc 71.8750 (72.0833) lr 5.7422e-04 eta 0:01:39
epoch [34/50] batch [35/49] time 0.092 (0.119) data 0.000 (0.026) loss 0.9834 (1.1265) acc 71.8750 (72.2321) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.3213 (1.1372) acc 78.1250 (72.1875) lr 5.7422e-04 eta 0:01:31
epoch [34/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.021) loss 1.4277 (1.1352) acc 46.8750 (71.9444) lr 5.7422e-04 eta 0:01:29
epoch [35/50] batch [5/49] time 0.093 (0.289) data 0.000 (0.196) loss 0.7759 (1.0188) acc 90.6250 (78.7500) lr 5.1825e-04 eta 0:03:45
epoch [35/50] batch [10/49] time 0.092 (0.191) data 0.000 (0.098) loss 1.0918 (1.1096) acc 71.8750 (75.0000) lr 5.1825e-04 eta 0:02:27
epoch [35/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.065) loss 0.9131 (1.0902) acc 75.0000 (74.3750) lr 5.1825e-04 eta 0:02:01
epoch [35/50] batch [20/49] time 0.095 (0.142) data 0.000 (0.049) loss 0.9302 (1.0924) acc 75.0000 (73.2812) lr 5.1825e-04 eta 0:01:48
epoch [35/50] batch [25/49] time 0.092 (0.132) data 0.000 (0.040) loss 1.1152 (1.0980) acc 68.7500 (73.2500) lr 5.1825e-04 eta 0:01:40
epoch [35/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.2236 (1.0962) acc 68.7500 (73.2292) lr 5.1825e-04 eta 0:01:34
epoch [35/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.0684 (1.0982) acc 75.0000 (73.3036) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.025) loss 1.3789 (1.0995) acc 68.7500 (73.5156) lr 5.1825e-04 eta 0:01:27
epoch [35/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.3262 (1.1003) acc 62.5000 (73.4028) lr 5.1825e-04 eta 0:01:24
epoch [36/50] batch [5/49] time 0.093 (0.266) data 0.000 (0.173) loss 0.8892 (1.0201) acc 81.2500 (74.3750) lr 4.6417e-04 eta 0:03:14
epoch [36/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.086) loss 0.7163 (1.1172) acc 90.6250 (76.2500) lr 4.6417e-04 eta 0:02:10
epoch [36/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.058) loss 0.8218 (1.0381) acc 78.1250 (77.5000) lr 4.6417e-04 eta 0:01:48
epoch [36/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.043) loss 0.9839 (1.0574) acc 71.8750 (75.6250) lr 4.6417e-04 eta 0:01:37
epoch [36/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.2500 (1.1008) acc 71.8750 (74.8750) lr 4.6417e-04 eta 0:01:30
epoch [36/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 0.8325 (1.0997) acc 81.2500 (74.2708) lr 4.6417e-04 eta 0:01:26
epoch [36/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.6523 (1.1369) acc 53.1250 (72.5000) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 0.8154 (1.1140) acc 75.0000 (73.2812) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.1943 (1.1179) acc 71.8750 (72.8472) lr 4.6417e-04 eta 0:01:17
epoch [37/50] batch [5/49] time 0.093 (0.259) data 0.000 (0.166) loss 1.0840 (1.1186) acc 68.7500 (69.3750) lr 4.1221e-04 eta 0:02:56
epoch [37/50] batch [10/49] time 0.093 (0.176) data 0.000 (0.083) loss 1.2529 (1.1136) acc 62.5000 (70.3125) lr 4.1221e-04 eta 0:01:58
epoch [37/50] batch [15/49] time 0.094 (0.149) data 0.000 (0.055) loss 0.8979 (1.0793) acc 81.2500 (71.8750) lr 4.1221e-04 eta 0:01:39
epoch [37/50] batch [20/49] time 0.092 (0.135) data 0.000 (0.042) loss 1.2480 (1.0637) acc 78.1250 (72.8125) lr 4.1221e-04 eta 0:01:29
epoch [37/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.0879 (1.0296) acc 78.1250 (75.0000) lr 4.1221e-04 eta 0:01:23
epoch [37/50] batch [30/49] time 0.096 (0.121) data 0.000 (0.028) loss 1.2539 (1.0471) acc 71.8750 (74.5833) lr 4.1221e-04 eta 0:01:19
epoch [37/50] batch [35/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.0938 (1.0631) acc 78.1250 (73.9286) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [40/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.2256 (1.0657) acc 78.1250 (74.3750) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.3809 (1.0746) acc 62.5000 (74.4444) lr 4.1221e-04 eta 0:01:11
epoch [38/50] batch [5/49] time 0.093 (0.247) data 0.000 (0.152) loss 1.0664 (1.0068) acc 62.5000 (73.1250) lr 3.6258e-04 eta 0:02:35
epoch [38/50] batch [10/49] time 0.093 (0.170) data 0.000 (0.076) loss 1.0869 (1.0011) acc 78.1250 (75.3125) lr 3.6258e-04 eta 0:01:46
epoch [38/50] batch [15/49] time 0.093 (0.144) data 0.000 (0.051) loss 1.1484 (1.0342) acc 71.8750 (73.3333) lr 3.6258e-04 eta 0:01:29
epoch [38/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 0.8950 (1.0214) acc 78.1250 (74.0625) lr 3.6258e-04 eta 0:01:20
epoch [38/50] batch [25/49] time 0.092 (0.124) data 0.000 (0.031) loss 1.0664 (1.0343) acc 71.8750 (73.7500) lr 3.6258e-04 eta 0:01:15
epoch [38/50] batch [30/49] time 0.092 (0.119) data 0.000 (0.026) loss 0.8013 (1.0227) acc 81.2500 (74.4792) lr 3.6258e-04 eta 0:01:12
epoch [38/50] batch [35/49] time 0.092 (0.115) data 0.000 (0.022) loss 0.7710 (1.0017) acc 78.1250 (75.1786) lr 3.6258e-04 eta 0:01:09
epoch [38/50] batch [40/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.1562 (1.0096) acc 65.6250 (74.8438) lr 3.6258e-04 eta 0:01:06
epoch [38/50] batch [45/49] time 0.092 (0.110) data 0.000 (0.017) loss 1.2275 (1.0242) acc 81.2500 (74.5139) lr 3.6258e-04 eta 0:01:05
epoch [39/50] batch [5/49] time 0.092 (0.261) data 0.000 (0.168) loss 1.1914 (1.1403) acc 62.5000 (70.6250) lr 3.1545e-04 eta 0:02:32
epoch [39/50] batch [10/49] time 0.093 (0.177) data 0.000 (0.084) loss 0.9854 (1.0532) acc 71.8750 (74.6875) lr 3.1545e-04 eta 0:01:42
epoch [39/50] batch [15/49] time 0.092 (0.149) data 0.000 (0.056) loss 1.3115 (1.0409) acc 62.5000 (74.5833) lr 3.1545e-04 eta 0:01:25
epoch [39/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.042) loss 1.0645 (1.0121) acc 75.0000 (75.1562) lr 3.1545e-04 eta 0:01:16
epoch [39/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.034) loss 0.7632 (1.0032) acc 81.2500 (75.7500) lr 3.1545e-04 eta 0:01:11
epoch [39/50] batch [30/49] time 0.093 (0.121) data 0.000 (0.028) loss 0.9468 (0.9923) acc 81.2500 (76.3542) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.0361 (1.0021) acc 78.1250 (76.0714) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.1689 (1.0077) acc 68.7500 (76.2500) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.019) loss 1.0771 (1.0099) acc 65.6250 (75.6944) lr 3.1545e-04 eta 0:01:00
epoch [40/50] batch [5/49] time 0.093 (0.269) data 0.000 (0.176) loss 1.2598 (1.1784) acc 65.6250 (67.5000) lr 2.7103e-04 eta 0:02:23
epoch [40/50] batch [10/49] time 0.092 (0.181) data 0.000 (0.088) loss 1.2627 (1.1642) acc 65.6250 (70.0000) lr 2.7103e-04 eta 0:01:35
epoch [40/50] batch [15/49] time 0.093 (0.152) data 0.000 (0.059) loss 1.1367 (1.1712) acc 71.8750 (70.0000) lr 2.7103e-04 eta 0:01:19
epoch [40/50] batch [20/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.0566 (1.1378) acc 78.1250 (71.7188) lr 2.7103e-04 eta 0:01:11
epoch [40/50] batch [25/49] time 0.092 (0.128) data 0.000 (0.035) loss 0.8813 (1.1102) acc 75.0000 (72.5000) lr 2.7103e-04 eta 0:01:05
epoch [40/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.030) loss 1.2012 (1.1243) acc 81.2500 (72.7083) lr 2.7103e-04 eta 0:01:02
epoch [40/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4600 (1.1285) acc 65.6250 (72.3214) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 0.8066 (1.1145) acc 87.5000 (72.8906) lr 2.7103e-04 eta 0:00:57
epoch [40/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.8589 (1.1103) acc 75.0000 (73.1944) lr 2.7103e-04 eta 0:00:55
epoch [41/50] batch [5/49] time 0.092 (0.252) data 0.000 (0.160) loss 1.0762 (1.1121) acc 84.3750 (75.0000) lr 2.2949e-04 eta 0:02:02
epoch [41/50] batch [10/49] time 0.093 (0.172) data 0.000 (0.080) loss 1.2188 (1.1592) acc 65.6250 (71.8750) lr 2.2949e-04 eta 0:01:22
epoch [41/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.053) loss 0.9785 (1.1459) acc 78.1250 (72.5000) lr 2.2949e-04 eta 0:01:09
epoch [41/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 0.9106 (1.1228) acc 78.1250 (73.2812) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [25/49] time 0.092 (0.125) data 0.000 (0.032) loss 0.9868 (1.1455) acc 62.5000 (72.2500) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 0.7607 (1.1100) acc 90.6250 (73.6458) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [35/49] time 0.095 (0.116) data 0.000 (0.023) loss 1.6270 (1.1240) acc 62.5000 (72.9464) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.3447 (1.1233) acc 75.0000 (73.2031) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.1143 (1.1355) acc 68.7500 (72.8472) lr 2.2949e-04 eta 0:00:49
epoch [42/50] batch [5/49] time 0.093 (0.263) data 0.000 (0.170) loss 1.1484 (1.0427) acc 75.0000 (74.3750) lr 1.9098e-04 eta 0:01:54
epoch [42/50] batch [10/49] time 0.097 (0.178) data 0.004 (0.085) loss 0.9243 (1.0147) acc 84.3750 (76.5625) lr 1.9098e-04 eta 0:01:16
epoch [42/50] batch [15/49] time 0.096 (0.150) data 0.000 (0.057) loss 0.6484 (0.9724) acc 90.6250 (77.5000) lr 1.9098e-04 eta 0:01:03
epoch [42/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.043) loss 0.9746 (1.0041) acc 71.8750 (76.7188) lr 1.9098e-04 eta 0:00:57
epoch [42/50] batch [25/49] time 0.092 (0.127) data 0.000 (0.034) loss 1.2715 (1.0269) acc 68.7500 (76.5000) lr 1.9098e-04 eta 0:00:52
epoch [42/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 0.9551 (1.0536) acc 84.3750 (76.6667) lr 1.9098e-04 eta 0:00:49
epoch [42/50] batch [35/49] time 0.092 (0.117) data 0.000 (0.025) loss 0.9844 (1.0543) acc 81.2500 (76.1607) lr 1.9098e-04 eta 0:00:47
epoch [42/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.022) loss 1.0166 (1.0532) acc 65.6250 (76.0938) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0098 (1.0469) acc 78.1250 (76.7361) lr 1.9098e-04 eta 0:00:44
epoch [43/50] batch [5/49] time 0.092 (0.258) data 0.000 (0.164) loss 1.1289 (1.0057) acc 68.7500 (76.8750) lr 1.5567e-04 eta 0:01:39
epoch [43/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.082) loss 1.0020 (1.0434) acc 81.2500 (76.2500) lr 1.5567e-04 eta 0:01:07
epoch [43/50] batch [15/49] time 0.092 (0.148) data 0.000 (0.055) loss 1.3389 (1.0720) acc 62.5000 (74.1667) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [20/49] time 0.094 (0.134) data 0.000 (0.041) loss 1.5029 (1.0851) acc 53.1250 (73.5938) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.033) loss 0.7173 (1.0346) acc 81.2500 (75.1250) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [30/49] time 0.092 (0.120) data 0.000 (0.028) loss 1.0488 (1.0215) acc 71.8750 (75.4167) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [35/49] time 0.094 (0.116) data 0.001 (0.024) loss 1.0254 (1.0229) acc 81.2500 (75.2679) lr 1.5567e-04 eta 0:00:41
epoch [43/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.021) loss 0.7974 (1.0193) acc 84.3750 (75.0781) lr 1.5567e-04 eta 0:00:39
epoch [43/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.019) loss 1.2764 (1.0427) acc 78.1250 (74.6528) lr 1.5567e-04 eta 0:00:38
epoch [44/50] batch [5/49] time 0.093 (0.254) data 0.000 (0.161) loss 0.8608 (1.0219) acc 81.2500 (77.5000) lr 1.2369e-04 eta 0:01:25
epoch [44/50] batch [10/49] time 0.092 (0.173) data 0.000 (0.081) loss 1.2549 (1.0182) acc 62.5000 (76.8750) lr 1.2369e-04 eta 0:00:57
epoch [44/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.054) loss 1.1777 (0.9957) acc 71.8750 (78.1250) lr 1.2369e-04 eta 0:00:48
epoch [44/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.0996 (0.9957) acc 68.7500 (77.3438) lr 1.2369e-04 eta 0:00:42
epoch [44/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.032) loss 0.7710 (1.0079) acc 87.5000 (76.8750) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2236 (1.0231) acc 78.1250 (76.9792) lr 1.2369e-04 eta 0:00:37
epoch [44/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.8901 (1.0274) acc 71.8750 (76.6964) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [40/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.3076 (1.0242) acc 75.0000 (76.7969) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [45/49] time 0.094 (0.111) data 0.000 (0.018) loss 1.1191 (1.0226) acc 71.8750 (76.9444) lr 1.2369e-04 eta 0:00:33
epoch [45/50] batch [5/49] time 0.093 (0.283) data 0.000 (0.189) loss 1.2207 (0.9931) acc 75.0000 (79.3750) lr 9.5173e-05 eta 0:01:21
epoch [45/50] batch [10/49] time 0.092 (0.188) data 0.000 (0.095) loss 1.0771 (1.0251) acc 71.8750 (78.4375) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [15/49] time 0.093 (0.157) data 0.000 (0.063) loss 0.7705 (0.9965) acc 84.3750 (77.7083) lr 9.5173e-05 eta 0:00:43
epoch [45/50] batch [20/49] time 0.093 (0.141) data 0.000 (0.047) loss 1.2266 (1.0163) acc 71.8750 (77.3438) lr 9.5173e-05 eta 0:00:38
epoch [45/50] batch [25/49] time 0.094 (0.131) data 0.000 (0.038) loss 0.9116 (1.0082) acc 84.3750 (77.5000) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.032) loss 0.9258 (0.9961) acc 84.3750 (77.9167) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.3721 (1.0409) acc 65.6250 (76.8750) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 0.7095 (1.0218) acc 87.5000 (77.2656) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.021) loss 0.9526 (1.0181) acc 78.1250 (77.3611) lr 9.5173e-05 eta 0:00:28
epoch [46/50] batch [5/49] time 0.093 (0.292) data 0.000 (0.198) loss 0.8779 (1.0432) acc 84.3750 (76.2500) lr 7.0224e-05 eta 0:01:10
epoch [46/50] batch [10/49] time 0.093 (0.192) data 0.000 (0.099) loss 1.0479 (1.0640) acc 78.1250 (76.2500) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [15/49] time 0.093 (0.160) data 0.000 (0.066) loss 1.2744 (1.0882) acc 71.8750 (76.4583) lr 7.0224e-05 eta 0:00:36
epoch [46/50] batch [20/49] time 0.092 (0.143) data 0.000 (0.050) loss 0.6855 (1.0254) acc 90.6250 (78.2812) lr 7.0224e-05 eta 0:00:32
epoch [46/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 0.8457 (1.0163) acc 78.1250 (77.8750) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.0439 (1.0244) acc 71.8750 (76.8750) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [35/49] time 0.092 (0.122) data 0.000 (0.029) loss 0.7998 (1.0098) acc 81.2500 (77.5000) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 0.9507 (1.0120) acc 78.1250 (77.5781) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.0078 (1.0252) acc 78.1250 (77.5000) lr 7.0224e-05 eta 0:00:23
epoch [47/50] batch [5/49] time 0.093 (0.241) data 0.000 (0.148) loss 0.9873 (1.0225) acc 78.1250 (75.0000) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [10/49] time 0.094 (0.167) data 0.000 (0.074) loss 0.7954 (1.0256) acc 84.3750 (75.6250) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [15/49] time 0.092 (0.142) data 0.000 (0.050) loss 0.8682 (1.0407) acc 84.3750 (75.2083) lr 4.8943e-05 eta 0:00:25
epoch [47/50] batch [20/49] time 0.095 (0.130) data 0.000 (0.037) loss 0.9453 (1.0028) acc 75.0000 (76.5625) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [25/49] time 0.097 (0.123) data 0.000 (0.030) loss 1.3096 (1.0253) acc 68.7500 (75.8750) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [30/49] time 0.092 (0.118) data 0.000 (0.025) loss 0.7690 (1.0093) acc 87.5000 (76.7708) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [35/49] time 0.093 (0.114) data 0.000 (0.021) loss 0.9731 (1.0070) acc 71.8750 (76.7857) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [40/49] time 0.092 (0.111) data 0.000 (0.019) loss 0.9502 (1.0164) acc 81.2500 (76.6406) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [45/49] time 0.092 (0.109) data 0.000 (0.017) loss 0.9170 (1.0084) acc 75.0000 (76.8056) lr 4.8943e-05 eta 0:00:16
epoch [48/50] batch [5/49] time 0.092 (0.243) data 0.000 (0.150) loss 0.9585 (1.1448) acc 71.8750 (71.2500) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [10/49] time 0.093 (0.168) data 0.000 (0.075) loss 1.0430 (1.0975) acc 65.6250 (70.6250) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [15/49] time 0.093 (0.144) data 0.000 (0.050) loss 1.0498 (1.0482) acc 75.0000 (73.5417) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 0.8184 (1.0433) acc 84.3750 (74.0625) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [25/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.1094 (1.0223) acc 75.0000 (74.6250) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [30/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.1123 (1.0354) acc 71.8750 (74.4792) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [35/49] time 0.094 (0.115) data 0.000 (0.022) loss 0.8101 (1.0310) acc 84.3750 (74.8214) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [40/49] time 0.094 (0.112) data 0.000 (0.019) loss 1.2402 (1.0159) acc 68.7500 (75.7031) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 0.9497 (1.0018) acc 78.1250 (75.9722) lr 3.1417e-05 eta 0:00:11
epoch [49/50] batch [5/49] time 0.093 (0.248) data 0.000 (0.155) loss 0.7017 (0.9437) acc 87.5000 (80.6250) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [10/49] time 0.095 (0.171) data 0.000 (0.078) loss 1.0664 (1.0053) acc 68.7500 (79.3750) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [15/49] time 0.094 (0.145) data 0.000 (0.052) loss 0.9658 (1.0414) acc 84.3750 (77.9167) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [20/49] time 0.093 (0.132) data 0.000 (0.039) loss 1.1133 (1.0325) acc 68.7500 (77.6562) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [25/49] time 0.096 (0.125) data 0.000 (0.031) loss 1.0811 (1.0374) acc 81.2500 (77.6250) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [30/49] time 0.097 (0.120) data 0.000 (0.026) loss 0.9448 (1.0205) acc 75.0000 (77.7083) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [35/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.1611 (1.0246) acc 71.8750 (77.5000) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.1572 (1.0216) acc 65.6250 (77.4219) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 0.8872 (1.0122) acc 81.2500 (77.5000) lr 1.7713e-05 eta 0:00:05
epoch [50/50] batch [5/49] time 0.096 (0.283) data 0.000 (0.189) loss 1.0391 (1.0630) acc 75.0000 (76.2500) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [10/49] time 0.093 (0.189) data 0.001 (0.095) loss 1.2412 (1.0853) acc 62.5000 (76.8750) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [15/49] time 0.093 (0.157) data 0.000 (0.063) loss 0.9492 (1.0573) acc 84.3750 (77.2917) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [20/49] time 0.094 (0.141) data 0.001 (0.048) loss 1.1787 (1.0588) acc 68.7500 (76.7188) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [25/49] time 0.093 (0.132) data 0.000 (0.038) loss 0.9741 (1.0415) acc 78.1250 (77.1250) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.2607 (1.0291) acc 78.1250 (77.6042) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.027) loss 1.1172 (1.0138) acc 75.0000 (77.6786) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.0195 (1.0158) acc 78.1250 (77.5781) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.021) loss 0.6445 (1.0194) acc 93.7500 (77.9861) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:10<01:24, 10.61s/it] 22%|██▏       | 2/9 [00:11<00:33,  4.77s/it] 33%|███▎      | 3/9 [00:11<00:17,  2.86s/it] 44%|████▍     | 4/9 [00:12<00:09,  1.96s/it] 56%|█████▌    | 5/9 [00:13<00:05,  1.47s/it] 67%|██████▋   | 6/9 [00:13<00:03,  1.17s/it] 78%|███████▊  | 7/9 [00:14<00:01,  1.02it/s] 89%|████████▉ | 8/9 [00:14<00:00,  1.17it/s]100%|██████████| 9/9 [00:15<00:00,  1.67s/it]
=> result
* total: 4,002
* correct: 3,228
* accuracy: 80.7%
* error: 19.3%
* macro_f1: 80.4%
Elapsed: 0:04:54
Run this job and save the output to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/49] time 0.136 (0.356) data 0.004 (0.241) loss 3.0938 (3.2332) acc 50.0000 (45.0000) lr 1.0000e-05 eta 0:14:30
epoch [1/50] batch [10/49] time 0.093 (0.224) data 0.000 (0.121) loss 2.5781 (3.0623) acc 62.5000 (51.2500) lr 1.0000e-05 eta 0:09:07
epoch [1/50] batch [15/49] time 0.091 (0.181) data 0.000 (0.081) loss 3.5723 (3.0789) acc 28.1250 (48.9583) lr 1.0000e-05 eta 0:07:20
epoch [1/50] batch [20/49] time 0.094 (0.159) data 0.000 (0.061) loss 2.6758 (3.0353) acc 59.3750 (48.9062) lr 1.0000e-05 eta 0:06:25
epoch [1/50] batch [25/49] time 0.095 (0.146) data 0.003 (0.049) loss 3.0117 (3.0310) acc 46.8750 (49.7500) lr 1.0000e-05 eta 0:05:53
epoch [1/50] batch [30/49] time 0.097 (0.137) data 0.001 (0.041) loss 3.2422 (3.0661) acc 56.2500 (49.1667) lr 1.0000e-05 eta 0:05:32
epoch [1/50] batch [35/49] time 0.092 (0.131) data 0.000 (0.035) loss 3.0156 (3.0347) acc 37.5000 (48.8393) lr 1.0000e-05 eta 0:05:15
epoch [1/50] batch [40/49] time 0.092 (0.126) data 0.000 (0.031) loss 3.0527 (3.0262) acc 53.1250 (48.9844) lr 1.0000e-05 eta 0:05:03
epoch [1/50] batch [45/49] time 0.092 (0.122) data 0.000 (0.027) loss 2.6055 (3.0059) acc 62.5000 (48.5417) lr 1.0000e-05 eta 0:04:53
epoch [2/50] batch [5/49] time 0.096 (0.262) data 0.000 (0.168) loss 1.7451 (1.9498) acc 43.7500 (47.5000) lr 2.0000e-03 eta 0:10:28
epoch [2/50] batch [10/49] time 0.092 (0.178) data 0.000 (0.084) loss 1.9814 (1.9239) acc 59.3750 (50.3125) lr 2.0000e-03 eta 0:07:04
epoch [2/50] batch [15/49] time 0.092 (0.149) data 0.000 (0.056) loss 2.0625 (1.9234) acc 37.5000 (49.5833) lr 2.0000e-03 eta 0:05:55
epoch [2/50] batch [20/49] time 0.095 (0.135) data 0.001 (0.042) loss 1.7852 (1.8363) acc 46.8750 (51.8750) lr 2.0000e-03 eta 0:05:20
epoch [2/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.034) loss 1.8262 (1.8369) acc 50.0000 (51.3750) lr 2.0000e-03 eta 0:05:00
epoch [2/50] batch [30/49] time 0.096 (0.121) data 0.000 (0.028) loss 1.8213 (1.8327) acc 50.0000 (50.5208) lr 2.0000e-03 eta 0:04:46
epoch [2/50] batch [35/49] time 0.092 (0.117) data 0.000 (0.024) loss 2.0156 (1.8266) acc 40.6250 (50.0893) lr 2.0000e-03 eta 0:04:36
epoch [2/50] batch [40/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.5762 (1.8424) acc 43.7500 (49.1406) lr 2.0000e-03 eta 0:04:28
epoch [2/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.019) loss 1.4385 (1.8232) acc 62.5000 (49.5833) lr 2.0000e-03 eta 0:04:22
epoch [3/50] batch [5/49] time 0.096 (0.267) data 0.000 (0.174) loss 1.5938 (1.5543) acc 46.8750 (49.3750) lr 1.9980e-03 eta 0:10:27
epoch [3/50] batch [10/49] time 0.092 (0.180) data 0.000 (0.087) loss 2.1445 (1.5851) acc 40.6250 (51.5625) lr 1.9980e-03 eta 0:07:02
epoch [3/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.058) loss 1.6484 (1.5795) acc 53.1250 (49.7917) lr 1.9980e-03 eta 0:05:53
epoch [3/50] batch [20/49] time 0.092 (0.136) data 0.000 (0.044) loss 1.4287 (1.5846) acc 53.1250 (49.8438) lr 1.9980e-03 eta 0:05:18
epoch [3/50] batch [25/49] time 0.092 (0.128) data 0.000 (0.035) loss 1.8213 (1.5836) acc 46.8750 (50.8750) lr 1.9980e-03 eta 0:04:56
epoch [3/50] batch [30/49] time 0.093 (0.122) data 0.001 (0.029) loss 1.7422 (1.5973) acc 56.2500 (51.1458) lr 1.9980e-03 eta 0:04:42
epoch [3/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.5557 (1.5850) acc 56.2500 (51.4286) lr 1.9980e-03 eta 0:04:32
epoch [3/50] batch [40/49] time 0.092 (0.114) data 0.000 (0.022) loss 1.7295 (1.5908) acc 46.8750 (50.9375) lr 1.9980e-03 eta 0:04:24
epoch [3/50] batch [45/49] time 0.094 (0.112) data 0.000 (0.020) loss 1.4287 (1.5761) acc 53.1250 (51.4583) lr 1.9980e-03 eta 0:04:18
epoch [4/50] batch [5/49] time 0.093 (0.292) data 0.000 (0.197) loss 1.2666 (1.4883) acc 50.0000 (49.3750) lr 1.9921e-03 eta 0:11:11
epoch [4/50] batch [10/49] time 0.092 (0.193) data 0.000 (0.099) loss 1.4941 (1.5773) acc 53.1250 (50.6250) lr 1.9921e-03 eta 0:07:22
epoch [4/50] batch [15/49] time 0.092 (0.160) data 0.000 (0.066) loss 1.3545 (1.5423) acc 43.7500 (51.8750) lr 1.9921e-03 eta 0:06:05
epoch [4/50] batch [20/49] time 0.092 (0.143) data 0.000 (0.050) loss 1.7559 (1.5541) acc 43.7500 (52.1875) lr 1.9921e-03 eta 0:05:26
epoch [4/50] batch [25/49] time 0.092 (0.133) data 0.000 (0.040) loss 1.9922 (1.5753) acc 50.0000 (52.3750) lr 1.9921e-03 eta 0:05:02
epoch [4/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.1650 (1.5770) acc 62.5000 (52.3958) lr 1.9921e-03 eta 0:04:46
epoch [4/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.029) loss 1.7402 (1.5715) acc 53.1250 (52.7679) lr 1.9921e-03 eta 0:04:35
epoch [4/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.5918 (1.5618) acc 50.0000 (53.5156) lr 1.9921e-03 eta 0:04:26
epoch [4/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.3994 (1.5665) acc 50.0000 (53.1250) lr 1.9921e-03 eta 0:04:19
epoch [5/50] batch [5/49] time 0.094 (0.281) data 0.000 (0.187) loss 1.3584 (1.5090) acc 59.3750 (55.6250) lr 1.9823e-03 eta 0:10:30
epoch [5/50] batch [10/49] time 0.093 (0.187) data 0.000 (0.094) loss 1.3770 (1.5301) acc 53.1250 (52.8125) lr 1.9823e-03 eta 0:06:59
epoch [5/50] batch [15/49] time 0.093 (0.156) data 0.000 (0.063) loss 1.4248 (1.5021) acc 59.3750 (52.9167) lr 1.9823e-03 eta 0:05:49
epoch [5/50] batch [20/49] time 0.093 (0.140) data 0.001 (0.047) loss 1.3789 (1.4678) acc 56.2500 (54.2188) lr 1.9823e-03 eta 0:05:13
epoch [5/50] batch [25/49] time 0.092 (0.131) data 0.000 (0.038) loss 1.6689 (1.4909) acc 43.7500 (53.6250) lr 1.9823e-03 eta 0:04:51
epoch [5/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.7256 (1.5171) acc 43.7500 (51.6667) lr 1.9823e-03 eta 0:04:36
epoch [5/50] batch [35/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.4834 (1.5185) acc 56.2500 (52.1429) lr 1.9823e-03 eta 0:04:26
epoch [5/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.3711 (1.5192) acc 56.2500 (52.4219) lr 1.9823e-03 eta 0:04:18
epoch [5/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.3232 (1.4899) acc 68.7500 (53.8194) lr 1.9823e-03 eta 0:04:11
epoch [6/50] batch [5/49] time 0.093 (0.291) data 0.000 (0.197) loss 1.3320 (1.3529) acc 65.6250 (63.1250) lr 1.9686e-03 eta 0:10:40
epoch [6/50] batch [10/49] time 0.092 (0.192) data 0.000 (0.099) loss 1.4629 (1.3740) acc 53.1250 (58.4375) lr 1.9686e-03 eta 0:07:00
epoch [6/50] batch [15/49] time 0.094 (0.159) data 0.000 (0.066) loss 1.5742 (1.4610) acc 43.7500 (55.8333) lr 1.9686e-03 eta 0:05:47
epoch [6/50] batch [20/49] time 0.092 (0.142) data 0.000 (0.050) loss 1.3340 (1.4558) acc 53.1250 (55.3125) lr 1.9686e-03 eta 0:05:10
epoch [6/50] batch [25/49] time 0.092 (0.132) data 0.000 (0.040) loss 1.5781 (1.4561) acc 53.1250 (55.2500) lr 1.9686e-03 eta 0:04:48
epoch [6/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.3438 (1.4532) acc 53.1250 (55.3125) lr 1.9686e-03 eta 0:04:33
epoch [6/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.1201 (1.4488) acc 75.0000 (55.6250) lr 1.9686e-03 eta 0:04:22
epoch [6/50] batch [40/49] time 0.095 (0.117) data 0.000 (0.025) loss 1.5664 (1.4587) acc 40.6250 (55.0000) lr 1.9686e-03 eta 0:04:14
epoch [6/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.4395 (1.4501) acc 50.0000 (55.2778) lr 1.9686e-03 eta 0:04:07
epoch [7/50] batch [5/49] time 0.095 (0.277) data 0.000 (0.183) loss 1.4795 (1.3734) acc 68.7500 (60.0000) lr 1.9511e-03 eta 0:09:56
epoch [7/50] batch [10/49] time 0.093 (0.185) data 0.000 (0.092) loss 1.5195 (1.4234) acc 46.8750 (57.5000) lr 1.9511e-03 eta 0:06:37
epoch [7/50] batch [15/49] time 0.092 (0.154) data 0.000 (0.061) loss 1.3145 (1.4212) acc 59.3750 (57.7083) lr 1.9511e-03 eta 0:05:30
epoch [7/50] batch [20/49] time 0.092 (0.139) data 0.000 (0.046) loss 1.2100 (1.4057) acc 71.8750 (57.9688) lr 1.9511e-03 eta 0:04:56
epoch [7/50] batch [25/49] time 0.092 (0.130) data 0.000 (0.037) loss 1.7783 (1.4351) acc 43.7500 (56.7500) lr 1.9511e-03 eta 0:04:36
epoch [7/50] batch [30/49] time 0.092 (0.124) data 0.000 (0.031) loss 1.4580 (1.4247) acc 50.0000 (56.4583) lr 1.9511e-03 eta 0:04:22
epoch [7/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.3115 (1.4130) acc 46.8750 (56.7857) lr 1.9511e-03 eta 0:04:12
epoch [7/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.4883 (1.4092) acc 50.0000 (56.6406) lr 1.9511e-03 eta 0:04:05
epoch [7/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.021) loss 1.3789 (1.4159) acc 56.2500 (56.2500) lr 1.9511e-03 eta 0:03:58
epoch [8/50] batch [5/49] time 0.093 (0.266) data 0.000 (0.173) loss 1.5654 (1.5168) acc 43.7500 (52.5000) lr 1.9298e-03 eta 0:09:18
epoch [8/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.087) loss 1.6387 (1.4857) acc 50.0000 (56.5625) lr 1.9298e-03 eta 0:06:16
epoch [8/50] batch [15/49] time 0.097 (0.151) data 0.000 (0.058) loss 1.6807 (1.4775) acc 53.1250 (55.6250) lr 1.9298e-03 eta 0:05:15
epoch [8/50] batch [20/49] time 0.092 (0.136) data 0.000 (0.043) loss 1.5654 (1.5083) acc 53.1250 (53.5938) lr 1.9298e-03 eta 0:04:44
epoch [8/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.4590 (1.4657) acc 59.3750 (55.1250) lr 1.9298e-03 eta 0:04:25
epoch [8/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.3691 (1.4674) acc 71.8750 (54.8958) lr 1.9298e-03 eta 0:04:12
epoch [8/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.1699 (1.4496) acc 62.5000 (54.4643) lr 1.9298e-03 eta 0:04:03
epoch [8/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.3379 (1.4315) acc 59.3750 (55.4688) lr 1.9298e-03 eta 0:03:56
epoch [8/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.0635 (1.4121) acc 62.5000 (55.4861) lr 1.9298e-03 eta 0:03:51
epoch [9/50] batch [5/49] time 0.093 (0.294) data 0.000 (0.200) loss 1.3525 (1.2816) acc 59.3750 (56.8750) lr 1.9048e-03 eta 0:10:03
epoch [9/50] batch [10/49] time 0.093 (0.194) data 0.000 (0.100) loss 1.4980 (1.4101) acc 65.6250 (56.2500) lr 1.9048e-03 eta 0:06:36
epoch [9/50] batch [15/49] time 0.094 (0.160) data 0.000 (0.067) loss 1.2197 (1.3284) acc 56.2500 (59.1667) lr 1.9048e-03 eta 0:05:26
epoch [9/50] batch [20/49] time 0.093 (0.143) data 0.000 (0.050) loss 1.1123 (1.3451) acc 68.7500 (58.2812) lr 1.9048e-03 eta 0:04:51
epoch [9/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.4443 (1.3324) acc 53.1250 (59.0000) lr 1.9048e-03 eta 0:04:30
epoch [9/50] batch [30/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.4609 (1.3457) acc 56.2500 (57.9167) lr 1.9048e-03 eta 0:04:16
epoch [9/50] batch [35/49] time 0.093 (0.122) data 0.000 (0.029) loss 1.4414 (1.3498) acc 50.0000 (57.5000) lr 1.9048e-03 eta 0:04:06
epoch [9/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.9072 (1.3761) acc 56.2500 (57.5781) lr 1.9048e-03 eta 0:03:58
epoch [9/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.023) loss 1.7393 (1.3894) acc 46.8750 (57.0833) lr 1.9048e-03 eta 0:03:52
epoch [10/50] batch [5/49] time 0.093 (0.269) data 0.000 (0.175) loss 0.9697 (1.2053) acc 71.8750 (63.1250) lr 1.8763e-03 eta 0:09:00
epoch [10/50] batch [10/49] time 0.097 (0.182) data 0.000 (0.088) loss 1.6768 (1.3815) acc 59.3750 (60.6250) lr 1.8763e-03 eta 0:06:03
epoch [10/50] batch [15/49] time 0.093 (0.152) data 0.000 (0.059) loss 1.2705 (1.3423) acc 68.7500 (62.9167) lr 1.8763e-03 eta 0:05:03
epoch [10/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.044) loss 1.2412 (1.3163) acc 65.6250 (62.8125) lr 1.8763e-03 eta 0:04:33
epoch [10/50] batch [25/49] time 0.092 (0.129) data 0.000 (0.035) loss 1.3975 (1.3400) acc 59.3750 (61.2500) lr 1.8763e-03 eta 0:04:15
epoch [10/50] batch [30/49] time 0.094 (0.123) data 0.000 (0.029) loss 1.2041 (1.3588) acc 56.2500 (60.4167) lr 1.8763e-03 eta 0:04:02
epoch [10/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.5479 (1.3603) acc 50.0000 (60.4464) lr 1.8763e-03 eta 0:03:53
epoch [10/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.4629 (1.3532) acc 65.6250 (61.0938) lr 1.8763e-03 eta 0:03:46
epoch [10/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.2979 (1.3479) acc 53.1250 (60.6250) lr 1.8763e-03 eta 0:03:41
epoch [11/50] batch [5/49] time 0.093 (0.311) data 0.000 (0.217) loss 1.0820 (1.4162) acc 65.6250 (54.3750) lr 1.8443e-03 eta 0:10:07
epoch [11/50] batch [10/49] time 0.092 (0.202) data 0.000 (0.109) loss 1.2471 (1.3152) acc 68.7500 (59.6875) lr 1.8443e-03 eta 0:06:33
epoch [11/50] batch [15/49] time 0.093 (0.166) data 0.000 (0.073) loss 1.1846 (1.2947) acc 56.2500 (61.0417) lr 1.8443e-03 eta 0:05:22
epoch [11/50] batch [20/49] time 0.093 (0.148) data 0.000 (0.055) loss 1.2549 (1.3357) acc 62.5000 (59.6875) lr 1.8443e-03 eta 0:04:46
epoch [11/50] batch [25/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.1191 (1.3062) acc 65.6250 (60.7500) lr 1.8443e-03 eta 0:04:24
epoch [11/50] batch [30/49] time 0.094 (0.130) data 0.000 (0.037) loss 1.1602 (1.3384) acc 68.7500 (59.7917) lr 1.8443e-03 eta 0:04:10
epoch [11/50] batch [35/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.7354 (1.3458) acc 53.1250 (59.9107) lr 1.8443e-03 eta 0:03:59
epoch [11/50] batch [40/49] time 0.094 (0.120) data 0.001 (0.028) loss 1.7998 (1.3583) acc 50.0000 (59.7656) lr 1.8443e-03 eta 0:03:51
epoch [11/50] batch [45/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.3711 (1.3463) acc 56.2500 (59.6528) lr 1.8443e-03 eta 0:03:44
epoch [12/50] batch [5/49] time 0.092 (0.271) data 0.000 (0.177) loss 1.6924 (1.3860) acc 46.8750 (58.7500) lr 1.8090e-03 eta 0:08:36
epoch [12/50] batch [10/49] time 0.095 (0.182) data 0.000 (0.089) loss 1.4346 (1.3849) acc 62.5000 (59.6875) lr 1.8090e-03 eta 0:05:45
epoch [12/50] batch [15/49] time 0.092 (0.152) data 0.000 (0.059) loss 1.0381 (1.4049) acc 65.6250 (58.3333) lr 1.8090e-03 eta 0:04:48
epoch [12/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.044) loss 1.2041 (1.3790) acc 75.0000 (59.6875) lr 1.8090e-03 eta 0:04:20
epoch [12/50] batch [25/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.0723 (1.3357) acc 71.8750 (60.7500) lr 1.8090e-03 eta 0:04:02
epoch [12/50] batch [30/49] time 0.092 (0.123) data 0.000 (0.030) loss 1.4170 (1.3353) acc 56.2500 (60.9375) lr 1.8090e-03 eta 0:03:50
epoch [12/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.2725 (1.3278) acc 50.0000 (60.9821) lr 1.8090e-03 eta 0:03:42
epoch [12/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.0645 (1.3108) acc 75.0000 (61.4844) lr 1.8090e-03 eta 0:03:35
epoch [12/50] batch [45/49] time 0.094 (0.113) data 0.000 (0.020) loss 1.3438 (1.3102) acc 62.5000 (61.6667) lr 1.8090e-03 eta 0:03:30
epoch [13/50] batch [5/49] time 0.093 (0.291) data 0.000 (0.197) loss 1.6064 (1.4160) acc 50.0000 (58.1250) lr 1.7705e-03 eta 0:09:00
epoch [13/50] batch [10/49] time 0.093 (0.192) data 0.000 (0.098) loss 1.2988 (1.3529) acc 68.7500 (63.1250) lr 1.7705e-03 eta 0:05:55
epoch [13/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.066) loss 1.0195 (1.3126) acc 68.7500 (63.3333) lr 1.7705e-03 eta 0:04:53
epoch [13/50] batch [20/49] time 0.092 (0.143) data 0.000 (0.049) loss 1.2480 (1.3379) acc 65.6250 (62.9688) lr 1.7705e-03 eta 0:04:23
epoch [13/50] batch [25/49] time 0.096 (0.133) data 0.000 (0.040) loss 1.2617 (1.3675) acc 65.6250 (62.2500) lr 1.7705e-03 eta 0:04:04
epoch [13/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.4932 (1.3400) acc 53.1250 (63.0208) lr 1.7705e-03 eta 0:03:51
epoch [13/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.3584 (1.3205) acc 62.5000 (63.3036) lr 1.7705e-03 eta 0:03:41
epoch [13/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.6191 (1.3401) acc 56.2500 (62.4219) lr 1.7705e-03 eta 0:03:34
epoch [13/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.7773 (1.3489) acc 53.1250 (62.6389) lr 1.7705e-03 eta 0:03:29
epoch [14/50] batch [5/49] time 0.095 (0.290) data 0.000 (0.195) loss 2.0234 (1.5156) acc 37.5000 (58.1250) lr 1.7290e-03 eta 0:08:44
epoch [14/50] batch [10/49] time 0.092 (0.191) data 0.000 (0.098) loss 1.3760 (1.4817) acc 68.7500 (59.3750) lr 1.7290e-03 eta 0:05:44
epoch [14/50] batch [15/49] time 0.092 (0.158) data 0.000 (0.065) loss 1.5283 (1.4321) acc 59.3750 (57.5000) lr 1.7290e-03 eta 0:04:44
epoch [14/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.049) loss 1.3398 (1.4206) acc 59.3750 (58.1250) lr 1.7290e-03 eta 0:04:14
epoch [14/50] batch [25/49] time 0.092 (0.132) data 0.000 (0.039) loss 1.2354 (1.4255) acc 68.7500 (59.5000) lr 1.7290e-03 eta 0:03:56
epoch [14/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.033) loss 0.9419 (1.3861) acc 68.7500 (60.8333) lr 1.7290e-03 eta 0:03:43
epoch [14/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.3877 (1.3790) acc 56.2500 (60.8036) lr 1.7290e-03 eta 0:03:34
epoch [14/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.025) loss 1.3574 (1.3645) acc 59.3750 (60.9375) lr 1.7290e-03 eta 0:03:27
epoch [14/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.022) loss 1.4336 (1.3609) acc 56.2500 (60.4167) lr 1.7290e-03 eta 0:03:22
epoch [15/50] batch [5/49] time 0.096 (0.266) data 0.000 (0.170) loss 1.4883 (1.3996) acc 68.7500 (61.2500) lr 1.6845e-03 eta 0:07:47
epoch [15/50] batch [10/49] time 0.094 (0.179) data 0.000 (0.085) loss 1.5654 (1.4135) acc 56.2500 (61.8750) lr 1.6845e-03 eta 0:05:14
epoch [15/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.057) loss 1.3584 (1.3954) acc 59.3750 (62.5000) lr 1.6845e-03 eta 0:04:23
epoch [15/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.043) loss 0.9321 (1.3657) acc 68.7500 (62.6562) lr 1.6845e-03 eta 0:03:57
epoch [15/50] batch [25/49] time 0.095 (0.127) data 0.000 (0.034) loss 1.4561 (1.3718) acc 40.6250 (60.7500) lr 1.6845e-03 eta 0:03:41
epoch [15/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 1.7295 (1.3582) acc 50.0000 (60.6250) lr 1.6845e-03 eta 0:03:31
epoch [15/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.0791 (1.3211) acc 62.5000 (61.8750) lr 1.6845e-03 eta 0:03:23
epoch [15/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.021) loss 1.1025 (1.3280) acc 71.8750 (61.7969) lr 1.6845e-03 eta 0:03:17
epoch [15/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.6982 (1.3409) acc 56.2500 (61.5972) lr 1.6845e-03 eta 0:03:12
epoch [16/50] batch [5/49] time 0.092 (0.267) data 0.000 (0.173) loss 1.2686 (1.3279) acc 68.7500 (64.3750) lr 1.6374e-03 eta 0:07:36
epoch [16/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.087) loss 1.2773 (1.3019) acc 62.5000 (62.8125) lr 1.6374e-03 eta 0:05:07
epoch [16/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.058) loss 1.3350 (1.2758) acc 56.2500 (63.3333) lr 1.6374e-03 eta 0:04:17
epoch [16/50] batch [20/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.1504 (1.2926) acc 71.8750 (63.2812) lr 1.6374e-03 eta 0:03:51
epoch [16/50] batch [25/49] time 0.094 (0.128) data 0.000 (0.035) loss 1.3535 (1.2881) acc 62.5000 (63.5000) lr 1.6374e-03 eta 0:03:36
epoch [16/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.1299 (1.2835) acc 65.6250 (63.9583) lr 1.6374e-03 eta 0:03:25
epoch [16/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.2100 (1.2891) acc 71.8750 (64.1071) lr 1.6374e-03 eta 0:03:18
epoch [16/50] batch [40/49] time 0.094 (0.115) data 0.000 (0.022) loss 1.6885 (1.2874) acc 50.0000 (64.1406) lr 1.6374e-03 eta 0:03:12
epoch [16/50] batch [45/49] time 0.094 (0.112) data 0.000 (0.020) loss 1.3857 (1.2854) acc 56.2500 (63.9583) lr 1.6374e-03 eta 0:03:07
epoch [17/50] batch [5/49] time 0.094 (0.308) data 0.000 (0.215) loss 1.5430 (1.2406) acc 56.2500 (67.5000) lr 1.5878e-03 eta 0:08:31
epoch [17/50] batch [10/49] time 0.093 (0.201) data 0.000 (0.108) loss 1.2891 (1.2568) acc 78.1250 (65.9375) lr 1.5878e-03 eta 0:05:33
epoch [17/50] batch [15/49] time 0.092 (0.165) data 0.000 (0.072) loss 1.0352 (1.2695) acc 75.0000 (64.3750) lr 1.5878e-03 eta 0:04:32
epoch [17/50] batch [20/49] time 0.093 (0.147) data 0.000 (0.054) loss 1.3281 (1.2525) acc 53.1250 (63.5938) lr 1.5878e-03 eta 0:04:02
epoch [17/50] batch [25/49] time 0.092 (0.136) data 0.000 (0.043) loss 1.4004 (1.2412) acc 56.2500 (64.2500) lr 1.5878e-03 eta 0:03:43
epoch [17/50] batch [30/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.5762 (1.2305) acc 56.2500 (64.5833) lr 1.5878e-03 eta 0:03:31
epoch [17/50] batch [35/49] time 0.092 (0.124) data 0.000 (0.031) loss 1.4209 (1.2316) acc 71.8750 (65.0000) lr 1.5878e-03 eta 0:03:21
epoch [17/50] batch [40/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.4658 (1.2294) acc 65.6250 (65.6250) lr 1.5878e-03 eta 0:03:14
epoch [17/50] batch [45/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.3594 (1.2392) acc 59.3750 (65.1389) lr 1.5878e-03 eta 0:03:09
epoch [18/50] batch [5/49] time 0.092 (0.309) data 0.000 (0.217) loss 1.5410 (1.3279) acc 56.2500 (65.0000) lr 1.5358e-03 eta 0:08:18
epoch [18/50] batch [10/49] time 0.093 (0.201) data 0.001 (0.108) loss 1.4160 (1.2379) acc 65.6250 (66.8750) lr 1.5358e-03 eta 0:05:23
epoch [18/50] batch [15/49] time 0.093 (0.165) data 0.001 (0.072) loss 1.6240 (1.2699) acc 53.1250 (65.4167) lr 1.5358e-03 eta 0:04:25
epoch [18/50] batch [20/49] time 0.092 (0.147) data 0.000 (0.054) loss 1.3389 (1.2735) acc 59.3750 (64.5312) lr 1.5358e-03 eta 0:03:55
epoch [18/50] batch [25/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.0586 (1.2477) acc 78.1250 (65.1250) lr 1.5358e-03 eta 0:03:37
epoch [18/50] batch [30/49] time 0.093 (0.130) data 0.000 (0.036) loss 1.3975 (1.3004) acc 53.1250 (63.4375) lr 1.5358e-03 eta 0:03:25
epoch [18/50] batch [35/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.2803 (1.3060) acc 68.7500 (62.6786) lr 1.5358e-03 eta 0:03:16
epoch [18/50] batch [40/49] time 0.095 (0.120) data 0.000 (0.027) loss 1.3340 (1.2999) acc 62.5000 (62.3438) lr 1.5358e-03 eta 0:03:10
epoch [18/50] batch [45/49] time 0.094 (0.117) data 0.000 (0.024) loss 1.4844 (1.2871) acc 50.0000 (62.9861) lr 1.5358e-03 eta 0:03:04
epoch [19/50] batch [5/49] time 0.093 (0.255) data 0.000 (0.159) loss 1.1670 (1.2381) acc 71.8750 (66.2500) lr 1.4818e-03 eta 0:06:37
epoch [19/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.080) loss 1.3252 (1.2331) acc 62.5000 (65.6250) lr 1.4818e-03 eta 0:04:31
epoch [19/50] batch [15/49] time 0.094 (0.147) data 0.001 (0.053) loss 1.1152 (1.2061) acc 68.7500 (66.4583) lr 1.4818e-03 eta 0:03:48
epoch [19/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.040) loss 1.4277 (1.2334) acc 50.0000 (65.0000) lr 1.4818e-03 eta 0:03:27
epoch [19/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.032) loss 1.5176 (1.2533) acc 50.0000 (63.8750) lr 1.4818e-03 eta 0:03:14
epoch [19/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2373 (1.2358) acc 59.3750 (64.2708) lr 1.4818e-03 eta 0:03:05
epoch [19/50] batch [35/49] time 0.094 (0.117) data 0.000 (0.023) loss 1.3428 (1.2211) acc 62.5000 (64.2857) lr 1.4818e-03 eta 0:02:58
epoch [19/50] batch [40/49] time 0.094 (0.114) data 0.000 (0.020) loss 1.2441 (1.2133) acc 62.5000 (64.7656) lr 1.4818e-03 eta 0:02:53
epoch [19/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.2119 (1.2279) acc 71.8750 (64.7222) lr 1.4818e-03 eta 0:02:49
epoch [20/50] batch [5/49] time 0.092 (0.252) data 0.000 (0.158) loss 1.4346 (1.3227) acc 56.2500 (59.3750) lr 1.4258e-03 eta 0:06:21
epoch [20/50] batch [10/49] time 0.093 (0.173) data 0.000 (0.079) loss 1.1406 (1.2641) acc 59.3750 (63.4375) lr 1.4258e-03 eta 0:04:20
epoch [20/50] batch [15/49] time 0.092 (0.146) data 0.000 (0.053) loss 1.0029 (1.2561) acc 65.6250 (64.7917) lr 1.4258e-03 eta 0:03:39
epoch [20/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.3936 (1.2659) acc 59.3750 (64.2188) lr 1.4258e-03 eta 0:03:19
epoch [20/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.2324 (1.2467) acc 56.2500 (64.7500) lr 1.4258e-03 eta 0:03:06
epoch [20/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.027) loss 1.4453 (1.2407) acc 62.5000 (65.4167) lr 1.4258e-03 eta 0:02:57
epoch [20/50] batch [35/49] time 0.095 (0.116) data 0.001 (0.023) loss 1.3721 (1.2335) acc 53.1250 (65.6250) lr 1.4258e-03 eta 0:02:51
epoch [20/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.4258 (1.2394) acc 56.2500 (65.3125) lr 1.4258e-03 eta 0:02:47
epoch [20/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 0.9917 (1.2321) acc 71.8750 (65.4861) lr 1.4258e-03 eta 0:02:43
epoch [21/50] batch [5/49] time 0.093 (0.267) data 0.000 (0.173) loss 1.0586 (1.2215) acc 75.0000 (64.3750) lr 1.3681e-03 eta 0:06:30
epoch [21/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.087) loss 1.0635 (1.2437) acc 62.5000 (64.3750) lr 1.3681e-03 eta 0:04:22
epoch [21/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.058) loss 1.4658 (1.2456) acc 62.5000 (65.8333) lr 1.3681e-03 eta 0:03:40
epoch [21/50] batch [20/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.1836 (1.2034) acc 65.6250 (67.3438) lr 1.3681e-03 eta 0:03:17
epoch [21/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.4355 (1.2215) acc 56.2500 (67.3750) lr 1.3681e-03 eta 0:03:04
epoch [21/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.1543 (1.2168) acc 65.6250 (67.1875) lr 1.3681e-03 eta 0:02:55
epoch [21/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.0957 (1.2195) acc 65.6250 (66.2500) lr 1.3681e-03 eta 0:02:49
epoch [21/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.4805 (1.2333) acc 65.6250 (66.0938) lr 1.3681e-03 eta 0:02:44
epoch [21/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.020) loss 1.2656 (1.2388) acc 65.6250 (66.1806) lr 1.3681e-03 eta 0:02:39
epoch [22/50] batch [5/49] time 0.096 (0.280) data 0.000 (0.186) loss 1.2744 (1.2174) acc 68.7500 (71.2500) lr 1.3090e-03 eta 0:06:35
epoch [22/50] batch [10/49] time 0.092 (0.187) data 0.000 (0.093) loss 1.1592 (1.1772) acc 71.8750 (71.2500) lr 1.3090e-03 eta 0:04:23
epoch [22/50] batch [15/49] time 0.095 (0.155) data 0.000 (0.062) loss 1.2646 (1.2145) acc 78.1250 (70.0000) lr 1.3090e-03 eta 0:03:38
epoch [22/50] batch [20/49] time 0.093 (0.140) data 0.000 (0.047) loss 1.0859 (1.2091) acc 71.8750 (69.3750) lr 1.3090e-03 eta 0:03:15
epoch [22/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.037) loss 1.2559 (1.2234) acc 65.6250 (69.2500) lr 1.3090e-03 eta 0:03:01
epoch [22/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.6582 (1.2523) acc 62.5000 (68.0208) lr 1.3090e-03 eta 0:02:52
epoch [22/50] batch [35/49] time 0.096 (0.120) data 0.000 (0.027) loss 1.4062 (1.2627) acc 65.6250 (67.9464) lr 1.3090e-03 eta 0:02:45
epoch [22/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.024) loss 1.2227 (1.2545) acc 65.6250 (67.8906) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.0840 (1.2593) acc 71.8750 (67.2917) lr 1.3090e-03 eta 0:02:36
epoch [23/50] batch [5/49] time 0.095 (0.269) data 0.000 (0.175) loss 1.2188 (1.0898) acc 59.3750 (71.8750) lr 1.2487e-03 eta 0:06:08
epoch [23/50] batch [10/49] time 0.093 (0.182) data 0.000 (0.088) loss 0.8472 (1.0877) acc 87.5000 (71.8750) lr 1.2487e-03 eta 0:04:07
epoch [23/50] batch [15/49] time 0.093 (0.152) data 0.000 (0.059) loss 1.0703 (1.1134) acc 71.8750 (72.2917) lr 1.2487e-03 eta 0:03:26
epoch [23/50] batch [20/49] time 0.092 (0.138) data 0.000 (0.044) loss 0.9927 (1.1350) acc 78.1250 (71.7188) lr 1.2487e-03 eta 0:03:05
epoch [23/50] batch [25/49] time 0.094 (0.129) data 0.000 (0.035) loss 1.5391 (1.2142) acc 46.8750 (69.0000) lr 1.2487e-03 eta 0:02:53
epoch [23/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.2451 (1.2079) acc 59.3750 (68.8542) lr 1.2487e-03 eta 0:02:44
epoch [23/50] batch [35/49] time 0.093 (0.119) data 0.001 (0.025) loss 2.1133 (1.2443) acc 53.1250 (68.3036) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [40/49] time 0.095 (0.115) data 0.000 (0.022) loss 1.4932 (1.2430) acc 71.8750 (68.4375) lr 1.2487e-03 eta 0:02:33
epoch [23/50] batch [45/49] time 0.094 (0.113) data 0.000 (0.020) loss 1.2832 (1.2310) acc 59.3750 (68.4722) lr 1.2487e-03 eta 0:02:29
epoch [24/50] batch [5/49] time 0.093 (0.254) data 0.000 (0.160) loss 1.0723 (1.1545) acc 65.6250 (72.5000) lr 1.1874e-03 eta 0:05:34
epoch [24/50] batch [10/49] time 0.093 (0.174) data 0.001 (0.080) loss 0.9067 (1.1609) acc 78.1250 (71.2500) lr 1.1874e-03 eta 0:03:48
epoch [24/50] batch [15/49] time 0.093 (0.147) data 0.000 (0.054) loss 1.1953 (1.2018) acc 68.7500 (68.9583) lr 1.1874e-03 eta 0:03:12
epoch [24/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.040) loss 1.5137 (1.2082) acc 68.7500 (68.4375) lr 1.1874e-03 eta 0:02:54
epoch [24/50] batch [25/49] time 0.094 (0.126) data 0.000 (0.032) loss 1.3389 (1.2197) acc 65.6250 (67.3750) lr 1.1874e-03 eta 0:02:43
epoch [24/50] batch [30/49] time 0.093 (0.121) data 0.000 (0.027) loss 1.2148 (1.2136) acc 71.8750 (67.8125) lr 1.1874e-03 eta 0:02:35
epoch [24/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.023) loss 1.2402 (1.2254) acc 68.7500 (67.5000) lr 1.1874e-03 eta 0:02:30
epoch [24/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.020) loss 0.9604 (1.2339) acc 78.1250 (67.5781) lr 1.1874e-03 eta 0:02:25
epoch [24/50] batch [45/49] time 0.094 (0.111) data 0.000 (0.018) loss 1.0801 (1.2435) acc 78.1250 (67.0139) lr 1.1874e-03 eta 0:02:22
epoch [25/50] batch [5/49] time 0.093 (0.259) data 0.000 (0.166) loss 1.0176 (1.0824) acc 75.0000 (76.8750) lr 1.1253e-03 eta 0:05:28
epoch [25/50] batch [10/49] time 0.094 (0.176) data 0.000 (0.083) loss 1.0977 (1.0520) acc 62.5000 (75.0000) lr 1.1253e-03 eta 0:03:42
epoch [25/50] batch [15/49] time 0.096 (0.149) data 0.000 (0.055) loss 1.5791 (1.1294) acc 59.3750 (71.0417) lr 1.1253e-03 eta 0:03:07
epoch [25/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.042) loss 1.4160 (1.1139) acc 71.8750 (72.1875) lr 1.1253e-03 eta 0:02:49
epoch [25/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.033) loss 1.2627 (1.1346) acc 62.5000 (71.5000) lr 1.1253e-03 eta 0:02:38
epoch [25/50] batch [30/49] time 0.093 (0.121) data 0.000 (0.028) loss 0.8970 (1.1453) acc 65.6250 (70.3125) lr 1.1253e-03 eta 0:02:30
epoch [25/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.2803 (1.1592) acc 75.0000 (70.5357) lr 1.1253e-03 eta 0:02:25
epoch [25/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.2051 (1.1694) acc 65.6250 (69.6875) lr 1.1253e-03 eta 0:02:20
epoch [25/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0879 (1.1764) acc 78.1250 (69.3056) lr 1.1253e-03 eta 0:02:17
epoch [26/50] batch [5/49] time 0.095 (0.248) data 0.000 (0.152) loss 1.0371 (1.0395) acc 68.7500 (71.2500) lr 1.0628e-03 eta 0:05:02
epoch [26/50] batch [10/49] time 0.094 (0.171) data 0.000 (0.076) loss 1.1709 (1.0478) acc 62.5000 (72.1875) lr 1.0628e-03 eta 0:03:27
epoch [26/50] batch [15/49] time 0.093 (0.145) data 0.001 (0.051) loss 1.2930 (1.0742) acc 59.3750 (71.0417) lr 1.0628e-03 eta 0:02:55
epoch [26/50] batch [20/49] time 0.097 (0.132) data 0.000 (0.038) loss 1.0020 (1.0826) acc 75.0000 (72.1875) lr 1.0628e-03 eta 0:02:39
epoch [26/50] batch [25/49] time 0.093 (0.124) data 0.000 (0.031) loss 0.8237 (1.0735) acc 78.1250 (71.5000) lr 1.0628e-03 eta 0:02:29
epoch [26/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.6445 (1.1168) acc 62.5000 (71.0417) lr 1.0628e-03 eta 0:02:22
epoch [26/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.022) loss 1.0674 (1.1217) acc 81.2500 (71.2500) lr 1.0628e-03 eta 0:02:17
epoch [26/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.019) loss 0.9048 (1.1143) acc 75.0000 (71.1719) lr 1.0628e-03 eta 0:02:13
epoch [26/50] batch [45/49] time 0.095 (0.111) data 0.000 (0.017) loss 1.2480 (1.1232) acc 68.7500 (70.6944) lr 1.0628e-03 eta 0:02:10
epoch [27/50] batch [5/49] time 0.097 (0.297) data 0.000 (0.203) loss 1.0020 (1.2869) acc 75.0000 (65.6250) lr 1.0000e-03 eta 0:05:47
epoch [27/50] batch [10/49] time 0.094 (0.196) data 0.000 (0.102) loss 1.2568 (1.3045) acc 56.2500 (65.6250) lr 1.0000e-03 eta 0:03:48
epoch [27/50] batch [15/49] time 0.093 (0.162) data 0.000 (0.068) loss 1.2285 (1.2038) acc 78.1250 (68.5417) lr 1.0000e-03 eta 0:03:07
epoch [27/50] batch [20/49] time 0.093 (0.145) data 0.000 (0.051) loss 0.8838 (1.1652) acc 78.1250 (69.5312) lr 1.0000e-03 eta 0:02:47
epoch [27/50] batch [25/49] time 0.094 (0.135) data 0.000 (0.041) loss 1.2529 (1.1825) acc 62.5000 (68.7500) lr 1.0000e-03 eta 0:02:34
epoch [27/50] batch [30/49] time 0.093 (0.128) data 0.000 (0.034) loss 1.3535 (1.1972) acc 62.5000 (68.3333) lr 1.0000e-03 eta 0:02:26
epoch [27/50] batch [35/49] time 0.095 (0.123) data 0.000 (0.029) loss 0.9126 (1.1837) acc 81.2500 (69.0179) lr 1.0000e-03 eta 0:02:20
epoch [27/50] batch [40/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.1592 (1.1741) acc 62.5000 (68.9062) lr 1.0000e-03 eta 0:02:15
epoch [27/50] batch [45/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9072 (1.1717) acc 71.8750 (68.7500) lr 1.0000e-03 eta 0:02:11
epoch [28/50] batch [5/49] time 0.093 (0.270) data 0.000 (0.176) loss 1.2578 (1.2207) acc 68.7500 (70.0000) lr 9.3721e-04 eta 0:05:02
epoch [28/50] batch [10/49] time 0.093 (0.182) data 0.000 (0.088) loss 1.5293 (1.2473) acc 65.6250 (70.0000) lr 9.3721e-04 eta 0:03:23
epoch [28/50] batch [15/49] time 0.093 (0.152) data 0.000 (0.059) loss 1.0117 (1.2251) acc 75.0000 (70.2083) lr 9.3721e-04 eta 0:02:49
epoch [28/50] batch [20/49] time 0.094 (0.138) data 0.000 (0.045) loss 1.0195 (1.1663) acc 71.8750 (72.0312) lr 9.3721e-04 eta 0:02:32
epoch [28/50] batch [25/49] time 0.093 (0.129) data 0.001 (0.036) loss 1.2900 (1.1647) acc 62.5000 (71.6250) lr 9.3721e-04 eta 0:02:22
epoch [28/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.2432 (1.1699) acc 62.5000 (71.4583) lr 9.3721e-04 eta 0:02:15
epoch [28/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 0.9468 (1.1602) acc 78.1250 (71.5179) lr 9.3721e-04 eta 0:02:09
epoch [28/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.022) loss 1.2480 (1.1469) acc 71.8750 (71.9531) lr 9.3721e-04 eta 0:02:05
epoch [28/50] batch [45/49] time 0.094 (0.113) data 0.000 (0.020) loss 1.0234 (1.1588) acc 71.8750 (71.2500) lr 9.3721e-04 eta 0:02:02
epoch [29/50] batch [5/49] time 0.095 (0.255) data 0.000 (0.161) loss 1.0059 (1.0054) acc 78.1250 (76.2500) lr 8.7467e-04 eta 0:04:34
epoch [29/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.081) loss 1.2393 (1.0473) acc 68.7500 (75.3125) lr 8.7467e-04 eta 0:03:06
epoch [29/50] batch [15/49] time 0.093 (0.148) data 0.000 (0.054) loss 1.5986 (1.1133) acc 71.8750 (73.9583) lr 8.7467e-04 eta 0:02:37
epoch [29/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.041) loss 0.8374 (1.0906) acc 81.2500 (73.5938) lr 8.7467e-04 eta 0:02:21
epoch [29/50] batch [25/49] time 0.094 (0.126) data 0.000 (0.033) loss 1.4189 (1.1124) acc 62.5000 (73.2500) lr 8.7467e-04 eta 0:02:12
epoch [29/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2090 (1.1253) acc 71.8750 (71.8750) lr 8.7467e-04 eta 0:02:06
epoch [29/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.023) loss 0.7905 (1.1418) acc 78.1250 (71.3393) lr 8.7467e-04 eta 0:02:01
epoch [29/50] batch [40/49] time 0.092 (0.114) data 0.000 (0.020) loss 1.1670 (1.1622) acc 68.7500 (70.3125) lr 8.7467e-04 eta 0:01:57
epoch [29/50] batch [45/49] time 0.092 (0.111) data 0.000 (0.018) loss 1.0303 (1.1642) acc 68.7500 (70.3472) lr 8.7467e-04 eta 0:01:54
epoch [30/50] batch [5/49] time 0.095 (0.270) data 0.000 (0.176) loss 1.0605 (1.0778) acc 68.7500 (73.7500) lr 8.1262e-04 eta 0:04:36
epoch [30/50] batch [10/49] time 0.094 (0.182) data 0.000 (0.088) loss 0.9131 (1.0457) acc 71.8750 (72.5000) lr 8.1262e-04 eta 0:03:05
epoch [30/50] batch [15/49] time 0.094 (0.153) data 0.000 (0.059) loss 0.8164 (1.0807) acc 84.3750 (72.7083) lr 8.1262e-04 eta 0:02:34
epoch [30/50] batch [20/49] time 0.095 (0.138) data 0.000 (0.044) loss 0.9995 (1.0629) acc 75.0000 (72.9688) lr 8.1262e-04 eta 0:02:19
epoch [30/50] batch [25/49] time 0.093 (0.129) data 0.001 (0.036) loss 1.1963 (1.0954) acc 75.0000 (71.8750) lr 8.1262e-04 eta 0:02:09
epoch [30/50] batch [30/49] time 0.096 (0.124) data 0.000 (0.030) loss 1.4287 (1.1174) acc 56.2500 (71.0417) lr 8.1262e-04 eta 0:02:03
epoch [30/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 0.9351 (1.1177) acc 78.1250 (70.6250) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [40/49] time 0.094 (0.116) data 0.000 (0.022) loss 1.1475 (1.1410) acc 68.7500 (69.9219) lr 8.1262e-04 eta 0:01:54
epoch [30/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.020) loss 1.1074 (1.1464) acc 71.8750 (69.4444) lr 8.1262e-04 eta 0:01:51
epoch [31/50] batch [5/49] time 0.095 (0.251) data 0.000 (0.157) loss 1.0791 (1.2053) acc 71.8750 (68.1250) lr 7.5131e-04 eta 0:04:04
epoch [31/50] batch [10/49] time 0.094 (0.173) data 0.000 (0.079) loss 0.8569 (1.1902) acc 84.3750 (69.3750) lr 7.5131e-04 eta 0:02:47
epoch [31/50] batch [15/49] time 0.093 (0.146) data 0.000 (0.053) loss 1.1133 (1.1728) acc 75.0000 (71.8750) lr 7.5131e-04 eta 0:02:20
epoch [31/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 0.7827 (1.1364) acc 87.5000 (73.7500) lr 7.5131e-04 eta 0:02:07
epoch [31/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.6064 (1.1371) acc 53.1250 (71.8750) lr 7.5131e-04 eta 0:01:59
epoch [31/50] batch [30/49] time 0.094 (0.120) data 0.000 (0.027) loss 1.5020 (1.1724) acc 68.7500 (71.4583) lr 7.5131e-04 eta 0:01:54
epoch [31/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9541 (1.1431) acc 81.2500 (71.7857) lr 7.5131e-04 eta 0:01:49
epoch [31/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.8521 (1.1526) acc 71.8750 (71.7969) lr 7.5131e-04 eta 0:01:46
epoch [31/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.6680 (1.1699) acc 62.5000 (71.2500) lr 7.5131e-04 eta 0:01:43
epoch [32/50] batch [5/49] time 0.094 (0.344) data 0.001 (0.250) loss 1.3574 (1.1661) acc 68.7500 (66.2500) lr 6.9098e-04 eta 0:05:18
epoch [32/50] batch [10/49] time 0.093 (0.219) data 0.000 (0.125) loss 1.7461 (1.1811) acc 56.2500 (65.9375) lr 6.9098e-04 eta 0:03:21
epoch [32/50] batch [15/49] time 0.094 (0.177) data 0.000 (0.084) loss 0.7856 (1.1205) acc 84.3750 (68.9583) lr 6.9098e-04 eta 0:02:42
epoch [32/50] batch [20/49] time 0.094 (0.156) data 0.000 (0.063) loss 1.0293 (1.0900) acc 71.8750 (70.0000) lr 6.9098e-04 eta 0:02:22
epoch [32/50] batch [25/49] time 0.093 (0.144) data 0.000 (0.050) loss 1.1709 (1.0893) acc 68.7500 (70.2500) lr 6.9098e-04 eta 0:02:10
epoch [32/50] batch [30/49] time 0.093 (0.135) data 0.000 (0.042) loss 0.9185 (1.0897) acc 84.3750 (71.7708) lr 6.9098e-04 eta 0:02:01
epoch [32/50] batch [35/49] time 0.094 (0.129) data 0.000 (0.036) loss 1.1309 (1.1009) acc 68.7500 (71.3393) lr 6.9098e-04 eta 0:01:55
epoch [32/50] batch [40/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.1289 (1.1104) acc 75.0000 (70.8594) lr 6.9098e-04 eta 0:01:51
epoch [32/50] batch [45/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.0273 (1.1216) acc 81.2500 (70.1389) lr 6.9098e-04 eta 0:01:47
epoch [33/50] batch [5/49] time 0.094 (0.256) data 0.000 (0.163) loss 0.7046 (1.0150) acc 90.6250 (76.8750) lr 6.3188e-04 eta 0:03:44
epoch [33/50] batch [10/49] time 0.095 (0.175) data 0.000 (0.082) loss 0.8267 (1.1058) acc 84.3750 (75.9375) lr 6.3188e-04 eta 0:02:32
epoch [33/50] batch [15/49] time 0.092 (0.147) data 0.000 (0.054) loss 0.7471 (1.0608) acc 90.6250 (77.5000) lr 6.3188e-04 eta 0:02:07
epoch [33/50] batch [20/49] time 0.092 (0.134) data 0.001 (0.041) loss 1.2891 (1.0752) acc 62.5000 (75.0000) lr 6.3188e-04 eta 0:01:55
epoch [33/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.1270 (1.0935) acc 65.6250 (73.8750) lr 6.3188e-04 eta 0:01:47
epoch [33/50] batch [30/49] time 0.093 (0.120) data 0.001 (0.027) loss 1.0117 (1.0941) acc 75.0000 (73.7500) lr 6.3188e-04 eta 0:01:42
epoch [33/50] batch [35/49] time 0.094 (0.116) data 0.001 (0.024) loss 0.9956 (1.0845) acc 87.5000 (74.8214) lr 6.3188e-04 eta 0:01:38
epoch [33/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.021) loss 0.7822 (1.0652) acc 90.6250 (75.2344) lr 6.3188e-04 eta 0:01:35
epoch [33/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 0.7559 (1.0822) acc 87.5000 (74.4444) lr 6.3188e-04 eta 0:01:33
epoch [34/50] batch [5/49] time 0.093 (0.261) data 0.000 (0.167) loss 0.9790 (1.0824) acc 78.1250 (70.0000) lr 5.7422e-04 eta 0:03:36
epoch [34/50] batch [10/49] time 0.095 (0.178) data 0.000 (0.084) loss 0.8003 (1.0743) acc 78.1250 (71.2500) lr 5.7422e-04 eta 0:02:26
epoch [34/50] batch [15/49] time 0.094 (0.150) data 0.000 (0.056) loss 1.4238 (1.0852) acc 62.5000 (72.9167) lr 5.7422e-04 eta 0:02:02
epoch [34/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.042) loss 0.8267 (1.0400) acc 81.2500 (74.3750) lr 5.7422e-04 eta 0:01:50
epoch [34/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.3105 (1.0675) acc 71.8750 (74.1250) lr 5.7422e-04 eta 0:01:42
epoch [34/50] batch [30/49] time 0.094 (0.122) data 0.000 (0.028) loss 1.3174 (1.0763) acc 65.6250 (73.9583) lr 5.7422e-04 eta 0:01:37
epoch [34/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.024) loss 1.4316 (1.0921) acc 65.6250 (73.3929) lr 5.7422e-04 eta 0:01:33
epoch [34/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.021) loss 1.5439 (1.0950) acc 71.8750 (73.5938) lr 5.7422e-04 eta 0:01:30
epoch [34/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.7012 (1.1060) acc 56.2500 (73.4722) lr 5.7422e-04 eta 0:01:28
epoch [35/50] batch [5/49] time 0.094 (0.265) data 0.000 (0.172) loss 1.0293 (1.0571) acc 68.7500 (77.5000) lr 5.1825e-04 eta 0:03:26
epoch [35/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.086) loss 0.6455 (0.9947) acc 84.3750 (76.5625) lr 5.1825e-04 eta 0:02:18
epoch [35/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.057) loss 1.2422 (1.0483) acc 68.7500 (73.7500) lr 5.1825e-04 eta 0:01:55
epoch [35/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.043) loss 1.3535 (1.0850) acc 53.1250 (72.6562) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.0176 (1.0925) acc 68.7500 (72.3750) lr 5.1825e-04 eta 0:01:36
epoch [35/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 0.9253 (1.1143) acc 75.0000 (71.9792) lr 5.1825e-04 eta 0:01:31
epoch [35/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4961 (1.1183) acc 62.5000 (71.8750) lr 5.1825e-04 eta 0:01:28
epoch [35/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 0.8867 (1.1026) acc 81.2500 (72.6562) lr 5.1825e-04 eta 0:01:25
epoch [35/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0391 (1.1081) acc 65.6250 (72.1528) lr 5.1825e-04 eta 0:01:22
epoch [36/50] batch [5/49] time 0.092 (0.267) data 0.000 (0.173) loss 1.1191 (1.1449) acc 75.0000 (74.3750) lr 4.6417e-04 eta 0:03:14
epoch [36/50] batch [10/49] time 0.095 (0.180) data 0.000 (0.086) loss 1.1299 (1.1293) acc 78.1250 (73.1250) lr 4.6417e-04 eta 0:02:10
epoch [36/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.058) loss 1.0195 (1.1010) acc 81.2500 (73.5417) lr 4.6417e-04 eta 0:01:48
epoch [36/50] batch [20/49] time 0.096 (0.137) data 0.000 (0.043) loss 1.4395 (1.1215) acc 68.7500 (72.6562) lr 4.6417e-04 eta 0:01:37
epoch [36/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.2812 (1.1278) acc 68.7500 (72.6250) lr 4.6417e-04 eta 0:01:30
epoch [36/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.0352 (1.1308) acc 75.0000 (72.5000) lr 4.6417e-04 eta 0:01:26
epoch [36/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.3496 (1.1299) acc 62.5000 (72.3214) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 0.8042 (1.1220) acc 78.1250 (72.1094) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.3047 (1.1330) acc 65.6250 (71.5972) lr 4.6417e-04 eta 0:01:17
epoch [37/50] batch [5/49] time 0.093 (0.255) data 0.000 (0.162) loss 0.8960 (0.9647) acc 90.6250 (80.6250) lr 4.1221e-04 eta 0:02:53
epoch [37/50] batch [10/49] time 0.093 (0.174) data 0.000 (0.081) loss 1.3984 (1.0374) acc 56.2500 (76.8750) lr 4.1221e-04 eta 0:01:57
epoch [37/50] batch [15/49] time 0.092 (0.148) data 0.000 (0.054) loss 1.1328 (1.0450) acc 68.7500 (75.4167) lr 4.1221e-04 eta 0:01:39
epoch [37/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.041) loss 1.2695 (1.0325) acc 75.0000 (75.6250) lr 4.1221e-04 eta 0:01:29
epoch [37/50] batch [25/49] time 0.092 (0.126) data 0.000 (0.033) loss 0.9287 (1.0387) acc 75.0000 (74.6250) lr 4.1221e-04 eta 0:01:22
epoch [37/50] batch [30/49] time 0.096 (0.120) data 0.000 (0.027) loss 1.1133 (1.0520) acc 84.3750 (74.7917) lr 4.1221e-04 eta 0:01:19
epoch [37/50] batch [35/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.0293 (1.0739) acc 71.8750 (74.1964) lr 4.1221e-04 eta 0:01:15
epoch [37/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.3018 (1.0766) acc 68.7500 (74.2969) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.0752 (1.1012) acc 78.1250 (73.5417) lr 4.1221e-04 eta 0:01:11
epoch [38/50] batch [5/49] time 0.093 (0.241) data 0.000 (0.147) loss 1.3848 (1.2439) acc 75.0000 (71.8750) lr 3.6258e-04 eta 0:02:32
epoch [38/50] batch [10/49] time 0.092 (0.167) data 0.000 (0.074) loss 0.7280 (1.1270) acc 93.7500 (74.3750) lr 3.6258e-04 eta 0:01:44
epoch [38/50] batch [15/49] time 0.092 (0.142) data 0.000 (0.049) loss 1.2900 (1.1185) acc 68.7500 (75.4167) lr 3.6258e-04 eta 0:01:28
epoch [38/50] batch [20/49] time 0.093 (0.130) data 0.000 (0.037) loss 1.7266 (1.0962) acc 62.5000 (76.4062) lr 3.6258e-04 eta 0:01:20
epoch [38/50] batch [25/49] time 0.092 (0.122) data 0.000 (0.030) loss 1.0957 (1.0769) acc 68.7500 (75.8750) lr 3.6258e-04 eta 0:01:14
epoch [38/50] batch [30/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.1230 (1.0607) acc 71.8750 (76.1458) lr 3.6258e-04 eta 0:01:11
epoch [38/50] batch [35/49] time 0.096 (0.114) data 0.000 (0.021) loss 0.9521 (1.0703) acc 84.3750 (75.8929) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [40/49] time 0.094 (0.112) data 0.000 (0.019) loss 0.9536 (1.0660) acc 71.8750 (76.1719) lr 3.6258e-04 eta 0:01:06
epoch [38/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 0.9023 (1.0601) acc 81.2500 (76.1111) lr 3.6258e-04 eta 0:01:04
epoch [39/50] batch [5/49] time 0.096 (0.258) data 0.000 (0.163) loss 0.9209 (1.0133) acc 78.1250 (76.2500) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [10/49] time 0.093 (0.176) data 0.000 (0.082) loss 1.1826 (1.0765) acc 65.6250 (74.0625) lr 3.1545e-04 eta 0:01:41
epoch [39/50] batch [15/49] time 0.093 (0.148) data 0.000 (0.055) loss 1.1904 (1.0982) acc 68.7500 (74.3750) lr 3.1545e-04 eta 0:01:25
epoch [39/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.041) loss 0.9561 (1.0806) acc 68.7500 (73.9062) lr 3.1545e-04 eta 0:01:16
epoch [39/50] batch [25/49] time 0.096 (0.126) data 0.000 (0.033) loss 0.9722 (1.0644) acc 81.2500 (74.6250) lr 3.1545e-04 eta 0:01:11
epoch [39/50] batch [30/49] time 0.093 (0.121) data 0.000 (0.027) loss 0.8545 (1.0646) acc 81.2500 (74.4792) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 0.9541 (1.0502) acc 81.2500 (75.1786) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.1113 (1.0458) acc 71.8750 (74.9219) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [45/49] time 0.094 (0.111) data 0.000 (0.018) loss 0.9819 (1.0339) acc 78.1250 (75.4167) lr 3.1545e-04 eta 0:01:00
epoch [40/50] batch [5/49] time 0.093 (0.292) data 0.000 (0.199) loss 1.4609 (1.1600) acc 75.0000 (73.7500) lr 2.7103e-04 eta 0:02:35
epoch [40/50] batch [10/49] time 0.093 (0.193) data 0.000 (0.100) loss 1.2314 (1.1299) acc 65.6250 (73.7500) lr 2.7103e-04 eta 0:01:41
epoch [40/50] batch [15/49] time 0.093 (0.160) data 0.000 (0.066) loss 1.1367 (1.1222) acc 68.7500 (72.5000) lr 2.7103e-04 eta 0:01:23
epoch [40/50] batch [20/49] time 0.093 (0.143) data 0.000 (0.050) loss 1.0459 (1.0865) acc 81.2500 (74.0625) lr 2.7103e-04 eta 0:01:14
epoch [40/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.1816 (1.0843) acc 75.0000 (74.5000) lr 2.7103e-04 eta 0:01:08
epoch [40/50] batch [30/49] time 0.095 (0.127) data 0.000 (0.033) loss 1.1709 (1.0799) acc 81.2500 (75.0000) lr 2.7103e-04 eta 0:01:04
epoch [40/50] batch [35/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.0752 (1.0711) acc 62.5000 (75.0000) lr 2.7103e-04 eta 0:01:01
epoch [40/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.1289 (1.0623) acc 75.0000 (75.5469) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 0.8203 (1.0647) acc 81.2500 (75.2083) lr 2.7103e-04 eta 0:00:56
epoch [41/50] batch [5/49] time 0.095 (0.250) data 0.000 (0.156) loss 1.1289 (1.1368) acc 68.7500 (72.5000) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [10/49] time 0.093 (0.171) data 0.000 (0.078) loss 1.0000 (1.0430) acc 71.8750 (74.3750) lr 2.2949e-04 eta 0:01:22
epoch [41/50] batch [15/49] time 0.092 (0.145) data 0.000 (0.052) loss 1.2969 (1.0276) acc 75.0000 (75.0000) lr 2.2949e-04 eta 0:01:09
epoch [41/50] batch [20/49] time 0.093 (0.132) data 0.000 (0.039) loss 1.1016 (1.0544) acc 71.8750 (74.0625) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [25/49] time 0.092 (0.125) data 0.000 (0.032) loss 0.9751 (1.0335) acc 75.0000 (74.7500) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.2939 (1.0317) acc 65.6250 (75.3125) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9839 (1.0443) acc 81.2500 (74.9107) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.4824 (1.0483) acc 50.0000 (74.6875) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.2402 (1.0600) acc 65.6250 (74.1667) lr 2.2949e-04 eta 0:00:49
epoch [42/50] batch [5/49] time 0.092 (0.247) data 0.000 (0.153) loss 1.1348 (0.9969) acc 78.1250 (78.1250) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [10/49] time 0.095 (0.170) data 0.000 (0.077) loss 1.1152 (1.0466) acc 81.2500 (75.9375) lr 1.9098e-04 eta 0:01:13
epoch [42/50] batch [15/49] time 0.093 (0.144) data 0.000 (0.051) loss 0.9448 (1.0064) acc 78.1250 (75.8333) lr 1.9098e-04 eta 0:01:01
epoch [42/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.2666 (1.0561) acc 71.8750 (75.0000) lr 1.9098e-04 eta 0:00:55
epoch [42/50] batch [25/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.0518 (1.0410) acc 78.1250 (76.0000) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.0029 (1.0351) acc 68.7500 (75.7292) lr 1.9098e-04 eta 0:00:48
epoch [42/50] batch [35/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.0156 (1.0345) acc 78.1250 (75.8036) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [40/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0518 (1.0395) acc 84.3750 (75.5469) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 1.2285 (1.0434) acc 62.5000 (75.6944) lr 1.9098e-04 eta 0:00:43
epoch [43/50] batch [5/49] time 0.094 (0.259) data 0.000 (0.163) loss 1.0801 (1.0853) acc 75.0000 (76.2500) lr 1.5567e-04 eta 0:01:40
epoch [43/50] batch [10/49] time 0.094 (0.176) data 0.000 (0.082) loss 1.0947 (1.0540) acc 71.8750 (77.1875) lr 1.5567e-04 eta 0:01:07
epoch [43/50] batch [15/49] time 0.098 (0.149) data 0.000 (0.055) loss 0.7974 (1.0256) acc 81.2500 (77.2917) lr 1.5567e-04 eta 0:00:56
epoch [43/50] batch [20/49] time 0.094 (0.136) data 0.000 (0.041) loss 0.9268 (1.0463) acc 71.8750 (76.7188) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [25/49] time 0.095 (0.127) data 0.001 (0.033) loss 1.3643 (1.0516) acc 71.8750 (75.8750) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [30/49] time 0.097 (0.122) data 0.000 (0.028) loss 1.6514 (1.0591) acc 65.6250 (75.5208) lr 1.5567e-04 eta 0:00:44
epoch [43/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.024) loss 0.8643 (1.0334) acc 84.3750 (76.3393) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [40/49] time 0.094 (0.115) data 0.000 (0.021) loss 0.6895 (1.0286) acc 84.3750 (76.4844) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [45/49] time 0.094 (0.112) data 0.000 (0.018) loss 1.0020 (1.0212) acc 78.1250 (76.5972) lr 1.5567e-04 eta 0:00:38
epoch [44/50] batch [5/49] time 0.093 (0.266) data 0.000 (0.173) loss 1.0469 (1.1442) acc 62.5000 (72.5000) lr 1.2369e-04 eta 0:01:30
epoch [44/50] batch [10/49] time 0.093 (0.180) data 0.000 (0.087) loss 1.0752 (1.0636) acc 75.0000 (75.6250) lr 1.2369e-04 eta 0:00:59
epoch [44/50] batch [15/49] time 0.092 (0.151) data 0.000 (0.058) loss 1.0557 (1.1070) acc 71.8750 (73.9583) lr 1.2369e-04 eta 0:00:49
epoch [44/50] batch [20/49] time 0.092 (0.136) data 0.000 (0.043) loss 0.7715 (1.0684) acc 84.3750 (75.3125) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [25/49] time 0.092 (0.127) data 0.000 (0.035) loss 1.0947 (1.0490) acc 68.7500 (75.2500) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [30/49] time 0.094 (0.122) data 0.000 (0.029) loss 1.1406 (1.0362) acc 75.0000 (75.4167) lr 1.2369e-04 eta 0:00:38
epoch [44/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.025) loss 0.8901 (1.0318) acc 81.2500 (75.6250) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.022) loss 1.0234 (1.0262) acc 75.0000 (75.6250) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0127 (1.0293) acc 75.0000 (75.4861) lr 1.2369e-04 eta 0:00:33
epoch [45/50] batch [5/49] time 0.094 (0.273) data 0.000 (0.179) loss 1.1191 (1.1881) acc 71.8750 (71.8750) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [10/49] time 0.097 (0.183) data 0.000 (0.090) loss 1.0518 (1.1626) acc 68.7500 (71.8750) lr 9.5173e-05 eta 0:00:52
epoch [45/50] batch [15/49] time 0.094 (0.153) data 0.000 (0.060) loss 0.8862 (1.1052) acc 90.6250 (75.0000) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [20/49] time 0.092 (0.139) data 0.000 (0.045) loss 1.1943 (1.1287) acc 68.7500 (72.6562) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [25/49] time 0.094 (0.129) data 0.000 (0.036) loss 1.1191 (1.1407) acc 65.6250 (72.3750) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.0850 (1.1104) acc 71.8750 (72.7083) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.2578 (1.1048) acc 71.8750 (72.6786) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.023) loss 0.9229 (1.0866) acc 81.2500 (73.5938) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.5088 (1.0717) acc 59.3750 (74.0972) lr 9.5173e-05 eta 0:00:28
epoch [46/50] batch [5/49] time 0.093 (0.246) data 0.000 (0.151) loss 0.9546 (1.1194) acc 78.1250 (73.7500) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [10/49] time 0.092 (0.169) data 0.000 (0.076) loss 0.9824 (1.0868) acc 75.0000 (75.3125) lr 7.0224e-05 eta 0:00:39
epoch [46/50] batch [15/49] time 0.093 (0.144) data 0.000 (0.051) loss 1.1445 (1.0177) acc 68.7500 (77.9167) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.2305 (1.0681) acc 78.1250 (76.7188) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [25/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.0068 (1.0453) acc 68.7500 (76.0000) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [30/49] time 0.093 (0.118) data 0.000 (0.026) loss 1.1680 (1.0230) acc 65.6250 (76.4583) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [35/49] time 0.093 (0.115) data 0.000 (0.022) loss 0.9551 (1.0088) acc 75.0000 (76.4286) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [40/49] time 0.093 (0.112) data 0.000 (0.019) loss 0.7266 (1.0092) acc 78.1250 (76.3281) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 1.0615 (1.0042) acc 71.8750 (76.2500) lr 7.0224e-05 eta 0:00:22
epoch [47/50] batch [5/49] time 0.093 (0.248) data 0.001 (0.153) loss 0.8589 (0.9361) acc 75.0000 (78.1250) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [10/49] time 0.093 (0.171) data 0.000 (0.077) loss 1.1680 (0.9924) acc 71.8750 (75.9375) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [15/49] time 0.094 (0.145) data 0.000 (0.052) loss 1.1279 (1.0542) acc 68.7500 (75.2083) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [20/49] time 0.093 (0.132) data 0.000 (0.039) loss 0.7710 (1.0053) acc 87.5000 (77.1875) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [25/49] time 0.093 (0.125) data 0.000 (0.031) loss 1.1943 (1.0137) acc 71.8750 (76.5000) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.0293 (1.0160) acc 78.1250 (76.8750) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [35/49] time 0.094 (0.116) data 0.000 (0.022) loss 1.0996 (1.0093) acc 68.7500 (76.7857) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.9917 (1.0006) acc 75.0000 (76.8750) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.017) loss 0.7729 (1.0127) acc 81.2500 (76.6667) lr 4.8943e-05 eta 0:00:16
epoch [48/50] batch [5/49] time 0.096 (0.277) data 0.000 (0.183) loss 0.8286 (0.9885) acc 81.2500 (79.3750) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [10/49] time 0.094 (0.186) data 0.000 (0.092) loss 0.7900 (0.9847) acc 87.5000 (77.8125) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [15/49] time 0.093 (0.155) data 0.000 (0.061) loss 1.0234 (1.0145) acc 78.1250 (75.6250) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [20/49] time 0.092 (0.140) data 0.000 (0.046) loss 1.0449 (1.0288) acc 68.7500 (75.9375) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [25/49] time 0.093 (0.131) data 0.000 (0.037) loss 0.9609 (1.0181) acc 68.7500 (75.3750) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.1943 (1.0046) acc 71.8750 (76.1458) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.026) loss 1.0684 (1.0246) acc 68.7500 (75.6250) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.023) loss 0.8604 (1.0157) acc 84.3750 (76.1719) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.2295 (1.0176) acc 71.8750 (76.2500) lr 3.1417e-05 eta 0:00:11
epoch [49/50] batch [5/49] time 0.094 (0.243) data 0.000 (0.149) loss 1.1250 (1.0892) acc 75.0000 (75.0000) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [10/49] time 0.093 (0.169) data 0.000 (0.075) loss 1.1533 (1.0928) acc 65.6250 (74.0625) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [15/49] time 0.094 (0.144) data 0.001 (0.050) loss 0.9297 (1.0203) acc 84.3750 (76.6667) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [20/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.0127 (1.0413) acc 68.7500 (75.1562) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [25/49] time 0.093 (0.124) data 0.000 (0.030) loss 1.2031 (1.0342) acc 78.1250 (75.7500) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [30/49] time 0.094 (0.119) data 0.000 (0.025) loss 1.2070 (1.0511) acc 65.6250 (74.8958) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [35/49] time 0.094 (0.115) data 0.000 (0.022) loss 0.9731 (1.0468) acc 65.6250 (75.0000) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [40/49] time 0.093 (0.112) data 0.000 (0.019) loss 0.8823 (1.0512) acc 81.2500 (74.9219) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [45/49] time 0.093 (0.110) data 0.000 (0.017) loss 0.7246 (1.0372) acc 87.5000 (75.2083) lr 1.7713e-05 eta 0:00:05
epoch [50/50] batch [5/49] time 0.093 (0.256) data 0.000 (0.162) loss 0.8672 (1.0547) acc 81.2500 (73.1250) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.081) loss 0.9980 (1.0342) acc 75.0000 (75.0000) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [15/49] time 0.094 (0.148) data 0.000 (0.054) loss 0.6865 (1.0367) acc 84.3750 (74.3750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [20/49] time 0.093 (0.134) data 0.000 (0.041) loss 1.0391 (1.0211) acc 62.5000 (73.5938) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [25/49] time 0.096 (0.126) data 0.000 (0.033) loss 0.8008 (1.0333) acc 78.1250 (73.6250) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 0.6357 (1.0209) acc 87.5000 (74.3750) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9995 (1.0040) acc 75.0000 (75.3571) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.021) loss 1.4951 (1.0147) acc 71.8750 (75.5469) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 0.8223 (1.0173) acc 87.5000 (75.9028) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:07<00:57,  7.22s/it] 22%|██▏       | 2/9 [00:07<00:23,  3.32s/it] 33%|███▎      | 3/9 [00:08<00:12,  2.07s/it] 44%|████▍     | 4/9 [00:08<00:07,  1.49s/it] 56%|█████▌    | 5/9 [00:09<00:04,  1.16s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.03it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.19it/s] 89%|████████▉ | 8/9 [00:11<00:00,  1.31it/s]100%|██████████| 9/9 [00:11<00:00,  1.28s/it]
=> result
* total: 4,002
* correct: 3,219
* accuracy: 80.4%
* error: 19.6%
* macro_f1: 80.2%
Elapsed: 0:04:51
Run this job and save the output to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/49] time 0.093 (0.341) data 0.000 (0.224) loss 3.4707 (3.5359) acc 43.7500 (48.1250) lr 1.0000e-05 eta 0:13:54
epoch [1/50] batch [10/49] time 0.098 (0.218) data 0.000 (0.112) loss 3.7227 (3.6701) acc 28.1250 (42.8125) lr 1.0000e-05 eta 0:08:51
epoch [1/50] batch [15/49] time 0.092 (0.176) data 0.000 (0.075) loss 3.6172 (3.5818) acc 37.5000 (44.1667) lr 1.0000e-05 eta 0:07:08
epoch [1/50] batch [20/49] time 0.093 (0.155) data 0.000 (0.056) loss 2.7324 (3.4919) acc 56.2500 (45.9375) lr 1.0000e-05 eta 0:06:17
epoch [1/50] batch [25/49] time 0.094 (0.143) data 0.000 (0.045) loss 3.1289 (3.4049) acc 50.0000 (48.5000) lr 1.0000e-05 eta 0:05:46
epoch [1/50] batch [30/49] time 0.093 (0.134) data 0.001 (0.038) loss 3.3594 (3.4110) acc 25.0000 (45.5208) lr 1.0000e-05 eta 0:05:25
epoch [1/50] batch [35/49] time 0.095 (0.129) data 0.000 (0.032) loss 3.3066 (3.3473) acc 34.3750 (45.6250) lr 1.0000e-05 eta 0:05:10
epoch [1/50] batch [40/49] time 0.092 (0.124) data 0.000 (0.028) loss 2.5020 (3.3056) acc 53.1250 (45.4688) lr 1.0000e-05 eta 0:04:58
epoch [1/50] batch [45/49] time 0.092 (0.120) data 0.000 (0.025) loss 2.9297 (3.2555) acc 46.8750 (46.0417) lr 1.0000e-05 eta 0:04:49
epoch [2/50] batch [5/49] time 0.094 (0.292) data 0.000 (0.199) loss 2.2617 (2.2687) acc 50.0000 (54.3750) lr 2.0000e-03 eta 0:11:38
epoch [2/50] batch [10/49] time 0.092 (0.193) data 0.000 (0.100) loss 1.7139 (2.1591) acc 46.8750 (51.2500) lr 2.0000e-03 eta 0:07:40
epoch [2/50] batch [15/49] time 0.092 (0.160) data 0.000 (0.067) loss 2.3789 (2.1633) acc 28.1250 (47.5000) lr 2.0000e-03 eta 0:06:20
epoch [2/50] batch [20/49] time 0.092 (0.143) data 0.000 (0.050) loss 1.6055 (2.0532) acc 56.2500 (49.0625) lr 2.0000e-03 eta 0:05:40
epoch [2/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.9873 (2.0170) acc 53.1250 (49.3750) lr 2.0000e-03 eta 0:05:15
epoch [2/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.3574 (1.9579) acc 53.1250 (49.6875) lr 2.0000e-03 eta 0:04:59
epoch [2/50] batch [35/49] time 0.095 (0.121) data 0.000 (0.029) loss 1.5244 (1.9276) acc 50.0000 (49.6429) lr 2.0000e-03 eta 0:04:47
epoch [2/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.3359 (1.8941) acc 62.5000 (49.9219) lr 2.0000e-03 eta 0:04:37
epoch [2/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.2061 (1.8568) acc 46.8750 (49.7222) lr 2.0000e-03 eta 0:04:30
epoch [3/50] batch [5/49] time 0.092 (0.274) data 0.000 (0.181) loss 1.6133 (1.5635) acc 59.3750 (56.2500) lr 1.9980e-03 eta 0:10:42
epoch [3/50] batch [10/49] time 0.095 (0.183) data 0.000 (0.091) loss 1.3818 (1.5164) acc 53.1250 (55.6250) lr 1.9980e-03 eta 0:07:08
epoch [3/50] batch [15/49] time 0.094 (0.153) data 0.000 (0.061) loss 1.7529 (1.5067) acc 34.3750 (54.5833) lr 1.9980e-03 eta 0:05:57
epoch [3/50] batch [20/49] time 0.091 (0.138) data 0.000 (0.045) loss 2.2852 (1.5491) acc 37.5000 (54.2188) lr 1.9980e-03 eta 0:05:21
epoch [3/50] batch [25/49] time 0.095 (0.129) data 0.000 (0.036) loss 1.4922 (1.5330) acc 56.2500 (55.0000) lr 1.9980e-03 eta 0:04:59
epoch [3/50] batch [30/49] time 0.092 (0.123) data 0.000 (0.030) loss 1.5879 (1.5443) acc 46.8750 (54.5833) lr 1.9980e-03 eta 0:04:45
epoch [3/50] batch [35/49] time 0.092 (0.119) data 0.000 (0.026) loss 1.8457 (1.5513) acc 37.5000 (53.3929) lr 1.9980e-03 eta 0:04:34
epoch [3/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.023) loss 1.4277 (1.5446) acc 50.0000 (53.6719) lr 1.9980e-03 eta 0:04:26
epoch [3/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.5801 (1.5484) acc 46.8750 (52.9861) lr 1.9980e-03 eta 0:04:19
epoch [4/50] batch [5/49] time 0.096 (0.275) data 0.005 (0.183) loss 1.8848 (1.6043) acc 50.0000 (53.1250) lr 1.9921e-03 eta 0:10:32
epoch [4/50] batch [10/49] time 0.092 (0.184) data 0.000 (0.092) loss 1.9971 (1.6137) acc 40.6250 (51.5625) lr 1.9921e-03 eta 0:07:02
epoch [4/50] batch [15/49] time 0.092 (0.154) data 0.000 (0.061) loss 1.2402 (1.5948) acc 65.6250 (51.8750) lr 1.9921e-03 eta 0:05:51
epoch [4/50] batch [20/49] time 0.092 (0.138) data 0.000 (0.046) loss 1.3877 (1.5710) acc 59.3750 (52.1875) lr 1.9921e-03 eta 0:05:15
epoch [4/50] batch [25/49] time 0.092 (0.129) data 0.000 (0.037) loss 1.5244 (1.5361) acc 65.6250 (53.0000) lr 1.9921e-03 eta 0:04:53
epoch [4/50] batch [30/49] time 0.092 (0.123) data 0.000 (0.031) loss 1.7871 (1.5279) acc 43.7500 (53.3333) lr 1.9921e-03 eta 0:04:39
epoch [4/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.026) loss 1.8369 (1.5334) acc 43.7500 (52.4107) lr 1.9921e-03 eta 0:04:28
epoch [4/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.023) loss 1.4414 (1.5348) acc 65.6250 (53.0469) lr 1.9921e-03 eta 0:04:20
epoch [4/50] batch [45/49] time 0.091 (0.113) data 0.000 (0.021) loss 1.2734 (1.5366) acc 65.6250 (52.6389) lr 1.9921e-03 eta 0:04:14
epoch [5/50] batch [5/49] time 0.096 (0.268) data 0.000 (0.173) loss 1.4189 (1.4699) acc 59.3750 (58.1250) lr 1.9823e-03 eta 0:10:03
epoch [5/50] batch [10/49] time 0.093 (0.181) data 0.000 (0.087) loss 1.2295 (1.4659) acc 56.2500 (56.8750) lr 1.9823e-03 eta 0:06:45
epoch [5/50] batch [15/49] time 0.096 (0.152) data 0.000 (0.058) loss 1.5996 (1.4988) acc 56.2500 (55.2083) lr 1.9823e-03 eta 0:05:40
epoch [5/50] batch [20/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.5430 (1.4898) acc 59.3750 (56.2500) lr 1.9823e-03 eta 0:05:06
epoch [5/50] batch [25/49] time 0.092 (0.128) data 0.000 (0.035) loss 1.6992 (1.5105) acc 50.0000 (54.6250) lr 1.9823e-03 eta 0:04:45
epoch [5/50] batch [30/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.7480 (1.5094) acc 43.7500 (54.8958) lr 1.9823e-03 eta 0:04:31
epoch [5/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.9375 (1.5140) acc 40.6250 (54.4643) lr 1.9823e-03 eta 0:04:21
epoch [5/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.3398 (1.5145) acc 62.5000 (53.4375) lr 1.9823e-03 eta 0:04:13
epoch [5/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.5166 (1.4924) acc 46.8750 (53.8194) lr 1.9823e-03 eta 0:04:07
epoch [6/50] batch [5/49] time 0.093 (0.269) data 0.000 (0.176) loss 1.4668 (1.4236) acc 56.2500 (55.6250) lr 1.9686e-03 eta 0:09:52
epoch [6/50] batch [10/49] time 0.092 (0.182) data 0.000 (0.088) loss 1.8770 (1.4936) acc 43.7500 (53.4375) lr 1.9686e-03 eta 0:06:38
epoch [6/50] batch [15/49] time 0.094 (0.152) data 0.000 (0.059) loss 0.9878 (1.4644) acc 68.7500 (54.7917) lr 1.9686e-03 eta 0:05:33
epoch [6/50] batch [20/49] time 0.094 (0.137) data 0.000 (0.044) loss 1.3389 (1.4327) acc 50.0000 (55.4688) lr 1.9686e-03 eta 0:05:00
epoch [6/50] batch [25/49] time 0.093 (0.129) data 0.001 (0.035) loss 1.4238 (1.4556) acc 56.2500 (56.0000) lr 1.9686e-03 eta 0:04:40
epoch [6/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.5264 (1.4553) acc 53.1250 (56.7708) lr 1.9686e-03 eta 0:04:26
epoch [6/50] batch [35/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.6885 (1.4822) acc 46.8750 (56.0714) lr 1.9686e-03 eta 0:04:16
epoch [6/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.9863 (1.4799) acc 50.0000 (56.4844) lr 1.9686e-03 eta 0:04:09
epoch [6/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.4941 (1.4528) acc 46.8750 (57.0139) lr 1.9686e-03 eta 0:04:03
epoch [7/50] batch [5/49] time 0.092 (0.271) data 0.000 (0.177) loss 1.6143 (1.2842) acc 59.3750 (57.5000) lr 1.9511e-03 eta 0:09:42
epoch [7/50] batch [10/49] time 0.092 (0.182) data 0.000 (0.089) loss 1.9229 (1.3533) acc 43.7500 (55.9375) lr 1.9511e-03 eta 0:06:29
epoch [7/50] batch [15/49] time 0.093 (0.152) data 0.000 (0.059) loss 1.2520 (1.3295) acc 46.8750 (57.2917) lr 1.9511e-03 eta 0:05:25
epoch [7/50] batch [20/49] time 0.094 (0.138) data 0.000 (0.045) loss 1.3232 (1.3299) acc 62.5000 (57.8125) lr 1.9511e-03 eta 0:04:53
epoch [7/50] batch [25/49] time 0.096 (0.129) data 0.000 (0.036) loss 1.5771 (1.3319) acc 53.1250 (58.5000) lr 1.9511e-03 eta 0:04:34
epoch [7/50] batch [30/49] time 0.092 (0.123) data 0.000 (0.030) loss 1.3154 (1.3528) acc 62.5000 (57.6042) lr 1.9511e-03 eta 0:04:21
epoch [7/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.4512 (1.3464) acc 34.3750 (57.8571) lr 1.9511e-03 eta 0:04:11
epoch [7/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.5371 (1.3624) acc 59.3750 (57.3438) lr 1.9511e-03 eta 0:04:04
epoch [7/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.5078 (1.3696) acc 53.1250 (57.5694) lr 1.9511e-03 eta 0:03:58
epoch [8/50] batch [5/49] time 0.094 (0.280) data 0.000 (0.187) loss 0.9780 (1.3177) acc 75.0000 (58.1250) lr 1.9298e-03 eta 0:09:48
epoch [8/50] batch [10/49] time 0.093 (0.187) data 0.000 (0.093) loss 1.4502 (1.4040) acc 56.2500 (56.5625) lr 1.9298e-03 eta 0:06:31
epoch [8/50] batch [15/49] time 0.092 (0.155) data 0.000 (0.062) loss 1.4258 (1.4021) acc 65.6250 (57.0833) lr 1.9298e-03 eta 0:05:24
epoch [8/50] batch [20/49] time 0.092 (0.140) data 0.000 (0.047) loss 1.1592 (1.3738) acc 62.5000 (57.8125) lr 1.9298e-03 eta 0:04:51
epoch [8/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.038) loss 1.3721 (1.3839) acc 59.3750 (58.5000) lr 1.9298e-03 eta 0:04:31
epoch [8/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.3828 (1.3658) acc 65.6250 (59.3750) lr 1.9298e-03 eta 0:04:17
epoch [8/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.5322 (1.3892) acc 65.6250 (58.4821) lr 1.9298e-03 eta 0:04:08
epoch [8/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.024) loss 1.4219 (1.3874) acc 53.1250 (58.2031) lr 1.9298e-03 eta 0:04:00
epoch [8/50] batch [45/49] time 0.094 (0.114) data 0.000 (0.021) loss 1.0479 (1.3784) acc 78.1250 (59.3056) lr 1.9298e-03 eta 0:03:54
epoch [9/50] batch [5/49] time 0.093 (0.289) data 0.000 (0.195) loss 1.0830 (1.3113) acc 62.5000 (53.7500) lr 1.9048e-03 eta 0:09:52
epoch [9/50] batch [10/49] time 0.094 (0.191) data 0.000 (0.098) loss 1.3691 (1.3729) acc 53.1250 (54.0625) lr 1.9048e-03 eta 0:06:32
epoch [9/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.065) loss 1.6006 (1.3827) acc 43.7500 (55.2083) lr 1.9048e-03 eta 0:05:24
epoch [9/50] batch [20/49] time 0.093 (0.143) data 0.000 (0.049) loss 1.1699 (1.3685) acc 68.7500 (56.2500) lr 1.9048e-03 eta 0:04:50
epoch [9/50] batch [25/49] time 0.092 (0.133) data 0.000 (0.040) loss 1.2920 (1.3925) acc 62.5000 (56.5000) lr 1.9048e-03 eta 0:04:29
epoch [9/50] batch [30/49] time 0.092 (0.126) data 0.000 (0.033) loss 1.0166 (1.3879) acc 65.6250 (56.5625) lr 1.9048e-03 eta 0:04:15
epoch [9/50] batch [35/49] time 0.094 (0.121) data 0.000 (0.028) loss 1.3721 (1.3695) acc 68.7500 (57.4107) lr 1.9048e-03 eta 0:04:05
epoch [9/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.3105 (1.3615) acc 56.2500 (58.0469) lr 1.9048e-03 eta 0:03:57
epoch [9/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.0605 (1.3339) acc 65.6250 (58.9583) lr 1.9048e-03 eta 0:03:51
epoch [10/50] batch [5/49] time 0.092 (0.289) data 0.000 (0.195) loss 1.4043 (1.4596) acc 59.3750 (60.0000) lr 1.8763e-03 eta 0:09:38
epoch [10/50] batch [10/49] time 0.092 (0.191) data 0.000 (0.098) loss 1.4424 (1.3740) acc 50.0000 (60.9375) lr 1.8763e-03 eta 0:06:22
epoch [10/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.065) loss 1.2666 (1.3660) acc 59.3750 (60.0000) lr 1.8763e-03 eta 0:05:16
epoch [10/50] batch [20/49] time 0.094 (0.142) data 0.000 (0.049) loss 1.5127 (1.3726) acc 46.8750 (58.9062) lr 1.8763e-03 eta 0:04:43
epoch [10/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.039) loss 1.6016 (1.3663) acc 53.1250 (59.1250) lr 1.8763e-03 eta 0:04:23
epoch [10/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.4199 (1.3629) acc 53.1250 (59.3750) lr 1.8763e-03 eta 0:04:09
epoch [10/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.4619 (1.3821) acc 56.2500 (58.7500) lr 1.8763e-03 eta 0:03:59
epoch [10/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4785 (1.3704) acc 56.2500 (58.6719) lr 1.8763e-03 eta 0:03:52
epoch [10/50] batch [45/49] time 0.092 (0.115) data 0.000 (0.022) loss 1.5293 (1.3721) acc 56.2500 (58.5417) lr 1.8763e-03 eta 0:03:45
epoch [11/50] batch [5/49] time 0.094 (0.302) data 0.000 (0.207) loss 1.0801 (1.2967) acc 71.8750 (60.6250) lr 1.8443e-03 eta 0:09:49
epoch [11/50] batch [10/49] time 0.096 (0.197) data 0.000 (0.104) loss 1.0801 (1.4041) acc 81.2500 (59.0625) lr 1.8443e-03 eta 0:06:25
epoch [11/50] batch [15/49] time 0.093 (0.162) data 0.001 (0.069) loss 1.4678 (1.4456) acc 56.2500 (56.6667) lr 1.8443e-03 eta 0:05:16
epoch [11/50] batch [20/49] time 0.093 (0.145) data 0.001 (0.052) loss 1.3545 (1.4323) acc 62.5000 (57.5000) lr 1.8443e-03 eta 0:04:42
epoch [11/50] batch [25/49] time 0.096 (0.135) data 0.000 (0.042) loss 1.3076 (1.4108) acc 56.2500 (58.2500) lr 1.8443e-03 eta 0:04:21
epoch [11/50] batch [30/49] time 0.093 (0.128) data 0.000 (0.035) loss 1.1973 (1.3752) acc 75.0000 (59.3750) lr 1.8443e-03 eta 0:04:07
epoch [11/50] batch [35/49] time 0.096 (0.123) data 0.001 (0.030) loss 1.0742 (1.3522) acc 71.8750 (60.4464) lr 1.8443e-03 eta 0:03:57
epoch [11/50] batch [40/49] time 0.093 (0.120) data 0.000 (0.026) loss 1.5381 (1.3594) acc 56.2500 (60.3125) lr 1.8443e-03 eta 0:03:49
epoch [11/50] batch [45/49] time 0.093 (0.117) data 0.000 (0.023) loss 1.4121 (1.3459) acc 62.5000 (60.7639) lr 1.8443e-03 eta 0:03:43
epoch [12/50] batch [5/49] time 0.092 (0.288) data 0.000 (0.195) loss 1.3252 (1.2451) acc 56.2500 (61.2500) lr 1.8090e-03 eta 0:09:09
epoch [12/50] batch [10/49] time 0.096 (0.191) data 0.000 (0.098) loss 1.5723 (1.3325) acc 50.0000 (58.7500) lr 1.8090e-03 eta 0:06:03
epoch [12/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.065) loss 1.2031 (1.3033) acc 62.5000 (61.4583) lr 1.8090e-03 eta 0:05:00
epoch [12/50] batch [20/49] time 0.092 (0.142) data 0.000 (0.049) loss 1.3320 (1.2951) acc 62.5000 (61.8750) lr 1.8090e-03 eta 0:04:29
epoch [12/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.039) loss 1.3604 (1.2925) acc 56.2500 (61.7500) lr 1.8090e-03 eta 0:04:09
epoch [12/50] batch [30/49] time 0.096 (0.126) data 0.000 (0.033) loss 1.0586 (1.3031) acc 53.1250 (61.1458) lr 1.8090e-03 eta 0:03:57
epoch [12/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.2363 (1.3069) acc 53.1250 (60.8036) lr 1.8090e-03 eta 0:03:47
epoch [12/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.5303 (1.3253) acc 50.0000 (60.5469) lr 1.8090e-03 eta 0:03:40
epoch [12/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.5605 (1.3273) acc 62.5000 (61.1806) lr 1.8090e-03 eta 0:03:34
epoch [13/50] batch [5/49] time 0.093 (0.322) data 0.000 (0.228) loss 1.1475 (1.2121) acc 71.8750 (65.6250) lr 1.7705e-03 eta 0:09:58
epoch [13/50] batch [10/49] time 0.093 (0.208) data 0.000 (0.114) loss 1.3896 (1.3903) acc 65.6250 (63.1250) lr 1.7705e-03 eta 0:06:24
epoch [13/50] batch [15/49] time 0.093 (0.169) data 0.000 (0.076) loss 0.9668 (1.3854) acc 71.8750 (62.0833) lr 1.7705e-03 eta 0:05:12
epoch [13/50] batch [20/49] time 0.093 (0.150) data 0.000 (0.057) loss 1.4102 (1.3416) acc 53.1250 (62.3438) lr 1.7705e-03 eta 0:04:36
epoch [13/50] batch [25/49] time 0.093 (0.139) data 0.000 (0.046) loss 1.4209 (1.3884) acc 53.1250 (60.6250) lr 1.7705e-03 eta 0:04:14
epoch [13/50] batch [30/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.5029 (1.3965) acc 53.1250 (60.3125) lr 1.7705e-03 eta 0:04:00
epoch [13/50] batch [35/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.4902 (1.3817) acc 56.2500 (60.4464) lr 1.7705e-03 eta 0:03:49
epoch [13/50] batch [40/49] time 0.092 (0.121) data 0.000 (0.029) loss 1.3730 (1.3496) acc 68.7500 (61.2500) lr 1.7705e-03 eta 0:03:41
epoch [13/50] batch [45/49] time 0.093 (0.118) data 0.000 (0.026) loss 1.6963 (1.3630) acc 59.3750 (60.9028) lr 1.7705e-03 eta 0:03:34
epoch [14/50] batch [5/49] time 0.095 (0.297) data 0.000 (0.203) loss 1.1787 (1.2865) acc 59.3750 (60.0000) lr 1.7290e-03 eta 0:08:57
epoch [14/50] batch [10/49] time 0.092 (0.195) data 0.000 (0.102) loss 1.3369 (1.2906) acc 68.7500 (62.8125) lr 1.7290e-03 eta 0:05:51
epoch [14/50] batch [15/49] time 0.094 (0.162) data 0.001 (0.068) loss 1.2295 (1.2548) acc 71.8750 (66.2500) lr 1.7290e-03 eta 0:04:50
epoch [14/50] batch [20/49] time 0.092 (0.144) data 0.000 (0.051) loss 1.2354 (1.2685) acc 59.3750 (64.5312) lr 1.7290e-03 eta 0:04:18
epoch [14/50] batch [25/49] time 0.093 (0.134) data 0.000 (0.041) loss 1.4131 (1.2599) acc 50.0000 (63.8750) lr 1.7290e-03 eta 0:03:59
epoch [14/50] batch [30/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.2461 (1.2726) acc 59.3750 (62.7083) lr 1.7290e-03 eta 0:03:46
epoch [14/50] batch [35/49] time 0.092 (0.122) data 0.000 (0.029) loss 1.4248 (1.2900) acc 56.2500 (62.2321) lr 1.7290e-03 eta 0:03:37
epoch [14/50] batch [40/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.5908 (1.2892) acc 59.3750 (61.7969) lr 1.7290e-03 eta 0:03:30
epoch [14/50] batch [45/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.3242 (1.3027) acc 62.5000 (61.0417) lr 1.7290e-03 eta 0:03:24
epoch [15/50] batch [5/49] time 0.094 (0.290) data 0.000 (0.196) loss 1.2471 (1.4268) acc 65.6250 (58.7500) lr 1.6845e-03 eta 0:08:30
epoch [15/50] batch [10/49] time 0.092 (0.192) data 0.000 (0.098) loss 1.4961 (1.3894) acc 62.5000 (61.2500) lr 1.6845e-03 eta 0:05:35
epoch [15/50] batch [15/49] time 0.093 (0.159) data 0.000 (0.065) loss 1.2568 (1.3375) acc 56.2500 (61.0417) lr 1.6845e-03 eta 0:04:37
epoch [15/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.049) loss 1.2783 (1.3073) acc 53.1250 (60.9375) lr 1.6845e-03 eta 0:04:08
epoch [15/50] batch [25/49] time 0.093 (0.132) data 0.000 (0.039) loss 1.3604 (1.3067) acc 68.7500 (61.2500) lr 1.6845e-03 eta 0:03:50
epoch [15/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.3613 (1.3089) acc 50.0000 (60.8333) lr 1.6845e-03 eta 0:03:38
epoch [15/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.4297 (1.3181) acc 62.5000 (60.6250) lr 1.6845e-03 eta 0:03:29
epoch [15/50] batch [40/49] time 0.092 (0.118) data 0.000 (0.025) loss 1.2559 (1.3216) acc 62.5000 (60.7031) lr 1.6845e-03 eta 0:03:22
epoch [15/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.7129 (1.3326) acc 53.1250 (60.8333) lr 1.6845e-03 eta 0:03:17
epoch [16/50] batch [5/49] time 0.092 (0.276) data 0.000 (0.183) loss 1.2158 (1.3535) acc 65.6250 (62.5000) lr 1.6374e-03 eta 0:07:52
epoch [16/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.092) loss 1.3535 (1.3495) acc 65.6250 (62.5000) lr 1.6374e-03 eta 0:05:14
epoch [16/50] batch [15/49] time 0.093 (0.154) data 0.000 (0.061) loss 1.2451 (1.3068) acc 71.8750 (63.9583) lr 1.6374e-03 eta 0:04:21
epoch [16/50] batch [20/49] time 0.094 (0.139) data 0.000 (0.046) loss 1.2578 (1.2534) acc 65.6250 (66.5625) lr 1.6374e-03 eta 0:03:55
epoch [16/50] batch [25/49] time 0.094 (0.130) data 0.000 (0.037) loss 1.3125 (1.2659) acc 56.2500 (65.5000) lr 1.6374e-03 eta 0:03:39
epoch [16/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.4150 (1.2701) acc 53.1250 (64.8958) lr 1.6374e-03 eta 0:03:28
epoch [16/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.5928 (1.2946) acc 62.5000 (64.2857) lr 1.6374e-03 eta 0:03:20
epoch [16/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.3955 (1.2871) acc 65.6250 (63.9062) lr 1.6374e-03 eta 0:03:14
epoch [16/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.021) loss 1.3818 (1.3159) acc 62.5000 (63.5417) lr 1.6374e-03 eta 0:03:09
epoch [17/50] batch [5/49] time 0.093 (0.298) data 0.000 (0.203) loss 1.1582 (1.2450) acc 78.1250 (65.0000) lr 1.5878e-03 eta 0:08:14
epoch [17/50] batch [10/49] time 0.093 (0.195) data 0.000 (0.102) loss 1.0801 (1.2072) acc 59.3750 (65.0000) lr 1.5878e-03 eta 0:05:23
epoch [17/50] batch [15/49] time 0.092 (0.161) data 0.000 (0.068) loss 1.1504 (1.2328) acc 56.2500 (63.5417) lr 1.5878e-03 eta 0:04:25
epoch [17/50] batch [20/49] time 0.093 (0.144) data 0.000 (0.051) loss 1.4658 (1.2627) acc 65.6250 (64.3750) lr 1.5878e-03 eta 0:03:57
epoch [17/50] batch [25/49] time 0.095 (0.134) data 0.000 (0.041) loss 1.3857 (1.2731) acc 65.6250 (64.2500) lr 1.5878e-03 eta 0:03:40
epoch [17/50] batch [30/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.2129 (1.2705) acc 56.2500 (63.6458) lr 1.5878e-03 eta 0:03:28
epoch [17/50] batch [35/49] time 0.096 (0.123) data 0.000 (0.029) loss 1.2480 (1.2785) acc 59.3750 (63.3929) lr 1.5878e-03 eta 0:03:19
epoch [17/50] batch [40/49] time 0.093 (0.119) data 0.000 (0.026) loss 0.8237 (1.2730) acc 75.0000 (63.8281) lr 1.5878e-03 eta 0:03:13
epoch [17/50] batch [45/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.0156 (1.2694) acc 68.7500 (63.8889) lr 1.5878e-03 eta 0:03:07
epoch [18/50] batch [5/49] time 0.093 (0.261) data 0.000 (0.167) loss 1.1504 (1.2492) acc 71.8750 (65.6250) lr 1.5358e-03 eta 0:07:00
epoch [18/50] batch [10/49] time 0.093 (0.177) data 0.000 (0.084) loss 0.9619 (1.2319) acc 75.0000 (65.0000) lr 1.5358e-03 eta 0:04:44
epoch [18/50] batch [15/49] time 0.093 (0.149) data 0.000 (0.056) loss 1.3496 (1.2175) acc 68.7500 (66.2500) lr 1.5358e-03 eta 0:03:58
epoch [18/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.042) loss 0.8545 (1.2239) acc 78.1250 (65.9375) lr 1.5358e-03 eta 0:03:35
epoch [18/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.034) loss 0.8584 (1.1951) acc 81.2500 (66.8750) lr 1.5358e-03 eta 0:03:21
epoch [18/50] batch [30/49] time 0.094 (0.121) data 0.000 (0.028) loss 1.4414 (1.2041) acc 56.2500 (67.7083) lr 1.5358e-03 eta 0:03:12
epoch [18/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.1973 (1.2086) acc 65.6250 (66.8750) lr 1.5358e-03 eta 0:03:05
epoch [18/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 0.8232 (1.2042) acc 75.0000 (67.0312) lr 1.5358e-03 eta 0:03:00
epoch [18/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.2236 (1.1966) acc 62.5000 (66.3889) lr 1.5358e-03 eta 0:02:55
epoch [19/50] batch [5/49] time 0.099 (0.283) data 0.000 (0.188) loss 1.2666 (1.1836) acc 65.6250 (72.5000) lr 1.4818e-03 eta 0:07:21
epoch [19/50] batch [10/49] time 0.094 (0.188) data 0.001 (0.094) loss 1.0449 (1.1783) acc 62.5000 (69.6875) lr 1.4818e-03 eta 0:04:52
epoch [19/50] batch [15/49] time 0.093 (0.157) data 0.000 (0.063) loss 1.4600 (1.1993) acc 65.6250 (69.3750) lr 1.4818e-03 eta 0:04:03
epoch [19/50] batch [20/49] time 0.093 (0.141) data 0.000 (0.047) loss 1.2725 (1.2105) acc 68.7500 (68.5938) lr 1.4818e-03 eta 0:03:38
epoch [19/50] batch [25/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.3535 (1.2408) acc 65.6250 (67.3750) lr 1.4818e-03 eta 0:03:22
epoch [19/50] batch [30/49] time 0.092 (0.125) data 0.000 (0.032) loss 1.0859 (1.2280) acc 71.8750 (66.8750) lr 1.4818e-03 eta 0:03:12
epoch [19/50] batch [35/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.2979 (1.2333) acc 56.2500 (66.6071) lr 1.4818e-03 eta 0:03:04
epoch [19/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.8193 (1.2493) acc 53.1250 (65.6250) lr 1.4818e-03 eta 0:02:58
epoch [19/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.021) loss 1.2393 (1.2372) acc 59.3750 (65.9722) lr 1.4818e-03 eta 0:02:54
epoch [20/50] batch [5/49] time 0.093 (0.286) data 0.000 (0.193) loss 1.3828 (1.3410) acc 62.5000 (61.2500) lr 1.4258e-03 eta 0:07:13
epoch [20/50] batch [10/49] time 0.093 (0.190) data 0.000 (0.097) loss 1.3516 (1.2429) acc 68.7500 (65.6250) lr 1.4258e-03 eta 0:04:46
epoch [20/50] batch [15/49] time 0.093 (0.158) data 0.000 (0.065) loss 1.2725 (1.2479) acc 71.8750 (66.6667) lr 1.4258e-03 eta 0:03:57
epoch [20/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.048) loss 1.1357 (1.2192) acc 71.8750 (68.4375) lr 1.4258e-03 eta 0:03:32
epoch [20/50] batch [25/49] time 0.093 (0.132) data 0.000 (0.039) loss 0.9990 (1.2142) acc 68.7500 (67.8750) lr 1.4258e-03 eta 0:03:17
epoch [20/50] batch [30/49] time 0.094 (0.125) data 0.000 (0.032) loss 1.6074 (1.2569) acc 68.7500 (67.2917) lr 1.4258e-03 eta 0:03:06
epoch [20/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.3486 (1.2480) acc 62.5000 (67.5893) lr 1.4258e-03 eta 0:02:59
epoch [20/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.024) loss 1.2861 (1.2362) acc 65.6250 (68.1250) lr 1.4258e-03 eta 0:02:53
epoch [20/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.2441 (1.2217) acc 71.8750 (68.4028) lr 1.4258e-03 eta 0:02:49
epoch [21/50] batch [5/49] time 0.093 (0.286) data 0.000 (0.192) loss 1.3213 (1.0954) acc 68.7500 (68.1250) lr 1.3681e-03 eta 0:06:58
epoch [21/50] batch [10/49] time 0.092 (0.190) data 0.000 (0.096) loss 1.4463 (1.1572) acc 59.3750 (67.5000) lr 1.3681e-03 eta 0:04:36
epoch [21/50] batch [15/49] time 0.093 (0.158) data 0.000 (0.064) loss 1.3516 (1.2086) acc 53.1250 (65.0000) lr 1.3681e-03 eta 0:03:49
epoch [21/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.048) loss 1.2588 (1.2294) acc 56.2500 (64.5312) lr 1.3681e-03 eta 0:03:25
epoch [21/50] batch [25/49] time 0.093 (0.132) data 0.000 (0.039) loss 0.9043 (1.2017) acc 75.0000 (66.0000) lr 1.3681e-03 eta 0:03:11
epoch [21/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.032) loss 1.5371 (1.2197) acc 59.3750 (65.6250) lr 1.3681e-03 eta 0:03:00
epoch [21/50] batch [35/49] time 0.094 (0.121) data 0.000 (0.028) loss 1.1348 (1.1973) acc 71.8750 (66.0714) lr 1.3681e-03 eta 0:02:53
epoch [21/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.024) loss 1.1172 (1.2185) acc 59.3750 (65.0000) lr 1.3681e-03 eta 0:02:48
epoch [21/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.3779 (1.2092) acc 59.3750 (65.2778) lr 1.3681e-03 eta 0:02:43
epoch [22/50] batch [5/49] time 0.096 (0.273) data 0.000 (0.177) loss 1.1064 (1.0468) acc 75.0000 (75.0000) lr 1.3090e-03 eta 0:06:26
epoch [22/50] batch [10/49] time 0.092 (0.183) data 0.000 (0.089) loss 1.0732 (1.1541) acc 68.7500 (71.8750) lr 1.3090e-03 eta 0:04:18
epoch [22/50] batch [15/49] time 0.093 (0.154) data 0.000 (0.059) loss 1.0869 (1.1593) acc 65.6250 (71.4583) lr 1.3090e-03 eta 0:03:35
epoch [22/50] batch [20/49] time 0.093 (0.139) data 0.000 (0.045) loss 1.0527 (1.2047) acc 75.0000 (69.6875) lr 1.3090e-03 eta 0:03:14
epoch [22/50] batch [25/49] time 0.094 (0.130) data 0.000 (0.036) loss 0.9814 (1.1925) acc 75.0000 (69.1250) lr 1.3090e-03 eta 0:03:01
epoch [22/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 1.1914 (1.1857) acc 62.5000 (69.2708) lr 1.3090e-03 eta 0:02:52
epoch [22/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.2559 (1.1968) acc 71.8750 (68.6607) lr 1.3090e-03 eta 0:02:45
epoch [22/50] batch [40/49] time 0.094 (0.116) data 0.000 (0.023) loss 1.3877 (1.2049) acc 59.3750 (68.0469) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.2119 (1.2125) acc 62.5000 (68.1250) lr 1.3090e-03 eta 0:02:36
epoch [23/50] batch [5/49] time 0.093 (0.265) data 0.000 (0.171) loss 1.2891 (1.1063) acc 65.6250 (70.6250) lr 1.2487e-03 eta 0:06:02
epoch [23/50] batch [10/49] time 0.092 (0.179) data 0.000 (0.086) loss 0.9033 (1.1554) acc 78.1250 (69.3750) lr 1.2487e-03 eta 0:04:03
epoch [23/50] batch [15/49] time 0.093 (0.150) data 0.000 (0.057) loss 1.5156 (1.1845) acc 62.5000 (67.9167) lr 1.2487e-03 eta 0:03:23
epoch [23/50] batch [20/49] time 0.092 (0.136) data 0.000 (0.043) loss 1.2041 (1.1900) acc 78.1250 (69.0625) lr 1.2487e-03 eta 0:03:03
epoch [23/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.5371 (1.1974) acc 56.2500 (68.7500) lr 1.2487e-03 eta 0:02:51
epoch [23/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 1.2246 (1.1966) acc 62.5000 (68.6458) lr 1.2487e-03 eta 0:02:43
epoch [23/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.2842 (1.2111) acc 65.6250 (68.6607) lr 1.2487e-03 eta 0:02:37
epoch [23/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 0.8267 (1.1905) acc 87.5000 (69.1406) lr 1.2487e-03 eta 0:02:32
epoch [23/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.2666 (1.1912) acc 71.8750 (69.0972) lr 1.2487e-03 eta 0:02:28
epoch [24/50] batch [5/49] time 0.092 (0.314) data 0.000 (0.221) loss 1.1670 (1.2393) acc 81.2500 (68.1250) lr 1.1874e-03 eta 0:06:53
epoch [24/50] batch [10/49] time 0.094 (0.204) data 0.000 (0.111) loss 0.9443 (1.2281) acc 75.0000 (68.4375) lr 1.1874e-03 eta 0:04:27
epoch [24/50] batch [15/49] time 0.092 (0.167) data 0.000 (0.074) loss 1.1846 (1.1938) acc 71.8750 (69.3750) lr 1.1874e-03 eta 0:03:38
epoch [24/50] batch [20/49] time 0.092 (0.149) data 0.000 (0.056) loss 1.2871 (1.2014) acc 65.6250 (70.0000) lr 1.1874e-03 eta 0:03:13
epoch [24/50] batch [25/49] time 0.093 (0.137) data 0.000 (0.044) loss 1.5664 (1.2258) acc 62.5000 (69.0000) lr 1.1874e-03 eta 0:02:58
epoch [24/50] batch [30/49] time 0.094 (0.130) data 0.000 (0.037) loss 1.3447 (1.2155) acc 71.8750 (69.5833) lr 1.1874e-03 eta 0:02:48
epoch [24/50] batch [35/49] time 0.093 (0.125) data 0.000 (0.032) loss 1.4033 (1.2192) acc 50.0000 (68.9286) lr 1.1874e-03 eta 0:02:41
epoch [24/50] batch [40/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.2100 (1.2193) acc 65.6250 (68.8281) lr 1.1874e-03 eta 0:02:35
epoch [24/50] batch [45/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.4727 (1.2130) acc 53.1250 (69.0278) lr 1.1874e-03 eta 0:02:30
epoch [25/50] batch [5/49] time 0.093 (0.272) data 0.000 (0.178) loss 1.0488 (1.0608) acc 68.7500 (75.0000) lr 1.1253e-03 eta 0:05:45
epoch [25/50] batch [10/49] time 0.093 (0.183) data 0.000 (0.089) loss 1.2646 (1.0281) acc 59.3750 (73.7500) lr 1.1253e-03 eta 0:03:51
epoch [25/50] batch [15/49] time 0.099 (0.153) data 0.000 (0.060) loss 1.2051 (1.0711) acc 71.8750 (72.7083) lr 1.1253e-03 eta 0:03:13
epoch [25/50] batch [20/49] time 0.094 (0.139) data 0.000 (0.045) loss 0.9214 (1.0626) acc 78.1250 (73.5938) lr 1.1253e-03 eta 0:02:53
epoch [25/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.036) loss 1.4639 (1.1043) acc 62.5000 (71.8750) lr 1.1253e-03 eta 0:02:42
epoch [25/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 1.1074 (1.1113) acc 75.0000 (72.2917) lr 1.1253e-03 eta 0:02:33
epoch [25/50] batch [35/49] time 0.092 (0.119) data 0.000 (0.026) loss 1.1973 (1.1030) acc 71.8750 (72.5893) lr 1.1253e-03 eta 0:02:27
epoch [25/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.023) loss 1.6758 (1.1174) acc 56.2500 (71.9531) lr 1.1253e-03 eta 0:02:23
epoch [25/50] batch [45/49] time 0.092 (0.113) data 0.000 (0.020) loss 1.0547 (1.1188) acc 81.2500 (72.2222) lr 1.1253e-03 eta 0:02:19
epoch [26/50] batch [5/49] time 0.094 (0.291) data 0.001 (0.198) loss 0.9868 (1.1158) acc 78.1250 (71.2500) lr 1.0628e-03 eta 0:05:55
epoch [26/50] batch [10/49] time 0.094 (0.192) data 0.000 (0.099) loss 1.2480 (1.0979) acc 65.6250 (73.4375) lr 1.0628e-03 eta 0:03:53
epoch [26/50] batch [15/49] time 0.093 (0.160) data 0.000 (0.066) loss 1.3105 (1.1425) acc 62.5000 (71.0417) lr 1.0628e-03 eta 0:03:13
epoch [26/50] batch [20/49] time 0.093 (0.143) data 0.000 (0.050) loss 0.9497 (1.1201) acc 71.8750 (71.7188) lr 1.0628e-03 eta 0:02:52
epoch [26/50] batch [25/49] time 0.093 (0.133) data 0.000 (0.040) loss 0.9761 (1.1244) acc 78.1250 (72.1250) lr 1.0628e-03 eta 0:02:39
epoch [26/50] batch [30/49] time 0.095 (0.127) data 0.000 (0.033) loss 1.2939 (1.1487) acc 56.2500 (70.1042) lr 1.0628e-03 eta 0:02:31
epoch [26/50] batch [35/49] time 0.096 (0.122) data 0.000 (0.029) loss 1.2686 (1.1656) acc 62.5000 (69.6429) lr 1.0628e-03 eta 0:02:25
epoch [26/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.025) loss 1.2129 (1.1576) acc 59.3750 (69.3750) lr 1.0628e-03 eta 0:02:20
epoch [26/50] batch [45/49] time 0.092 (0.116) data 0.000 (0.022) loss 1.1055 (1.1577) acc 75.0000 (69.5139) lr 1.0628e-03 eta 0:02:16
epoch [27/50] batch [5/49] time 0.093 (0.275) data 0.000 (0.181) loss 1.3418 (1.0961) acc 75.0000 (74.3750) lr 1.0000e-03 eta 0:05:21
epoch [27/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.091) loss 0.9072 (1.0891) acc 75.0000 (72.8125) lr 1.0000e-03 eta 0:03:34
epoch [27/50] batch [15/49] time 0.093 (0.154) data 0.000 (0.061) loss 0.9482 (1.0687) acc 62.5000 (73.1250) lr 1.0000e-03 eta 0:02:58
epoch [27/50] batch [20/49] time 0.096 (0.139) data 0.000 (0.046) loss 0.9453 (1.0753) acc 78.1250 (72.8125) lr 1.0000e-03 eta 0:02:40
epoch [27/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.037) loss 0.9219 (1.0874) acc 84.3750 (73.0000) lr 1.0000e-03 eta 0:02:29
epoch [27/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 1.4512 (1.1056) acc 62.5000 (72.6042) lr 1.0000e-03 eta 0:02:21
epoch [27/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.1553 (1.1117) acc 65.6250 (72.3214) lr 1.0000e-03 eta 0:02:16
epoch [27/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.4238 (1.1327) acc 62.5000 (71.4844) lr 1.0000e-03 eta 0:02:11
epoch [27/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.020) loss 1.4951 (1.1449) acc 62.5000 (71.0417) lr 1.0000e-03 eta 0:02:08
epoch [28/50] batch [5/49] time 0.096 (0.296) data 0.000 (0.203) loss 1.3916 (1.1357) acc 59.3750 (71.8750) lr 9.3721e-04 eta 0:05:32
epoch [28/50] batch [10/49] time 0.093 (0.195) data 0.000 (0.101) loss 1.1465 (1.1610) acc 62.5000 (70.0000) lr 9.3721e-04 eta 0:03:37
epoch [28/50] batch [15/49] time 0.096 (0.161) data 0.000 (0.068) loss 1.0049 (1.1350) acc 75.0000 (70.2083) lr 9.3721e-04 eta 0:02:58
epoch [28/50] batch [20/49] time 0.093 (0.144) data 0.000 (0.051) loss 1.2871 (1.1574) acc 71.8750 (68.9062) lr 9.3721e-04 eta 0:02:39
epoch [28/50] batch [25/49] time 0.093 (0.134) data 0.000 (0.041) loss 1.0361 (1.1628) acc 71.8750 (68.3750) lr 9.3721e-04 eta 0:02:27
epoch [28/50] batch [30/49] time 0.098 (0.127) data 0.000 (0.034) loss 1.5049 (1.1887) acc 62.5000 (67.9167) lr 9.3721e-04 eta 0:02:19
epoch [28/50] batch [35/49] time 0.093 (0.122) data 0.000 (0.029) loss 0.8457 (1.1975) acc 75.0000 (67.3214) lr 9.3721e-04 eta 0:02:13
epoch [28/50] batch [40/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.1289 (1.1939) acc 68.7500 (67.4219) lr 9.3721e-04 eta 0:02:09
epoch [28/50] batch [45/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.8809 (1.1978) acc 56.2500 (67.6389) lr 9.3721e-04 eta 0:02:05
epoch [29/50] batch [5/49] time 0.093 (0.258) data 0.000 (0.165) loss 1.1670 (1.0277) acc 65.6250 (74.3750) lr 8.7467e-04 eta 0:04:37
epoch [29/50] batch [10/49] time 0.093 (0.176) data 0.000 (0.083) loss 1.3125 (1.1220) acc 65.6250 (71.5625) lr 8.7467e-04 eta 0:03:07
epoch [29/50] batch [15/49] time 0.094 (0.148) data 0.000 (0.055) loss 1.0186 (1.1500) acc 71.8750 (70.4167) lr 8.7467e-04 eta 0:02:37
epoch [29/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.041) loss 1.1309 (1.1709) acc 75.0000 (70.3125) lr 8.7467e-04 eta 0:02:22
epoch [29/50] batch [25/49] time 0.094 (0.126) data 0.000 (0.033) loss 1.0527 (1.1506) acc 65.6250 (70.8750) lr 8.7467e-04 eta 0:02:13
epoch [29/50] batch [30/49] time 0.098 (0.121) data 0.000 (0.028) loss 1.3408 (1.1602) acc 56.2500 (69.8958) lr 8.7467e-04 eta 0:02:06
epoch [29/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.0254 (1.1562) acc 78.1250 (70.3571) lr 8.7467e-04 eta 0:02:02
epoch [29/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.3408 (1.1437) acc 62.5000 (70.7812) lr 8.7467e-04 eta 0:01:58
epoch [29/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0469 (1.1410) acc 78.1250 (70.8333) lr 8.7467e-04 eta 0:01:55
epoch [30/50] batch [5/49] time 0.100 (0.257) data 0.000 (0.161) loss 0.9922 (1.0533) acc 87.5000 (73.7500) lr 8.1262e-04 eta 0:04:23
epoch [30/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.080) loss 0.8779 (1.0202) acc 87.5000 (75.9375) lr 8.1262e-04 eta 0:02:58
epoch [30/50] batch [15/49] time 0.094 (0.148) data 0.000 (0.054) loss 0.7471 (1.0614) acc 84.3750 (75.4167) lr 8.1262e-04 eta 0:02:30
epoch [30/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.041) loss 0.9521 (1.0805) acc 75.0000 (74.0625) lr 8.1262e-04 eta 0:02:15
epoch [30/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.032) loss 1.1846 (1.0893) acc 71.8750 (73.3750) lr 8.1262e-04 eta 0:02:06
epoch [30/50] batch [30/49] time 0.094 (0.121) data 0.001 (0.027) loss 0.8389 (1.0785) acc 78.1250 (73.8542) lr 8.1262e-04 eta 0:02:00
epoch [30/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.023) loss 0.9717 (1.0848) acc 81.2500 (73.6607) lr 8.1262e-04 eta 0:01:56
epoch [30/50] batch [40/49] time 0.092 (0.114) data 0.000 (0.020) loss 1.4443 (1.0866) acc 53.1250 (73.4375) lr 8.1262e-04 eta 0:01:52
epoch [30/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.018) loss 0.8462 (1.0865) acc 78.1250 (73.4722) lr 8.1262e-04 eta 0:01:49
epoch [31/50] batch [5/49] time 0.093 (0.244) data 0.000 (0.151) loss 1.2910 (1.0910) acc 62.5000 (74.3750) lr 7.5131e-04 eta 0:03:58
epoch [31/50] batch [10/49] time 0.092 (0.168) data 0.000 (0.076) loss 1.3105 (1.1470) acc 68.7500 (72.1875) lr 7.5131e-04 eta 0:02:43
epoch [31/50] batch [15/49] time 0.094 (0.143) data 0.000 (0.051) loss 1.3848 (1.1679) acc 65.6250 (70.8333) lr 7.5131e-04 eta 0:02:18
epoch [31/50] batch [20/49] time 0.095 (0.131) data 0.001 (0.038) loss 0.9751 (1.1522) acc 78.1250 (71.5625) lr 7.5131e-04 eta 0:02:05
epoch [31/50] batch [25/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.3096 (1.1320) acc 62.5000 (71.7500) lr 7.5131e-04 eta 0:01:57
epoch [31/50] batch [30/49] time 0.093 (0.119) data 0.000 (0.025) loss 0.9580 (1.1113) acc 71.8750 (72.3958) lr 7.5131e-04 eta 0:01:52
epoch [31/50] batch [35/49] time 0.093 (0.115) data 0.000 (0.022) loss 1.5625 (1.1122) acc 56.2500 (71.6964) lr 7.5131e-04 eta 0:01:48
epoch [31/50] batch [40/49] time 0.093 (0.112) data 0.000 (0.019) loss 0.7974 (1.0960) acc 84.3750 (72.2656) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [45/49] time 0.092 (0.110) data 0.000 (0.017) loss 0.9263 (1.1136) acc 81.2500 (71.3194) lr 7.5131e-04 eta 0:01:42
epoch [32/50] batch [5/49] time 0.093 (0.255) data 0.000 (0.162) loss 0.9575 (1.0565) acc 81.2500 (72.5000) lr 6.9098e-04 eta 0:03:56
epoch [32/50] batch [10/49] time 0.092 (0.174) data 0.000 (0.081) loss 0.9180 (1.1189) acc 81.2500 (73.1250) lr 6.9098e-04 eta 0:02:40
epoch [32/50] batch [15/49] time 0.093 (0.147) data 0.000 (0.054) loss 0.9717 (1.1040) acc 71.8750 (72.5000) lr 6.9098e-04 eta 0:02:14
epoch [32/50] batch [20/49] time 0.097 (0.134) data 0.000 (0.041) loss 0.8940 (1.1066) acc 78.1250 (72.6562) lr 6.9098e-04 eta 0:02:01
epoch [32/50] batch [25/49] time 0.093 (0.126) data 0.000 (0.033) loss 1.1777 (1.1130) acc 68.7500 (71.8750) lr 6.9098e-04 eta 0:01:53
epoch [32/50] batch [30/49] time 0.093 (0.120) data 0.000 (0.027) loss 1.2842 (1.1380) acc 68.7500 (71.4583) lr 6.9098e-04 eta 0:01:48
epoch [32/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.9473 (1.1313) acc 81.2500 (71.4286) lr 6.9098e-04 eta 0:01:44
epoch [32/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.4307 (1.1239) acc 65.6250 (71.6406) lr 6.9098e-04 eta 0:01:41
epoch [32/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.6738 (1.1253) acc 62.5000 (71.5278) lr 6.9098e-04 eta 0:01:38
epoch [33/50] batch [5/49] time 0.093 (0.281) data 0.000 (0.187) loss 1.1426 (1.0541) acc 71.8750 (74.3750) lr 6.3188e-04 eta 0:04:06
epoch [33/50] batch [10/49] time 0.093 (0.187) data 0.000 (0.094) loss 1.1328 (1.0454) acc 78.1250 (75.6250) lr 6.3188e-04 eta 0:02:43
epoch [33/50] batch [15/49] time 0.093 (0.156) data 0.000 (0.062) loss 0.8301 (0.9968) acc 84.3750 (76.4583) lr 6.3188e-04 eta 0:02:15
epoch [33/50] batch [20/49] time 0.093 (0.140) data 0.000 (0.047) loss 1.0977 (1.0306) acc 78.1250 (75.7812) lr 6.3188e-04 eta 0:02:01
epoch [33/50] batch [25/49] time 0.093 (0.131) data 0.000 (0.038) loss 1.0078 (1.0396) acc 78.1250 (74.6250) lr 6.3188e-04 eta 0:01:52
epoch [33/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.031) loss 1.0811 (1.0456) acc 75.0000 (74.2708) lr 6.3188e-04 eta 0:01:46
epoch [33/50] batch [35/49] time 0.095 (0.120) data 0.000 (0.027) loss 0.8027 (1.0406) acc 81.2500 (74.2857) lr 6.3188e-04 eta 0:01:41
epoch [33/50] batch [40/49] time 0.092 (0.117) data 0.000 (0.024) loss 0.7920 (1.0359) acc 87.5000 (74.7656) lr 6.3188e-04 eta 0:01:38
epoch [33/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.1016 (1.0389) acc 71.8750 (75.0694) lr 6.3188e-04 eta 0:01:35
epoch [34/50] batch [5/49] time 0.093 (0.281) data 0.000 (0.186) loss 0.9746 (1.0514) acc 71.8750 (72.5000) lr 5.7422e-04 eta 0:03:52
epoch [34/50] batch [10/49] time 0.093 (0.188) data 0.000 (0.094) loss 1.1152 (1.0567) acc 68.7500 (74.6875) lr 5.7422e-04 eta 0:02:34
epoch [34/50] batch [15/49] time 0.094 (0.157) data 0.000 (0.063) loss 1.2979 (1.1299) acc 71.8750 (73.1250) lr 5.7422e-04 eta 0:02:08
epoch [34/50] batch [20/49] time 0.098 (0.141) data 0.000 (0.047) loss 0.9380 (1.1074) acc 78.1250 (73.5938) lr 5.7422e-04 eta 0:01:55
epoch [34/50] batch [25/49] time 0.092 (0.132) data 0.000 (0.038) loss 0.7300 (1.0779) acc 84.3750 (74.2500) lr 5.7422e-04 eta 0:01:46
epoch [34/50] batch [30/49] time 0.093 (0.125) data 0.000 (0.031) loss 0.8716 (1.0866) acc 84.3750 (73.5417) lr 5.7422e-04 eta 0:01:40
epoch [34/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.027) loss 0.7349 (1.0791) acc 84.3750 (73.3036) lr 5.7422e-04 eta 0:01:36
epoch [34/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.1846 (1.0932) acc 75.0000 (72.9688) lr 5.7422e-04 eta 0:01:32
epoch [34/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 0.9341 (1.0608) acc 71.8750 (73.8889) lr 5.7422e-04 eta 0:01:30
epoch [35/50] batch [5/49] time 0.093 (0.274) data 0.000 (0.181) loss 0.8379 (1.1609) acc 84.3750 (73.7500) lr 5.1825e-04 eta 0:03:33
epoch [35/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.090) loss 1.2129 (1.1609) acc 78.1250 (73.4375) lr 5.1825e-04 eta 0:02:22
epoch [35/50] batch [15/49] time 0.094 (0.153) data 0.001 (0.060) loss 1.1035 (1.1322) acc 78.1250 (73.7500) lr 5.1825e-04 eta 0:01:57
epoch [35/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.045) loss 1.2646 (1.1634) acc 68.7500 (72.0312) lr 5.1825e-04 eta 0:01:45
epoch [35/50] batch [25/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.1211 (1.1622) acc 71.8750 (71.7500) lr 5.1825e-04 eta 0:01:38
epoch [35/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 1.1426 (1.1803) acc 75.0000 (71.7708) lr 5.1825e-04 eta 0:01:33
epoch [35/50] batch [35/49] time 0.095 (0.119) data 0.000 (0.026) loss 1.4170 (1.1695) acc 56.2500 (71.9643) lr 5.1825e-04 eta 0:01:29
epoch [35/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 0.7393 (1.1505) acc 84.3750 (72.2656) lr 5.1825e-04 eta 0:01:26
epoch [35/50] batch [45/49] time 0.094 (0.113) data 0.000 (0.020) loss 1.0957 (1.1537) acc 68.7500 (72.1528) lr 5.1825e-04 eta 0:01:23
epoch [36/50] batch [5/49] time 0.093 (0.283) data 0.000 (0.188) loss 0.6958 (0.9361) acc 87.5000 (78.7500) lr 4.6417e-04 eta 0:03:26
epoch [36/50] batch [10/49] time 0.093 (0.189) data 0.000 (0.094) loss 0.8257 (0.9830) acc 81.2500 (79.3750) lr 4.6417e-04 eta 0:02:16
epoch [36/50] batch [15/49] time 0.093 (0.157) data 0.000 (0.063) loss 1.0469 (0.9817) acc 71.8750 (78.7500) lr 4.6417e-04 eta 0:01:52
epoch [36/50] batch [20/49] time 0.093 (0.141) data 0.000 (0.047) loss 1.3848 (1.0205) acc 62.5000 (76.7188) lr 4.6417e-04 eta 0:01:40
epoch [36/50] batch [25/49] time 0.092 (0.131) data 0.000 (0.038) loss 1.0352 (1.0187) acc 68.7500 (76.0000) lr 4.6417e-04 eta 0:01:33
epoch [36/50] batch [30/49] time 0.097 (0.125) data 0.000 (0.032) loss 0.6875 (1.0375) acc 90.6250 (75.2083) lr 4.6417e-04 eta 0:01:28
epoch [36/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.027) loss 0.7832 (1.0427) acc 84.3750 (74.5536) lr 4.6417e-04 eta 0:01:24
epoch [36/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 1.3340 (1.0457) acc 65.6250 (74.2969) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [45/49] time 0.092 (0.114) data 0.000 (0.021) loss 0.9136 (1.0479) acc 81.2500 (74.4444) lr 4.6417e-04 eta 0:01:19
epoch [37/50] batch [5/49] time 0.092 (0.263) data 0.000 (0.169) loss 1.3193 (1.2199) acc 62.5000 (67.5000) lr 4.1221e-04 eta 0:02:58
epoch [37/50] batch [10/49] time 0.097 (0.178) data 0.000 (0.085) loss 1.0254 (1.1149) acc 71.8750 (72.1875) lr 4.1221e-04 eta 0:02:00
epoch [37/50] batch [15/49] time 0.093 (0.150) data 0.001 (0.057) loss 1.1025 (1.1088) acc 71.8750 (72.7083) lr 4.1221e-04 eta 0:01:40
epoch [37/50] batch [20/49] time 0.093 (0.136) data 0.000 (0.043) loss 0.6758 (1.0868) acc 87.5000 (73.4375) lr 4.1221e-04 eta 0:01:30
epoch [37/50] batch [25/49] time 0.095 (0.127) data 0.000 (0.034) loss 1.0771 (1.0835) acc 71.8750 (73.7500) lr 4.1221e-04 eta 0:01:24
epoch [37/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.029) loss 0.8042 (1.0500) acc 78.1250 (74.4792) lr 4.1221e-04 eta 0:01:19
epoch [37/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.025) loss 0.8169 (1.0404) acc 87.5000 (74.8214) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.021) loss 1.3916 (1.0687) acc 59.3750 (73.5938) lr 4.1221e-04 eta 0:01:14
epoch [37/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.3145 (1.0722) acc 65.6250 (73.4028) lr 4.1221e-04 eta 0:01:11
epoch [38/50] batch [5/49] time 0.093 (0.258) data 0.000 (0.164) loss 1.1631 (1.1235) acc 68.7500 (73.1250) lr 3.6258e-04 eta 0:02:43
epoch [38/50] batch [10/49] time 0.095 (0.177) data 0.000 (0.082) loss 1.3779 (1.1406) acc 68.7500 (72.8125) lr 3.6258e-04 eta 0:01:50
epoch [38/50] batch [15/49] time 0.093 (0.149) data 0.000 (0.055) loss 1.1191 (1.0884) acc 75.0000 (74.1667) lr 3.6258e-04 eta 0:01:32
epoch [38/50] batch [20/49] time 0.094 (0.136) data 0.001 (0.041) loss 0.9155 (1.0430) acc 65.6250 (74.8438) lr 3.6258e-04 eta 0:01:23
epoch [38/50] batch [25/49] time 0.095 (0.128) data 0.001 (0.033) loss 0.9888 (1.0472) acc 71.8750 (74.8750) lr 3.6258e-04 eta 0:01:18
epoch [38/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.028) loss 1.2119 (1.0628) acc 75.0000 (73.9583) lr 3.6258e-04 eta 0:01:13
epoch [38/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.024) loss 0.7676 (1.0556) acc 81.2500 (73.3929) lr 3.6258e-04 eta 0:01:10
epoch [38/50] batch [40/49] time 0.092 (0.115) data 0.000 (0.021) loss 0.7720 (1.0501) acc 87.5000 (73.5156) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [45/49] time 0.092 (0.112) data 0.000 (0.019) loss 1.0684 (1.0363) acc 56.2500 (73.5417) lr 3.6258e-04 eta 0:01:06
epoch [39/50] batch [5/49] time 0.092 (0.252) data 0.000 (0.158) loss 0.7246 (0.9197) acc 81.2500 (78.1250) lr 3.1545e-04 eta 0:02:26
epoch [39/50] batch [10/49] time 0.093 (0.173) data 0.000 (0.080) loss 1.1982 (1.0125) acc 68.7500 (75.0000) lr 3.1545e-04 eta 0:01:39
epoch [39/50] batch [15/49] time 0.093 (0.146) data 0.000 (0.053) loss 1.2148 (0.9887) acc 78.1250 (76.2500) lr 3.1545e-04 eta 0:01:23
epoch [39/50] batch [20/49] time 0.093 (0.133) data 0.000 (0.040) loss 1.1152 (0.9939) acc 62.5000 (76.4062) lr 3.1545e-04 eta 0:01:15
epoch [39/50] batch [25/49] time 0.097 (0.125) data 0.000 (0.032) loss 0.9565 (1.0225) acc 71.8750 (75.5000) lr 3.1545e-04 eta 0:01:10
epoch [39/50] batch [30/49] time 0.096 (0.120) data 0.000 (0.027) loss 1.5527 (1.0765) acc 56.2500 (73.6458) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [35/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.2676 (1.0764) acc 71.8750 (73.8393) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [40/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.2412 (1.0723) acc 62.5000 (73.9062) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [45/49] time 0.093 (0.111) data 0.000 (0.018) loss 1.0430 (1.0715) acc 75.0000 (73.8889) lr 3.1545e-04 eta 0:01:00
epoch [40/50] batch [5/49] time 0.095 (0.275) data 0.000 (0.180) loss 1.0781 (1.0482) acc 75.0000 (76.2500) lr 2.7103e-04 eta 0:02:26
epoch [40/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.090) loss 1.0107 (1.0023) acc 75.0000 (76.8750) lr 2.7103e-04 eta 0:01:37
epoch [40/50] batch [15/49] time 0.093 (0.154) data 0.000 (0.060) loss 1.2549 (1.0210) acc 68.7500 (76.2500) lr 2.7103e-04 eta 0:01:20
epoch [40/50] batch [20/49] time 0.093 (0.139) data 0.000 (0.045) loss 0.7925 (0.9998) acc 81.2500 (76.5625) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.036) loss 1.0332 (0.9981) acc 81.2500 (76.6250) lr 2.7103e-04 eta 0:01:06
epoch [40/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 1.1660 (1.0014) acc 59.3750 (76.0417) lr 2.7103e-04 eta 0:01:03
epoch [40/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.026) loss 1.3398 (1.0097) acc 59.3750 (75.8036) lr 2.7103e-04 eta 0:01:00
epoch [40/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.1152 (1.0272) acc 71.8750 (75.6250) lr 2.7103e-04 eta 0:00:58
epoch [40/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.020) loss 0.7305 (0.9993) acc 81.2500 (76.4583) lr 2.7103e-04 eta 0:00:56
epoch [41/50] batch [5/49] time 0.097 (0.273) data 0.000 (0.178) loss 0.6172 (0.9084) acc 90.6250 (76.8750) lr 2.2949e-04 eta 0:02:12
epoch [41/50] batch [10/49] time 0.093 (0.184) data 0.000 (0.089) loss 1.0918 (1.0519) acc 78.1250 (74.0625) lr 2.2949e-04 eta 0:01:28
epoch [41/50] batch [15/49] time 0.093 (0.153) data 0.000 (0.060) loss 0.8438 (1.0299) acc 81.2500 (74.1667) lr 2.2949e-04 eta 0:01:12
epoch [41/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.045) loss 0.8940 (1.0089) acc 81.2500 (75.3125) lr 2.2949e-04 eta 0:01:04
epoch [41/50] batch [25/49] time 0.093 (0.129) data 0.000 (0.036) loss 1.0244 (0.9994) acc 78.1250 (76.5000) lr 2.2949e-04 eta 0:01:00
epoch [41/50] batch [30/49] time 0.093 (0.123) data 0.000 (0.030) loss 0.8247 (0.9862) acc 81.2500 (76.4583) lr 2.2949e-04 eta 0:00:56
epoch [41/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 1.2646 (1.0031) acc 71.8750 (75.4464) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.3799 (1.0072) acc 65.6250 (75.2344) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 0.9297 (1.0034) acc 71.8750 (75.1389) lr 2.2949e-04 eta 0:00:50
epoch [42/50] batch [5/49] time 0.093 (0.280) data 0.000 (0.184) loss 1.2695 (1.1283) acc 71.8750 (71.2500) lr 1.9098e-04 eta 0:02:01
epoch [42/50] batch [10/49] time 0.093 (0.187) data 0.000 (0.092) loss 1.1182 (1.2110) acc 68.7500 (70.3125) lr 1.9098e-04 eta 0:01:20
epoch [42/50] batch [15/49] time 0.093 (0.156) data 0.000 (0.062) loss 1.0020 (1.1725) acc 78.1250 (72.7083) lr 1.9098e-04 eta 0:01:06
epoch [42/50] batch [20/49] time 0.093 (0.140) data 0.000 (0.046) loss 1.1055 (1.1172) acc 71.8750 (73.4375) lr 1.9098e-04 eta 0:00:58
epoch [42/50] batch [25/49] time 0.093 (0.131) data 0.000 (0.037) loss 0.8604 (1.0857) acc 84.3750 (74.5000) lr 1.9098e-04 eta 0:00:54
epoch [42/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.031) loss 0.9033 (1.0757) acc 84.3750 (74.3750) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [35/49] time 0.092 (0.120) data 0.000 (0.027) loss 1.1748 (1.0638) acc 62.5000 (74.7321) lr 1.9098e-04 eta 0:00:48
epoch [42/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.023) loss 1.0518 (1.0641) acc 75.0000 (74.8438) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.2393 (1.0648) acc 68.7500 (74.7222) lr 1.9098e-04 eta 0:00:45
epoch [43/50] batch [5/49] time 0.093 (0.258) data 0.000 (0.164) loss 1.1133 (1.2029) acc 75.0000 (71.2500) lr 1.5567e-04 eta 0:01:39
epoch [43/50] batch [10/49] time 0.093 (0.175) data 0.000 (0.082) loss 1.3115 (1.0906) acc 65.6250 (71.5625) lr 1.5567e-04 eta 0:01:06
epoch [43/50] batch [15/49] time 0.095 (0.148) data 0.000 (0.055) loss 1.1318 (1.0473) acc 75.0000 (73.1250) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [20/49] time 0.093 (0.135) data 0.000 (0.041) loss 1.1152 (1.0467) acc 78.1250 (74.6875) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.033) loss 1.1172 (1.0237) acc 68.7500 (75.3750) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [30/49] time 0.094 (0.121) data 0.000 (0.028) loss 1.0078 (1.0595) acc 81.2500 (74.2708) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [35/49] time 0.093 (0.117) data 0.000 (0.024) loss 0.9727 (1.0341) acc 75.0000 (75.1786) lr 1.5567e-04 eta 0:00:41
epoch [43/50] batch [40/49] time 0.093 (0.114) data 0.000 (0.021) loss 1.0137 (1.0426) acc 78.1250 (75.0000) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 0.8716 (1.0331) acc 87.5000 (75.1389) lr 1.5567e-04 eta 0:00:38
epoch [44/50] batch [5/49] time 0.093 (0.277) data 0.000 (0.181) loss 1.1816 (1.0201) acc 78.1250 (78.7500) lr 1.2369e-04 eta 0:01:33
epoch [44/50] batch [10/49] time 0.092 (0.185) data 0.000 (0.091) loss 1.0029 (1.0043) acc 81.2500 (78.1250) lr 1.2369e-04 eta 0:01:01
epoch [44/50] batch [15/49] time 0.093 (0.155) data 0.000 (0.061) loss 1.1318 (1.0050) acc 68.7500 (77.0833) lr 1.2369e-04 eta 0:00:50
epoch [44/50] batch [20/49] time 0.093 (0.140) data 0.000 (0.046) loss 1.2666 (1.0307) acc 71.8750 (76.8750) lr 1.2369e-04 eta 0:00:45
epoch [44/50] batch [25/49] time 0.098 (0.131) data 0.000 (0.037) loss 0.5605 (1.0102) acc 87.5000 (76.3750) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 0.7778 (1.0237) acc 78.1250 (75.8333) lr 1.2369e-04 eta 0:00:38
epoch [44/50] batch [35/49] time 0.093 (0.120) data 0.000 (0.026) loss 1.2002 (1.0287) acc 81.2500 (76.1607) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.023) loss 0.9229 (1.0244) acc 75.0000 (76.2500) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [45/49] time 0.096 (0.114) data 0.000 (0.020) loss 0.9326 (1.0243) acc 71.8750 (76.1111) lr 1.2369e-04 eta 0:00:33
epoch [45/50] batch [5/49] time 0.093 (0.286) data 0.000 (0.191) loss 1.0244 (1.0346) acc 68.7500 (75.6250) lr 9.5173e-05 eta 0:01:22
epoch [45/50] batch [10/49] time 0.093 (0.190) data 0.000 (0.096) loss 1.1523 (1.0330) acc 68.7500 (75.9375) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [15/49] time 0.093 (0.158) data 0.000 (0.064) loss 0.7881 (1.1381) acc 81.2500 (72.9167) lr 9.5173e-05 eta 0:00:44
epoch [45/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.048) loss 1.0703 (1.0873) acc 78.1250 (75.1562) lr 9.5173e-05 eta 0:00:38
epoch [45/50] batch [25/49] time 0.094 (0.132) data 0.000 (0.038) loss 1.0859 (1.0567) acc 81.2500 (76.1250) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [30/49] time 0.097 (0.125) data 0.000 (0.032) loss 0.8154 (1.0610) acc 75.0000 (75.6250) lr 9.5173e-05 eta 0:00:33
epoch [45/50] batch [35/49] time 0.092 (0.121) data 0.000 (0.028) loss 1.2939 (1.0610) acc 71.8750 (75.2679) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [40/49] time 0.093 (0.117) data 0.000 (0.024) loss 0.9136 (1.0469) acc 81.2500 (75.6250) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.021) loss 0.8447 (1.0446) acc 81.2500 (75.3472) lr 9.5173e-05 eta 0:00:28
epoch [46/50] batch [5/49] time 0.093 (0.286) data 0.000 (0.191) loss 1.2021 (0.9760) acc 68.7500 (75.6250) lr 7.0224e-05 eta 0:01:08
epoch [46/50] batch [10/49] time 0.096 (0.190) data 0.000 (0.096) loss 0.9551 (1.0433) acc 84.3750 (73.7500) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [15/49] time 0.093 (0.158) data 0.000 (0.064) loss 1.0117 (1.0225) acc 81.2500 (75.0000) lr 7.0224e-05 eta 0:00:36
epoch [46/50] batch [20/49] time 0.093 (0.142) data 0.000 (0.048) loss 1.1953 (1.0213) acc 78.1250 (75.6250) lr 7.0224e-05 eta 0:00:31
epoch [46/50] batch [25/49] time 0.093 (0.132) data 0.000 (0.038) loss 1.0840 (1.0412) acc 78.1250 (75.5000) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [30/49] time 0.093 (0.126) data 0.000 (0.032) loss 0.8252 (1.0192) acc 78.1250 (76.2500) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [35/49] time 0.093 (0.121) data 0.000 (0.028) loss 1.0010 (1.0119) acc 71.8750 (76.3393) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [40/49] time 0.093 (0.118) data 0.000 (0.024) loss 0.8389 (1.0082) acc 87.5000 (76.7188) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [45/49] time 0.093 (0.115) data 0.000 (0.021) loss 0.8755 (1.0110) acc 81.2500 (76.4583) lr 7.0224e-05 eta 0:00:22
epoch [47/50] batch [5/49] time 0.093 (0.262) data 0.000 (0.169) loss 0.9390 (1.0037) acc 81.2500 (76.2500) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [10/49] time 0.093 (0.178) data 0.000 (0.085) loss 0.8262 (0.9268) acc 84.3750 (78.4375) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [15/49] time 0.093 (0.150) data 0.000 (0.057) loss 1.0225 (0.9847) acc 78.1250 (76.4583) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [20/49] time 0.094 (0.136) data 0.001 (0.043) loss 1.0762 (1.0029) acc 62.5000 (76.2500) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [25/49] time 0.093 (0.127) data 0.000 (0.034) loss 1.4004 (1.0175) acc 65.6250 (76.2500) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [30/49] time 0.093 (0.122) data 0.000 (0.028) loss 1.1270 (1.0253) acc 68.7500 (75.8333) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [35/49] time 0.093 (0.118) data 0.000 (0.024) loss 0.7524 (1.0268) acc 81.2500 (75.5357) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.021) loss 0.7734 (1.0207) acc 87.5000 (75.6250) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [45/49] time 0.093 (0.112) data 0.000 (0.019) loss 1.0645 (1.0320) acc 75.0000 (75.3472) lr 4.8943e-05 eta 0:00:16
epoch [48/50] batch [5/49] time 0.094 (0.269) data 0.000 (0.175) loss 1.0039 (1.0013) acc 75.0000 (75.0000) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [10/49] time 0.094 (0.181) data 0.001 (0.088) loss 1.0664 (1.0296) acc 71.8750 (73.7500) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [15/49] time 0.093 (0.153) data 0.000 (0.058) loss 0.7266 (1.0140) acc 84.3750 (75.2083) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [20/49] time 0.093 (0.138) data 0.000 (0.044) loss 1.2061 (1.0241) acc 68.7500 (75.3125) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [25/49] time 0.100 (0.130) data 0.000 (0.036) loss 1.1221 (1.0447) acc 81.2500 (75.5000) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 1.2305 (1.0391) acc 71.8750 (75.7292) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [35/49] time 0.094 (0.120) data 0.000 (0.026) loss 0.7944 (1.0210) acc 87.5000 (76.8750) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [40/49] time 0.092 (0.116) data 0.000 (0.022) loss 0.8027 (1.0142) acc 81.2500 (77.0312) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [45/49] time 0.093 (0.114) data 0.000 (0.020) loss 1.1074 (1.0218) acc 65.6250 (76.1806) lr 3.1417e-05 eta 0:00:11
epoch [49/50] batch [5/49] time 0.093 (0.268) data 0.000 (0.173) loss 1.1172 (0.9717) acc 65.6250 (74.3750) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [10/49] time 0.093 (0.181) data 0.000 (0.087) loss 1.0996 (1.0235) acc 81.2500 (76.2500) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [15/49] time 0.093 (0.151) data 0.000 (0.058) loss 0.7437 (1.0077) acc 84.3750 (77.0833) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [20/49] time 0.093 (0.137) data 0.000 (0.044) loss 0.6528 (0.9635) acc 87.5000 (78.9062) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [25/49] time 0.093 (0.128) data 0.000 (0.035) loss 0.7300 (0.9563) acc 93.7500 (79.0000) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [30/49] time 0.098 (0.123) data 0.000 (0.029) loss 1.0195 (0.9601) acc 75.0000 (78.6458) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [35/49] time 0.094 (0.119) data 0.000 (0.025) loss 1.0977 (0.9683) acc 71.8750 (78.9286) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [40/49] time 0.093 (0.115) data 0.000 (0.022) loss 0.9365 (0.9773) acc 71.8750 (78.3594) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [45/49] time 0.093 (0.113) data 0.000 (0.020) loss 1.1680 (0.9816) acc 68.7500 (78.0556) lr 1.7713e-05 eta 0:00:05
epoch [50/50] batch [5/49] time 0.097 (0.274) data 0.000 (0.178) loss 0.8970 (0.9652) acc 71.8750 (75.6250) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [10/49] time 0.093 (0.183) data 0.000 (0.089) loss 1.1953 (0.9953) acc 68.7500 (76.8750) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [15/49] time 0.093 (0.154) data 0.000 (0.060) loss 1.3555 (1.0355) acc 62.5000 (74.5833) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [20/49] time 0.093 (0.139) data 0.000 (0.045) loss 1.0576 (1.0488) acc 75.0000 (74.8438) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [25/49] time 0.093 (0.130) data 0.000 (0.036) loss 0.7300 (1.0389) acc 81.2500 (74.3750) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [30/49] time 0.093 (0.124) data 0.000 (0.030) loss 0.8325 (1.0256) acc 81.2500 (75.1042) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [35/49] time 0.093 (0.119) data 0.000 (0.026) loss 0.7988 (1.0102) acc 87.5000 (75.5357) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [40/49] time 0.093 (0.116) data 0.000 (0.023) loss 1.1094 (1.0060) acc 71.8750 (75.4688) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.094 (0.113) data 0.000 (0.020) loss 0.8135 (0.9931) acc 81.2500 (76.1806) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:06<00:53,  6.69s/it] 22%|██▏       | 2/9 [00:07<00:21,  3.10s/it] 33%|███▎      | 3/9 [00:07<00:11,  1.95s/it] 44%|████▍     | 4/9 [00:08<00:07,  1.41s/it] 56%|█████▌    | 5/9 [00:09<00:04,  1.11s/it] 67%|██████▋   | 6/9 [00:09<00:02,  1.07it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.22it/s] 89%|████████▉ | 8/9 [00:10<00:00,  1.34it/s]100%|██████████| 9/9 [00:11<00:00,  1.22s/it]
=> result
* total: 4,002
* correct: 3,215
* accuracy: 80.3%
* error: 19.7%
* macro_f1: 80.0%
Elapsed: 0:04:52
Run this job and save the output to output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:11<01:35, 11.88s/it] 22%|██▏       | 2/9 [00:12<00:36,  5.23s/it] 33%|███▎      | 3/9 [00:13<00:18,  3.11s/it] 44%|████▍     | 4/9 [00:13<00:10,  2.11s/it] 56%|█████▌    | 5/9 [00:14<00:06,  1.56s/it] 67%|██████▋   | 6/9 [00:14<00:03,  1.23s/it] 78%|███████▊  | 7/9 [00:15<00:02,  1.02s/it] 89%|████████▉ | 8/9 [00:15<00:00,  1.14it/s]100%|██████████| 9/9 [00:16<00:00,  1.80s/it]
=> result
* total: 4,039
* correct: 3,006
* accuracy: 74.4%
* error: 25.6%
* macro_f1: 73.4%
Run this job and save the output to output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:05,  8.24s/it] 22%|██▏       | 2/9 [00:08<00:26,  3.73s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.29s/it] 44%|████▍     | 4/9 [00:09<00:08,  1.62s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.24s/it] 67%|██████▋   | 6/9 [00:11<00:03,  1.02s/it] 78%|███████▊  | 7/9 [00:11<00:01,  1.14it/s] 89%|████████▉ | 8/9 [00:12<00:00,  1.28it/s]100%|██████████| 9/9 [00:12<00:00,  1.75it/s]100%|██████████| 9/9 [00:12<00:00,  1.40s/it]
=> result
* total: 4,039
* correct: 3,000
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.3%
Run this job and save the output to output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:05,  8.16s/it] 22%|██▏       | 2/9 [00:08<00:25,  3.70s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.28s/it] 44%|████▍     | 4/9 [00:09<00:08,  1.61s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.24s/it] 67%|██████▋   | 6/9 [00:11<00:03,  1.01s/it] 78%|███████▊  | 7/9 [00:11<00:01,  1.15it/s] 89%|████████▉ | 8/9 [00:12<00:00,  1.28it/s]100%|██████████| 9/9 [00:12<00:00,  1.76it/s]100%|██████████| 9/9 [00:12<00:00,  1.39s/it]
=> result
* total: 4,039
* correct: 3,004
* accuracy: 74.4%
* error: 25.6%
* macro_f1: 73.4%
Run this job and save the output to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.070 (0.312) data 0.000 (0.210) loss 2.3359 (2.6063) acc 71.8750 (61.2500) lr 1.0000e-05 eta 0:06:43
epoch [1/50] batch [10/26] time 0.070 (0.191) data 0.000 (0.105) loss 2.3848 (2.5967) acc 62.5000 (60.3125) lr 1.0000e-05 eta 0:04:06
epoch [1/50] batch [15/26] time 0.070 (0.151) data 0.000 (0.070) loss 2.8555 (2.6234) acc 46.8750 (57.9167) lr 1.0000e-05 eta 0:03:13
epoch [1/50] batch [20/26] time 0.070 (0.130) data 0.000 (0.053) loss 2.6133 (2.6285) acc 46.8750 (56.5625) lr 1.0000e-05 eta 0:02:46
epoch [1/50] batch [25/26] time 0.071 (0.118) data 0.000 (0.042) loss 2.4805 (2.6190) acc 62.5000 (56.8750) lr 1.0000e-05 eta 0:02:31
epoch [2/50] batch [5/26] time 0.071 (0.260) data 0.000 (0.183) loss 1.7109 (1.9131) acc 65.6250 (63.1250) lr 2.0000e-03 eta 0:05:29
epoch [2/50] batch [10/26] time 0.077 (0.166) data 0.000 (0.092) loss 0.9214 (1.7686) acc 75.0000 (61.2500) lr 2.0000e-03 eta 0:03:29
epoch [2/50] batch [15/26] time 0.070 (0.134) data 0.000 (0.061) loss 1.3916 (1.6560) acc 65.6250 (62.5000) lr 2.0000e-03 eta 0:02:48
epoch [2/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.046) loss 1.3223 (1.5730) acc 65.6250 (63.4375) lr 2.0000e-03 eta 0:02:28
epoch [2/50] batch [25/26] time 0.071 (0.109) data 0.000 (0.037) loss 1.0645 (1.5311) acc 84.3750 (63.0000) lr 2.0000e-03 eta 0:02:15
epoch [3/50] batch [5/26] time 0.069 (0.217) data 0.000 (0.143) loss 1.0293 (1.2563) acc 75.0000 (68.7500) lr 1.9980e-03 eta 0:04:29
epoch [3/50] batch [10/26] time 0.070 (0.143) data 0.000 (0.072) loss 1.0898 (1.1256) acc 71.8750 (71.8750) lr 1.9980e-03 eta 0:02:57
epoch [3/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.048) loss 1.4863 (1.1898) acc 56.2500 (68.5417) lr 1.9980e-03 eta 0:02:26
epoch [3/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 1.1738 (1.2094) acc 71.8750 (67.6562) lr 1.9980e-03 eta 0:02:11
epoch [3/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.029) loss 1.0049 (1.1880) acc 71.8750 (67.6250) lr 1.9980e-03 eta 0:02:01
epoch [4/50] batch [5/26] time 0.070 (0.256) data 0.000 (0.184) loss 0.9443 (1.0123) acc 75.0000 (72.5000) lr 1.9921e-03 eta 0:05:11
epoch [4/50] batch [10/26] time 0.070 (0.163) data 0.000 (0.092) loss 1.0879 (1.0855) acc 68.7500 (69.0625) lr 1.9921e-03 eta 0:03:17
epoch [4/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.062) loss 1.0859 (1.0573) acc 62.5000 (70.0000) lr 1.9921e-03 eta 0:02:39
epoch [4/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.046) loss 0.9746 (1.0793) acc 71.8750 (69.8438) lr 1.9921e-03 eta 0:02:19
epoch [4/50] batch [25/26] time 0.069 (0.107) data 0.000 (0.037) loss 1.2666 (1.1274) acc 65.6250 (68.7500) lr 1.9921e-03 eta 0:02:08
epoch [5/50] batch [5/26] time 0.070 (0.233) data 0.000 (0.161) loss 0.8657 (1.0071) acc 75.0000 (73.7500) lr 1.9823e-03 eta 0:04:37
epoch [5/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.081) loss 1.2402 (1.0354) acc 62.5000 (71.8750) lr 1.9823e-03 eta 0:02:59
epoch [5/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.054) loss 0.7700 (1.0288) acc 78.1250 (71.6667) lr 1.9823e-03 eta 0:02:26
epoch [5/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.7759 (1.0598) acc 78.1250 (70.3125) lr 1.9823e-03 eta 0:02:10
epoch [5/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 1.5742 (1.0899) acc 62.5000 (69.2500) lr 1.9823e-03 eta 0:02:00
epoch [6/50] batch [5/26] time 0.071 (0.234) data 0.001 (0.161) loss 1.3896 (1.1052) acc 68.7500 (73.1250) lr 1.9686e-03 eta 0:04:32
epoch [6/50] batch [10/26] time 0.070 (0.152) data 0.000 (0.081) loss 0.8706 (1.0260) acc 71.8750 (71.8750) lr 1.9686e-03 eta 0:02:56
epoch [6/50] batch [15/26] time 0.071 (0.125) data 0.000 (0.054) loss 0.9092 (1.0067) acc 75.0000 (72.7083) lr 1.9686e-03 eta 0:02:24
epoch [6/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.041) loss 0.9434 (0.9922) acc 78.1250 (72.9688) lr 1.9686e-03 eta 0:02:07
epoch [6/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.7808 (1.0035) acc 81.2500 (73.2500) lr 1.9686e-03 eta 0:01:58
epoch [7/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.158) loss 0.9126 (1.0436) acc 75.0000 (75.6250) lr 1.9511e-03 eta 0:04:22
epoch [7/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.079) loss 0.7979 (0.9053) acc 78.1250 (77.8125) lr 1.9511e-03 eta 0:02:50
epoch [7/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.053) loss 0.9004 (0.9218) acc 62.5000 (75.8333) lr 1.9511e-03 eta 0:02:19
epoch [7/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.040) loss 0.8931 (0.9155) acc 81.2500 (76.5625) lr 1.9511e-03 eta 0:02:03
epoch [7/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.8169 (0.9262) acc 78.1250 (76.0000) lr 1.9511e-03 eta 0:01:54
epoch [8/50] batch [5/26] time 0.070 (0.258) data 0.000 (0.187) loss 0.6094 (0.9045) acc 84.3750 (75.0000) lr 1.9298e-03 eta 0:04:47
epoch [8/50] batch [10/26] time 0.072 (0.164) data 0.000 (0.093) loss 0.8789 (0.9665) acc 78.1250 (73.1250) lr 1.9298e-03 eta 0:03:02
epoch [8/50] batch [15/26] time 0.070 (0.133) data 0.000 (0.062) loss 0.8408 (0.9471) acc 75.0000 (74.3750) lr 1.9298e-03 eta 0:02:26
epoch [8/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.047) loss 1.0010 (0.9314) acc 81.2500 (75.7812) lr 1.9298e-03 eta 0:02:08
epoch [8/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.8618 (0.9291) acc 75.0000 (75.7500) lr 1.9298e-03 eta 0:01:57
epoch [9/50] batch [5/26] time 0.070 (0.223) data 0.000 (0.152) loss 0.9702 (0.8953) acc 75.0000 (76.8750) lr 1.9048e-03 eta 0:04:01
epoch [9/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.076) loss 1.3311 (0.9443) acc 62.5000 (76.5625) lr 1.9048e-03 eta 0:02:38
epoch [9/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.051) loss 0.7734 (0.8912) acc 84.3750 (77.2917) lr 1.9048e-03 eta 0:02:10
epoch [9/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.8301 (0.8760) acc 84.3750 (77.9688) lr 1.9048e-03 eta 0:01:56
epoch [9/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.031) loss 0.6782 (0.8889) acc 84.3750 (77.0000) lr 1.9048e-03 eta 0:01:47
epoch [10/50] batch [5/26] time 0.070 (0.223) data 0.000 (0.149) loss 0.7324 (0.7971) acc 90.6250 (78.1250) lr 1.8763e-03 eta 0:03:56
epoch [10/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.075) loss 0.8628 (0.8318) acc 71.8750 (77.1875) lr 1.8763e-03 eta 0:02:35
epoch [10/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.9893 (0.8734) acc 78.1250 (77.2917) lr 1.8763e-03 eta 0:02:07
epoch [10/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.6611 (0.8572) acc 81.2500 (77.5000) lr 1.8763e-03 eta 0:01:53
epoch [10/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.5811 (0.8516) acc 81.2500 (78.0000) lr 1.8763e-03 eta 0:01:45
epoch [11/50] batch [5/26] time 0.071 (0.238) data 0.000 (0.164) loss 1.0762 (0.9041) acc 68.7500 (76.8750) lr 1.8443e-03 eta 0:04:06
epoch [11/50] batch [10/26] time 0.070 (0.154) data 0.000 (0.082) loss 0.7046 (0.8301) acc 81.2500 (79.3750) lr 1.8443e-03 eta 0:02:38
epoch [11/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.8530 (0.8440) acc 75.0000 (79.1667) lr 1.8443e-03 eta 0:02:09
epoch [11/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.041) loss 0.8936 (0.8387) acc 78.1250 (79.8438) lr 1.8443e-03 eta 0:01:54
epoch [11/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 1.1045 (0.8346) acc 68.7500 (79.5000) lr 1.8443e-03 eta 0:01:45
epoch [12/50] batch [5/26] time 0.073 (0.225) data 0.000 (0.148) loss 1.0527 (0.8813) acc 81.2500 (82.5000) lr 1.8090e-03 eta 0:03:47
epoch [12/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.074) loss 0.6230 (0.8847) acc 84.3750 (80.6250) lr 1.8090e-03 eta 0:02:28
epoch [12/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.049) loss 0.6118 (0.8372) acc 90.6250 (82.9167) lr 1.8090e-03 eta 0:02:01
epoch [12/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.037) loss 0.8896 (0.7860) acc 87.5000 (84.5312) lr 1.8090e-03 eta 0:01:48
epoch [12/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.9565 (0.7890) acc 78.1250 (83.8750) lr 1.8090e-03 eta 0:01:40
epoch [13/50] batch [5/26] time 0.074 (0.221) data 0.000 (0.147) loss 1.1191 (1.0076) acc 71.8750 (73.1250) lr 1.7705e-03 eta 0:03:37
epoch [13/50] batch [10/26] time 0.075 (0.148) data 0.000 (0.073) loss 1.0625 (0.9268) acc 71.8750 (76.2500) lr 1.7705e-03 eta 0:02:24
epoch [13/50] batch [15/26] time 0.074 (0.123) data 0.000 (0.049) loss 0.6387 (0.8757) acc 87.5000 (77.7083) lr 1.7705e-03 eta 0:01:59
epoch [13/50] batch [20/26] time 0.073 (0.110) data 0.000 (0.037) loss 0.9609 (0.8614) acc 68.7500 (78.4375) lr 1.7705e-03 eta 0:01:46
epoch [13/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.029) loss 0.6006 (0.8541) acc 84.3750 (79.1250) lr 1.7705e-03 eta 0:01:38
epoch [14/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.155) loss 0.5703 (0.8296) acc 87.5000 (78.7500) lr 1.7290e-03 eta 0:03:39
epoch [14/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.077) loss 0.8486 (0.8131) acc 78.1250 (79.6875) lr 1.7290e-03 eta 0:02:22
epoch [14/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.052) loss 1.3721 (0.8350) acc 68.7500 (79.3750) lr 1.7290e-03 eta 0:01:57
epoch [14/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.8496 (0.8313) acc 84.3750 (79.6875) lr 1.7290e-03 eta 0:01:43
epoch [14/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.8721 (0.8087) acc 75.0000 (80.5000) lr 1.7290e-03 eta 0:01:35
epoch [15/50] batch [5/26] time 0.070 (0.218) data 0.000 (0.146) loss 0.7939 (0.8119) acc 87.5000 (84.3750) lr 1.6845e-03 eta 0:03:23
epoch [15/50] batch [10/26] time 0.070 (0.144) data 0.000 (0.073) loss 0.4490 (0.7759) acc 96.8750 (83.7500) lr 1.6845e-03 eta 0:02:13
epoch [15/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.049) loss 0.7959 (0.7671) acc 84.3750 (83.3333) lr 1.6845e-03 eta 0:01:50
epoch [15/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.8301 (0.7946) acc 84.3750 (82.1875) lr 1.6845e-03 eta 0:01:38
epoch [15/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.8213 (0.7910) acc 81.2500 (82.3750) lr 1.6845e-03 eta 0:01:30
epoch [16/50] batch [5/26] time 0.070 (0.231) data 0.000 (0.160) loss 1.1416 (0.7871) acc 75.0000 (81.8750) lr 1.6374e-03 eta 0:03:29
epoch [16/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.080) loss 0.7827 (0.7565) acc 78.1250 (81.8750) lr 1.6374e-03 eta 0:02:15
epoch [16/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.054) loss 0.6172 (0.7213) acc 90.6250 (83.5417) lr 1.6374e-03 eta 0:01:50
epoch [16/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 1.3203 (0.7417) acc 81.2500 (84.0625) lr 1.6374e-03 eta 0:01:38
epoch [16/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.8823 (0.7897) acc 75.0000 (81.8750) lr 1.6374e-03 eta 0:01:30
epoch [17/50] batch [5/26] time 0.070 (0.224) data 0.000 (0.151) loss 0.7505 (0.8932) acc 84.3750 (80.0000) lr 1.5878e-03 eta 0:03:16
epoch [17/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.075) loss 0.8657 (0.8391) acc 78.1250 (80.9375) lr 1.5878e-03 eta 0:02:08
epoch [17/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.8193 (0.8477) acc 87.5000 (81.0417) lr 1.5878e-03 eta 0:01:45
epoch [17/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.6704 (0.8177) acc 87.5000 (81.0938) lr 1.5878e-03 eta 0:01:33
epoch [17/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.7197 (0.7989) acc 81.2500 (81.3750) lr 1.5878e-03 eta 0:01:26
epoch [18/50] batch [5/26] time 0.071 (0.224) data 0.000 (0.151) loss 0.5415 (0.7157) acc 90.6250 (85.6250) lr 1.5358e-03 eta 0:03:10
epoch [18/50] batch [10/26] time 0.073 (0.148) data 0.001 (0.076) loss 0.6279 (0.7161) acc 90.6250 (85.3125) lr 1.5358e-03 eta 0:02:05
epoch [18/50] batch [15/26] time 0.072 (0.122) data 0.000 (0.050) loss 0.8750 (0.7718) acc 78.1250 (83.7500) lr 1.5358e-03 eta 0:01:42
epoch [18/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.7427 (0.7501) acc 78.1250 (83.5938) lr 1.5358e-03 eta 0:01:31
epoch [18/50] batch [25/26] time 0.073 (0.102) data 0.000 (0.030) loss 0.7119 (0.7249) acc 78.1250 (83.7500) lr 1.5358e-03 eta 0:01:24
epoch [19/50] batch [5/26] time 0.071 (0.235) data 0.000 (0.163) loss 0.8149 (0.8513) acc 78.1250 (78.7500) lr 1.4818e-03 eta 0:03:14
epoch [19/50] batch [10/26] time 0.071 (0.153) data 0.000 (0.082) loss 0.8281 (0.8124) acc 84.3750 (82.5000) lr 1.4818e-03 eta 0:02:05
epoch [19/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.055) loss 0.8721 (0.7923) acc 75.0000 (82.0833) lr 1.4818e-03 eta 0:01:42
epoch [19/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.041) loss 0.7495 (0.8036) acc 87.5000 (81.2500) lr 1.4818e-03 eta 0:01:31
epoch [19/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.6812 (0.8000) acc 87.5000 (81.3750) lr 1.4818e-03 eta 0:01:23
epoch [20/50] batch [5/26] time 0.071 (0.232) data 0.000 (0.157) loss 0.6270 (0.6225) acc 84.3750 (88.1250) lr 1.4258e-03 eta 0:03:06
epoch [20/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.079) loss 0.8149 (0.6804) acc 90.6250 (86.5625) lr 1.4258e-03 eta 0:02:00
epoch [20/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.8032 (0.7084) acc 84.3750 (85.6250) lr 1.4258e-03 eta 0:01:38
epoch [20/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.6240 (0.7063) acc 90.6250 (85.1562) lr 1.4258e-03 eta 0:01:27
epoch [20/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.7681 (0.7327) acc 81.2500 (84.2500) lr 1.4258e-03 eta 0:01:20
epoch [21/50] batch [5/26] time 0.071 (0.218) data 0.000 (0.146) loss 0.6152 (0.7970) acc 90.6250 (81.8750) lr 1.3681e-03 eta 0:02:48
epoch [21/50] batch [10/26] time 0.070 (0.144) data 0.000 (0.073) loss 0.8218 (0.7444) acc 78.1250 (84.6875) lr 1.3681e-03 eta 0:01:50
epoch [21/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.049) loss 0.9106 (0.7843) acc 81.2500 (83.1250) lr 1.3681e-03 eta 0:01:31
epoch [21/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.037) loss 0.8398 (0.7695) acc 81.2500 (84.3750) lr 1.3681e-03 eta 0:01:21
epoch [21/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.9194 (0.7725) acc 87.5000 (84.3750) lr 1.3681e-03 eta 0:01:15
epoch [22/50] batch [5/26] time 0.071 (0.243) data 0.000 (0.170) loss 0.6665 (0.7032) acc 90.6250 (88.1250) lr 1.3090e-03 eta 0:03:02
epoch [22/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.085) loss 0.6621 (0.6899) acc 87.5000 (86.5625) lr 1.3090e-03 eta 0:01:56
epoch [22/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.057) loss 0.6929 (0.6741) acc 78.1250 (86.0417) lr 1.3090e-03 eta 0:01:34
epoch [22/50] batch [20/26] time 0.071 (0.114) data 0.000 (0.043) loss 0.6460 (0.6650) acc 90.6250 (86.8750) lr 1.3090e-03 eta 0:01:23
epoch [22/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.9102 (0.6972) acc 75.0000 (85.8750) lr 1.3090e-03 eta 0:01:16
epoch [23/50] batch [5/26] time 0.071 (0.241) data 0.000 (0.169) loss 0.4375 (0.6571) acc 90.6250 (85.0000) lr 1.2487e-03 eta 0:02:54
epoch [23/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.6577 (0.6644) acc 90.6250 (86.8750) lr 1.2487e-03 eta 0:01:51
epoch [23/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.057) loss 0.4221 (0.6708) acc 90.6250 (86.2500) lr 1.2487e-03 eta 0:01:30
epoch [23/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.7617 (0.7187) acc 90.6250 (84.6875) lr 1.2487e-03 eta 0:01:19
epoch [23/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.7852 (0.7373) acc 81.2500 (84.5000) lr 1.2487e-03 eta 0:01:13
epoch [24/50] batch [5/26] time 0.071 (0.241) data 0.000 (0.167) loss 0.5342 (0.6031) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:02:48
epoch [24/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.9155 (0.6554) acc 81.2500 (88.1250) lr 1.1874e-03 eta 0:01:47
epoch [24/50] batch [15/26] time 0.071 (0.128) data 0.000 (0.056) loss 0.5562 (0.6466) acc 93.7500 (87.7083) lr 1.1874e-03 eta 0:01:27
epoch [24/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.042) loss 0.8291 (0.6623) acc 81.2500 (86.8750) lr 1.1874e-03 eta 0:01:17
epoch [24/50] batch [25/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.6035 (0.6607) acc 93.7500 (87.3750) lr 1.1874e-03 eta 0:01:11
epoch [25/50] batch [5/26] time 0.070 (0.265) data 0.000 (0.193) loss 0.7988 (0.7273) acc 87.5000 (84.3750) lr 1.1253e-03 eta 0:02:57
epoch [25/50] batch [10/26] time 0.070 (0.168) data 0.000 (0.097) loss 0.4873 (0.6823) acc 93.7500 (85.6250) lr 1.1253e-03 eta 0:01:51
epoch [25/50] batch [15/26] time 0.072 (0.136) data 0.000 (0.065) loss 0.7656 (0.6583) acc 78.1250 (86.4583) lr 1.1253e-03 eta 0:01:29
epoch [25/50] batch [20/26] time 0.071 (0.119) data 0.000 (0.049) loss 0.5459 (0.6544) acc 90.6250 (86.2500) lr 1.1253e-03 eta 0:01:18
epoch [25/50] batch [25/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.7461 (0.6621) acc 84.3750 (86.6250) lr 1.1253e-03 eta 0:01:11
epoch [26/50] batch [5/26] time 0.072 (0.236) data 0.000 (0.164) loss 1.0742 (0.7205) acc 78.1250 (85.0000) lr 1.0628e-03 eta 0:02:32
epoch [26/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.082) loss 0.5498 (0.6862) acc 93.7500 (85.9375) lr 1.0628e-03 eta 0:01:38
epoch [26/50] batch [15/26] time 0.071 (0.126) data 0.000 (0.055) loss 0.4714 (0.6465) acc 90.6250 (87.2917) lr 1.0628e-03 eta 0:01:20
epoch [26/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.041) loss 0.6162 (0.6781) acc 84.3750 (86.0938) lr 1.0628e-03 eta 0:01:10
epoch [26/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.8115 (0.7185) acc 81.2500 (84.7500) lr 1.0628e-03 eta 0:01:05
epoch [27/50] batch [5/26] time 0.071 (0.229) data 0.000 (0.155) loss 0.6221 (0.7165) acc 87.5000 (84.3750) lr 1.0000e-03 eta 0:02:21
epoch [27/50] batch [10/26] time 0.071 (0.150) data 0.000 (0.078) loss 0.6309 (0.6881) acc 87.5000 (85.0000) lr 1.0000e-03 eta 0:01:32
epoch [27/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.052) loss 0.6670 (0.6837) acc 87.5000 (85.2083) lr 1.0000e-03 eta 0:01:15
epoch [27/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.039) loss 0.7261 (0.6851) acc 84.3750 (85.4688) lr 1.0000e-03 eta 0:01:06
epoch [27/50] batch [25/26] time 0.072 (0.103) data 0.000 (0.031) loss 0.4651 (0.6696) acc 96.8750 (86.3750) lr 1.0000e-03 eta 0:01:01
epoch [28/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.148) loss 0.8716 (0.6036) acc 78.1250 (90.6250) lr 9.3721e-04 eta 0:02:11
epoch [28/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.074) loss 0.5356 (0.6630) acc 90.6250 (87.5000) lr 9.3721e-04 eta 0:01:25
epoch [28/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.8862 (0.7037) acc 78.1250 (86.0417) lr 9.3721e-04 eta 0:01:10
epoch [28/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.5605 (0.6640) acc 96.8750 (87.8125) lr 9.3721e-04 eta 0:01:02
epoch [28/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.5156 (0.6619) acc 93.7500 (87.7500) lr 9.3721e-04 eta 0:00:57
epoch [29/50] batch [5/26] time 0.070 (0.215) data 0.000 (0.144) loss 0.9644 (0.6796) acc 84.3750 (88.7500) lr 8.7467e-04 eta 0:02:01
epoch [29/50] batch [10/26] time 0.071 (0.143) data 0.000 (0.072) loss 1.1074 (0.7320) acc 71.8750 (85.3125) lr 8.7467e-04 eta 0:01:20
epoch [29/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.048) loss 1.0176 (0.7318) acc 78.1250 (85.4167) lr 8.7467e-04 eta 0:01:06
epoch [29/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.7559 (0.7193) acc 81.2500 (86.0938) lr 8.7467e-04 eta 0:00:58
epoch [29/50] batch [25/26] time 0.072 (0.100) data 0.000 (0.029) loss 0.6084 (0.7104) acc 87.5000 (86.1250) lr 8.7467e-04 eta 0:00:54
epoch [30/50] batch [5/26] time 0.071 (0.224) data 0.000 (0.150) loss 0.7266 (0.5898) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:02:01
epoch [30/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.075) loss 0.5195 (0.6404) acc 93.7500 (86.8750) lr 8.1262e-04 eta 0:01:19
epoch [30/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.050) loss 0.6011 (0.6264) acc 81.2500 (86.2500) lr 8.1262e-04 eta 0:01:04
epoch [30/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.6719 (0.6246) acc 87.5000 (86.8750) lr 8.1262e-04 eta 0:00:57
epoch [30/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.5400 (0.6024) acc 90.6250 (87.7500) lr 8.1262e-04 eta 0:00:52
epoch [31/50] batch [5/26] time 0.070 (0.210) data 0.000 (0.138) loss 0.5894 (0.6460) acc 81.2500 (85.0000) lr 7.5131e-04 eta 0:01:48
epoch [31/50] batch [10/26] time 0.070 (0.140) data 0.000 (0.069) loss 0.6758 (0.6552) acc 93.7500 (87.8125) lr 7.5131e-04 eta 0:01:11
epoch [31/50] batch [15/26] time 0.070 (0.117) data 0.000 (0.046) loss 0.5273 (0.6277) acc 87.5000 (86.8750) lr 7.5131e-04 eta 0:00:58
epoch [31/50] batch [20/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.5166 (0.6311) acc 93.7500 (87.3438) lr 7.5131e-04 eta 0:00:52
epoch [31/50] batch [25/26] time 0.071 (0.098) data 0.000 (0.028) loss 0.6309 (0.6359) acc 87.5000 (87.3750) lr 7.5131e-04 eta 0:00:48
epoch [32/50] batch [5/26] time 0.072 (0.219) data 0.000 (0.146) loss 0.6606 (0.6302) acc 84.3750 (87.5000) lr 6.9098e-04 eta 0:01:46
epoch [32/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.073) loss 0.5098 (0.6095) acc 90.6250 (86.8750) lr 6.9098e-04 eta 0:01:10
epoch [32/50] batch [15/26] time 0.072 (0.120) data 0.000 (0.049) loss 0.7974 (0.5999) acc 81.2500 (87.2917) lr 6.9098e-04 eta 0:00:57
epoch [32/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.7207 (0.6133) acc 81.2500 (87.1875) lr 6.9098e-04 eta 0:00:51
epoch [32/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.9600 (0.6321) acc 78.1250 (86.7500) lr 6.9098e-04 eta 0:00:46
epoch [33/50] batch [5/26] time 0.071 (0.219) data 0.000 (0.147) loss 0.4514 (0.6647) acc 96.8750 (86.8750) lr 6.3188e-04 eta 0:01:41
epoch [33/50] batch [10/26] time 0.070 (0.144) data 0.000 (0.074) loss 0.5801 (0.7063) acc 90.6250 (86.2500) lr 6.3188e-04 eta 0:01:06
epoch [33/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.049) loss 0.5337 (0.6885) acc 90.6250 (86.6667) lr 6.3188e-04 eta 0:00:54
epoch [33/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.037) loss 0.7378 (0.6929) acc 84.3750 (86.4062) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.030) loss 0.7729 (0.7039) acc 84.3750 (86.3750) lr 6.3188e-04 eta 0:00:44
epoch [34/50] batch [5/26] time 0.070 (0.213) data 0.000 (0.138) loss 0.4272 (0.5160) acc 96.8750 (92.5000) lr 5.7422e-04 eta 0:01:33
epoch [34/50] batch [10/26] time 0.072 (0.142) data 0.001 (0.069) loss 0.4253 (0.5865) acc 93.7500 (90.0000) lr 5.7422e-04 eta 0:01:01
epoch [34/50] batch [15/26] time 0.071 (0.118) data 0.000 (0.046) loss 0.7568 (0.6004) acc 81.2500 (88.7500) lr 5.7422e-04 eta 0:00:50
epoch [34/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.6978 (0.5938) acc 84.3750 (89.0625) lr 5.7422e-04 eta 0:00:44
epoch [34/50] batch [25/26] time 0.072 (0.099) data 0.000 (0.028) loss 0.5312 (0.6021) acc 93.7500 (89.0000) lr 5.7422e-04 eta 0:00:41
epoch [35/50] batch [5/26] time 0.070 (0.222) data 0.000 (0.147) loss 0.6450 (0.6317) acc 87.5000 (86.8750) lr 5.1825e-04 eta 0:01:31
epoch [35/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.074) loss 0.7437 (0.5900) acc 90.6250 (90.0000) lr 5.1825e-04 eta 0:00:59
epoch [35/50] batch [15/26] time 0.072 (0.121) data 0.000 (0.049) loss 1.0039 (0.6073) acc 65.6250 (88.7500) lr 5.1825e-04 eta 0:00:48
epoch [35/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.037) loss 0.6445 (0.6065) acc 84.3750 (88.4375) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.4463 (0.5857) acc 93.7500 (88.8750) lr 5.1825e-04 eta 0:00:39
epoch [36/50] batch [5/26] time 0.071 (0.204) data 0.000 (0.132) loss 0.4497 (0.5100) acc 93.7500 (93.1250) lr 4.6417e-04 eta 0:01:18
epoch [36/50] batch [10/26] time 0.074 (0.138) data 0.000 (0.066) loss 0.4675 (0.5622) acc 93.7500 (90.3125) lr 4.6417e-04 eta 0:00:52
epoch [36/50] batch [15/26] time 0.071 (0.116) data 0.000 (0.044) loss 0.5073 (0.5478) acc 96.8750 (90.8333) lr 4.6417e-04 eta 0:00:43
epoch [36/50] batch [20/26] time 0.071 (0.105) data 0.000 (0.033) loss 0.4248 (0.5407) acc 93.7500 (90.4688) lr 4.6417e-04 eta 0:00:38
epoch [36/50] batch [25/26] time 0.071 (0.098) data 0.000 (0.027) loss 0.4038 (0.5566) acc 96.8750 (89.3750) lr 4.6417e-04 eta 0:00:35
epoch [37/50] batch [5/26] time 0.070 (0.212) data 0.000 (0.138) loss 0.5024 (0.6514) acc 87.5000 (86.2500) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.069) loss 0.7485 (0.6317) acc 81.2500 (87.8125) lr 4.1221e-04 eta 0:00:50
epoch [37/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.046) loss 0.5273 (0.6026) acc 90.6250 (89.1667) lr 4.1221e-04 eta 0:00:41
epoch [37/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.4170 (0.5775) acc 93.7500 (90.0000) lr 4.1221e-04 eta 0:00:36
epoch [37/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.5840 (0.5825) acc 81.2500 (89.5000) lr 4.1221e-04 eta 0:00:33
epoch [38/50] batch [5/26] time 0.071 (0.206) data 0.000 (0.135) loss 0.6543 (0.6455) acc 87.5000 (86.2500) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [10/26] time 0.072 (0.138) data 0.001 (0.068) loss 0.4824 (0.6135) acc 84.3750 (86.5625) lr 3.6258e-04 eta 0:00:45
epoch [38/50] batch [15/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.6440 (0.6067) acc 90.6250 (87.7083) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [20/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.6235 (0.5891) acc 84.3750 (88.4375) lr 3.6258e-04 eta 0:00:33
epoch [38/50] batch [25/26] time 0.070 (0.098) data 0.000 (0.027) loss 0.6611 (0.5958) acc 90.6250 (88.5000) lr 3.6258e-04 eta 0:00:30
epoch [39/50] batch [5/26] time 0.070 (0.223) data 0.000 (0.151) loss 0.5107 (0.6623) acc 87.5000 (84.3750) lr 3.1545e-04 eta 0:01:08
epoch [39/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.075) loss 0.4937 (0.6037) acc 90.6250 (87.5000) lr 3.1545e-04 eta 0:00:44
epoch [39/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.4312 (0.5769) acc 93.7500 (89.3750) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.4890 (0.6023) acc 90.6250 (87.9688) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.5186 (0.5956) acc 90.6250 (88.5000) lr 3.1545e-04 eta 0:00:28
epoch [40/50] batch [5/26] time 0.071 (0.204) data 0.000 (0.132) loss 0.5713 (0.6724) acc 87.5000 (87.5000) lr 2.7103e-04 eta 0:00:57
epoch [40/50] batch [10/26] time 0.071 (0.137) data 0.000 (0.066) loss 0.3367 (0.6024) acc 93.7500 (89.6875) lr 2.7103e-04 eta 0:00:37
epoch [40/50] batch [15/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.6387 (0.6111) acc 90.6250 (89.3750) lr 2.7103e-04 eta 0:00:31
epoch [40/50] batch [20/26] time 0.071 (0.104) data 0.000 (0.033) loss 0.6475 (0.6161) acc 87.5000 (88.7500) lr 2.7103e-04 eta 0:00:27
epoch [40/50] batch [25/26] time 0.070 (0.097) data 0.000 (0.027) loss 0.7949 (0.6159) acc 84.3750 (88.2500) lr 2.7103e-04 eta 0:00:25
epoch [41/50] batch [5/26] time 0.070 (0.213) data 0.000 (0.141) loss 0.7998 (0.6684) acc 81.2500 (87.5000) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [10/26] time 0.071 (0.142) data 0.000 (0.071) loss 0.4653 (0.6175) acc 93.7500 (89.0625) lr 2.2949e-04 eta 0:00:35
epoch [41/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.4248 (0.6295) acc 93.7500 (88.1250) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.035) loss 1.2275 (0.6511) acc 71.8750 (87.5000) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.5601 (0.6337) acc 87.5000 (87.6250) lr 2.2949e-04 eta 0:00:23
epoch [42/50] batch [5/26] time 0.070 (0.200) data 0.000 (0.129) loss 0.5688 (0.7050) acc 90.6250 (87.5000) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [10/26] time 0.071 (0.135) data 0.000 (0.065) loss 0.4626 (0.6474) acc 93.7500 (89.0625) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [15/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.5312 (0.6670) acc 87.5000 (88.3333) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [20/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.4451 (0.6271) acc 93.7500 (88.9062) lr 1.9098e-04 eta 0:00:22
epoch [42/50] batch [25/26] time 0.071 (0.096) data 0.000 (0.026) loss 0.8550 (0.6370) acc 87.5000 (89.5000) lr 1.9098e-04 eta 0:00:20
epoch [43/50] batch [5/26] time 0.070 (0.212) data 0.000 (0.134) loss 0.4631 (0.5822) acc 90.6250 (89.3750) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [10/26] time 0.071 (0.142) data 0.001 (0.067) loss 0.7695 (0.5633) acc 81.2500 (89.3750) lr 1.5567e-04 eta 0:00:28
epoch [43/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.045) loss 0.5957 (0.6126) acc 84.3750 (88.3333) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.034) loss 0.5498 (0.5917) acc 87.5000 (88.9062) lr 1.5567e-04 eta 0:00:19
epoch [43/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.027) loss 0.6182 (0.5796) acc 87.5000 (89.2500) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.070 (0.224) data 0.000 (0.152) loss 0.3857 (0.6067) acc 96.8750 (88.7500) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.076) loss 0.5615 (0.6034) acc 84.3750 (89.0625) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.051) loss 0.7588 (0.6241) acc 78.1250 (87.9167) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.7588 (0.6233) acc 81.2500 (87.8125) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.031) loss 0.6899 (0.6474) acc 87.5000 (87.6250) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/26] time 0.070 (0.211) data 0.000 (0.138) loss 0.7539 (0.6608) acc 84.3750 (88.1250) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [10/26] time 0.071 (0.141) data 0.000 (0.069) loss 0.3533 (0.6149) acc 96.8750 (89.0625) lr 9.5173e-05 eta 0:00:20
epoch [45/50] batch [15/26] time 0.072 (0.117) data 0.000 (0.046) loss 0.5127 (0.5799) acc 90.6250 (89.7917) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [20/26] time 0.072 (0.106) data 0.000 (0.035) loss 0.5918 (0.5757) acc 81.2500 (89.2188) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.8413 (0.5859) acc 81.2500 (89.2500) lr 9.5173e-05 eta 0:00:12
epoch [46/50] batch [5/26] time 0.070 (0.207) data 0.000 (0.136) loss 0.6880 (0.5771) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [10/26] time 0.070 (0.139) data 0.000 (0.068) loss 0.6143 (0.5579) acc 90.6250 (90.9375) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [15/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.7603 (0.5863) acc 87.5000 (90.8333) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.5742 (0.5876) acc 84.3750 (90.1562) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/26] time 0.070 (0.097) data 0.000 (0.027) loss 0.5288 (0.6097) acc 90.6250 (89.3750) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.070 (0.216) data 0.000 (0.141) loss 0.6548 (0.4847) acc 87.5000 (92.5000) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [10/26] time 0.070 (0.143) data 0.000 (0.071) loss 0.4700 (0.4985) acc 93.7500 (93.4375) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/26] time 0.071 (0.119) data 0.000 (0.047) loss 0.7471 (0.5261) acc 84.3750 (92.2917) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.036) loss 0.9165 (0.5784) acc 75.0000 (90.6250) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.5742 (0.5775) acc 90.6250 (90.2500) lr 4.8943e-05 eta 0:00:07
epoch [48/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.147) loss 0.7188 (0.5833) acc 78.1250 (86.2500) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.074) loss 0.4653 (0.6321) acc 90.6250 (86.8750) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.5132 (0.6572) acc 87.5000 (86.4583) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.4961 (0.6309) acc 90.6250 (87.1875) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.5947 (0.6170) acc 87.5000 (87.7500) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.069 (0.214) data 0.000 (0.141) loss 0.6914 (0.6065) acc 84.3750 (88.7500) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.071) loss 0.5967 (0.6123) acc 96.8750 (89.3750) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.5127 (0.6120) acc 90.6250 (88.9583) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.036) loss 0.6782 (0.6074) acc 84.3750 (89.0625) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.028) loss 0.5176 (0.5976) acc 93.7500 (89.6250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.071 (0.205) data 0.000 (0.133) loss 0.6958 (0.5884) acc 84.3750 (90.6250) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.071 (0.137) data 0.000 (0.067) loss 0.8462 (0.6101) acc 78.1250 (90.0000) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.070 (0.115) data 0.000 (0.045) loss 0.6606 (0.5983) acc 87.5000 (89.7917) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.8618 (0.6030) acc 84.3750 (89.5312) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.070 (0.097) data 0.000 (0.027) loss 0.5127 (0.5860) acc 96.8750 (90.2500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:04<00:13,  4.49s/it] 50%|█████     | 2/4 [00:05<00:04,  2.19s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.45s/it]100%|██████████| 4/4 [00:06<00:00,  1.11s/it]100%|██████████| 4/4 [00:06<00:00,  1.59s/it]
=> result
* total: 1,934
* correct: 1,674
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.7%
Elapsed: 0:02:22
Run this job and save the output to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.071 (0.286) data 0.000 (0.180) loss 2.5391 (2.4281) acc 56.2500 (60.6250) lr 1.0000e-05 eta 0:06:10
epoch [1/50] batch [10/26] time 0.071 (0.179) data 0.000 (0.090) loss 2.9805 (2.4879) acc 40.6250 (57.8125) lr 1.0000e-05 eta 0:03:51
epoch [1/50] batch [15/26] time 0.071 (0.143) data 0.000 (0.060) loss 2.7324 (2.5109) acc 46.8750 (57.7083) lr 1.0000e-05 eta 0:03:03
epoch [1/50] batch [20/26] time 0.070 (0.125) data 0.000 (0.045) loss 2.2109 (2.4646) acc 53.1250 (57.3438) lr 1.0000e-05 eta 0:02:40
epoch [1/50] batch [25/26] time 0.075 (0.114) data 0.000 (0.036) loss 2.6484 (2.4216) acc 53.1250 (59.0000) lr 1.0000e-05 eta 0:02:25
epoch [2/50] batch [5/26] time 0.070 (0.227) data 0.000 (0.150) loss 1.6504 (1.8955) acc 65.6250 (64.3750) lr 2.0000e-03 eta 0:04:47
epoch [2/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.075) loss 1.7090 (1.7222) acc 59.3750 (64.3750) lr 2.0000e-03 eta 0:03:07
epoch [2/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.050) loss 1.4160 (1.6402) acc 68.7500 (64.5833) lr 2.0000e-03 eta 0:02:33
epoch [2/50] batch [20/26] time 0.069 (0.109) data 0.000 (0.038) loss 1.1826 (1.5517) acc 65.6250 (64.8438) lr 2.0000e-03 eta 0:02:16
epoch [2/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 1.1748 (1.5249) acc 78.1250 (64.2500) lr 2.0000e-03 eta 0:02:06
epoch [3/50] batch [5/26] time 0.070 (0.214) data 0.000 (0.142) loss 1.5332 (1.1322) acc 71.8750 (70.0000) lr 1.9980e-03 eta 0:04:25
epoch [3/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.071) loss 1.4189 (1.2229) acc 62.5000 (65.3125) lr 1.9980e-03 eta 0:02:55
epoch [3/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.048) loss 1.3564 (1.2257) acc 71.8750 (66.4583) lr 1.9980e-03 eta 0:02:25
epoch [3/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.036) loss 1.2715 (1.2239) acc 59.3750 (66.2500) lr 1.9980e-03 eta 0:02:10
epoch [3/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.029) loss 1.3516 (1.1952) acc 65.6250 (67.2500) lr 1.9980e-03 eta 0:02:00
epoch [4/50] batch [5/26] time 0.070 (0.219) data 0.000 (0.144) loss 1.2881 (1.1786) acc 62.5000 (65.0000) lr 1.9921e-03 eta 0:04:26
epoch [4/50] batch [10/26] time 0.070 (0.145) data 0.000 (0.072) loss 0.7632 (1.1905) acc 75.0000 (65.3125) lr 1.9921e-03 eta 0:02:55
epoch [4/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.8604 (1.1665) acc 87.5000 (66.6667) lr 1.9921e-03 eta 0:02:24
epoch [4/50] batch [20/26] time 0.069 (0.107) data 0.000 (0.036) loss 0.9985 (1.1281) acc 81.2500 (68.2812) lr 1.9921e-03 eta 0:02:09
epoch [4/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.7852 (1.1107) acc 81.2500 (68.7500) lr 1.9921e-03 eta 0:01:59
epoch [5/50] batch [5/26] time 0.073 (0.231) data 0.000 (0.160) loss 0.9883 (1.0567) acc 78.1250 (76.2500) lr 1.9823e-03 eta 0:04:35
epoch [5/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.080) loss 0.4590 (1.0164) acc 87.5000 (75.9375) lr 1.9823e-03 eta 0:02:58
epoch [5/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.8711 (0.9967) acc 87.5000 (75.6250) lr 1.9823e-03 eta 0:02:26
epoch [5/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.040) loss 0.8965 (1.0012) acc 71.8750 (74.2188) lr 1.9823e-03 eta 0:02:10
epoch [5/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.7197 (0.9980) acc 84.3750 (74.1250) lr 1.9823e-03 eta 0:02:00
epoch [6/50] batch [5/26] time 0.070 (0.222) data 0.000 (0.149) loss 0.9946 (0.9540) acc 68.7500 (72.5000) lr 1.9686e-03 eta 0:04:19
epoch [6/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.075) loss 1.0273 (0.9123) acc 71.8750 (74.3750) lr 1.9686e-03 eta 0:02:50
epoch [6/50] batch [15/26] time 0.069 (0.121) data 0.000 (0.050) loss 0.7100 (0.9694) acc 81.2500 (72.9167) lr 1.9686e-03 eta 0:02:19
epoch [6/50] batch [20/26] time 0.069 (0.108) data 0.000 (0.038) loss 0.9971 (0.9849) acc 68.7500 (72.9688) lr 1.9686e-03 eta 0:02:04
epoch [6/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.7290 (0.9580) acc 78.1250 (73.6250) lr 1.9686e-03 eta 0:01:55
epoch [7/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.145) loss 0.8511 (0.9313) acc 84.3750 (78.1250) lr 1.9511e-03 eta 0:04:11
epoch [7/50] batch [10/26] time 0.069 (0.146) data 0.000 (0.073) loss 0.8687 (0.8704) acc 87.5000 (80.6250) lr 1.9511e-03 eta 0:02:45
epoch [7/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.049) loss 0.8652 (0.8573) acc 78.1250 (79.7917) lr 1.9511e-03 eta 0:02:16
epoch [7/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 1.0605 (0.9037) acc 78.1250 (77.8125) lr 1.9511e-03 eta 0:02:01
epoch [7/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 1.0742 (0.8943) acc 75.0000 (78.3750) lr 1.9511e-03 eta 0:01:52
epoch [8/50] batch [5/26] time 0.071 (0.217) data 0.000 (0.143) loss 0.7163 (0.7449) acc 84.3750 (81.2500) lr 1.9298e-03 eta 0:04:01
epoch [8/50] batch [10/26] time 0.072 (0.144) data 0.000 (0.072) loss 0.5273 (0.8485) acc 90.6250 (78.4375) lr 1.9298e-03 eta 0:02:39
epoch [8/50] batch [15/26] time 0.069 (0.119) data 0.000 (0.048) loss 0.9556 (0.8756) acc 75.0000 (77.2917) lr 1.9298e-03 eta 0:02:11
epoch [8/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 1.0156 (0.8980) acc 56.2500 (75.3125) lr 1.9298e-03 eta 0:01:57
epoch [8/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.7134 (0.9181) acc 81.2500 (76.1250) lr 1.9298e-03 eta 0:01:49
epoch [9/50] batch [5/26] time 0.071 (0.270) data 0.000 (0.199) loss 0.9819 (0.8979) acc 71.8750 (79.3750) lr 1.9048e-03 eta 0:04:53
epoch [9/50] batch [10/26] time 0.072 (0.171) data 0.000 (0.100) loss 0.8970 (0.8223) acc 78.1250 (80.0000) lr 1.9048e-03 eta 0:03:04
epoch [9/50] batch [15/26] time 0.070 (0.137) data 0.000 (0.066) loss 1.1416 (0.8910) acc 75.0000 (77.7083) lr 1.9048e-03 eta 0:02:27
epoch [9/50] batch [20/26] time 0.073 (0.121) data 0.000 (0.050) loss 0.8335 (0.8777) acc 84.3750 (77.8125) lr 1.9048e-03 eta 0:02:09
epoch [9/50] batch [25/26] time 0.070 (0.111) data 0.000 (0.040) loss 1.0635 (0.8851) acc 78.1250 (77.6250) lr 1.9048e-03 eta 0:01:58
epoch [10/50] batch [5/26] time 0.071 (0.229) data 0.000 (0.157) loss 0.7471 (0.9330) acc 84.3750 (75.6250) lr 1.8763e-03 eta 0:04:03
epoch [10/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.079) loss 0.8286 (0.9430) acc 81.2500 (75.6250) lr 1.8763e-03 eta 0:02:38
epoch [10/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.053) loss 0.9194 (0.9359) acc 75.0000 (76.8750) lr 1.8763e-03 eta 0:02:09
epoch [10/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 1.0400 (0.9338) acc 78.1250 (77.6562) lr 1.8763e-03 eta 0:01:55
epoch [10/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.8960 (0.9424) acc 71.8750 (76.7500) lr 1.8763e-03 eta 0:01:46
epoch [11/50] batch [5/26] time 0.071 (0.252) data 0.000 (0.180) loss 0.7397 (0.8112) acc 84.3750 (82.5000) lr 1.8443e-03 eta 0:04:21
epoch [11/50] batch [10/26] time 0.074 (0.162) data 0.000 (0.090) loss 1.1094 (0.8828) acc 71.8750 (79.6875) lr 1.8443e-03 eta 0:02:46
epoch [11/50] batch [15/26] time 0.071 (0.132) data 0.000 (0.060) loss 1.0273 (0.8884) acc 75.0000 (78.9583) lr 1.8443e-03 eta 0:02:14
epoch [11/50] batch [20/26] time 0.070 (0.117) data 0.000 (0.045) loss 0.6274 (0.8576) acc 84.3750 (79.3750) lr 1.8443e-03 eta 0:01:58
epoch [11/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.6387 (0.8504) acc 84.3750 (79.2500) lr 1.8443e-03 eta 0:01:48
epoch [12/50] batch [5/26] time 0.070 (0.267) data 0.000 (0.189) loss 0.6602 (0.7711) acc 81.2500 (80.6250) lr 1.8090e-03 eta 0:04:29
epoch [12/50] batch [10/26] time 0.070 (0.169) data 0.000 (0.095) loss 0.6880 (0.7946) acc 81.2500 (79.6875) lr 1.8090e-03 eta 0:02:49
epoch [12/50] batch [15/26] time 0.070 (0.136) data 0.000 (0.063) loss 1.0010 (0.8242) acc 81.2500 (79.3750) lr 1.8090e-03 eta 0:02:15
epoch [12/50] batch [20/26] time 0.070 (0.119) data 0.000 (0.047) loss 0.8242 (0.8222) acc 81.2500 (79.8438) lr 1.8090e-03 eta 0:01:58
epoch [12/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.9639 (0.8242) acc 75.0000 (80.0000) lr 1.8090e-03 eta 0:01:48
epoch [13/50] batch [5/26] time 0.070 (0.261) data 0.000 (0.188) loss 0.7969 (0.7346) acc 90.6250 (86.8750) lr 1.7705e-03 eta 0:04:16
epoch [13/50] batch [10/26] time 0.070 (0.165) data 0.000 (0.094) loss 0.5352 (0.7381) acc 87.5000 (85.3125) lr 1.7705e-03 eta 0:02:41
epoch [13/50] batch [15/26] time 0.070 (0.134) data 0.000 (0.063) loss 1.0488 (0.8068) acc 78.1250 (82.0833) lr 1.7705e-03 eta 0:02:10
epoch [13/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.047) loss 1.0527 (0.7990) acc 75.0000 (82.3438) lr 1.7705e-03 eta 0:01:54
epoch [13/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.7432 (0.7869) acc 75.0000 (81.6250) lr 1.7705e-03 eta 0:01:44
epoch [14/50] batch [5/26] time 0.071 (0.267) data 0.000 (0.191) loss 0.7651 (0.8678) acc 84.3750 (80.0000) lr 1.7290e-03 eta 0:04:15
epoch [14/50] batch [10/26] time 0.071 (0.169) data 0.000 (0.096) loss 0.7861 (0.8398) acc 84.3750 (80.3125) lr 1.7290e-03 eta 0:02:40
epoch [14/50] batch [15/26] time 0.070 (0.136) data 0.000 (0.064) loss 0.8706 (0.8271) acc 78.1250 (80.2083) lr 1.7290e-03 eta 0:02:08
epoch [14/50] batch [20/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.7041 (0.8046) acc 84.3750 (81.4062) lr 1.7290e-03 eta 0:01:52
epoch [14/50] batch [25/26] time 0.070 (0.110) data 0.000 (0.038) loss 0.8745 (0.8123) acc 78.1250 (81.1250) lr 1.7290e-03 eta 0:01:42
epoch [15/50] batch [5/26] time 0.071 (0.228) data 0.000 (0.157) loss 0.8179 (0.7036) acc 81.2500 (84.3750) lr 1.6845e-03 eta 0:03:32
epoch [15/50] batch [10/26] time 0.073 (0.150) data 0.000 (0.079) loss 0.9688 (0.8147) acc 78.1250 (80.0000) lr 1.6845e-03 eta 0:02:18
epoch [15/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.053) loss 0.5312 (0.7725) acc 90.6250 (82.0833) lr 1.6845e-03 eta 0:01:53
epoch [15/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.9819 (0.8250) acc 68.7500 (79.6875) lr 1.6845e-03 eta 0:01:41
epoch [15/50] batch [25/26] time 0.072 (0.103) data 0.000 (0.032) loss 0.8706 (0.8371) acc 81.2500 (79.5000) lr 1.6845e-03 eta 0:01:33
epoch [16/50] batch [5/26] time 0.070 (0.253) data 0.000 (0.177) loss 0.5986 (0.7846) acc 87.5000 (82.5000) lr 1.6374e-03 eta 0:03:49
epoch [16/50] batch [10/26] time 0.073 (0.162) data 0.000 (0.089) loss 0.6255 (0.7895) acc 81.2500 (82.1875) lr 1.6374e-03 eta 0:02:25
epoch [16/50] batch [15/26] time 0.070 (0.132) data 0.000 (0.059) loss 0.6362 (0.7783) acc 84.3750 (82.7083) lr 1.6374e-03 eta 0:01:57
epoch [16/50] batch [20/26] time 0.071 (0.116) data 0.000 (0.045) loss 0.5166 (0.7709) acc 87.5000 (82.8125) lr 1.6374e-03 eta 0:01:43
epoch [16/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.8564 (0.7868) acc 78.1250 (81.5000) lr 1.6374e-03 eta 0:01:34
epoch [17/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.158) loss 0.8018 (0.9292) acc 75.0000 (75.6250) lr 1.5878e-03 eta 0:03:21
epoch [17/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.079) loss 0.9326 (0.8820) acc 84.3750 (78.4375) lr 1.5878e-03 eta 0:02:11
epoch [17/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.053) loss 0.6011 (0.8539) acc 87.5000 (79.5833) lr 1.5878e-03 eta 0:01:47
epoch [17/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.040) loss 0.9038 (0.8144) acc 75.0000 (80.1562) lr 1.5878e-03 eta 0:01:35
epoch [17/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.6641 (0.7988) acc 81.2500 (80.7500) lr 1.5878e-03 eta 0:01:27
epoch [18/50] batch [5/26] time 0.071 (0.216) data 0.000 (0.145) loss 0.7969 (0.7206) acc 81.2500 (83.7500) lr 1.5358e-03 eta 0:03:04
epoch [18/50] batch [10/26] time 0.071 (0.144) data 0.000 (0.073) loss 0.6357 (0.7371) acc 90.6250 (82.1875) lr 1.5358e-03 eta 0:02:02
epoch [18/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.049) loss 0.8496 (0.7497) acc 84.3750 (81.4583) lr 1.5358e-03 eta 0:01:40
epoch [18/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.037) loss 0.8315 (0.7796) acc 78.1250 (81.2500) lr 1.5358e-03 eta 0:01:29
epoch [18/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.8345 (0.7702) acc 84.3750 (81.3750) lr 1.5358e-03 eta 0:01:23
epoch [19/50] batch [5/26] time 0.070 (0.265) data 0.000 (0.191) loss 0.7852 (0.7833) acc 81.2500 (84.3750) lr 1.4818e-03 eta 0:03:38
epoch [19/50] batch [10/26] time 0.071 (0.168) data 0.000 (0.096) loss 0.7852 (0.7608) acc 81.2500 (82.1875) lr 1.4818e-03 eta 0:02:17
epoch [19/50] batch [15/26] time 0.071 (0.135) data 0.000 (0.064) loss 0.6084 (0.7382) acc 90.6250 (83.1250) lr 1.4818e-03 eta 0:01:50
epoch [19/50] batch [20/26] time 0.071 (0.119) data 0.000 (0.048) loss 0.6001 (0.7296) acc 87.5000 (83.4375) lr 1.4818e-03 eta 0:01:36
epoch [19/50] batch [25/26] time 0.071 (0.110) data 0.000 (0.038) loss 0.7021 (0.7362) acc 84.3750 (83.7500) lr 1.4818e-03 eta 0:01:28
epoch [20/50] batch [5/26] time 0.070 (0.258) data 0.001 (0.186) loss 0.4927 (0.8093) acc 93.7500 (81.8750) lr 1.4258e-03 eta 0:03:26
epoch [20/50] batch [10/26] time 0.071 (0.164) data 0.000 (0.093) loss 0.5625 (0.8028) acc 90.6250 (81.2500) lr 1.4258e-03 eta 0:02:10
epoch [20/50] batch [15/26] time 0.071 (0.133) data 0.000 (0.062) loss 0.7803 (0.8123) acc 87.5000 (80.8333) lr 1.4258e-03 eta 0:01:45
epoch [20/50] batch [20/26] time 0.071 (0.117) data 0.000 (0.047) loss 0.4392 (0.7604) acc 87.5000 (82.1875) lr 1.4258e-03 eta 0:01:32
epoch [20/50] batch [25/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.5791 (0.7404) acc 90.6250 (83.1250) lr 1.4258e-03 eta 0:01:24
epoch [21/50] batch [5/26] time 0.071 (0.234) data 0.000 (0.161) loss 1.1660 (0.7562) acc 65.6250 (81.8750) lr 1.3681e-03 eta 0:03:01
epoch [21/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.080) loss 0.3706 (0.6816) acc 93.7500 (85.0000) lr 1.3681e-03 eta 0:01:57
epoch [21/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.054) loss 0.6191 (0.6789) acc 87.5000 (85.4167) lr 1.3681e-03 eta 0:01:35
epoch [21/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.4966 (0.6984) acc 87.5000 (84.5312) lr 1.3681e-03 eta 0:01:24
epoch [21/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.6958 (0.7053) acc 81.2500 (84.2500) lr 1.3681e-03 eta 0:01:17
epoch [22/50] batch [5/26] time 0.072 (0.217) data 0.000 (0.144) loss 0.4419 (0.7603) acc 96.8750 (85.6250) lr 1.3090e-03 eta 0:02:42
epoch [22/50] batch [10/26] time 0.071 (0.144) data 0.001 (0.072) loss 0.6543 (0.7381) acc 84.3750 (85.0000) lr 1.3090e-03 eta 0:01:47
epoch [22/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.048) loss 0.6978 (0.7174) acc 87.5000 (85.2083) lr 1.3090e-03 eta 0:01:28
epoch [22/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.4819 (0.7145) acc 90.6250 (85.1562) lr 1.3090e-03 eta 0:01:18
epoch [22/50] batch [25/26] time 0.071 (0.100) data 0.000 (0.029) loss 0.4016 (0.6974) acc 90.6250 (84.7500) lr 1.3090e-03 eta 0:01:12
epoch [23/50] batch [5/26] time 0.071 (0.250) data 0.000 (0.174) loss 0.4358 (0.6572) acc 90.6250 (83.1250) lr 1.2487e-03 eta 0:03:00
epoch [23/50] batch [10/26] time 0.071 (0.160) data 0.000 (0.087) loss 0.9434 (0.7584) acc 78.1250 (81.5625) lr 1.2487e-03 eta 0:01:55
epoch [23/50] batch [15/26] time 0.071 (0.131) data 0.000 (0.058) loss 0.8838 (0.7447) acc 75.0000 (81.8750) lr 1.2487e-03 eta 0:01:33
epoch [23/50] batch [20/26] time 0.072 (0.116) data 0.000 (0.044) loss 0.4792 (0.7201) acc 96.8750 (83.5938) lr 1.2487e-03 eta 0:01:22
epoch [23/50] batch [25/26] time 0.071 (0.107) data 0.000 (0.035) loss 0.8013 (0.7313) acc 81.2500 (83.3750) lr 1.2487e-03 eta 0:01:15
epoch [24/50] batch [5/26] time 0.070 (0.232) data 0.000 (0.161) loss 0.7524 (0.6879) acc 81.2500 (82.5000) lr 1.1874e-03 eta 0:02:42
epoch [24/50] batch [10/26] time 0.072 (0.152) data 0.000 (0.081) loss 0.6152 (0.6899) acc 87.5000 (84.0625) lr 1.1874e-03 eta 0:01:44
epoch [24/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.054) loss 0.7305 (0.6816) acc 84.3750 (85.2083) lr 1.1874e-03 eta 0:01:25
epoch [24/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.8999 (0.7005) acc 78.1250 (84.3750) lr 1.1874e-03 eta 0:01:15
epoch [24/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.032) loss 0.6831 (0.6913) acc 81.2500 (84.8750) lr 1.1874e-03 eta 0:01:09
epoch [25/50] batch [5/26] time 0.071 (0.218) data 0.000 (0.141) loss 0.4763 (0.6492) acc 93.7500 (86.2500) lr 1.1253e-03 eta 0:02:25
epoch [25/50] batch [10/26] time 0.071 (0.144) data 0.000 (0.071) loss 0.9951 (0.7364) acc 81.2500 (83.7500) lr 1.1253e-03 eta 0:01:35
epoch [25/50] batch [15/26] time 0.072 (0.120) data 0.000 (0.047) loss 0.6777 (0.7242) acc 90.6250 (84.3750) lr 1.1253e-03 eta 0:01:19
epoch [25/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.035) loss 0.8955 (0.7338) acc 81.2500 (83.9062) lr 1.1253e-03 eta 0:01:10
epoch [25/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.028) loss 0.5933 (0.7186) acc 90.6250 (84.6250) lr 1.1253e-03 eta 0:01:05
epoch [26/50] batch [5/26] time 0.071 (0.222) data 0.000 (0.150) loss 0.9609 (0.6773) acc 75.0000 (84.3750) lr 1.0628e-03 eta 0:02:23
epoch [26/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.075) loss 0.8154 (0.6434) acc 65.6250 (83.4375) lr 1.0628e-03 eta 0:01:33
epoch [26/50] batch [15/26] time 0.072 (0.121) data 0.000 (0.050) loss 0.7046 (0.6593) acc 87.5000 (83.1250) lr 1.0628e-03 eta 0:01:16
epoch [26/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.038) loss 0.5762 (0.6281) acc 84.3750 (83.9062) lr 1.0628e-03 eta 0:01:08
epoch [26/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.7520 (0.6285) acc 81.2500 (84.1250) lr 1.0628e-03 eta 0:01:02
epoch [27/50] batch [5/26] time 0.071 (0.203) data 0.000 (0.129) loss 0.5557 (0.6265) acc 87.5000 (87.5000) lr 1.0000e-03 eta 0:02:05
epoch [27/50] batch [10/26] time 0.071 (0.137) data 0.001 (0.065) loss 0.9756 (0.6748) acc 78.1250 (85.0000) lr 1.0000e-03 eta 0:01:23
epoch [27/50] batch [15/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.7900 (0.6928) acc 84.3750 (84.7917) lr 1.0000e-03 eta 0:01:10
epoch [27/50] batch [20/26] time 0.070 (0.104) data 0.000 (0.032) loss 0.7412 (0.7070) acc 75.0000 (84.0625) lr 1.0000e-03 eta 0:01:02
epoch [27/50] batch [25/26] time 0.070 (0.097) data 0.000 (0.026) loss 0.7461 (0.7020) acc 87.5000 (84.6250) lr 1.0000e-03 eta 0:00:58
epoch [28/50] batch [5/26] time 0.073 (0.202) data 0.000 (0.128) loss 0.6113 (0.7674) acc 84.3750 (82.5000) lr 9.3721e-04 eta 0:01:59
epoch [28/50] batch [10/26] time 0.071 (0.136) data 0.000 (0.064) loss 0.4058 (0.6579) acc 84.3750 (85.9375) lr 9.3721e-04 eta 0:01:20
epoch [28/50] batch [15/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.7324 (0.6790) acc 78.1250 (85.8333) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [20/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.8896 (0.6711) acc 81.2500 (86.4062) lr 9.3721e-04 eta 0:00:59
epoch [28/50] batch [25/26] time 0.071 (0.097) data 0.000 (0.026) loss 0.9502 (0.6794) acc 84.3750 (86.3750) lr 9.3721e-04 eta 0:00:55
epoch [29/50] batch [5/26] time 0.071 (0.206) data 0.000 (0.134) loss 0.6250 (0.6029) acc 81.2500 (85.0000) lr 8.7467e-04 eta 0:01:56
epoch [29/50] batch [10/26] time 0.073 (0.139) data 0.000 (0.067) loss 0.5488 (0.6214) acc 96.8750 (86.8750) lr 8.7467e-04 eta 0:01:17
epoch [29/50] batch [15/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.6646 (0.6128) acc 84.3750 (87.2917) lr 8.7467e-04 eta 0:01:04
epoch [29/50] batch [20/26] time 0.071 (0.105) data 0.000 (0.034) loss 0.4978 (0.6429) acc 90.6250 (85.9375) lr 8.7467e-04 eta 0:00:57
epoch [29/50] batch [25/26] time 0.072 (0.098) data 0.000 (0.027) loss 0.6626 (0.6396) acc 81.2500 (86.0000) lr 8.7467e-04 eta 0:00:53
epoch [30/50] batch [5/26] time 0.070 (0.209) data 0.000 (0.138) loss 0.8325 (0.6476) acc 87.5000 (88.7500) lr 8.1262e-04 eta 0:01:53
epoch [30/50] batch [10/26] time 0.070 (0.140) data 0.000 (0.069) loss 0.8716 (0.6448) acc 84.3750 (87.1875) lr 8.1262e-04 eta 0:01:14
epoch [30/50] batch [15/26] time 0.070 (0.117) data 0.000 (0.046) loss 0.8345 (0.6704) acc 81.2500 (86.8750) lr 8.1262e-04 eta 0:01:01
epoch [30/50] batch [20/26] time 0.069 (0.105) data 0.000 (0.035) loss 0.8149 (0.6741) acc 84.3750 (86.7188) lr 8.1262e-04 eta 0:00:55
epoch [30/50] batch [25/26] time 0.069 (0.098) data 0.000 (0.028) loss 0.9805 (0.6835) acc 75.0000 (85.8750) lr 8.1262e-04 eta 0:00:51
epoch [31/50] batch [5/26] time 0.071 (0.198) data 0.000 (0.124) loss 0.3103 (0.5766) acc 96.8750 (89.3750) lr 7.5131e-04 eta 0:01:41
epoch [31/50] batch [10/26] time 0.071 (0.134) data 0.000 (0.062) loss 0.5586 (0.6312) acc 90.6250 (88.7500) lr 7.5131e-04 eta 0:01:08
epoch [31/50] batch [15/26] time 0.071 (0.113) data 0.000 (0.042) loss 0.4509 (0.6042) acc 90.6250 (88.7500) lr 7.5131e-04 eta 0:00:57
epoch [31/50] batch [20/26] time 0.071 (0.103) data 0.000 (0.031) loss 0.4766 (0.6008) acc 90.6250 (88.9062) lr 7.5131e-04 eta 0:00:51
epoch [31/50] batch [25/26] time 0.071 (0.096) data 0.000 (0.025) loss 0.7344 (0.6031) acc 81.2500 (88.8750) lr 7.5131e-04 eta 0:00:47
epoch [32/50] batch [5/26] time 0.071 (0.247) data 0.000 (0.172) loss 0.5752 (0.6046) acc 84.3750 (85.0000) lr 6.9098e-04 eta 0:02:00
epoch [32/50] batch [10/26] time 0.071 (0.159) data 0.001 (0.086) loss 0.4468 (0.5223) acc 96.8750 (89.6875) lr 6.9098e-04 eta 0:01:16
epoch [32/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.057) loss 0.4717 (0.5280) acc 87.5000 (89.3750) lr 6.9098e-04 eta 0:01:01
epoch [32/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.6968 (0.5415) acc 87.5000 (89.0625) lr 6.9098e-04 eta 0:00:54
epoch [32/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.8516 (0.5663) acc 78.1250 (88.5000) lr 6.9098e-04 eta 0:00:49
epoch [33/50] batch [5/26] time 0.071 (0.216) data 0.000 (0.143) loss 0.5635 (0.6341) acc 93.7500 (89.3750) lr 6.3188e-04 eta 0:01:39
epoch [33/50] batch [10/26] time 0.071 (0.143) data 0.000 (0.072) loss 0.7192 (0.6477) acc 87.5000 (88.1250) lr 6.3188e-04 eta 0:01:05
epoch [33/50] batch [15/26] time 0.071 (0.119) data 0.000 (0.048) loss 0.6133 (0.6539) acc 81.2500 (86.8750) lr 6.3188e-04 eta 0:00:53
epoch [33/50] batch [20/26] time 0.071 (0.107) data 0.000 (0.036) loss 1.0938 (0.6887) acc 68.7500 (85.3125) lr 6.3188e-04 eta 0:00:47
epoch [33/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.5479 (0.6839) acc 90.6250 (85.6250) lr 6.3188e-04 eta 0:00:44
epoch [34/50] batch [5/26] time 0.072 (0.228) data 0.000 (0.154) loss 0.4563 (0.5334) acc 93.7500 (91.8750) lr 5.7422e-04 eta 0:01:39
epoch [34/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.077) loss 0.7056 (0.5984) acc 90.6250 (89.0625) lr 5.7422e-04 eta 0:01:04
epoch [34/50] batch [15/26] time 0.071 (0.123) data 0.000 (0.052) loss 0.5146 (0.6489) acc 84.3750 (86.4583) lr 5.7422e-04 eta 0:00:52
epoch [34/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.4832 (0.6528) acc 87.5000 (86.7188) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.4265 (0.6283) acc 93.7500 (88.0000) lr 5.7422e-04 eta 0:00:42
epoch [35/50] batch [5/26] time 0.072 (0.210) data 0.000 (0.138) loss 0.5381 (0.6242) acc 84.3750 (87.5000) lr 5.1825e-04 eta 0:01:26
epoch [35/50] batch [10/26] time 0.070 (0.140) data 0.000 (0.069) loss 0.6587 (0.6250) acc 84.3750 (87.5000) lr 5.1825e-04 eta 0:00:56
epoch [35/50] batch [15/26] time 0.071 (0.117) data 0.000 (0.046) loss 0.4954 (0.6108) acc 93.7500 (88.1250) lr 5.1825e-04 eta 0:00:46
epoch [35/50] batch [20/26] time 0.071 (0.105) data 0.000 (0.035) loss 0.5874 (0.6377) acc 93.7500 (87.1875) lr 5.1825e-04 eta 0:00:41
epoch [35/50] batch [25/26] time 0.071 (0.098) data 0.000 (0.028) loss 0.6123 (0.6214) acc 87.5000 (87.5000) lr 5.1825e-04 eta 0:00:38
epoch [36/50] batch [5/26] time 0.069 (0.216) data 0.000 (0.144) loss 0.3083 (0.4858) acc 100.0000 (93.7500) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [10/26] time 0.070 (0.143) data 0.000 (0.072) loss 0.6729 (0.6010) acc 84.3750 (88.4375) lr 4.6417e-04 eta 0:00:54
epoch [36/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.048) loss 0.9326 (0.6308) acc 75.0000 (86.4583) lr 4.6417e-04 eta 0:00:44
epoch [36/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.5581 (0.6392) acc 90.6250 (86.2500) lr 4.6417e-04 eta 0:00:39
epoch [36/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.029) loss 0.6133 (0.6348) acc 93.7500 (86.6250) lr 4.6417e-04 eta 0:00:36
epoch [37/50] batch [5/26] time 0.072 (0.223) data 0.000 (0.150) loss 0.6025 (0.6239) acc 90.6250 (87.5000) lr 4.1221e-04 eta 0:01:20
epoch [37/50] batch [10/26] time 0.073 (0.147) data 0.000 (0.075) loss 0.8325 (0.6198) acc 75.0000 (87.1875) lr 4.1221e-04 eta 0:00:52
epoch [37/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.050) loss 0.5762 (0.6073) acc 84.3750 (87.7083) lr 4.1221e-04 eta 0:00:42
epoch [37/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.7085 (0.6159) acc 87.5000 (87.8125) lr 4.1221e-04 eta 0:00:37
epoch [37/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.8105 (0.6075) acc 75.0000 (88.2500) lr 4.1221e-04 eta 0:00:34
epoch [38/50] batch [5/26] time 0.070 (0.204) data 0.000 (0.133) loss 0.5693 (0.6206) acc 90.6250 (85.0000) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [10/26] time 0.071 (0.137) data 0.000 (0.066) loss 0.5571 (0.5894) acc 90.6250 (87.8125) lr 3.6258e-04 eta 0:00:45
epoch [38/50] batch [15/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.6211 (0.5753) acc 90.6250 (88.9583) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [20/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.3997 (0.5713) acc 96.8750 (89.2188) lr 3.6258e-04 eta 0:00:33
epoch [38/50] batch [25/26] time 0.070 (0.097) data 0.000 (0.027) loss 0.7437 (0.6232) acc 90.6250 (87.6250) lr 3.6258e-04 eta 0:00:30
epoch [39/50] batch [5/26] time 0.072 (0.218) data 0.000 (0.147) loss 0.4683 (0.5651) acc 96.8750 (90.0000) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.074) loss 0.4797 (0.5523) acc 93.7500 (90.0000) lr 3.1545e-04 eta 0:00:43
epoch [39/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.049) loss 0.3237 (0.5734) acc 100.0000 (89.5833) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.037) loss 0.4636 (0.5703) acc 90.6250 (89.5312) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.030) loss 0.5947 (0.5783) acc 90.6250 (89.0000) lr 3.1545e-04 eta 0:00:28
epoch [40/50] batch [5/26] time 0.070 (0.237) data 0.000 (0.161) loss 0.8047 (0.5976) acc 81.2500 (88.1250) lr 2.7103e-04 eta 0:01:06
epoch [40/50] batch [10/26] time 0.071 (0.154) data 0.000 (0.080) loss 0.5986 (0.6111) acc 90.6250 (87.5000) lr 2.7103e-04 eta 0:00:42
epoch [40/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.054) loss 0.6465 (0.5830) acc 90.6250 (88.5417) lr 2.7103e-04 eta 0:00:34
epoch [40/50] batch [20/26] time 0.071 (0.112) data 0.000 (0.040) loss 0.3298 (0.5652) acc 93.7500 (89.3750) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [25/26] time 0.071 (0.104) data 0.000 (0.032) loss 0.5010 (0.5486) acc 87.5000 (89.6250) lr 2.7103e-04 eta 0:00:27
epoch [41/50] batch [5/26] time 0.070 (0.211) data 0.000 (0.139) loss 0.4629 (0.5611) acc 90.6250 (88.7500) lr 2.2949e-04 eta 0:00:53
epoch [41/50] batch [10/26] time 0.072 (0.141) data 0.000 (0.070) loss 0.4700 (0.5910) acc 93.7500 (87.5000) lr 2.2949e-04 eta 0:00:35
epoch [41/50] batch [15/26] time 0.071 (0.118) data 0.000 (0.047) loss 0.4197 (0.5578) acc 93.7500 (89.3750) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.5698 (0.5951) acc 90.6250 (88.7500) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.028) loss 0.8003 (0.6016) acc 81.2500 (88.3750) lr 2.2949e-04 eta 0:00:23
epoch [42/50] batch [5/26] time 0.072 (0.222) data 0.000 (0.150) loss 0.5234 (0.5566) acc 96.8750 (90.0000) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [10/26] time 0.072 (0.147) data 0.000 (0.075) loss 0.4678 (0.6203) acc 96.8750 (88.4375) lr 1.9098e-04 eta 0:00:32
epoch [42/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.050) loss 0.6069 (0.6312) acc 87.5000 (87.5000) lr 1.9098e-04 eta 0:00:26
epoch [42/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.4822 (0.6232) acc 93.7500 (87.3438) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.6772 (0.6363) acc 87.5000 (87.0000) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.071 (0.244) data 0.000 (0.172) loss 0.8208 (0.5170) acc 84.3750 (91.8750) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [10/26] time 0.073 (0.158) data 0.000 (0.086) loss 0.3740 (0.5352) acc 93.7500 (90.9375) lr 1.5567e-04 eta 0:00:31
epoch [43/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.058) loss 0.6509 (0.5315) acc 87.5000 (91.0417) lr 1.5567e-04 eta 0:00:24
epoch [43/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.043) loss 0.5747 (0.5374) acc 93.7500 (91.2500) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.6162 (0.5438) acc 87.5000 (90.2500) lr 1.5567e-04 eta 0:00:19
epoch [44/50] batch [5/26] time 0.070 (0.213) data 0.000 (0.136) loss 0.4749 (0.5463) acc 96.8750 (89.3750) lr 1.2369e-04 eta 0:00:37
epoch [44/50] batch [10/26] time 0.071 (0.142) data 0.000 (0.068) loss 0.5498 (0.5767) acc 90.6250 (89.3750) lr 1.2369e-04 eta 0:00:24
epoch [44/50] batch [15/26] time 0.071 (0.118) data 0.000 (0.045) loss 0.6963 (0.5569) acc 90.6250 (89.5833) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.034) loss 0.7549 (0.5798) acc 84.3750 (88.9062) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.027) loss 0.5718 (0.5979) acc 93.7500 (88.1250) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/26] time 0.072 (0.245) data 0.000 (0.170) loss 0.8994 (0.7272) acc 81.2500 (85.6250) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [10/26] time 0.071 (0.158) data 0.000 (0.085) loss 0.4873 (0.6709) acc 90.6250 (86.8750) lr 9.5173e-05 eta 0:00:23
epoch [45/50] batch [15/26] time 0.071 (0.129) data 0.000 (0.057) loss 0.3425 (0.6207) acc 96.8750 (88.1250) lr 9.5173e-05 eta 0:00:18
epoch [45/50] batch [20/26] time 0.073 (0.115) data 0.000 (0.043) loss 0.5654 (0.6324) acc 90.6250 (87.3438) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.034) loss 0.6064 (0.6040) acc 90.6250 (88.6250) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.071 (0.217) data 0.000 (0.139) loss 0.7227 (0.6672) acc 84.3750 (86.2500) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.070) loss 0.3977 (0.6069) acc 96.8750 (87.1875) lr 7.0224e-05 eta 0:00:17
epoch [46/50] batch [15/26] time 0.071 (0.120) data 0.000 (0.047) loss 0.6411 (0.5879) acc 84.3750 (87.7083) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/26] time 0.071 (0.108) data 0.000 (0.035) loss 0.5610 (0.5938) acc 93.7500 (88.1250) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.028) loss 0.4165 (0.5932) acc 90.6250 (88.1250) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.071 (0.209) data 0.000 (0.134) loss 0.4114 (0.4459) acc 93.7500 (92.5000) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [10/26] time 0.071 (0.140) data 0.000 (0.067) loss 0.4509 (0.4905) acc 93.7500 (91.8750) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/26] time 0.071 (0.117) data 0.000 (0.045) loss 0.8262 (0.5627) acc 90.6250 (91.2500) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.6846 (0.5762) acc 81.2500 (90.3125) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [25/26] time 0.071 (0.098) data 0.000 (0.027) loss 0.5176 (0.5846) acc 93.7500 (89.8750) lr 4.8943e-05 eta 0:00:07
epoch [48/50] batch [5/26] time 0.071 (0.213) data 0.000 (0.138) loss 0.3730 (0.4953) acc 93.7500 (93.1250) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [10/26] time 0.071 (0.142) data 0.000 (0.069) loss 0.6064 (0.5404) acc 87.5000 (91.5625) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.046) loss 0.5176 (0.5555) acc 90.6250 (91.2500) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.8398 (0.5860) acc 90.6250 (90.4688) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.7559 (0.5953) acc 81.2500 (89.8750) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.072 (0.208) data 0.000 (0.136) loss 0.5264 (0.6173) acc 87.5000 (88.1250) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [10/26] time 0.071 (0.139) data 0.000 (0.068) loss 0.4683 (0.5765) acc 90.6250 (90.0000) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [15/26] time 0.070 (0.116) data 0.000 (0.046) loss 0.5244 (0.6000) acc 90.6250 (89.7917) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.6250 (0.5731) acc 78.1250 (89.6875) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.098) data 0.000 (0.027) loss 0.5493 (0.5687) acc 90.6250 (89.5000) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.072 (0.221) data 0.000 (0.145) loss 0.5752 (0.6524) acc 90.6250 (84.3750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.072 (0.146) data 0.000 (0.073) loss 0.5991 (0.6091) acc 90.6250 (86.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.5020 (0.6208) acc 90.6250 (86.6667) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.036) loss 0.6826 (0.6007) acc 84.3750 (87.1875) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.072 (0.101) data 0.000 (0.029) loss 0.5322 (0.5846) acc 96.8750 (88.5000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:10,  3.57s/it] 50%|█████     | 2/4 [00:04<00:03,  1.81s/it] 75%|███████▌  | 3/4 [00:04<00:01,  1.25s/it]100%|██████████| 4/4 [00:05<00:00,  1.02it/s]100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
=> result
* total: 1,934
* correct: 1,704
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.4%
Elapsed: 0:02:22
Run this job and save the output to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.070 (0.291) data 0.000 (0.176) loss 2.5488 (2.6031) acc 62.5000 (56.8750) lr 1.0000e-05 eta 0:06:17
epoch [1/50] batch [10/26] time 0.071 (0.181) data 0.000 (0.088) loss 2.5020 (2.4902) acc 56.2500 (60.6250) lr 1.0000e-05 eta 0:03:53
epoch [1/50] batch [15/26] time 0.070 (0.144) data 0.000 (0.059) loss 2.4082 (2.4935) acc 56.2500 (60.2083) lr 1.0000e-05 eta 0:03:05
epoch [1/50] batch [20/26] time 0.070 (0.126) data 0.000 (0.044) loss 2.0820 (2.3953) acc 75.0000 (62.6562) lr 1.0000e-05 eta 0:02:40
epoch [1/50] batch [25/26] time 0.071 (0.115) data 0.000 (0.035) loss 1.8711 (2.3512) acc 78.1250 (62.8750) lr 1.0000e-05 eta 0:02:26
epoch [2/50] batch [5/26] time 0.069 (0.244) data 0.000 (0.172) loss 1.5977 (1.6855) acc 71.8750 (68.1250) lr 2.0000e-03 eta 0:05:09
epoch [2/50] batch [10/26] time 0.077 (0.158) data 0.000 (0.086) loss 1.1895 (1.5210) acc 75.0000 (68.1250) lr 2.0000e-03 eta 0:03:19
epoch [2/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.058) loss 1.4521 (1.4961) acc 62.5000 (66.4583) lr 2.0000e-03 eta 0:02:41
epoch [2/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 1.5547 (1.4399) acc 62.5000 (67.0312) lr 2.0000e-03 eta 0:02:22
epoch [2/50] batch [25/26] time 0.069 (0.105) data 0.000 (0.035) loss 1.6318 (1.4450) acc 59.3750 (66.5000) lr 2.0000e-03 eta 0:02:11
epoch [3/50] batch [5/26] time 0.071 (0.265) data 0.001 (0.191) loss 1.2812 (1.2425) acc 71.8750 (66.8750) lr 1.9980e-03 eta 0:05:30
epoch [3/50] batch [10/26] time 0.070 (0.168) data 0.000 (0.096) loss 1.1211 (1.1736) acc 68.7500 (66.8750) lr 1.9980e-03 eta 0:03:27
epoch [3/50] batch [15/26] time 0.070 (0.135) data 0.000 (0.064) loss 1.1465 (1.2362) acc 68.7500 (64.7917) lr 1.9980e-03 eta 0:02:46
epoch [3/50] batch [20/26] time 0.070 (0.119) data 0.000 (0.048) loss 1.3320 (1.2250) acc 68.7500 (65.6250) lr 1.9980e-03 eta 0:02:26
epoch [3/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.039) loss 1.1113 (1.2490) acc 68.7500 (65.3750) lr 1.9980e-03 eta 0:02:13
epoch [4/50] batch [5/26] time 0.069 (0.254) data 0.000 (0.181) loss 1.0703 (1.1390) acc 75.0000 (69.3750) lr 1.9921e-03 eta 0:05:08
epoch [4/50] batch [10/26] time 0.070 (0.162) data 0.000 (0.091) loss 0.6562 (1.1113) acc 87.5000 (69.0625) lr 1.9921e-03 eta 0:03:16
epoch [4/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.061) loss 1.0137 (1.0708) acc 75.0000 (69.5833) lr 1.9921e-03 eta 0:02:38
epoch [4/50] batch [20/26] time 0.070 (0.116) data 0.000 (0.045) loss 0.8145 (1.0509) acc 75.0000 (71.2500) lr 1.9921e-03 eta 0:02:19
epoch [4/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.9883 (1.0749) acc 71.8750 (70.1250) lr 1.9921e-03 eta 0:02:07
epoch [5/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.158) loss 1.1348 (1.1076) acc 68.7500 (70.0000) lr 1.9823e-03 eta 0:04:34
epoch [5/50] batch [10/26] time 0.070 (0.151) data 0.000 (0.079) loss 0.6289 (1.0930) acc 81.2500 (70.9375) lr 1.9823e-03 eta 0:02:58
epoch [5/50] batch [15/26] time 0.069 (0.124) data 0.000 (0.053) loss 1.1006 (1.0664) acc 68.7500 (70.0000) lr 1.9823e-03 eta 0:02:26
epoch [5/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 1.2422 (1.0757) acc 65.6250 (70.3125) lr 1.9823e-03 eta 0:02:09
epoch [5/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.032) loss 0.8633 (1.0610) acc 81.2500 (71.5000) lr 1.9823e-03 eta 0:01:59
epoch [6/50] batch [5/26] time 0.071 (0.242) data 0.000 (0.170) loss 0.6973 (1.0706) acc 87.5000 (71.2500) lr 1.9686e-03 eta 0:04:41
epoch [6/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.9292 (1.0121) acc 71.8750 (71.2500) lr 1.9686e-03 eta 0:03:01
epoch [6/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 1.2080 (1.0026) acc 59.3750 (71.0417) lr 1.9686e-03 eta 0:02:27
epoch [6/50] batch [20/26] time 0.071 (0.113) data 0.000 (0.043) loss 0.4854 (0.9736) acc 87.5000 (72.1875) lr 1.9686e-03 eta 0:02:10
epoch [6/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.8086 (0.9354) acc 87.5000 (74.1250) lr 1.9686e-03 eta 0:01:59
epoch [7/50] batch [5/26] time 0.070 (0.236) data 0.000 (0.161) loss 1.0410 (0.8391) acc 75.0000 (80.0000) lr 1.9511e-03 eta 0:04:29
epoch [7/50] batch [10/26] time 0.072 (0.153) data 0.000 (0.081) loss 0.7822 (0.9398) acc 65.6250 (75.6250) lr 1.9511e-03 eta 0:02:54
epoch [7/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.054) loss 0.7227 (0.8954) acc 87.5000 (77.5000) lr 1.9511e-03 eta 0:02:21
epoch [7/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.040) loss 1.2715 (0.9379) acc 65.6250 (76.2500) lr 1.9511e-03 eta 0:02:05
epoch [7/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 1.2051 (0.9574) acc 68.7500 (75.3750) lr 1.9511e-03 eta 0:01:55
epoch [8/50] batch [5/26] time 0.071 (0.229) data 0.000 (0.156) loss 0.8350 (0.8806) acc 81.2500 (77.5000) lr 1.9298e-03 eta 0:04:15
epoch [8/50] batch [10/26] time 0.072 (0.150) data 0.000 (0.078) loss 1.1514 (1.0280) acc 65.6250 (74.0625) lr 1.9298e-03 eta 0:02:46
epoch [8/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.052) loss 0.6113 (0.9764) acc 81.2500 (75.8333) lr 1.9298e-03 eta 0:02:16
epoch [8/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.5713 (0.9195) acc 81.2500 (76.5625) lr 1.9298e-03 eta 0:02:01
epoch [8/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.6738 (0.9127) acc 84.3750 (76.2500) lr 1.9298e-03 eta 0:01:51
epoch [9/50] batch [5/26] time 0.070 (0.262) data 0.000 (0.192) loss 0.8252 (0.8139) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:04:45
epoch [9/50] batch [10/26] time 0.074 (0.167) data 0.000 (0.096) loss 0.8506 (0.8326) acc 81.2500 (78.7500) lr 1.9048e-03 eta 0:03:00
epoch [9/50] batch [15/26] time 0.070 (0.135) data 0.000 (0.064) loss 0.8516 (0.8795) acc 78.1250 (78.1250) lr 1.9048e-03 eta 0:02:25
epoch [9/50] batch [20/26] time 0.070 (0.118) data 0.000 (0.048) loss 0.6084 (0.8692) acc 93.7500 (78.7500) lr 1.9048e-03 eta 0:02:06
epoch [9/50] batch [25/26] time 0.070 (0.109) data 0.000 (0.039) loss 0.8926 (0.8722) acc 78.1250 (78.2500) lr 1.9048e-03 eta 0:01:56
epoch [10/50] batch [5/26] time 0.072 (0.243) data 0.000 (0.170) loss 0.8228 (0.7783) acc 81.2500 (80.0000) lr 1.8763e-03 eta 0:04:18
epoch [10/50] batch [10/26] time 0.074 (0.157) data 0.000 (0.085) loss 0.8218 (0.8023) acc 78.1250 (80.3125) lr 1.8763e-03 eta 0:02:46
epoch [10/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 1.0791 (0.8310) acc 68.7500 (79.7917) lr 1.8763e-03 eta 0:02:14
epoch [10/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.7939 (0.8281) acc 78.1250 (80.0000) lr 1.8763e-03 eta 0:01:59
epoch [10/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.9424 (0.8473) acc 78.1250 (79.1250) lr 1.8763e-03 eta 0:01:49
epoch [11/50] batch [5/26] time 0.071 (0.248) data 0.000 (0.174) loss 0.7002 (0.9320) acc 87.5000 (76.8750) lr 1.8443e-03 eta 0:04:16
epoch [11/50] batch [10/26] time 0.070 (0.159) data 0.000 (0.087) loss 0.6626 (0.7866) acc 84.3750 (81.8750) lr 1.8443e-03 eta 0:02:43
epoch [11/50] batch [15/26] time 0.072 (0.130) data 0.000 (0.058) loss 0.6313 (0.7830) acc 87.5000 (82.5000) lr 1.8443e-03 eta 0:02:12
epoch [11/50] batch [20/26] time 0.070 (0.115) data 0.000 (0.044) loss 0.6592 (0.7815) acc 90.6250 (82.5000) lr 1.8443e-03 eta 0:01:57
epoch [11/50] batch [25/26] time 0.070 (0.106) data 0.000 (0.035) loss 1.0527 (0.8101) acc 71.8750 (80.8750) lr 1.8443e-03 eta 0:01:47
epoch [12/50] batch [5/26] time 0.072 (0.235) data 0.000 (0.162) loss 1.0596 (0.8031) acc 71.8750 (81.2500) lr 1.8090e-03 eta 0:03:56
epoch [12/50] batch [10/26] time 0.070 (0.152) data 0.000 (0.081) loss 1.0186 (0.7667) acc 75.0000 (82.1875) lr 1.8090e-03 eta 0:02:32
epoch [12/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.054) loss 0.7788 (0.7719) acc 71.8750 (81.0417) lr 1.8090e-03 eta 0:02:04
epoch [12/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.041) loss 0.6660 (0.7853) acc 84.3750 (80.9375) lr 1.8090e-03 eta 0:01:50
epoch [12/50] batch [25/26] time 0.071 (0.103) data 0.000 (0.033) loss 1.0947 (0.8076) acc 71.8750 (80.5000) lr 1.8090e-03 eta 0:01:42
epoch [13/50] batch [5/26] time 0.071 (0.239) data 0.000 (0.166) loss 0.6172 (0.8200) acc 87.5000 (80.6250) lr 1.7705e-03 eta 0:03:55
epoch [13/50] batch [10/26] time 0.070 (0.155) data 0.000 (0.083) loss 1.0430 (0.8002) acc 71.8750 (80.6250) lr 1.7705e-03 eta 0:02:31
epoch [13/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.6895 (0.8005) acc 81.2500 (80.8333) lr 1.7705e-03 eta 0:02:03
epoch [13/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.5576 (0.7904) acc 87.5000 (81.4062) lr 1.7705e-03 eta 0:01:48
epoch [13/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.5664 (0.7871) acc 90.6250 (81.5000) lr 1.7705e-03 eta 0:01:40
epoch [14/50] batch [5/26] time 0.071 (0.244) data 0.000 (0.173) loss 0.5957 (0.7980) acc 87.5000 (78.7500) lr 1.7290e-03 eta 0:03:53
epoch [14/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.087) loss 0.6362 (0.7020) acc 81.2500 (83.1250) lr 1.7290e-03 eta 0:02:29
epoch [14/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.058) loss 0.7300 (0.7517) acc 90.6250 (82.9167) lr 1.7290e-03 eta 0:02:01
epoch [14/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.044) loss 0.6284 (0.7889) acc 90.6250 (81.4062) lr 1.7290e-03 eta 0:01:47
epoch [14/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.5449 (0.7850) acc 81.2500 (81.3750) lr 1.7290e-03 eta 0:01:38
epoch [15/50] batch [5/26] time 0.074 (0.248) data 0.000 (0.177) loss 0.9282 (0.8892) acc 78.1250 (78.7500) lr 1.6845e-03 eta 0:03:51
epoch [15/50] batch [10/26] time 0.071 (0.160) data 0.000 (0.089) loss 1.0205 (0.9246) acc 78.1250 (80.0000) lr 1.6845e-03 eta 0:02:27
epoch [15/50] batch [15/26] time 0.070 (0.130) data 0.000 (0.059) loss 0.9448 (0.8685) acc 84.3750 (81.4583) lr 1.6845e-03 eta 0:01:59
epoch [15/50] batch [20/26] time 0.071 (0.115) data 0.000 (0.044) loss 0.8433 (0.8137) acc 78.1250 (82.6562) lr 1.6845e-03 eta 0:01:45
epoch [15/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.036) loss 0.5957 (0.7759) acc 84.3750 (83.2500) lr 1.6845e-03 eta 0:01:36
epoch [16/50] batch [5/26] time 0.070 (0.241) data 0.000 (0.168) loss 0.4893 (0.7593) acc 90.6250 (83.7500) lr 1.6374e-03 eta 0:03:38
epoch [16/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.6196 (0.7332) acc 90.6250 (84.6875) lr 1.6374e-03 eta 0:02:20
epoch [16/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.5859 (0.7459) acc 84.3750 (84.1667) lr 1.6374e-03 eta 0:01:54
epoch [16/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.6548 (0.7595) acc 84.3750 (84.2188) lr 1.6374e-03 eta 0:01:40
epoch [16/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.034) loss 0.5303 (0.7445) acc 90.6250 (84.1250) lr 1.6374e-03 eta 0:01:32
epoch [17/50] batch [5/26] time 0.071 (0.252) data 0.000 (0.179) loss 0.7261 (0.5099) acc 81.2500 (89.3750) lr 1.5878e-03 eta 0:03:41
epoch [17/50] batch [10/26] time 0.071 (0.161) data 0.000 (0.090) loss 0.3745 (0.6053) acc 96.8750 (88.1250) lr 1.5878e-03 eta 0:02:20
epoch [17/50] batch [15/26] time 0.070 (0.131) data 0.000 (0.060) loss 0.8291 (0.6563) acc 87.5000 (86.4583) lr 1.5878e-03 eta 0:01:53
epoch [17/50] batch [20/26] time 0.071 (0.116) data 0.000 (0.045) loss 1.1035 (0.7156) acc 62.5000 (84.2188) lr 1.5878e-03 eta 0:01:40
epoch [17/50] batch [25/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.5977 (0.7027) acc 87.5000 (84.5000) lr 1.5878e-03 eta 0:01:31
epoch [18/50] batch [5/26] time 0.070 (0.230) data 0.000 (0.156) loss 0.9141 (0.7154) acc 78.1250 (83.7500) lr 1.5358e-03 eta 0:03:16
epoch [18/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.078) loss 0.5283 (0.6642) acc 90.6250 (85.9375) lr 1.5358e-03 eta 0:02:07
epoch [18/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.7915 (0.6891) acc 87.5000 (85.8333) lr 1.5358e-03 eta 0:01:44
epoch [18/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.039) loss 0.8955 (0.6725) acc 81.2500 (86.4062) lr 1.5358e-03 eta 0:01:32
epoch [18/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.8296 (0.6842) acc 71.8750 (85.1250) lr 1.5358e-03 eta 0:01:24
epoch [19/50] batch [5/26] time 0.071 (0.225) data 0.000 (0.152) loss 0.7344 (0.7106) acc 84.3750 (86.2500) lr 1.4818e-03 eta 0:03:05
epoch [19/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.076) loss 0.4475 (0.6569) acc 90.6250 (88.1250) lr 1.4818e-03 eta 0:02:01
epoch [19/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.9648 (0.6586) acc 75.0000 (87.5000) lr 1.4818e-03 eta 0:01:39
epoch [19/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4951 (0.6774) acc 93.7500 (86.2500) lr 1.4818e-03 eta 0:01:28
epoch [19/50] batch [25/26] time 0.069 (0.101) data 0.000 (0.031) loss 0.5234 (0.6966) acc 87.5000 (86.0000) lr 1.4818e-03 eta 0:01:21
epoch [20/50] batch [5/26] time 0.071 (0.231) data 0.000 (0.153) loss 0.6421 (0.7287) acc 84.3750 (83.1250) lr 1.4258e-03 eta 0:03:04
epoch [20/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.077) loss 0.3879 (0.6397) acc 93.7500 (85.9375) lr 1.4258e-03 eta 0:02:00
epoch [20/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.051) loss 0.5020 (0.6284) acc 87.5000 (86.4583) lr 1.4258e-03 eta 0:01:38
epoch [20/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.038) loss 1.1836 (0.6922) acc 68.7500 (84.2188) lr 1.4258e-03 eta 0:01:27
epoch [20/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.031) loss 0.7520 (0.6793) acc 75.0000 (84.5000) lr 1.4258e-03 eta 0:01:20
epoch [21/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.170) loss 0.7676 (0.6892) acc 87.5000 (88.7500) lr 1.3681e-03 eta 0:03:08
epoch [21/50] batch [10/26] time 0.071 (0.157) data 0.000 (0.085) loss 0.6289 (0.6672) acc 78.1250 (87.1875) lr 1.3681e-03 eta 0:02:00
epoch [21/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.057) loss 0.7554 (0.6441) acc 81.2500 (87.0833) lr 1.3681e-03 eta 0:01:37
epoch [21/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.043) loss 0.7183 (0.6559) acc 84.3750 (86.5625) lr 1.3681e-03 eta 0:01:26
epoch [21/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.6416 (0.6603) acc 90.6250 (87.2500) lr 1.3681e-03 eta 0:01:19
epoch [22/50] batch [5/26] time 0.070 (0.243) data 0.000 (0.173) loss 0.5107 (0.6835) acc 90.6250 (86.2500) lr 1.3090e-03 eta 0:03:01
epoch [22/50] batch [10/26] time 0.069 (0.157) data 0.000 (0.087) loss 1.0859 (0.7227) acc 71.8750 (84.0625) lr 1.3090e-03 eta 0:01:56
epoch [22/50] batch [15/26] time 0.070 (0.128) data 0.000 (0.058) loss 0.8247 (0.7250) acc 84.3750 (84.7917) lr 1.3090e-03 eta 0:01:34
epoch [22/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.043) loss 0.4719 (0.6872) acc 90.6250 (86.4062) lr 1.3090e-03 eta 0:01:23
epoch [22/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.035) loss 0.8237 (0.7125) acc 84.3750 (86.0000) lr 1.3090e-03 eta 0:01:16
epoch [23/50] batch [5/26] time 0.071 (0.232) data 0.000 (0.160) loss 0.7656 (0.6562) acc 81.2500 (87.5000) lr 1.2487e-03 eta 0:02:47
epoch [23/50] batch [10/26] time 0.071 (0.151) data 0.000 (0.080) loss 0.7139 (0.6875) acc 87.5000 (86.5625) lr 1.2487e-03 eta 0:01:48
epoch [23/50] batch [15/26] time 0.070 (0.124) data 0.000 (0.053) loss 0.6685 (0.6925) acc 90.6250 (86.0417) lr 1.2487e-03 eta 0:01:28
epoch [23/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.040) loss 0.6899 (0.7254) acc 87.5000 (85.3125) lr 1.2487e-03 eta 0:01:18
epoch [23/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.8643 (0.7078) acc 81.2500 (86.0000) lr 1.2487e-03 eta 0:01:12
epoch [24/50] batch [5/26] time 0.070 (0.235) data 0.000 (0.164) loss 0.6133 (0.5550) acc 84.3750 (89.3750) lr 1.1874e-03 eta 0:02:43
epoch [24/50] batch [10/26] time 0.071 (0.153) data 0.000 (0.082) loss 0.7202 (0.6165) acc 90.6250 (89.0625) lr 1.1874e-03 eta 0:01:45
epoch [24/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.055) loss 0.6763 (0.6353) acc 87.5000 (88.9583) lr 1.1874e-03 eta 0:01:25
epoch [24/50] batch [20/26] time 0.071 (0.111) data 0.000 (0.041) loss 0.5195 (0.5986) acc 93.7500 (89.8438) lr 1.1874e-03 eta 0:01:16
epoch [24/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.6250 (0.5979) acc 87.5000 (89.6250) lr 1.1874e-03 eta 0:01:09
epoch [25/50] batch [5/26] time 0.070 (0.229) data 0.000 (0.156) loss 0.4844 (0.6705) acc 90.6250 (87.5000) lr 1.1253e-03 eta 0:02:33
epoch [25/50] batch [10/26] time 0.070 (0.150) data 0.000 (0.078) loss 0.4871 (0.6300) acc 93.7500 (87.8125) lr 1.1253e-03 eta 0:01:39
epoch [25/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.052) loss 0.6304 (0.6479) acc 81.2500 (87.0833) lr 1.1253e-03 eta 0:01:21
epoch [25/50] batch [20/26] time 0.071 (0.110) data 0.000 (0.039) loss 0.4658 (0.6655) acc 93.7500 (86.8750) lr 1.1253e-03 eta 0:01:12
epoch [25/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.8833 (0.6769) acc 78.1250 (86.7500) lr 1.1253e-03 eta 0:01:06
epoch [26/50] batch [5/26] time 0.072 (0.246) data 0.000 (0.173) loss 0.5908 (0.5923) acc 84.3750 (88.1250) lr 1.0628e-03 eta 0:02:38
epoch [26/50] batch [10/26] time 0.070 (0.158) data 0.000 (0.087) loss 0.5918 (0.5596) acc 87.5000 (88.7500) lr 1.0628e-03 eta 0:01:41
epoch [26/50] batch [15/26] time 0.070 (0.129) data 0.000 (0.058) loss 0.5059 (0.6123) acc 90.6250 (87.2917) lr 1.0628e-03 eta 0:01:21
epoch [26/50] batch [20/26] time 0.070 (0.114) data 0.000 (0.043) loss 0.7031 (0.6099) acc 81.2500 (87.5000) lr 1.0628e-03 eta 0:01:12
epoch [26/50] batch [25/26] time 0.071 (0.106) data 0.000 (0.035) loss 0.5527 (0.6204) acc 84.3750 (87.5000) lr 1.0628e-03 eta 0:01:05
epoch [27/50] batch [5/26] time 0.070 (0.225) data 0.000 (0.152) loss 0.4893 (0.5317) acc 90.6250 (90.0000) lr 1.0000e-03 eta 0:02:19
epoch [27/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.076) loss 0.6309 (0.5656) acc 90.6250 (90.3125) lr 1.0000e-03 eta 0:01:30
epoch [27/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.6245 (0.5497) acc 87.5000 (90.4167) lr 1.0000e-03 eta 0:01:14
epoch [27/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.038) loss 0.3574 (0.5735) acc 93.7500 (89.5312) lr 1.0000e-03 eta 0:01:06
epoch [27/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.9272 (0.6065) acc 75.0000 (88.1250) lr 1.0000e-03 eta 0:01:00
epoch [28/50] batch [5/26] time 0.072 (0.234) data 0.000 (0.162) loss 0.5137 (0.5911) acc 90.6250 (90.0000) lr 9.3721e-04 eta 0:02:18
epoch [28/50] batch [10/26] time 0.070 (0.152) data 0.000 (0.081) loss 0.7529 (0.6733) acc 81.2500 (86.5625) lr 9.3721e-04 eta 0:01:29
epoch [28/50] batch [15/26] time 0.070 (0.125) data 0.000 (0.054) loss 0.5591 (0.6456) acc 93.7500 (87.5000) lr 9.3721e-04 eta 0:01:12
epoch [28/50] batch [20/26] time 0.073 (0.111) data 0.000 (0.041) loss 0.3818 (0.6490) acc 96.8750 (87.0312) lr 9.3721e-04 eta 0:01:04
epoch [28/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.033) loss 0.6899 (0.6200) acc 87.5000 (88.0000) lr 9.3721e-04 eta 0:00:59
epoch [29/50] batch [5/26] time 0.071 (0.227) data 0.000 (0.153) loss 0.6899 (0.6958) acc 84.3750 (82.5000) lr 8.7467e-04 eta 0:02:08
epoch [29/50] batch [10/26] time 0.071 (0.149) data 0.000 (0.077) loss 0.5718 (0.7036) acc 96.8750 (85.6250) lr 8.7467e-04 eta 0:01:23
epoch [29/50] batch [15/26] time 0.070 (0.123) data 0.000 (0.051) loss 0.8398 (0.7071) acc 75.0000 (84.5833) lr 8.7467e-04 eta 0:01:08
epoch [29/50] batch [20/26] time 0.070 (0.110) data 0.000 (0.038) loss 0.7100 (0.6989) acc 84.3750 (84.5312) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [25/26] time 0.070 (0.102) data 0.000 (0.031) loss 0.5649 (0.7025) acc 87.5000 (84.3750) lr 8.7467e-04 eta 0:00:55
epoch [30/50] batch [5/26] time 0.070 (0.221) data 0.000 (0.149) loss 0.7339 (0.5890) acc 84.3750 (88.1250) lr 8.1262e-04 eta 0:01:59
epoch [30/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.075) loss 0.4717 (0.6042) acc 96.8750 (88.1250) lr 8.1262e-04 eta 0:01:18
epoch [30/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.7144 (0.6198) acc 84.3750 (87.2917) lr 8.1262e-04 eta 0:01:04
epoch [30/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 1.0508 (0.6490) acc 78.1250 (86.2500) lr 8.1262e-04 eta 0:00:56
epoch [30/50] batch [25/26] time 0.072 (0.101) data 0.000 (0.030) loss 0.4761 (0.6293) acc 90.6250 (86.6250) lr 8.1262e-04 eta 0:00:52
epoch [31/50] batch [5/26] time 0.071 (0.211) data 0.000 (0.138) loss 0.5430 (0.6596) acc 90.6250 (86.8750) lr 7.5131e-04 eta 0:01:48
epoch [31/50] batch [10/26] time 0.070 (0.141) data 0.000 (0.069) loss 0.6860 (0.6443) acc 81.2500 (87.5000) lr 7.5131e-04 eta 0:01:11
epoch [31/50] batch [15/26] time 0.071 (0.117) data 0.000 (0.046) loss 0.5386 (0.6370) acc 93.7500 (87.7083) lr 7.5131e-04 eta 0:00:59
epoch [31/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.5444 (0.6382) acc 90.6250 (87.9688) lr 7.5131e-04 eta 0:00:52
epoch [31/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.5029 (0.6320) acc 90.6250 (87.8750) lr 7.5131e-04 eta 0:00:48
epoch [32/50] batch [5/26] time 0.071 (0.220) data 0.000 (0.145) loss 0.7041 (0.6961) acc 87.5000 (86.8750) lr 6.9098e-04 eta 0:01:47
epoch [32/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.073) loss 0.8359 (0.7042) acc 81.2500 (85.9375) lr 6.9098e-04 eta 0:01:10
epoch [32/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.7397 (0.6485) acc 84.3750 (86.8750) lr 6.9098e-04 eta 0:00:57
epoch [32/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.6523 (0.6633) acc 84.3750 (86.0938) lr 6.9098e-04 eta 0:00:51
epoch [32/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.029) loss 0.4622 (0.6485) acc 87.5000 (86.7500) lr 6.9098e-04 eta 0:00:47
epoch [33/50] batch [5/26] time 0.070 (0.213) data 0.000 (0.142) loss 0.5259 (0.6948) acc 93.7500 (88.1250) lr 6.3188e-04 eta 0:01:38
epoch [33/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.071) loss 0.5410 (0.6295) acc 93.7500 (89.3750) lr 6.3188e-04 eta 0:01:04
epoch [33/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.047) loss 0.7422 (0.6287) acc 87.5000 (88.9583) lr 6.3188e-04 eta 0:00:53
epoch [33/50] batch [20/26] time 0.071 (0.106) data 0.000 (0.036) loss 0.6030 (0.6360) acc 90.6250 (89.0625) lr 6.3188e-04 eta 0:00:47
epoch [33/50] batch [25/26] time 0.071 (0.099) data 0.000 (0.029) loss 0.5840 (0.6145) acc 90.6250 (89.6250) lr 6.3188e-04 eta 0:00:43
epoch [34/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.149) loss 0.5059 (0.6105) acc 93.7500 (88.1250) lr 5.7422e-04 eta 0:01:36
epoch [34/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.075) loss 0.7324 (0.5885) acc 87.5000 (88.4375) lr 5.7422e-04 eta 0:01:02
epoch [34/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.050) loss 0.7959 (0.6113) acc 84.3750 (87.7083) lr 5.7422e-04 eta 0:00:51
epoch [34/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.6860 (0.5902) acc 87.5000 (88.2812) lr 5.7422e-04 eta 0:00:45
epoch [34/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.030) loss 0.5024 (0.5925) acc 93.7500 (87.7500) lr 5.7422e-04 eta 0:00:41
epoch [35/50] batch [5/26] time 0.070 (0.220) data 0.000 (0.149) loss 0.7729 (0.6292) acc 84.3750 (90.0000) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [10/26] time 0.070 (0.145) data 0.000 (0.075) loss 0.3828 (0.6056) acc 96.8750 (90.0000) lr 5.1825e-04 eta 0:00:58
epoch [35/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.050) loss 0.7573 (0.6246) acc 78.1250 (88.7500) lr 5.1825e-04 eta 0:00:48
epoch [35/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.037) loss 0.7935 (0.6274) acc 78.1250 (87.3438) lr 5.1825e-04 eta 0:00:42
epoch [35/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.030) loss 0.7246 (0.6171) acc 84.3750 (87.6250) lr 5.1825e-04 eta 0:00:39
epoch [36/50] batch [5/26] time 0.071 (0.221) data 0.000 (0.148) loss 0.6670 (0.6929) acc 87.5000 (86.8750) lr 4.6417e-04 eta 0:01:25
epoch [36/50] batch [10/26] time 0.070 (0.146) data 0.000 (0.074) loss 0.7837 (0.7062) acc 81.2500 (85.9375) lr 4.6417e-04 eta 0:00:55
epoch [36/50] batch [15/26] time 0.072 (0.121) data 0.000 (0.050) loss 0.5234 (0.6667) acc 93.7500 (87.7083) lr 4.6417e-04 eta 0:00:45
epoch [36/50] batch [20/26] time 0.072 (0.109) data 0.000 (0.037) loss 0.6147 (0.6310) acc 84.3750 (88.7500) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.3591 (0.6180) acc 96.8750 (89.2500) lr 4.6417e-04 eta 0:00:36
epoch [37/50] batch [5/26] time 0.070 (0.213) data 0.000 (0.138) loss 0.7671 (0.6835) acc 84.3750 (89.3750) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [10/26] time 0.070 (0.142) data 0.000 (0.069) loss 0.7217 (0.6423) acc 81.2500 (88.1250) lr 4.1221e-04 eta 0:00:50
epoch [37/50] batch [15/26] time 0.070 (0.118) data 0.000 (0.046) loss 0.3933 (0.5976) acc 93.7500 (89.1667) lr 4.1221e-04 eta 0:00:41
epoch [37/50] batch [20/26] time 0.070 (0.106) data 0.000 (0.035) loss 0.5527 (0.5772) acc 87.5000 (89.3750) lr 4.1221e-04 eta 0:00:36
epoch [37/50] batch [25/26] time 0.070 (0.099) data 0.000 (0.028) loss 0.4858 (0.5733) acc 90.6250 (89.5000) lr 4.1221e-04 eta 0:00:33
epoch [38/50] batch [5/26] time 0.070 (0.241) data 0.000 (0.171) loss 0.5371 (0.5746) acc 90.6250 (88.1250) lr 3.6258e-04 eta 0:01:20
epoch [38/50] batch [10/26] time 0.070 (0.156) data 0.000 (0.085) loss 0.5918 (0.6431) acc 81.2500 (86.5625) lr 3.6258e-04 eta 0:00:51
epoch [38/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.057) loss 0.7852 (0.6419) acc 84.3750 (86.6667) lr 3.6258e-04 eta 0:00:41
epoch [38/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.043) loss 0.9102 (0.6349) acc 81.2500 (86.5625) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 1.0273 (0.6297) acc 75.0000 (87.0000) lr 3.6258e-04 eta 0:00:32
epoch [39/50] batch [5/26] time 0.070 (0.221) data 0.000 (0.150) loss 0.6577 (0.5713) acc 81.2500 (89.3750) lr 3.1545e-04 eta 0:01:07
epoch [39/50] batch [10/26] time 0.071 (0.146) data 0.000 (0.075) loss 0.6753 (0.6287) acc 90.6250 (88.7500) lr 3.1545e-04 eta 0:00:44
epoch [39/50] batch [15/26] time 0.070 (0.121) data 0.000 (0.050) loss 0.7168 (0.5921) acc 87.5000 (90.0000) lr 3.1545e-04 eta 0:00:35
epoch [39/50] batch [20/26] time 0.070 (0.108) data 0.000 (0.038) loss 0.3025 (0.6051) acc 100.0000 (89.2188) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.030) loss 0.6211 (0.5954) acc 87.5000 (89.2500) lr 3.1545e-04 eta 0:00:28
epoch [40/50] batch [5/26] time 0.071 (0.240) data 0.000 (0.168) loss 0.6597 (0.6284) acc 90.6250 (89.3750) lr 2.7103e-04 eta 0:01:07
epoch [40/50] batch [10/26] time 0.071 (0.156) data 0.000 (0.084) loss 0.7798 (0.6693) acc 87.5000 (88.4375) lr 2.7103e-04 eta 0:00:42
epoch [40/50] batch [15/26] time 0.070 (0.127) data 0.000 (0.056) loss 0.6807 (0.6553) acc 84.3750 (88.1250) lr 2.7103e-04 eta 0:00:34
epoch [40/50] batch [20/26] time 0.070 (0.113) data 0.000 (0.042) loss 0.7144 (0.6427) acc 84.3750 (88.5938) lr 2.7103e-04 eta 0:00:30
epoch [40/50] batch [25/26] time 0.070 (0.105) data 0.000 (0.034) loss 0.5327 (0.6170) acc 87.5000 (89.1250) lr 2.7103e-04 eta 0:00:27
epoch [41/50] batch [5/26] time 0.073 (0.237) data 0.000 (0.163) loss 0.5088 (0.6588) acc 90.6250 (86.8750) lr 2.2949e-04 eta 0:01:00
epoch [41/50] batch [10/26] time 0.070 (0.154) data 0.000 (0.082) loss 0.5381 (0.6059) acc 90.6250 (88.4375) lr 2.2949e-04 eta 0:00:38
epoch [41/50] batch [15/26] time 0.070 (0.126) data 0.000 (0.055) loss 0.4087 (0.5800) acc 96.8750 (89.7917) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [20/26] time 0.070 (0.112) data 0.000 (0.041) loss 0.4155 (0.5665) acc 93.7500 (90.1562) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [25/26] time 0.070 (0.104) data 0.000 (0.033) loss 0.4287 (0.5808) acc 93.7500 (89.8750) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/26] time 0.071 (0.231) data 0.000 (0.157) loss 0.3145 (0.5746) acc 96.8750 (88.7500) lr 1.9098e-04 eta 0:00:52
epoch [42/50] batch [10/26] time 0.073 (0.151) data 0.000 (0.079) loss 0.7852 (0.5760) acc 81.2500 (89.3750) lr 1.9098e-04 eta 0:00:33
epoch [42/50] batch [15/26] time 0.071 (0.124) data 0.000 (0.053) loss 0.5244 (0.5797) acc 90.6250 (89.5833) lr 1.9098e-04 eta 0:00:27
epoch [42/50] batch [20/26] time 0.070 (0.111) data 0.000 (0.039) loss 0.3171 (0.5677) acc 100.0000 (90.6250) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [25/26] time 0.070 (0.103) data 0.000 (0.032) loss 0.8882 (0.5704) acc 84.3750 (90.8750) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/26] time 0.070 (0.216) data 0.000 (0.144) loss 0.4592 (0.5285) acc 93.7500 (90.0000) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [10/26] time 0.071 (0.143) data 0.000 (0.072) loss 0.6235 (0.5963) acc 87.5000 (88.7500) lr 1.5567e-04 eta 0:00:28
epoch [43/50] batch [15/26] time 0.070 (0.119) data 0.000 (0.048) loss 0.5068 (0.5825) acc 84.3750 (89.1667) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.5044 (0.5849) acc 90.6250 (88.7500) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [25/26] time 0.072 (0.099) data 0.000 (0.029) loss 0.7065 (0.5868) acc 84.3750 (88.6250) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/26] time 0.072 (0.224) data 0.000 (0.152) loss 0.8804 (0.6885) acc 81.2500 (87.5000) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [10/26] time 0.070 (0.147) data 0.000 (0.076) loss 0.6270 (0.6856) acc 87.5000 (86.8750) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.051) loss 0.6851 (0.6704) acc 87.5000 (86.6667) lr 1.2369e-04 eta 0:00:20
epoch [44/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.8833 (0.6846) acc 84.3750 (86.7188) lr 1.2369e-04 eta 0:00:17
epoch [44/50] batch [25/26] time 0.070 (0.101) data 0.000 (0.031) loss 0.4119 (0.6442) acc 96.8750 (87.8750) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/26] time 0.071 (0.218) data 0.000 (0.143) loss 0.6187 (0.5005) acc 84.3750 (92.5000) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [10/26] time 0.071 (0.145) data 0.000 (0.072) loss 0.3496 (0.4989) acc 100.0000 (92.5000) lr 9.5173e-05 eta 0:00:21
epoch [45/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.7441 (0.5242) acc 90.6250 (91.0417) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.5908 (0.5447) acc 90.6250 (90.7812) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.5293 (0.5579) acc 90.6250 (90.1250) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/26] time 0.071 (0.222) data 0.000 (0.146) loss 0.5283 (0.6263) acc 93.7500 (90.0000) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [10/26] time 0.071 (0.147) data 0.000 (0.073) loss 0.4019 (0.5846) acc 93.7500 (90.0000) lr 7.0224e-05 eta 0:00:17
epoch [46/50] batch [15/26] time 0.071 (0.121) data 0.000 (0.049) loss 0.7417 (0.5989) acc 87.5000 (89.7917) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [20/26] time 0.071 (0.109) data 0.000 (0.037) loss 0.3159 (0.6020) acc 100.0000 (90.0000) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.029) loss 0.4155 (0.5895) acc 90.6250 (90.3750) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/26] time 0.070 (0.217) data 0.000 (0.143) loss 0.4041 (0.5325) acc 96.8750 (90.6250) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [10/26] time 0.071 (0.144) data 0.000 (0.071) loss 0.6416 (0.6148) acc 81.2500 (87.8125) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [15/26] time 0.070 (0.120) data 0.000 (0.048) loss 0.5259 (0.5821) acc 87.5000 (88.7500) lr 4.8943e-05 eta 0:00:10
epoch [47/50] batch [20/26] time 0.070 (0.107) data 0.000 (0.036) loss 0.5718 (0.5886) acc 93.7500 (88.7500) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [25/26] time 0.070 (0.100) data 0.000 (0.029) loss 0.7402 (0.5726) acc 87.5000 (89.6250) lr 4.8943e-05 eta 0:00:07
epoch [48/50] batch [5/26] time 0.071 (0.224) data 0.001 (0.151) loss 0.7222 (0.7179) acc 84.3750 (84.3750) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.076) loss 0.4692 (0.6275) acc 96.8750 (89.0625) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.050) loss 0.7441 (0.5876) acc 84.3750 (89.7917) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.6904 (0.6097) acc 81.2500 (88.5938) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.030) loss 0.4880 (0.6006) acc 93.7500 (88.6250) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/26] time 0.070 (0.226) data 0.000 (0.156) loss 0.5718 (0.5540) acc 90.6250 (89.3750) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [10/26] time 0.070 (0.148) data 0.000 (0.078) loss 0.4717 (0.5492) acc 96.8750 (91.8750) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [15/26] time 0.070 (0.122) data 0.000 (0.052) loss 0.4807 (0.5664) acc 93.7500 (90.8333) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.039) loss 0.3931 (0.5444) acc 93.7500 (90.7812) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [25/26] time 0.071 (0.101) data 0.000 (0.031) loss 0.3179 (0.5353) acc 100.0000 (91.1250) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/26] time 0.070 (0.225) data 0.000 (0.150) loss 0.6758 (0.6146) acc 87.5000 (89.3750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [10/26] time 0.071 (0.148) data 0.000 (0.075) loss 0.8486 (0.6286) acc 81.2500 (88.1250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [15/26] time 0.071 (0.122) data 0.000 (0.050) loss 0.3950 (0.5993) acc 93.7500 (88.3333) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/26] time 0.070 (0.109) data 0.000 (0.038) loss 0.4763 (0.6235) acc 96.8750 (88.4375) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/26] time 0.072 (0.102) data 0.000 (0.030) loss 0.3809 (0.6366) acc 96.8750 (87.2500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:10,  3.45s/it] 50%|█████     | 2/4 [00:04<00:03,  1.76s/it] 75%|███████▌  | 3/4 [00:04<00:01,  1.22s/it]100%|██████████| 4/4 [00:05<00:00,  1.04it/s]100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
=> result
* total: 1,934
* correct: 1,671
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.5%
Elapsed: 0:02:23
Run this job and save the output to output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:04<00:12,  4.20s/it] 50%|█████     | 2/4 [00:04<00:04,  2.06s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.38s/it]100%|██████████| 4/4 [00:05<00:00,  1.02s/it]100%|██████████| 4/4 [00:05<00:00,  1.49s/it]
=> result
* total: 1,849
* correct: 1,489
* accuracy: 80.5%
* error: 19.5%
* macro_f1: 79.3%
Run this job and save the output to output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.73s/it] 50%|█████     | 2/4 [00:04<00:03,  1.87s/it] 75%|███████▌  | 3/4 [00:04<00:01,  1.28s/it]100%|██████████| 4/4 [00:05<00:00,  1.04it/s]100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
=> result
* total: 1,849
* correct: 1,495
* accuracy: 80.9%
* error: 19.1%
* macro_f1: 79.2%
Run this job and save the output to output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4_eval/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  4.00s/it] 50%|█████     | 2/4 [00:04<00:03,  1.98s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.34s/it]100%|██████████| 4/4 [00:05<00:00,  1.01it/s]100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
=> result
* total: 1,849
* correct: 1,513
* accuracy: 81.8%
* error: 18.2%
* macro_f1: 80.0%
Run this job and save the output to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      796
# test     9,950
---------  ------
['abbey', 'airplane_cabin', 'airport_terminal', 'alley', 'amphitheater', 'amusement_arcade', 'amusement_park', 'anechoic_chamber', 'outdoor apartment_building', 'indoor apse', 'aquarium', 'aqueduct', 'arch', 'archive', 'outdoor arrival_gate', 'art_gallery', 'art_school', 'art_studio', 'assembly_line', 'outdoor athletic_field', 'public atrium', 'attic', 'auditorium', 'auto_factory', 'badlands', 'indoor badminton_court', 'baggage_claim', 'shop bakery', 'exterior balcony', 'interior balcony', 'ball_pit', 'ballroom', 'bamboo_forest', 'banquet_hall', 'bar', 'barn', 'barndoor', 'baseball_field', 'basement', 'basilica', 'outdoor basketball_court', 'bathroom', 'batters_box', 'bayou', 'indoor bazaar', 'outdoor bazaar', 'beach', 'beauty_salon', 'bedroom', 'berth', 'biology_laboratory', 'indoor bistro', 'boardwalk', 'boat_deck', 'boathouse', 'bookstore', 'indoor booth', 'botanical_garden', 'indoor bow_window', 'outdoor bow_window', 'bowling_alley', 'boxing_ring', 'indoor brewery', 'bridge', 'building_facade', 'bullring', 'burial_chamber', 'bus_interior', 'butchers_shop', 'butte', 'outdoor cabin', 'cafeteria', 'campsite', 'campus', 'natural canal', 'urban canal', 'candy_store', 'canyon', 'backseat car_interior', 'frontseat car_interior', 'carrousel', 'indoor casino', 'castle', 'catacomb', 'indoor cathedral', 'outdoor cathedral', 'indoor cavern', 'cemetery', 'chalet', 'cheese_factory', 'chemistry_lab', 'indoor chicken_coop', 'outdoor chicken_coop', 'childs_room', 'indoor church', 'outdoor church', 'classroom', 'clean_room', 'cliff', 'indoor cloister', 'closet', 'clothing_store', 'coast', 'cockpit', 'coffee_shop', 'computer_room', 'conference_center', 'conference_room', 'construction_site', 'control_room', 'outdoor control_tower', 'corn_field', 'corral', 'corridor', 'cottage_garden', 'courthouse', 'courtroom', 'courtyard', 'exterior covered_bridge', 'creek', 'crevasse', 'crosswalk', 'office cubicle', 'dam', 'delicatessen', 'dentists_office', 'sand desert', 'vegetation desert', 'indoor diner', 'outdoor diner', 'home dinette', 'vehicle dinette', 'dining_car', 'dining_room', 'discotheque', 'dock', 'outdoor doorway', 'dorm_room', 'driveway', 'outdoor driving_range', 'drugstore', 'electrical_substation', 'door elevator', 'interior elevator', 'elevator_shaft', 'engine_room', 'indoor escalator', 'excavation', 'indoor factory', 'fairway', 'fastfood_restaurant', 'cultivated field', 'wild field', 'fire_escape', 'fire_station', 'indoor firing_range', 'fishpond', 'indoor florist_shop', 'food_court', 'broadleaf forest', 'needleleaf forest', 'forest_path', 'forest_road', 'formal_garden', 'fountain', 'galley', 'game_room', 'indoor garage', 'garbage_dump', 'gas_station', 'exterior gazebo', 'indoor general_store', 'outdoor general_store', 'gift_shop', 'golf_course', 'indoor greenhouse', 'outdoor greenhouse', 'indoor gymnasium', 'indoor hangar', 'outdoor hangar', 'harbor', 'hayfield', 'heliport', 'herb_garden', 'highway', 'hill', 'home_office', 'hospital', 'hospital_room', 'hot_spring', 'outdoor hot_tub', 'outdoor hotel', 'hotel_room', 'house', 'outdoor hunting_lodge', 'ice_cream_parlor', 'ice_floe', 'ice_shelf', 'indoor ice_skating_rink']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abbey.', 'X X X X airplane cabin.', 'X X X X airport terminal.', 'X X X X alley.', 'X X X X amphitheater.', 'X X X X amusement arcade.', 'X X X X amusement park.', 'X X X X anechoic chamber.', 'X X X X outdoor apartment building.', 'X X X X indoor apse.', 'X X X X aquarium.', 'X X X X aqueduct.', 'X X X X arch.', 'X X X X archive.', 'X X X X outdoor arrival gate.', 'X X X X art gallery.', 'X X X X art school.', 'X X X X art studio.', 'X X X X assembly line.', 'X X X X outdoor athletic field.', 'X X X X public atrium.', 'X X X X attic.', 'X X X X auditorium.', 'X X X X auto factory.', 'X X X X badlands.', 'X X X X indoor badminton court.', 'X X X X baggage claim.', 'X X X X shop bakery.', 'X X X X exterior balcony.', 'X X X X interior balcony.', 'X X X X ball pit.', 'X X X X ballroom.', 'X X X X bamboo forest.', 'X X X X banquet hall.', 'X X X X bar.', 'X X X X barn.', 'X X X X barndoor.', 'X X X X baseball field.', 'X X X X basement.', 'X X X X basilica.', 'X X X X outdoor basketball court.', 'X X X X bathroom.', 'X X X X batters box.', 'X X X X bayou.', 'X X X X indoor bazaar.', 'X X X X outdoor bazaar.', 'X X X X beach.', 'X X X X beauty salon.', 'X X X X bedroom.', 'X X X X berth.', 'X X X X biology laboratory.', 'X X X X indoor bistro.', 'X X X X boardwalk.', 'X X X X boat deck.', 'X X X X boathouse.', 'X X X X bookstore.', 'X X X X indoor booth.', 'X X X X botanical garden.', 'X X X X indoor bow window.', 'X X X X outdoor bow window.', 'X X X X bowling alley.', 'X X X X boxing ring.', 'X X X X indoor brewery.', 'X X X X bridge.', 'X X X X building facade.', 'X X X X bullring.', 'X X X X burial chamber.', 'X X X X bus interior.', 'X X X X butchers shop.', 'X X X X butte.', 'X X X X outdoor cabin.', 'X X X X cafeteria.', 'X X X X campsite.', 'X X X X campus.', 'X X X X natural canal.', 'X X X X urban canal.', 'X X X X candy store.', 'X X X X canyon.', 'X X X X backseat car interior.', 'X X X X frontseat car interior.', 'X X X X carrousel.', 'X X X X indoor casino.', 'X X X X castle.', 'X X X X catacomb.', 'X X X X indoor cathedral.', 'X X X X outdoor cathedral.', 'X X X X indoor cavern.', 'X X X X cemetery.', 'X X X X chalet.', 'X X X X cheese factory.', 'X X X X chemistry lab.', 'X X X X indoor chicken coop.', 'X X X X outdoor chicken coop.', 'X X X X childs room.', 'X X X X indoor church.', 'X X X X outdoor church.', 'X X X X classroom.', 'X X X X clean room.', 'X X X X cliff.', 'X X X X indoor cloister.', 'X X X X closet.', 'X X X X clothing store.', 'X X X X coast.', 'X X X X cockpit.', 'X X X X coffee shop.', 'X X X X computer room.', 'X X X X conference center.', 'X X X X conference room.', 'X X X X construction site.', 'X X X X control room.', 'X X X X outdoor control tower.', 'X X X X corn field.', 'X X X X corral.', 'X X X X corridor.', 'X X X X cottage garden.', 'X X X X courthouse.', 'X X X X courtroom.', 'X X X X courtyard.', 'X X X X exterior covered bridge.', 'X X X X creek.', 'X X X X crevasse.', 'X X X X crosswalk.', 'X X X X office cubicle.', 'X X X X dam.', 'X X X X delicatessen.', 'X X X X dentists office.', 'X X X X sand desert.', 'X X X X vegetation desert.', 'X X X X indoor diner.', 'X X X X outdoor diner.', 'X X X X home dinette.', 'X X X X vehicle dinette.', 'X X X X dining car.', 'X X X X dining room.', 'X X X X discotheque.', 'X X X X dock.', 'X X X X outdoor doorway.', 'X X X X dorm room.', 'X X X X driveway.', 'X X X X outdoor driving range.', 'X X X X drugstore.', 'X X X X electrical substation.', 'X X X X door elevator.', 'X X X X interior elevator.', 'X X X X elevator shaft.', 'X X X X engine room.', 'X X X X indoor escalator.', 'X X X X excavation.', 'X X X X indoor factory.', 'X X X X fairway.', 'X X X X fastfood restaurant.', 'X X X X cultivated field.', 'X X X X wild field.', 'X X X X fire escape.', 'X X X X fire station.', 'X X X X indoor firing range.', 'X X X X fishpond.', 'X X X X indoor florist shop.', 'X X X X food court.', 'X X X X broadleaf forest.', 'X X X X needleleaf forest.', 'X X X X forest path.', 'X X X X forest road.', 'X X X X formal garden.', 'X X X X fountain.', 'X X X X galley.', 'X X X X game room.', 'X X X X indoor garage.', 'X X X X garbage dump.', 'X X X X gas station.', 'X X X X exterior gazebo.', 'X X X X indoor general store.', 'X X X X outdoor general store.', 'X X X X gift shop.', 'X X X X golf course.', 'X X X X indoor greenhouse.', 'X X X X outdoor greenhouse.', 'X X X X indoor gymnasium.', 'X X X X indoor hangar.', 'X X X X outdoor hangar.', 'X X X X harbor.', 'X X X X hayfield.', 'X X X X heliport.', 'X X X X herb garden.', 'X X X X highway.', 'X X X X hill.', 'X X X X home office.', 'X X X X hospital.', 'X X X X hospital room.', 'X X X X hot spring.', 'X X X X outdoor hot tub.', 'X X X X outdoor hotel.', 'X X X X hotel room.', 'X X X X house.', 'X X X X outdoor hunting lodge.', 'X X X X ice cream parlor.', 'X X X X ice floe.', 'X X X X ice shelf.', 'X X X X indoor ice skating rink.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/25] batch [5/100] time 0.140 (0.653) data 0.000 (0.492) loss 3.0156 (3.4145) acc 59.3750 (53.1250) lr 1.0000e-05 eta 0:27:09
epoch [1/25] batch [10/100] time 0.140 (0.454) data 0.000 (0.304) loss 3.5938 (3.4465) acc 40.6250 (48.7500) lr 1.0000e-05 eta 0:18:50
epoch [1/25] batch [15/100] time 0.140 (0.379) data 0.000 (0.232) loss 3.3867 (3.3658) acc 50.0000 (50.0000) lr 1.0000e-05 eta 0:15:42
epoch [1/25] batch [20/100] time 0.140 (0.380) data 0.000 (0.235) loss 3.6660 (3.3539) acc 40.6250 (49.8438) lr 1.0000e-05 eta 0:15:42
epoch [1/25] batch [25/100] time 0.140 (0.332) data 0.000 (0.188) loss 3.3848 (3.3398) acc 46.8750 (49.7500) lr 1.0000e-05 eta 0:13:41
epoch [1/25] batch [30/100] time 0.140 (0.358) data 0.000 (0.214) loss 3.3652 (3.3110) acc 50.0000 (49.8958) lr 1.0000e-05 eta 0:14:43
epoch [1/25] batch [35/100] time 0.470 (0.336) data 0.329 (0.193) loss 2.8164 (3.2703) acc 56.2500 (50.6250) lr 1.0000e-05 eta 0:13:48
epoch [1/25] batch [40/100] time 0.140 (0.312) data 0.000 (0.169) loss 3.1953 (3.2766) acc 46.8750 (50.3906) lr 1.0000e-05 eta 0:12:47
epoch [1/25] batch [45/100] time 0.141 (0.330) data 0.000 (0.187) loss 2.9336 (3.2500) acc 62.5000 (50.9028) lr 1.0000e-05 eta 0:13:30
epoch [1/25] batch [50/100] time 0.144 (0.329) data 0.000 (0.186) loss 3.0977 (3.2454) acc 59.3750 (51.2500) lr 1.0000e-05 eta 0:13:25
epoch [1/25] batch [55/100] time 0.145 (0.330) data 0.000 (0.188) loss 2.7617 (3.2336) acc 68.7500 (51.5909) lr 1.0000e-05 eta 0:13:27
epoch [1/25] batch [60/100] time 0.139 (0.334) data 0.000 (0.191) loss 2.8789 (3.2086) acc 56.2500 (52.0312) lr 1.0000e-05 eta 0:13:34
epoch [1/25] batch [65/100] time 0.140 (0.319) data 0.000 (0.177) loss 2.6602 (3.1869) acc 65.6250 (52.3558) lr 1.0000e-05 eta 0:12:56
epoch [1/25] batch [70/100] time 0.141 (0.353) data 0.000 (0.210) loss 2.8008 (3.1616) acc 56.2500 (52.9018) lr 1.0000e-05 eta 0:14:16
epoch [1/25] batch [75/100] time 0.399 (0.342) data 0.257 (0.200) loss 2.6816 (3.1272) acc 46.8750 (53.2500) lr 1.0000e-05 eta 0:13:49
epoch [1/25] batch [80/100] time 0.140 (0.334) data 0.000 (0.192) loss 3.1211 (3.1147) acc 56.2500 (53.2031) lr 1.0000e-05 eta 0:13:27
epoch [1/25] batch [85/100] time 0.142 (0.364) data 0.000 (0.222) loss 2.7188 (3.0869) acc 53.1250 (53.4926) lr 1.0000e-05 eta 0:14:38
epoch [1/25] batch [90/100] time 0.141 (0.351) data 0.000 (0.209) loss 2.8164 (3.0784) acc 59.3750 (53.6806) lr 1.0000e-05 eta 0:14:06
epoch [1/25] batch [95/100] time 0.140 (0.342) data 0.000 (0.200) loss 3.1094 (3.0526) acc 46.8750 (54.0461) lr 1.0000e-05 eta 0:13:43
epoch [1/25] batch [100/100] time 0.212 (0.333) data 0.000 (0.191) loss 3.1758 (3.0411) acc 50.0000 (54.1250) lr 2.0000e-03 eta 0:13:19
epoch [2/25] batch [5/100] time 0.141 (0.555) data 0.000 (0.412) loss 1.3662 (1.7986) acc 68.7500 (70.0000) lr 2.0000e-03 eta 0:22:09
epoch [2/25] batch [10/100] time 0.141 (0.349) data 0.000 (0.206) loss 1.4209 (1.6983) acc 65.6250 (68.4375) lr 2.0000e-03 eta 0:13:55
epoch [2/25] batch [15/100] time 0.141 (0.280) data 0.000 (0.138) loss 1.7246 (1.6316) acc 59.3750 (67.5000) lr 2.0000e-03 eta 0:11:07
epoch [2/25] batch [20/100] time 0.141 (0.245) data 0.000 (0.103) loss 1.8975 (1.6260) acc 59.3750 (66.4062) lr 2.0000e-03 eta 0:09:43
epoch [2/25] batch [25/100] time 0.142 (0.225) data 0.001 (0.083) loss 1.1689 (1.5817) acc 71.8750 (66.2500) lr 2.0000e-03 eta 0:08:53
epoch [2/25] batch [30/100] time 0.141 (0.211) data 0.000 (0.069) loss 1.5107 (1.5555) acc 68.7500 (66.3542) lr 2.0000e-03 eta 0:08:20
epoch [2/25] batch [35/100] time 0.141 (0.201) data 0.000 (0.059) loss 1.3340 (1.5139) acc 75.0000 (67.4107) lr 2.0000e-03 eta 0:07:55
epoch [2/25] batch [40/100] time 0.144 (0.194) data 0.000 (0.052) loss 1.2129 (1.4794) acc 68.7500 (67.8906) lr 2.0000e-03 eta 0:07:37
epoch [2/25] batch [45/100] time 0.141 (0.188) data 0.000 (0.046) loss 1.7607 (1.4827) acc 59.3750 (66.9444) lr 2.0000e-03 eta 0:07:22
epoch [2/25] batch [50/100] time 0.141 (0.183) data 0.000 (0.042) loss 1.3379 (1.4504) acc 56.2500 (67.0000) lr 2.0000e-03 eta 0:07:10
epoch [2/25] batch [55/100] time 0.141 (0.180) data 0.000 (0.038) loss 1.2568 (1.4205) acc 68.7500 (67.3864) lr 2.0000e-03 eta 0:07:00
epoch [2/25] batch [60/100] time 0.142 (0.176) data 0.001 (0.035) loss 1.2061 (1.4050) acc 71.8750 (67.6562) lr 2.0000e-03 eta 0:06:52
epoch [2/25] batch [65/100] time 0.141 (0.174) data 0.000 (0.032) loss 0.9595 (1.3974) acc 75.0000 (67.7885) lr 2.0000e-03 eta 0:06:45
epoch [2/25] batch [70/100] time 0.141 (0.171) data 0.000 (0.030) loss 1.0859 (1.3735) acc 68.7500 (67.9464) lr 2.0000e-03 eta 0:06:39
epoch [2/25] batch [75/100] time 0.145 (0.170) data 0.004 (0.028) loss 0.9717 (1.3542) acc 81.2500 (68.2500) lr 2.0000e-03 eta 0:06:34
epoch [2/25] batch [80/100] time 0.142 (0.168) data 0.000 (0.026) loss 1.0527 (1.3416) acc 71.8750 (68.2812) lr 2.0000e-03 eta 0:06:29
epoch [2/25] batch [85/100] time 0.141 (0.166) data 0.000 (0.025) loss 1.1211 (1.3364) acc 68.7500 (68.2721) lr 2.0000e-03 eta 0:06:24
epoch [2/25] batch [90/100] time 0.141 (0.165) data 0.000 (0.023) loss 1.0781 (1.3322) acc 71.8750 (68.0903) lr 2.0000e-03 eta 0:06:20
epoch [2/25] batch [95/100] time 0.141 (0.164) data 0.000 (0.022) loss 0.8550 (1.3254) acc 78.1250 (68.1579) lr 2.0000e-03 eta 0:06:17
epoch [2/25] batch [100/100] time 0.123 (0.162) data 0.000 (0.021) loss 1.3213 (1.3183) acc 62.5000 (68.2500) lr 1.9921e-03 eta 0:06:13
epoch [3/25] batch [5/100] time 0.147 (0.384) data 0.000 (0.241) loss 1.1982 (1.0188) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:14:41
epoch [3/25] batch [10/100] time 0.146 (0.263) data 0.001 (0.121) loss 0.7764 (1.1390) acc 81.2500 (70.6250) lr 1.9921e-03 eta 0:10:03
epoch [3/25] batch [15/100] time 0.142 (0.223) data 0.000 (0.081) loss 0.8789 (1.1103) acc 81.2500 (70.2083) lr 1.9921e-03 eta 0:08:28
epoch [3/25] batch [20/100] time 0.141 (0.202) data 0.000 (0.061) loss 1.2793 (1.1154) acc 68.7500 (71.2500) lr 1.9921e-03 eta 0:07:41
epoch [3/25] batch [25/100] time 0.142 (0.190) data 0.000 (0.048) loss 1.1133 (1.1115) acc 78.1250 (71.8750) lr 1.9921e-03 eta 0:07:13
epoch [3/25] batch [30/100] time 0.142 (0.182) data 0.000 (0.040) loss 1.4062 (1.1315) acc 62.5000 (70.7292) lr 1.9921e-03 eta 0:06:53
epoch [3/25] batch [35/100] time 0.143 (0.177) data 0.000 (0.035) loss 1.3086 (1.1466) acc 59.3750 (70.0893) lr 1.9921e-03 eta 0:06:40
epoch [3/25] batch [40/100] time 0.141 (0.172) data 0.000 (0.030) loss 1.2598 (1.1330) acc 62.5000 (70.3125) lr 1.9921e-03 eta 0:06:29
epoch [3/25] batch [45/100] time 0.144 (0.169) data 0.001 (0.027) loss 0.5820 (1.1346) acc 87.5000 (70.1389) lr 1.9921e-03 eta 0:06:21
epoch [3/25] batch [50/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.5859 (1.1219) acc 87.5000 (70.6875) lr 1.9921e-03 eta 0:06:14
epoch [3/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.1875 (1.1226) acc 65.6250 (70.6818) lr 1.9921e-03 eta 0:06:09
epoch [3/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.020) loss 0.6514 (1.1156) acc 87.5000 (70.6771) lr 1.9921e-03 eta 0:06:04
epoch [3/25] batch [65/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.5308 (1.1161) acc 87.5000 (70.8173) lr 1.9921e-03 eta 0:05:59
epoch [3/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.018) loss 1.2754 (1.1249) acc 71.8750 (70.4464) lr 1.9921e-03 eta 0:05:55
epoch [3/25] batch [75/100] time 0.141 (0.158) data 0.000 (0.016) loss 1.1572 (1.1197) acc 71.8750 (70.6667) lr 1.9921e-03 eta 0:05:52
epoch [3/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.2734 (1.1206) acc 59.3750 (70.5859) lr 1.9921e-03 eta 0:05:49
epoch [3/25] batch [85/100] time 0.143 (0.156) data 0.000 (0.015) loss 1.2393 (1.1210) acc 62.5000 (70.4412) lr 1.9921e-03 eta 0:05:46
epoch [3/25] batch [90/100] time 0.141 (0.156) data 0.000 (0.014) loss 1.2656 (1.1251) acc 59.3750 (70.3819) lr 1.9921e-03 eta 0:05:44
epoch [3/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.9238 (1.1232) acc 65.6250 (70.4605) lr 1.9921e-03 eta 0:05:41
epoch [3/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.012) loss 0.7549 (1.1150) acc 75.0000 (70.6875) lr 1.9686e-03 eta 0:05:38
epoch [4/25] batch [5/100] time 0.142 (0.386) data 0.000 (0.244) loss 0.7954 (1.0800) acc 75.0000 (67.5000) lr 1.9686e-03 eta 0:14:07
epoch [4/25] batch [10/100] time 0.141 (0.264) data 0.000 (0.122) loss 1.2852 (1.1256) acc 78.1250 (67.5000) lr 1.9686e-03 eta 0:09:38
epoch [4/25] batch [15/100] time 0.142 (0.223) data 0.000 (0.082) loss 1.3311 (1.1116) acc 59.3750 (67.7083) lr 1.9686e-03 eta 0:08:08
epoch [4/25] batch [20/100] time 0.142 (0.203) data 0.000 (0.061) loss 1.2451 (1.0994) acc 68.7500 (68.7500) lr 1.9686e-03 eta 0:07:22
epoch [4/25] batch [25/100] time 0.145 (0.191) data 0.000 (0.049) loss 1.2969 (1.0890) acc 68.7500 (69.2500) lr 1.9686e-03 eta 0:06:55
epoch [4/25] batch [30/100] time 0.142 (0.183) data 0.000 (0.041) loss 1.2920 (1.0704) acc 71.8750 (70.7292) lr 1.9686e-03 eta 0:06:36
epoch [4/25] batch [35/100] time 0.144 (0.177) data 0.001 (0.035) loss 0.7778 (1.0508) acc 81.2500 (71.5179) lr 1.9686e-03 eta 0:06:23
epoch [4/25] batch [40/100] time 0.143 (0.173) data 0.001 (0.031) loss 0.7856 (1.0344) acc 81.2500 (72.6562) lr 1.9686e-03 eta 0:06:13
epoch [4/25] batch [45/100] time 0.142 (0.169) data 0.000 (0.027) loss 1.1250 (1.0465) acc 65.6250 (72.2222) lr 1.9686e-03 eta 0:06:05
epoch [4/25] batch [50/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.9385 (1.0530) acc 78.1250 (71.9375) lr 1.9686e-03 eta 0:05:58
epoch [4/25] batch [55/100] time 0.142 (0.165) data 0.001 (0.023) loss 1.4600 (1.0581) acc 62.5000 (72.2159) lr 1.9686e-03 eta 0:05:53
epoch [4/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.021) loss 1.0254 (1.0563) acc 81.2500 (72.2396) lr 1.9686e-03 eta 0:05:48
epoch [4/25] batch [65/100] time 0.144 (0.161) data 0.000 (0.019) loss 0.8555 (1.0672) acc 84.3750 (72.1635) lr 1.9686e-03 eta 0:05:44
epoch [4/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.018) loss 1.0840 (1.0654) acc 75.0000 (72.2321) lr 1.9686e-03 eta 0:05:40
epoch [4/25] batch [75/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.8706 (1.0531) acc 68.7500 (72.3333) lr 1.9686e-03 eta 0:05:37
epoch [4/25] batch [80/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.3164 (1.0478) acc 62.5000 (72.3828) lr 1.9686e-03 eta 0:05:34
epoch [4/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.2070 (1.0510) acc 68.7500 (72.2794) lr 1.9686e-03 eta 0:05:31
epoch [4/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.1055 (1.0566) acc 62.5000 (72.2222) lr 1.9686e-03 eta 0:05:29
epoch [4/25] batch [95/100] time 0.143 (0.155) data 0.000 (0.013) loss 0.8955 (1.0530) acc 78.1250 (72.5000) lr 1.9686e-03 eta 0:05:26
epoch [4/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.013) loss 1.1367 (1.0534) acc 56.2500 (72.3125) lr 1.9298e-03 eta 0:05:24
epoch [5/25] batch [5/100] time 0.143 (0.543) data 0.000 (0.398) loss 0.9126 (1.0959) acc 75.0000 (71.8750) lr 1.9298e-03 eta 0:18:56
epoch [5/25] batch [10/100] time 0.144 (0.343) data 0.000 (0.199) loss 1.1631 (1.0349) acc 68.7500 (74.3750) lr 1.9298e-03 eta 0:11:56
epoch [5/25] batch [15/100] time 0.144 (0.276) data 0.000 (0.133) loss 1.2129 (1.0201) acc 62.5000 (74.1667) lr 1.9298e-03 eta 0:09:36
epoch [5/25] batch [20/100] time 0.144 (0.243) data 0.000 (0.100) loss 1.3008 (1.0525) acc 56.2500 (73.4375) lr 1.9298e-03 eta 0:08:25
epoch [5/25] batch [25/100] time 0.142 (0.223) data 0.000 (0.080) loss 1.2471 (1.0721) acc 71.8750 (72.8750) lr 1.9298e-03 eta 0:07:42
epoch [5/25] batch [30/100] time 0.142 (0.209) data 0.000 (0.067) loss 0.9893 (1.0589) acc 75.0000 (72.8125) lr 1.9298e-03 eta 0:07:13
epoch [5/25] batch [35/100] time 0.142 (0.200) data 0.000 (0.057) loss 1.2627 (1.0592) acc 62.5000 (72.8571) lr 1.9298e-03 eta 0:06:52
epoch [5/25] batch [40/100] time 0.145 (0.193) data 0.000 (0.050) loss 1.2305 (1.0489) acc 68.7500 (73.0469) lr 1.9298e-03 eta 0:06:37
epoch [5/25] batch [45/100] time 0.142 (0.187) data 0.000 (0.044) loss 0.8247 (1.0466) acc 81.2500 (73.3333) lr 1.9298e-03 eta 0:06:24
epoch [5/25] batch [50/100] time 0.143 (0.183) data 0.000 (0.040) loss 1.0508 (1.0473) acc 68.7500 (73.1250) lr 1.9298e-03 eta 0:06:14
epoch [5/25] batch [55/100] time 0.142 (0.179) data 0.000 (0.036) loss 1.0645 (1.0354) acc 68.7500 (73.6364) lr 1.9298e-03 eta 0:06:06
epoch [5/25] batch [60/100] time 0.142 (0.176) data 0.000 (0.033) loss 0.8613 (1.0366) acc 68.7500 (73.4896) lr 1.9298e-03 eta 0:05:59
epoch [5/25] batch [65/100] time 0.142 (0.173) data 0.000 (0.031) loss 1.3447 (1.0482) acc 56.2500 (72.9327) lr 1.9298e-03 eta 0:05:53
epoch [5/25] batch [70/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.9199 (1.0452) acc 75.0000 (72.8125) lr 1.9298e-03 eta 0:05:47
epoch [5/25] batch [75/100] time 0.142 (0.169) data 0.000 (0.027) loss 1.2891 (1.0425) acc 59.3750 (72.8750) lr 1.9298e-03 eta 0:05:43
epoch [5/25] batch [80/100] time 0.142 (0.168) data 0.000 (0.025) loss 0.9419 (1.0435) acc 75.0000 (72.9297) lr 1.9298e-03 eta 0:05:38
epoch [5/25] batch [85/100] time 0.143 (0.166) data 0.000 (0.024) loss 0.8037 (1.0419) acc 81.2500 (72.9779) lr 1.9298e-03 eta 0:05:34
epoch [5/25] batch [90/100] time 0.144 (0.165) data 0.000 (0.022) loss 1.3271 (1.0417) acc 56.2500 (72.9861) lr 1.9298e-03 eta 0:05:31
epoch [5/25] batch [95/100] time 0.142 (0.164) data 0.000 (0.021) loss 1.1211 (1.0393) acc 81.2500 (73.0921) lr 1.9298e-03 eta 0:05:28
epoch [5/25] batch [100/100] time 0.124 (0.163) data 0.000 (0.020) loss 0.7876 (1.0366) acc 62.5000 (73.0000) lr 1.8763e-03 eta 0:05:25
epoch [6/25] batch [5/100] time 0.142 (0.439) data 0.000 (0.296) loss 0.7715 (0.9145) acc 84.3750 (75.0000) lr 1.8763e-03 eta 0:14:36
epoch [6/25] batch [10/100] time 0.141 (0.291) data 0.000 (0.148) loss 1.0371 (0.9956) acc 78.1250 (74.6875) lr 1.8763e-03 eta 0:09:39
epoch [6/25] batch [15/100] time 0.144 (0.242) data 0.001 (0.099) loss 1.2168 (0.9839) acc 68.7500 (75.6250) lr 1.8763e-03 eta 0:07:59
epoch [6/25] batch [20/100] time 0.143 (0.217) data 0.000 (0.074) loss 0.9233 (0.9980) acc 78.1250 (75.4688) lr 1.8763e-03 eta 0:07:09
epoch [6/25] batch [25/100] time 0.143 (0.202) data 0.000 (0.060) loss 1.0264 (1.0116) acc 71.8750 (74.8750) lr 1.8763e-03 eta 0:06:39
epoch [6/25] batch [30/100] time 0.142 (0.192) data 0.000 (0.050) loss 0.7637 (0.9991) acc 75.0000 (74.8958) lr 1.8763e-03 eta 0:06:19
epoch [6/25] batch [35/100] time 0.143 (0.185) data 0.000 (0.043) loss 0.7256 (0.9766) acc 87.5000 (75.1786) lr 1.8763e-03 eta 0:06:04
epoch [6/25] batch [40/100] time 0.142 (0.180) data 0.000 (0.037) loss 1.3818 (0.9867) acc 65.6250 (74.9219) lr 1.8763e-03 eta 0:05:52
epoch [6/25] batch [45/100] time 0.142 (0.176) data 0.000 (0.033) loss 1.0791 (0.9813) acc 71.8750 (75.0000) lr 1.8763e-03 eta 0:05:43
epoch [6/25] batch [50/100] time 0.142 (0.173) data 0.000 (0.030) loss 1.1631 (0.9971) acc 65.6250 (74.6250) lr 1.8763e-03 eta 0:05:36
epoch [6/25] batch [55/100] time 0.143 (0.170) data 0.000 (0.027) loss 0.9502 (1.0008) acc 71.8750 (74.6023) lr 1.8763e-03 eta 0:05:30
epoch [6/25] batch [60/100] time 0.142 (0.168) data 0.001 (0.025) loss 0.8921 (0.9828) acc 84.3750 (75.1042) lr 1.8763e-03 eta 0:05:25
epoch [6/25] batch [65/100] time 0.143 (0.166) data 0.000 (0.023) loss 0.7129 (0.9775) acc 84.3750 (75.1442) lr 1.8763e-03 eta 0:05:20
epoch [6/25] batch [70/100] time 0.143 (0.164) data 0.000 (0.021) loss 1.5381 (0.9900) acc 53.1250 (74.6429) lr 1.8763e-03 eta 0:05:16
epoch [6/25] batch [75/100] time 0.144 (0.163) data 0.000 (0.020) loss 1.1670 (0.9803) acc 75.0000 (74.8750) lr 1.8763e-03 eta 0:05:13
epoch [6/25] batch [80/100] time 0.145 (0.161) data 0.000 (0.019) loss 1.0078 (0.9851) acc 78.1250 (74.8828) lr 1.8763e-03 eta 0:05:10
epoch [6/25] batch [85/100] time 0.143 (0.160) data 0.000 (0.018) loss 0.7842 (0.9718) acc 81.2500 (75.4044) lr 1.8763e-03 eta 0:05:07
epoch [6/25] batch [90/100] time 0.143 (0.159) data 0.000 (0.017) loss 1.0420 (0.9674) acc 71.8750 (75.3819) lr 1.8763e-03 eta 0:05:04
epoch [6/25] batch [95/100] time 0.142 (0.159) data 0.000 (0.016) loss 0.8770 (0.9709) acc 75.0000 (75.3289) lr 1.8763e-03 eta 0:05:01
epoch [6/25] batch [100/100] time 0.125 (0.158) data 0.000 (0.015) loss 1.1260 (0.9673) acc 56.2500 (75.2188) lr 1.8090e-03 eta 0:04:59
epoch [7/25] batch [5/100] time 0.143 (0.430) data 0.000 (0.286) loss 0.9170 (0.8192) acc 78.1250 (79.3750) lr 1.8090e-03 eta 0:13:34
epoch [7/25] batch [10/100] time 0.143 (0.286) data 0.000 (0.143) loss 0.9526 (0.8369) acc 81.2500 (79.3750) lr 1.8090e-03 eta 0:09:01
epoch [7/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.096) loss 0.9424 (0.9165) acc 81.2500 (76.8750) lr 1.8090e-03 eta 0:07:29
epoch [7/25] batch [20/100] time 0.142 (0.214) data 0.000 (0.072) loss 0.7988 (0.8971) acc 81.2500 (76.7188) lr 1.8090e-03 eta 0:06:43
epoch [7/25] batch [25/100] time 0.142 (0.200) data 0.000 (0.057) loss 0.5391 (0.8835) acc 90.6250 (77.2500) lr 1.8090e-03 eta 0:06:14
epoch [7/25] batch [30/100] time 0.143 (0.190) data 0.000 (0.048) loss 1.1123 (0.8794) acc 65.6250 (77.7083) lr 1.8090e-03 eta 0:05:56
epoch [7/25] batch [35/100] time 0.143 (0.184) data 0.000 (0.041) loss 1.4775 (0.8935) acc 65.6250 (77.2321) lr 1.8090e-03 eta 0:05:42
epoch [7/25] batch [40/100] time 0.142 (0.179) data 0.000 (0.036) loss 1.0195 (0.9003) acc 68.7500 (76.7188) lr 1.8090e-03 eta 0:05:32
epoch [7/25] batch [45/100] time 0.142 (0.175) data 0.000 (0.032) loss 0.9263 (0.9109) acc 75.0000 (76.7361) lr 1.8090e-03 eta 0:05:23
epoch [7/25] batch [50/100] time 0.142 (0.171) data 0.000 (0.029) loss 1.0859 (0.9140) acc 68.7500 (76.8125) lr 1.8090e-03 eta 0:05:16
epoch [7/25] batch [55/100] time 0.142 (0.169) data 0.000 (0.026) loss 0.8701 (0.9258) acc 78.1250 (76.7045) lr 1.8090e-03 eta 0:05:11
epoch [7/25] batch [60/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.8022 (0.9260) acc 78.1250 (76.3542) lr 1.8090e-03 eta 0:05:06
epoch [7/25] batch [65/100] time 0.143 (0.165) data 0.000 (0.022) loss 0.8682 (0.9260) acc 84.3750 (76.6346) lr 1.8090e-03 eta 0:05:02
epoch [7/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8154 (0.9253) acc 81.2500 (76.4732) lr 1.8090e-03 eta 0:04:58
epoch [7/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.019) loss 1.5225 (0.9479) acc 71.8750 (75.7917) lr 1.8090e-03 eta 0:04:55
epoch [7/25] batch [80/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.5405 (0.9528) acc 90.6250 (75.8594) lr 1.8090e-03 eta 0:04:52
epoch [7/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.8530 (0.9453) acc 78.1250 (76.1029) lr 1.8090e-03 eta 0:04:49
epoch [7/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.3135 (0.9612) acc 68.7500 (75.7292) lr 1.8090e-03 eta 0:04:46
epoch [7/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.9097 (0.9632) acc 81.2500 (75.6250) lr 1.8090e-03 eta 0:04:44
epoch [7/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 1.3662 (0.9704) acc 75.0000 (75.3438) lr 1.7290e-03 eta 0:04:41
epoch [8/25] batch [5/100] time 0.142 (0.413) data 0.000 (0.270) loss 1.0605 (0.9728) acc 78.1250 (76.2500) lr 1.7290e-03 eta 0:12:21
epoch [8/25] batch [10/100] time 0.142 (0.278) data 0.000 (0.135) loss 0.9897 (0.9222) acc 71.8750 (76.2500) lr 1.7290e-03 eta 0:08:17
epoch [8/25] batch [15/100] time 0.143 (0.233) data 0.001 (0.090) loss 1.1045 (0.9087) acc 65.6250 (75.8333) lr 1.7290e-03 eta 0:06:55
epoch [8/25] batch [20/100] time 0.143 (0.210) data 0.000 (0.068) loss 0.6445 (0.8795) acc 84.3750 (76.4062) lr 1.7290e-03 eta 0:06:13
epoch [8/25] batch [25/100] time 0.142 (0.197) data 0.000 (0.054) loss 0.9419 (0.8731) acc 81.2500 (77.0000) lr 1.7290e-03 eta 0:05:48
epoch [8/25] batch [30/100] time 0.143 (0.188) data 0.000 (0.045) loss 0.6255 (0.8638) acc 87.5000 (77.2917) lr 1.7290e-03 eta 0:05:32
epoch [8/25] batch [35/100] time 0.143 (0.181) data 0.000 (0.039) loss 1.2197 (0.8728) acc 75.0000 (77.0536) lr 1.7290e-03 eta 0:05:19
epoch [8/25] batch [40/100] time 0.141 (0.176) data 0.000 (0.034) loss 1.0859 (0.8916) acc 71.8750 (76.6406) lr 1.7290e-03 eta 0:05:10
epoch [8/25] batch [45/100] time 0.142 (0.173) data 0.000 (0.030) loss 0.7256 (0.9001) acc 84.3750 (76.3889) lr 1.7290e-03 eta 0:05:02
epoch [8/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.027) loss 1.4375 (0.9063) acc 68.7500 (76.5625) lr 1.7290e-03 eta 0:04:56
epoch [8/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.6387 (0.9051) acc 81.2500 (76.7045) lr 1.7290e-03 eta 0:04:51
epoch [8/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6904 (0.9079) acc 90.6250 (76.7188) lr 1.7290e-03 eta 0:04:47
epoch [8/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.9741 (0.9193) acc 78.1250 (76.5385) lr 1.7290e-03 eta 0:04:43
epoch [8/25] batch [70/100] time 0.143 (0.162) data 0.000 (0.020) loss 0.8271 (0.9259) acc 75.0000 (76.5179) lr 1.7290e-03 eta 0:04:39
epoch [8/25] batch [75/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.7607 (0.9252) acc 81.2500 (76.6667) lr 1.7290e-03 eta 0:04:36
epoch [8/25] batch [80/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.8525 (0.9289) acc 78.1250 (76.4453) lr 1.7290e-03 eta 0:04:34
epoch [8/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.0879 (0.9278) acc 81.2500 (76.7279) lr 1.7290e-03 eta 0:04:31
epoch [8/25] batch [90/100] time 0.143 (0.158) data 0.000 (0.015) loss 1.0742 (0.9341) acc 75.0000 (76.5278) lr 1.7290e-03 eta 0:04:29
epoch [8/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.7695 (0.9316) acc 75.0000 (76.3487) lr 1.7290e-03 eta 0:04:27
epoch [8/25] batch [100/100] time 0.123 (0.156) data 0.000 (0.014) loss 1.1807 (0.9370) acc 81.2500 (76.3125) lr 1.6374e-03 eta 0:04:24
epoch [9/25] batch [5/100] time 0.143 (0.399) data 0.000 (0.257) loss 0.7520 (0.9251) acc 87.5000 (78.1250) lr 1.6374e-03 eta 0:11:16
epoch [9/25] batch [10/100] time 0.143 (0.271) data 0.000 (0.129) loss 0.9609 (0.9681) acc 68.7500 (75.0000) lr 1.6374e-03 eta 0:07:37
epoch [9/25] batch [15/100] time 0.146 (0.228) data 0.000 (0.086) loss 0.7793 (0.9312) acc 78.1250 (75.8333) lr 1.6374e-03 eta 0:06:24
epoch [9/25] batch [20/100] time 0.143 (0.207) data 0.000 (0.065) loss 0.8013 (0.9492) acc 78.1250 (75.6250) lr 1.6374e-03 eta 0:05:47
epoch [9/25] batch [25/100] time 0.144 (0.194) data 0.001 (0.052) loss 0.9639 (0.9141) acc 71.8750 (76.6250) lr 1.6374e-03 eta 0:05:24
epoch [9/25] batch [30/100] time 0.146 (0.185) data 0.000 (0.043) loss 1.0098 (0.9148) acc 78.1250 (76.9792) lr 1.6374e-03 eta 0:05:09
epoch [9/25] batch [35/100] time 0.142 (0.179) data 0.000 (0.037) loss 0.5771 (0.8797) acc 78.1250 (77.4107) lr 1.6374e-03 eta 0:04:58
epoch [9/25] batch [40/100] time 0.142 (0.175) data 0.000 (0.032) loss 0.7832 (0.8722) acc 84.3750 (77.8906) lr 1.6374e-03 eta 0:04:49
epoch [9/25] batch [45/100] time 0.144 (0.171) data 0.000 (0.029) loss 1.2793 (0.8899) acc 75.0000 (77.5000) lr 1.6374e-03 eta 0:04:43
epoch [9/25] batch [50/100] time 0.143 (0.168) data 0.000 (0.026) loss 0.9126 (0.9017) acc 68.7500 (76.3750) lr 1.6374e-03 eta 0:04:37
epoch [9/25] batch [55/100] time 0.142 (0.166) data 0.001 (0.024) loss 1.2383 (0.9234) acc 65.6250 (75.6250) lr 1.6374e-03 eta 0:04:33
epoch [9/25] batch [60/100] time 0.143 (0.164) data 0.001 (0.022) loss 0.8794 (0.9314) acc 81.2500 (75.6771) lr 1.6374e-03 eta 0:04:29
epoch [9/25] batch [65/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.2803 (0.9249) acc 71.8750 (76.0096) lr 1.6374e-03 eta 0:04:25
epoch [9/25] batch [70/100] time 0.146 (0.161) data 0.000 (0.019) loss 1.1357 (0.9262) acc 71.8750 (75.8482) lr 1.6374e-03 eta 0:04:22
epoch [9/25] batch [75/100] time 0.143 (0.160) data 0.001 (0.018) loss 0.6641 (0.9237) acc 87.5000 (76.1250) lr 1.6374e-03 eta 0:04:19
epoch [9/25] batch [80/100] time 0.143 (0.159) data 0.000 (0.016) loss 1.0918 (0.9302) acc 78.1250 (76.0547) lr 1.6374e-03 eta 0:04:17
epoch [9/25] batch [85/100] time 0.143 (0.158) data 0.000 (0.016) loss 0.7617 (0.9278) acc 81.2500 (76.0662) lr 1.6374e-03 eta 0:04:14
epoch [9/25] batch [90/100] time 0.143 (0.157) data 0.000 (0.015) loss 0.6709 (0.9347) acc 78.1250 (75.8333) lr 1.6374e-03 eta 0:04:12
epoch [9/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9873 (0.9324) acc 75.0000 (75.8224) lr 1.6374e-03 eta 0:04:10
epoch [9/25] batch [100/100] time 0.124 (0.155) data 0.000 (0.013) loss 0.7847 (0.9365) acc 81.2500 (75.7812) lr 1.5358e-03 eta 0:04:08
epoch [10/25] batch [5/100] time 0.145 (0.372) data 0.000 (0.228) loss 1.3887 (0.9267) acc 65.6250 (76.2500) lr 1.5358e-03 eta 0:09:52
epoch [10/25] batch [10/100] time 0.142 (0.257) data 0.000 (0.114) loss 1.0332 (0.8764) acc 71.8750 (77.1875) lr 1.5358e-03 eta 0:06:48
epoch [10/25] batch [15/100] time 0.143 (0.219) data 0.000 (0.076) loss 0.9351 (0.8794) acc 68.7500 (77.0833) lr 1.5358e-03 eta 0:05:46
epoch [10/25] batch [20/100] time 0.143 (0.200) data 0.000 (0.057) loss 0.6572 (0.8880) acc 87.5000 (77.6562) lr 1.5358e-03 eta 0:05:15
epoch [10/25] batch [25/100] time 0.143 (0.188) data 0.000 (0.046) loss 0.7886 (0.8976) acc 84.3750 (78.1250) lr 1.5358e-03 eta 0:04:56
epoch [10/25] batch [30/100] time 0.142 (0.181) data 0.000 (0.038) loss 0.7432 (0.8898) acc 75.0000 (77.6042) lr 1.5358e-03 eta 0:04:43
epoch [10/25] batch [35/100] time 0.144 (0.175) data 0.000 (0.033) loss 0.9019 (0.8812) acc 81.2500 (77.5000) lr 1.5358e-03 eta 0:04:34
epoch [10/25] batch [40/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.9199 (0.8916) acc 71.8750 (77.2656) lr 1.5358e-03 eta 0:04:26
epoch [10/25] batch [45/100] time 0.141 (0.168) data 0.000 (0.026) loss 0.9224 (0.8923) acc 71.8750 (77.0833) lr 1.5358e-03 eta 0:04:20
epoch [10/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.9243 (0.8796) acc 71.8750 (77.5000) lr 1.5358e-03 eta 0:04:16
epoch [10/25] batch [55/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.8271 (0.8843) acc 78.1250 (77.1023) lr 1.5358e-03 eta 0:04:12
epoch [10/25] batch [60/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.7739 (0.8821) acc 75.0000 (76.9271) lr 1.5358e-03 eta 0:04:08
epoch [10/25] batch [65/100] time 0.142 (0.160) data 0.000 (0.018) loss 1.1738 (0.8968) acc 71.8750 (76.5385) lr 1.5358e-03 eta 0:04:05
epoch [10/25] batch [70/100] time 0.141 (0.159) data 0.000 (0.017) loss 0.8110 (0.8965) acc 75.0000 (76.4732) lr 1.5358e-03 eta 0:04:02
epoch [10/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.015) loss 1.0801 (0.9017) acc 75.0000 (76.5000) lr 1.5358e-03 eta 0:04:00
epoch [10/25] batch [80/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.6729 (0.9053) acc 81.2500 (76.3672) lr 1.5358e-03 eta 0:03:57
epoch [10/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.2158 (0.9185) acc 62.5000 (75.9926) lr 1.5358e-03 eta 0:03:55
epoch [10/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 1.0039 (0.9126) acc 68.7500 (76.2153) lr 1.5358e-03 eta 0:03:54
epoch [10/25] batch [95/100] time 0.143 (0.154) data 0.000 (0.012) loss 0.7466 (0.9027) acc 75.0000 (76.4803) lr 1.5358e-03 eta 0:03:52
epoch [10/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.012) loss 1.2119 (0.9087) acc 56.2500 (76.3750) lr 1.4258e-03 eta 0:03:50
epoch [11/25] batch [5/100] time 0.143 (0.426) data 0.000 (0.282) loss 0.9116 (0.8267) acc 62.5000 (78.7500) lr 1.4258e-03 eta 0:10:36
epoch [11/25] batch [10/100] time 0.141 (0.284) data 0.000 (0.141) loss 0.8179 (0.8250) acc 84.3750 (78.7500) lr 1.4258e-03 eta 0:07:03
epoch [11/25] batch [15/100] time 0.142 (0.237) data 0.001 (0.094) loss 1.0410 (0.8162) acc 75.0000 (78.9583) lr 1.4258e-03 eta 0:05:51
epoch [11/25] batch [20/100] time 0.143 (0.213) data 0.000 (0.071) loss 1.0234 (0.8060) acc 71.8750 (79.2188) lr 1.4258e-03 eta 0:05:15
epoch [11/25] batch [25/100] time 0.142 (0.199) data 0.000 (0.057) loss 0.9009 (0.8277) acc 68.7500 (78.3750) lr 1.4258e-03 eta 0:04:53
epoch [11/25] batch [30/100] time 0.143 (0.190) data 0.001 (0.047) loss 0.8330 (0.8301) acc 81.2500 (78.4375) lr 1.4258e-03 eta 0:04:38
epoch [11/25] batch [35/100] time 0.143 (0.183) data 0.001 (0.041) loss 1.0498 (0.8553) acc 81.2500 (77.4107) lr 1.4258e-03 eta 0:04:28
epoch [11/25] batch [40/100] time 0.143 (0.178) data 0.001 (0.036) loss 0.9150 (0.8423) acc 81.2500 (78.1250) lr 1.4258e-03 eta 0:04:19
epoch [11/25] batch [45/100] time 0.145 (0.174) data 0.000 (0.032) loss 0.9692 (0.8371) acc 68.7500 (78.2639) lr 1.4258e-03 eta 0:04:13
epoch [11/25] batch [50/100] time 0.144 (0.171) data 0.000 (0.029) loss 0.8628 (0.8464) acc 84.3750 (78.2500) lr 1.4258e-03 eta 0:04:07
epoch [11/25] batch [55/100] time 0.143 (0.168) data 0.000 (0.026) loss 0.6157 (0.8476) acc 84.3750 (78.3523) lr 1.4258e-03 eta 0:04:03
epoch [11/25] batch [60/100] time 0.143 (0.166) data 0.000 (0.024) loss 1.0195 (0.8570) acc 62.5000 (78.1250) lr 1.4258e-03 eta 0:03:59
epoch [11/25] batch [65/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.1982 (0.8654) acc 68.7500 (77.7404) lr 1.4258e-03 eta 0:03:55
epoch [11/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.7993 (0.8682) acc 84.3750 (77.6339) lr 1.4258e-03 eta 0:03:52
epoch [11/25] batch [75/100] time 0.143 (0.161) data 0.000 (0.019) loss 0.6416 (0.8651) acc 81.2500 (77.7083) lr 1.4258e-03 eta 0:03:50
epoch [11/25] batch [80/100] time 0.143 (0.160) data 0.000 (0.018) loss 0.7700 (0.8667) acc 78.1250 (77.5000) lr 1.4258e-03 eta 0:03:47
epoch [11/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.7153 (0.8730) acc 87.5000 (77.4265) lr 1.4258e-03 eta 0:03:45
epoch [11/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.3369 (0.8722) acc 65.6250 (77.5347) lr 1.4258e-03 eta 0:03:43
epoch [11/25] batch [95/100] time 0.141 (0.157) data 0.000 (0.015) loss 1.3164 (0.8685) acc 68.7500 (77.7632) lr 1.4258e-03 eta 0:03:41
epoch [11/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.7573 (0.8757) acc 75.0000 (77.4688) lr 1.3090e-03 eta 0:03:39
epoch [12/25] batch [5/100] time 0.142 (0.396) data 0.000 (0.254) loss 0.8652 (0.9068) acc 78.1250 (75.6250) lr 1.3090e-03 eta 0:09:12
epoch [12/25] batch [10/100] time 0.144 (0.270) data 0.001 (0.127) loss 1.1406 (0.9050) acc 75.0000 (77.5000) lr 1.3090e-03 eta 0:06:15
epoch [12/25] batch [15/100] time 0.147 (0.228) data 0.001 (0.085) loss 0.7231 (0.9101) acc 84.3750 (76.4583) lr 1.3090e-03 eta 0:05:15
epoch [12/25] batch [20/100] time 0.142 (0.207) data 0.000 (0.064) loss 1.0430 (0.8622) acc 78.1250 (78.5938) lr 1.3090e-03 eta 0:04:44
epoch [12/25] batch [25/100] time 0.142 (0.194) data 0.000 (0.051) loss 0.5186 (0.8951) acc 84.3750 (77.7500) lr 1.3090e-03 eta 0:04:26
epoch [12/25] batch [30/100] time 0.142 (0.185) data 0.000 (0.043) loss 0.4702 (0.8816) acc 90.6250 (77.9167) lr 1.3090e-03 eta 0:04:13
epoch [12/25] batch [35/100] time 0.142 (0.179) data 0.000 (0.037) loss 1.0000 (0.8774) acc 78.1250 (77.8571) lr 1.3090e-03 eta 0:04:04
epoch [12/25] batch [40/100] time 0.143 (0.175) data 0.000 (0.032) loss 1.2861 (0.8881) acc 62.5000 (77.8906) lr 1.3090e-03 eta 0:03:57
epoch [12/25] batch [45/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.5127 (0.8754) acc 81.2500 (77.9167) lr 1.3090e-03 eta 0:03:51
epoch [12/25] batch [50/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.9829 (0.8824) acc 75.0000 (77.4375) lr 1.3090e-03 eta 0:03:47
epoch [12/25] batch [55/100] time 0.143 (0.166) data 0.000 (0.023) loss 1.1816 (0.8916) acc 59.3750 (76.8182) lr 1.3090e-03 eta 0:03:43
epoch [12/25] batch [60/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.0547 (0.9068) acc 71.8750 (76.5625) lr 1.3090e-03 eta 0:03:39
epoch [12/25] batch [65/100] time 0.146 (0.162) data 0.001 (0.020) loss 0.5283 (0.9045) acc 87.5000 (76.7788) lr 1.3090e-03 eta 0:03:36
epoch [12/25] batch [70/100] time 0.143 (0.161) data 0.000 (0.019) loss 0.7964 (0.9043) acc 81.2500 (76.9643) lr 1.3090e-03 eta 0:03:34
epoch [12/25] batch [75/100] time 0.142 (0.160) data 0.000 (0.017) loss 0.7949 (0.8982) acc 81.2500 (77.2083) lr 1.3090e-03 eta 0:03:31
epoch [12/25] batch [80/100] time 0.144 (0.159) data 0.000 (0.016) loss 1.0059 (0.8893) acc 78.1250 (77.5391) lr 1.3090e-03 eta 0:03:29
epoch [12/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.6704 (0.8923) acc 90.6250 (77.3162) lr 1.3090e-03 eta 0:03:27
epoch [12/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.014) loss 0.7456 (0.8870) acc 75.0000 (77.2569) lr 1.3090e-03 eta 0:03:25
epoch [12/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9209 (0.8832) acc 75.0000 (77.1711) lr 1.3090e-03 eta 0:03:23
epoch [12/25] batch [100/100] time 0.124 (0.155) data 0.000 (0.013) loss 0.5713 (0.8786) acc 93.7500 (77.3438) lr 1.1874e-03 eta 0:03:21
epoch [13/25] batch [5/100] time 0.143 (0.400) data 0.000 (0.257) loss 1.0391 (0.9207) acc 78.1250 (80.0000) lr 1.1874e-03 eta 0:08:38
epoch [13/25] batch [10/100] time 0.142 (0.271) data 0.000 (0.129) loss 1.1240 (0.8499) acc 68.7500 (80.3125) lr 1.1874e-03 eta 0:05:50
epoch [13/25] batch [15/100] time 0.142 (0.228) data 0.000 (0.086) loss 0.8286 (0.8724) acc 78.1250 (79.7917) lr 1.1874e-03 eta 0:04:53
epoch [13/25] batch [20/100] time 0.142 (0.207) data 0.000 (0.065) loss 0.8350 (0.8699) acc 81.2500 (80.0000) lr 1.1874e-03 eta 0:04:24
epoch [13/25] batch [25/100] time 0.142 (0.194) data 0.000 (0.052) loss 0.7861 (0.8718) acc 78.1250 (79.7500) lr 1.1874e-03 eta 0:04:07
epoch [13/25] batch [30/100] time 0.143 (0.185) data 0.001 (0.043) loss 1.0430 (0.8835) acc 71.8750 (79.6875) lr 1.1874e-03 eta 0:03:55
epoch [13/25] batch [35/100] time 0.143 (0.179) data 0.000 (0.037) loss 0.6914 (0.8546) acc 78.1250 (80.0893) lr 1.1874e-03 eta 0:03:46
epoch [13/25] batch [40/100] time 0.143 (0.175) data 0.000 (0.032) loss 0.9531 (0.8487) acc 81.2500 (80.1562) lr 1.1874e-03 eta 0:03:40
epoch [13/25] batch [45/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.7808 (0.8317) acc 78.1250 (80.6250) lr 1.1874e-03 eta 0:03:34
epoch [13/25] batch [50/100] time 0.143 (0.168) data 0.000 (0.026) loss 0.8906 (0.8349) acc 84.3750 (80.3750) lr 1.1874e-03 eta 0:03:30
epoch [13/25] batch [55/100] time 0.144 (0.166) data 0.000 (0.024) loss 1.3184 (0.8513) acc 75.0000 (80.2841) lr 1.1874e-03 eta 0:03:26
epoch [13/25] batch [60/100] time 0.142 (0.164) data 0.001 (0.022) loss 0.7368 (0.8576) acc 78.1250 (79.7917) lr 1.1874e-03 eta 0:03:23
epoch [13/25] batch [65/100] time 0.144 (0.162) data 0.000 (0.020) loss 0.7480 (0.8601) acc 81.2500 (79.4712) lr 1.1874e-03 eta 0:03:20
epoch [13/25] batch [70/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.6841 (0.8563) acc 78.1250 (79.5982) lr 1.1874e-03 eta 0:03:18
epoch [13/25] batch [75/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.7134 (0.8612) acc 75.0000 (79.3333) lr 1.1874e-03 eta 0:03:15
epoch [13/25] batch [80/100] time 0.141 (0.159) data 0.000 (0.016) loss 0.7905 (0.8571) acc 81.2500 (79.4922) lr 1.1874e-03 eta 0:03:13
epoch [13/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.7690 (0.8626) acc 78.1250 (79.4485) lr 1.1874e-03 eta 0:03:11
epoch [13/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.2725 (0.8732) acc 59.3750 (79.0278) lr 1.1874e-03 eta 0:03:09
epoch [13/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.7163 (0.8753) acc 84.3750 (78.9803) lr 1.1874e-03 eta 0:03:08
epoch [13/25] batch [100/100] time 0.123 (0.155) data 0.000 (0.013) loss 0.9502 (0.8787) acc 68.7500 (78.9375) lr 1.0628e-03 eta 0:03:06
epoch [14/25] batch [5/100] time 0.142 (0.414) data 0.000 (0.272) loss 1.0566 (0.8926) acc 75.0000 (76.8750) lr 1.0628e-03 eta 0:08:15
epoch [14/25] batch [10/100] time 0.142 (0.278) data 0.000 (0.136) loss 0.7441 (0.8677) acc 84.3750 (78.4375) lr 1.0628e-03 eta 0:05:31
epoch [14/25] batch [15/100] time 0.142 (0.233) data 0.000 (0.091) loss 0.8950 (0.8514) acc 75.0000 (78.9583) lr 1.0628e-03 eta 0:04:36
epoch [14/25] batch [20/100] time 0.141 (0.210) data 0.000 (0.068) loss 0.7183 (0.8191) acc 87.5000 (80.4688) lr 1.0628e-03 eta 0:04:08
epoch [14/25] batch [25/100] time 0.142 (0.197) data 0.000 (0.055) loss 0.8970 (0.8151) acc 75.0000 (80.2500) lr 1.0628e-03 eta 0:03:51
epoch [14/25] batch [30/100] time 0.145 (0.188) data 0.000 (0.046) loss 0.8262 (0.8512) acc 75.0000 (79.1667) lr 1.0628e-03 eta 0:03:39
epoch [14/25] batch [35/100] time 0.142 (0.181) data 0.000 (0.039) loss 0.7632 (0.8366) acc 78.1250 (79.7321) lr 1.0628e-03 eta 0:03:31
epoch [14/25] batch [40/100] time 0.142 (0.176) data 0.000 (0.034) loss 0.7769 (0.8280) acc 75.0000 (79.9219) lr 1.0628e-03 eta 0:03:24
epoch [14/25] batch [45/100] time 0.143 (0.173) data 0.000 (0.031) loss 0.8828 (0.8196) acc 78.1250 (80.0000) lr 1.0628e-03 eta 0:03:19
epoch [14/25] batch [50/100] time 0.147 (0.170) data 0.000 (0.028) loss 0.7339 (0.8221) acc 84.3750 (80.1875) lr 1.0628e-03 eta 0:03:15
epoch [14/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.9863 (0.8287) acc 78.1250 (80.1136) lr 1.0628e-03 eta 0:03:11
epoch [14/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6255 (0.8358) acc 84.3750 (79.8438) lr 1.0628e-03 eta 0:03:08
epoch [14/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.6035 (0.8246) acc 78.1250 (79.9038) lr 1.0628e-03 eta 0:03:05
epoch [14/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.0342 (0.8303) acc 78.1250 (79.9107) lr 1.0628e-03 eta 0:03:02
epoch [14/25] batch [75/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.9458 (0.8304) acc 78.1250 (79.8750) lr 1.0628e-03 eta 0:03:00
epoch [14/25] batch [80/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.9858 (0.8430) acc 78.1250 (79.6875) lr 1.0628e-03 eta 0:02:58
epoch [14/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.6855 (0.8478) acc 87.5000 (79.3382) lr 1.0628e-03 eta 0:02:56
epoch [14/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.6860 (0.8431) acc 84.3750 (79.5139) lr 1.0628e-03 eta 0:02:54
epoch [14/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.5234 (0.8497) acc 90.6250 (79.2763) lr 1.0628e-03 eta 0:02:53
epoch [14/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 1.2148 (0.8567) acc 56.2500 (79.0000) lr 9.3721e-04 eta 0:02:51
epoch [15/25] batch [5/100] time 0.142 (0.384) data 0.000 (0.241) loss 0.9365 (0.8671) acc 71.8750 (78.1250) lr 9.3721e-04 eta 0:06:59
epoch [15/25] batch [10/100] time 0.142 (0.263) data 0.000 (0.121) loss 0.9429 (0.9210) acc 78.1250 (77.1875) lr 9.3721e-04 eta 0:04:46
epoch [15/25] batch [15/100] time 0.142 (0.223) data 0.000 (0.081) loss 0.9316 (0.9090) acc 84.3750 (78.5417) lr 9.3721e-04 eta 0:04:01
epoch [15/25] batch [20/100] time 0.142 (0.203) data 0.000 (0.061) loss 0.4702 (0.8602) acc 93.7500 (80.4688) lr 9.3721e-04 eta 0:03:39
epoch [15/25] batch [25/100] time 0.142 (0.191) data 0.000 (0.049) loss 0.8613 (0.8535) acc 78.1250 (79.3750) lr 9.3721e-04 eta 0:03:24
epoch [15/25] batch [30/100] time 0.142 (0.183) data 0.000 (0.040) loss 0.7070 (0.8749) acc 87.5000 (78.5417) lr 9.3721e-04 eta 0:03:15
epoch [15/25] batch [35/100] time 0.142 (0.177) data 0.000 (0.035) loss 1.0811 (0.8744) acc 68.7500 (78.4821) lr 9.3721e-04 eta 0:03:08
epoch [15/25] batch [40/100] time 0.144 (0.173) data 0.001 (0.030) loss 0.7993 (0.8808) acc 71.8750 (77.8906) lr 9.3721e-04 eta 0:03:03
epoch [15/25] batch [45/100] time 0.142 (0.169) data 0.000 (0.027) loss 1.3623 (0.8844) acc 65.6250 (77.5694) lr 9.3721e-04 eta 0:02:58
epoch [15/25] batch [50/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.7056 (0.8599) acc 78.1250 (78.1250) lr 9.3721e-04 eta 0:02:54
epoch [15/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.2178 (0.8583) acc 71.8750 (78.1818) lr 9.3721e-04 eta 0:02:51
epoch [15/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.020) loss 0.8232 (0.8622) acc 84.3750 (78.0729) lr 9.3721e-04 eta 0:02:49
epoch [15/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.8970 (0.8590) acc 84.3750 (78.1731) lr 9.3721e-04 eta 0:02:46
epoch [15/25] batch [70/100] time 0.141 (0.160) data 0.000 (0.018) loss 0.5361 (0.8582) acc 93.7500 (78.4821) lr 9.3721e-04 eta 0:02:44
epoch [15/25] batch [75/100] time 0.144 (0.158) data 0.000 (0.016) loss 0.4973 (0.8469) acc 87.5000 (78.7917) lr 9.3721e-04 eta 0:02:42
epoch [15/25] batch [80/100] time 0.148 (0.157) data 0.000 (0.015) loss 0.8276 (0.8514) acc 75.0000 (78.5156) lr 9.3721e-04 eta 0:02:40
epoch [15/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.9316 (0.8551) acc 71.8750 (78.5294) lr 9.3721e-04 eta 0:02:38
epoch [15/25] batch [90/100] time 0.144 (0.156) data 0.000 (0.014) loss 0.7583 (0.8563) acc 81.2500 (78.3333) lr 9.3721e-04 eta 0:02:37
epoch [15/25] batch [95/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.6772 (0.8507) acc 81.2500 (78.3882) lr 9.3721e-04 eta 0:02:35
epoch [15/25] batch [100/100] time 0.125 (0.154) data 0.000 (0.012) loss 0.5557 (0.8420) acc 81.2500 (78.5625) lr 8.1262e-04 eta 0:02:34
epoch [16/25] batch [5/100] time 0.145 (0.534) data 0.000 (0.389) loss 0.7593 (0.8647) acc 87.5000 (78.1250) lr 8.1262e-04 eta 0:08:50
epoch [16/25] batch [10/100] time 0.144 (0.339) data 0.001 (0.195) loss 0.8945 (0.8330) acc 78.1250 (79.6875) lr 8.1262e-04 eta 0:05:35
epoch [16/25] batch [15/100] time 0.143 (0.273) data 0.001 (0.130) loss 0.6987 (0.8019) acc 90.6250 (81.4583) lr 8.1262e-04 eta 0:04:29
epoch [16/25] batch [20/100] time 0.143 (0.241) data 0.000 (0.098) loss 0.8945 (0.7986) acc 75.0000 (80.9375) lr 8.1262e-04 eta 0:03:56
epoch [16/25] batch [25/100] time 0.142 (0.221) data 0.000 (0.078) loss 0.6938 (0.7866) acc 84.3750 (81.1250) lr 8.1262e-04 eta 0:03:35
epoch [16/25] batch [30/100] time 0.142 (0.208) data 0.000 (0.065) loss 0.7915 (0.7798) acc 81.2500 (80.9375) lr 8.1262e-04 eta 0:03:21
epoch [16/25] batch [35/100] time 0.142 (0.199) data 0.000 (0.056) loss 0.9463 (0.7690) acc 78.1250 (81.6071) lr 8.1262e-04 eta 0:03:11
epoch [16/25] batch [40/100] time 0.143 (0.192) data 0.001 (0.049) loss 0.5898 (0.7671) acc 84.3750 (81.7969) lr 8.1262e-04 eta 0:03:04
epoch [16/25] batch [45/100] time 0.142 (0.186) data 0.000 (0.044) loss 0.7612 (0.7732) acc 84.3750 (81.8750) lr 8.1262e-04 eta 0:02:58
epoch [16/25] batch [50/100] time 0.143 (0.182) data 0.000 (0.039) loss 1.0293 (0.7804) acc 78.1250 (81.7500) lr 8.1262e-04 eta 0:02:52
epoch [16/25] batch [55/100] time 0.142 (0.178) data 0.001 (0.036) loss 0.9478 (0.7799) acc 65.6250 (81.4773) lr 8.1262e-04 eta 0:02:48
epoch [16/25] batch [60/100] time 0.143 (0.175) data 0.001 (0.033) loss 0.8711 (0.7896) acc 81.2500 (81.0417) lr 8.1262e-04 eta 0:02:44
epoch [16/25] batch [65/100] time 0.142 (0.173) data 0.000 (0.030) loss 0.8271 (0.7964) acc 87.5000 (80.9135) lr 8.1262e-04 eta 0:02:41
epoch [16/25] batch [70/100] time 0.142 (0.171) data 0.000 (0.028) loss 0.6230 (0.7891) acc 81.2500 (80.9821) lr 8.1262e-04 eta 0:02:38
epoch [16/25] batch [75/100] time 0.142 (0.169) data 0.000 (0.026) loss 0.7441 (0.7830) acc 81.2500 (81.1667) lr 8.1262e-04 eta 0:02:36
epoch [16/25] batch [80/100] time 0.144 (0.167) data 0.000 (0.025) loss 0.7856 (0.7801) acc 87.5000 (81.3672) lr 8.1262e-04 eta 0:02:33
epoch [16/25] batch [85/100] time 0.147 (0.166) data 0.000 (0.023) loss 1.1025 (0.7835) acc 81.2500 (81.2132) lr 8.1262e-04 eta 0:02:31
epoch [16/25] batch [90/100] time 0.143 (0.165) data 0.000 (0.022) loss 0.7261 (0.7806) acc 81.2500 (81.3194) lr 8.1262e-04 eta 0:02:29
epoch [16/25] batch [95/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8105 (0.7849) acc 75.0000 (81.1842) lr 8.1262e-04 eta 0:02:27
epoch [16/25] batch [100/100] time 0.124 (0.162) data 0.000 (0.020) loss 0.7036 (0.7871) acc 75.0000 (80.9375) lr 6.9098e-04 eta 0:02:25
epoch [17/25] batch [5/100] time 0.142 (0.343) data 0.000 (0.200) loss 0.6958 (0.8083) acc 84.3750 (78.7500) lr 6.9098e-04 eta 0:05:06
epoch [17/25] batch [10/100] time 0.142 (0.243) data 0.000 (0.100) loss 0.8892 (0.8434) acc 71.8750 (77.5000) lr 6.9098e-04 eta 0:03:35
epoch [17/25] batch [15/100] time 0.142 (0.209) data 0.000 (0.067) loss 0.5410 (0.7924) acc 87.5000 (79.1667) lr 6.9098e-04 eta 0:03:04
epoch [17/25] batch [20/100] time 0.146 (0.192) data 0.000 (0.050) loss 1.0068 (0.7733) acc 75.0000 (80.3125) lr 6.9098e-04 eta 0:02:49
epoch [17/25] batch [25/100] time 0.142 (0.182) data 0.000 (0.040) loss 0.8296 (0.7696) acc 71.8750 (79.8750) lr 6.9098e-04 eta 0:02:39
epoch [17/25] batch [30/100] time 0.146 (0.176) data 0.001 (0.034) loss 0.6147 (0.7480) acc 90.6250 (80.7292) lr 6.9098e-04 eta 0:02:32
epoch [17/25] batch [35/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.8501 (0.7673) acc 75.0000 (80.4464) lr 6.9098e-04 eta 0:02:27
epoch [17/25] batch [40/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.4299 (0.7744) acc 96.8750 (80.7031) lr 6.9098e-04 eta 0:02:24
epoch [17/25] batch [45/100] time 0.143 (0.165) data 0.000 (0.023) loss 0.5854 (0.7741) acc 84.3750 (80.4167) lr 6.9098e-04 eta 0:02:20
epoch [17/25] batch [50/100] time 0.145 (0.163) data 0.000 (0.020) loss 1.2637 (0.7828) acc 68.7500 (80.3125) lr 6.9098e-04 eta 0:02:18
epoch [17/25] batch [55/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.9883 (0.7825) acc 81.2500 (80.2841) lr 6.9098e-04 eta 0:02:15
epoch [17/25] batch [60/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.7739 (0.7766) acc 71.8750 (80.4688) lr 6.9098e-04 eta 0:02:13
epoch [17/25] batch [65/100] time 0.144 (0.158) data 0.000 (0.016) loss 1.0352 (0.7896) acc 71.8750 (80.1923) lr 6.9098e-04 eta 0:02:11
epoch [17/25] batch [70/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.1172 (0.8034) acc 75.0000 (79.7321) lr 6.9098e-04 eta 0:02:10
epoch [17/25] batch [75/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9224 (0.8027) acc 78.1250 (79.6667) lr 6.9098e-04 eta 0:02:08
epoch [17/25] batch [80/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.4922 (0.7992) acc 87.5000 (79.8438) lr 6.9098e-04 eta 0:02:07
epoch [17/25] batch [85/100] time 0.143 (0.154) data 0.000 (0.012) loss 0.8325 (0.8036) acc 84.3750 (79.8162) lr 6.9098e-04 eta 0:02:05
epoch [17/25] batch [90/100] time 0.141 (0.154) data 0.000 (0.011) loss 0.6250 (0.7994) acc 87.5000 (80.2778) lr 6.9098e-04 eta 0:02:04
epoch [17/25] batch [95/100] time 0.142 (0.153) data 0.000 (0.011) loss 0.3938 (0.7997) acc 93.7500 (80.3618) lr 6.9098e-04 eta 0:02:03
epoch [17/25] batch [100/100] time 0.124 (0.152) data 0.000 (0.010) loss 1.0117 (0.7983) acc 75.0000 (80.2500) lr 5.7422e-04 eta 0:02:01
epoch [18/25] batch [5/100] time 0.143 (0.373) data 0.000 (0.229) loss 0.7563 (0.7541) acc 78.1250 (83.1250) lr 5.7422e-04 eta 0:04:56
epoch [18/25] batch [10/100] time 0.142 (0.258) data 0.000 (0.115) loss 0.8291 (0.7960) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:03:23
epoch [18/25] batch [15/100] time 0.142 (0.219) data 0.000 (0.076) loss 0.8008 (0.7877) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:02:51
epoch [18/25] batch [20/100] time 0.141 (0.200) data 0.000 (0.057) loss 0.9219 (0.7907) acc 71.8750 (80.1562) lr 5.7422e-04 eta 0:02:35
epoch [18/25] batch [25/100] time 0.142 (0.188) data 0.000 (0.046) loss 0.8447 (0.8011) acc 87.5000 (80.3750) lr 5.7422e-04 eta 0:02:25
epoch [18/25] batch [30/100] time 0.142 (0.181) data 0.000 (0.038) loss 0.5679 (0.8028) acc 87.5000 (80.7292) lr 5.7422e-04 eta 0:02:19
epoch [18/25] batch [35/100] time 0.143 (0.175) data 0.001 (0.033) loss 0.4692 (0.7817) acc 93.7500 (81.5179) lr 5.7422e-04 eta 0:02:13
epoch [18/25] batch [40/100] time 0.142 (0.171) data 0.000 (0.029) loss 1.2041 (0.7721) acc 71.8750 (81.7969) lr 5.7422e-04 eta 0:02:09
epoch [18/25] batch [45/100] time 0.145 (0.168) data 0.000 (0.026) loss 1.0078 (0.7720) acc 78.1250 (81.5972) lr 5.7422e-04 eta 0:02:06
epoch [18/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.8618 (0.7602) acc 81.2500 (81.9375) lr 5.7422e-04 eta 0:02:03
epoch [18/25] batch [55/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.9468 (0.7720) acc 71.8750 (81.7614) lr 5.7422e-04 eta 0:02:01
epoch [18/25] batch [60/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.9902 (0.7693) acc 78.1250 (81.7188) lr 5.7422e-04 eta 0:01:59
epoch [18/25] batch [65/100] time 0.143 (0.160) data 0.000 (0.018) loss 1.2910 (0.7762) acc 75.0000 (81.2981) lr 5.7422e-04 eta 0:01:57
epoch [18/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.8135 (0.7816) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:01:55
epoch [18/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9287 (0.7794) acc 78.1250 (81.2917) lr 5.7422e-04 eta 0:01:54
epoch [18/25] batch [80/100] time 0.143 (0.157) data 0.000 (0.015) loss 1.1279 (0.7830) acc 71.8750 (81.1719) lr 5.7422e-04 eta 0:01:52
epoch [18/25] batch [85/100] time 0.144 (0.156) data 0.000 (0.014) loss 0.7783 (0.7754) acc 84.3750 (81.5074) lr 5.7422e-04 eta 0:01:51
epoch [18/25] batch [90/100] time 0.143 (0.155) data 0.000 (0.013) loss 0.7358 (0.7788) acc 84.3750 (81.3889) lr 5.7422e-04 eta 0:01:50
epoch [18/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.6479 (0.7771) acc 84.3750 (81.5132) lr 5.7422e-04 eta 0:01:48
epoch [18/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.012) loss 0.7183 (0.7813) acc 81.2500 (81.3438) lr 4.6417e-04 eta 0:01:47
epoch [19/25] batch [5/100] time 0.142 (0.496) data 0.000 (0.354) loss 0.5625 (0.6369) acc 84.3750 (83.1250) lr 4.6417e-04 eta 0:05:44
epoch [19/25] batch [10/100] time 0.142 (0.319) data 0.000 (0.177) loss 0.6646 (0.7227) acc 87.5000 (82.5000) lr 4.6417e-04 eta 0:03:40
epoch [19/25] batch [15/100] time 0.145 (0.260) data 0.000 (0.118) loss 1.1885 (0.7585) acc 71.8750 (82.7083) lr 4.6417e-04 eta 0:02:58
epoch [19/25] batch [20/100] time 0.141 (0.231) data 0.000 (0.089) loss 0.8613 (0.7876) acc 75.0000 (81.7188) lr 4.6417e-04 eta 0:02:36
epoch [19/25] batch [25/100] time 0.142 (0.213) data 0.000 (0.071) loss 0.6348 (0.7805) acc 84.3750 (82.0000) lr 4.6417e-04 eta 0:02:23
epoch [19/25] batch [30/100] time 0.142 (0.201) data 0.000 (0.059) loss 0.9727 (0.7610) acc 75.0000 (81.9792) lr 4.6417e-04 eta 0:02:14
epoch [19/25] batch [35/100] time 0.143 (0.193) data 0.000 (0.051) loss 0.5713 (0.7623) acc 87.5000 (81.6964) lr 4.6417e-04 eta 0:02:08
epoch [19/25] batch [40/100] time 0.142 (0.186) data 0.000 (0.045) loss 1.0996 (0.7577) acc 71.8750 (81.9531) lr 4.6417e-04 eta 0:02:03
epoch [19/25] batch [45/100] time 0.142 (0.182) data 0.000 (0.040) loss 1.0957 (0.7490) acc 65.6250 (82.2222) lr 4.6417e-04 eta 0:01:58
epoch [19/25] batch [50/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.6865 (0.7540) acc 87.5000 (82.1250) lr 4.6417e-04 eta 0:01:55
epoch [19/25] batch [55/100] time 0.142 (0.174) data 0.000 (0.032) loss 0.5981 (0.7539) acc 90.6250 (81.9886) lr 4.6417e-04 eta 0:01:52
epoch [19/25] batch [60/100] time 0.141 (0.172) data 0.000 (0.030) loss 0.8926 (0.7578) acc 84.3750 (82.0833) lr 4.6417e-04 eta 0:01:49
epoch [19/25] batch [65/100] time 0.141 (0.169) data 0.000 (0.028) loss 0.8057 (0.7512) acc 81.2500 (82.3077) lr 4.6417e-04 eta 0:01:47
epoch [19/25] batch [70/100] time 0.142 (0.167) data 0.000 (0.026) loss 0.8022 (0.7581) acc 81.2500 (82.0089) lr 4.6417e-04 eta 0:01:45
epoch [19/25] batch [75/100] time 0.141 (0.166) data 0.000 (0.024) loss 1.0605 (0.7607) acc 75.0000 (81.8750) lr 4.6417e-04 eta 0:01:43
epoch [19/25] batch [80/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.2109 (0.7676) acc 68.7500 (81.7578) lr 4.6417e-04 eta 0:01:41
epoch [19/25] batch [85/100] time 0.146 (0.163) data 0.000 (0.021) loss 0.7573 (0.7588) acc 90.6250 (82.0588) lr 4.6417e-04 eta 0:01:40
epoch [19/25] batch [90/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.5586 (0.7552) acc 81.2500 (81.9444) lr 4.6417e-04 eta 0:01:38
epoch [19/25] batch [95/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.7891 (0.7590) acc 90.6250 (82.0066) lr 4.6417e-04 eta 0:01:37
epoch [19/25] batch [100/100] time 0.124 (0.160) data 0.000 (0.018) loss 1.4834 (0.7753) acc 62.5000 (81.5312) lr 3.6258e-04 eta 0:01:35
epoch [20/25] batch [5/100] time 0.142 (0.372) data 0.000 (0.229) loss 1.0928 (0.8341) acc 78.1250 (80.0000) lr 3.6258e-04 eta 0:03:41
epoch [20/25] batch [10/100] time 0.142 (0.257) data 0.000 (0.115) loss 0.6738 (0.8241) acc 78.1250 (80.9375) lr 3.6258e-04 eta 0:02:31
epoch [20/25] batch [15/100] time 0.142 (0.219) data 0.000 (0.077) loss 0.7539 (0.7962) acc 81.2500 (81.6667) lr 3.6258e-04 eta 0:02:07
epoch [20/25] batch [20/100] time 0.142 (0.199) data 0.000 (0.058) loss 1.0723 (0.8167) acc 71.8750 (80.9375) lr 3.6258e-04 eta 0:01:55
epoch [20/25] batch [25/100] time 0.142 (0.188) data 0.000 (0.046) loss 0.4885 (0.7891) acc 96.8750 (81.3750) lr 3.6258e-04 eta 0:01:48
epoch [20/25] batch [30/100] time 0.142 (0.180) data 0.000 (0.038) loss 0.5664 (0.7635) acc 81.2500 (81.9792) lr 3.6258e-04 eta 0:01:42
epoch [20/25] batch [35/100] time 0.142 (0.175) data 0.000 (0.033) loss 1.1562 (0.7911) acc 81.2500 (81.7857) lr 3.6258e-04 eta 0:01:38
epoch [20/25] batch [40/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.4304 (0.7798) acc 93.7500 (82.2656) lr 3.6258e-04 eta 0:01:35
epoch [20/25] batch [45/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.4353 (0.7639) acc 84.3750 (82.4306) lr 3.6258e-04 eta 0:01:33
epoch [20/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6045 (0.7691) acc 84.3750 (82.1875) lr 3.6258e-04 eta 0:01:30
epoch [20/25] batch [55/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.6489 (0.7647) acc 93.7500 (82.3864) lr 3.6258e-04 eta 0:01:28
epoch [20/25] batch [60/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.8809 (0.7554) acc 75.0000 (82.6562) lr 3.6258e-04 eta 0:01:27
epoch [20/25] batch [65/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.6211 (0.7491) acc 84.3750 (82.8846) lr 3.6258e-04 eta 0:01:25
epoch [20/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.8447 (0.7538) acc 78.1250 (82.7679) lr 3.6258e-04 eta 0:01:24
epoch [20/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.6465 (0.7530) acc 87.5000 (82.7917) lr 3.6258e-04 eta 0:01:22
epoch [20/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.8188 (0.7540) acc 78.1250 (82.8125) lr 3.6258e-04 eta 0:01:21
epoch [20/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.1768 (0.7697) acc 65.6250 (82.2426) lr 3.6258e-04 eta 0:01:20
epoch [20/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.8223 (0.7751) acc 87.5000 (82.0139) lr 3.6258e-04 eta 0:01:19
epoch [20/25] batch [95/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.8506 (0.7725) acc 78.1250 (82.0395) lr 3.6258e-04 eta 0:01:17
epoch [20/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.012) loss 0.4873 (0.7704) acc 93.7500 (82.1250) lr 2.7103e-04 eta 0:01:16
epoch [21/25] batch [5/100] time 0.145 (0.366) data 0.000 (0.224) loss 0.6362 (0.6091) acc 84.3750 (83.7500) lr 2.7103e-04 eta 0:03:01
epoch [21/25] batch [10/100] time 0.142 (0.254) data 0.000 (0.112) loss 0.9941 (0.7027) acc 81.2500 (83.4375) lr 2.7103e-04 eta 0:02:04
epoch [21/25] batch [15/100] time 0.142 (0.217) data 0.000 (0.075) loss 0.8887 (0.7568) acc 81.2500 (82.7083) lr 2.7103e-04 eta 0:01:45
epoch [21/25] batch [20/100] time 0.142 (0.198) data 0.000 (0.056) loss 0.8989 (0.7806) acc 75.0000 (81.7188) lr 2.7103e-04 eta 0:01:35
epoch [21/25] batch [25/100] time 0.142 (0.187) data 0.000 (0.045) loss 0.5107 (0.7942) acc 90.6250 (81.5000) lr 2.7103e-04 eta 0:01:28
epoch [21/25] batch [30/100] time 0.142 (0.179) data 0.000 (0.037) loss 0.9575 (0.7679) acc 78.1250 (82.3958) lr 2.7103e-04 eta 0:01:24
epoch [21/25] batch [35/100] time 0.142 (0.174) data 0.000 (0.032) loss 0.4731 (0.7560) acc 90.6250 (82.4107) lr 2.7103e-04 eta 0:01:20
epoch [21/25] batch [40/100] time 0.142 (0.170) data 0.000 (0.028) loss 0.9521 (0.7609) acc 71.8750 (82.4219) lr 2.7103e-04 eta 0:01:18
epoch [21/25] batch [45/100] time 0.143 (0.167) data 0.000 (0.025) loss 0.6665 (0.7533) acc 90.6250 (82.3611) lr 2.7103e-04 eta 0:01:15
epoch [21/25] batch [50/100] time 0.142 (0.164) data 0.000 (0.023) loss 0.7007 (0.7508) acc 81.2500 (82.2500) lr 2.7103e-04 eta 0:01:13
epoch [21/25] batch [55/100] time 0.143 (0.162) data 0.000 (0.021) loss 0.7944 (0.7491) acc 75.0000 (82.2159) lr 2.7103e-04 eta 0:01:12
epoch [21/25] batch [60/100] time 0.145 (0.161) data 0.000 (0.019) loss 0.6035 (0.7599) acc 81.2500 (81.6146) lr 2.7103e-04 eta 0:01:10
epoch [21/25] batch [65/100] time 0.142 (0.159) data 0.001 (0.017) loss 0.7983 (0.7587) acc 78.1250 (81.6827) lr 2.7103e-04 eta 0:01:09
epoch [21/25] batch [70/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9585 (0.7563) acc 84.3750 (81.7857) lr 2.7103e-04 eta 0:01:08
epoch [21/25] batch [75/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.6367 (0.7549) acc 84.3750 (81.9167) lr 2.7103e-04 eta 0:01:06
epoch [21/25] batch [80/100] time 0.141 (0.156) data 0.000 (0.014) loss 0.7231 (0.7533) acc 78.1250 (81.8359) lr 2.7103e-04 eta 0:01:05
epoch [21/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.7329 (0.7491) acc 78.1250 (81.9485) lr 2.7103e-04 eta 0:01:04
epoch [21/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.7446 (0.7454) acc 78.1250 (81.9444) lr 2.7103e-04 eta 0:01:03
epoch [21/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.6089 (0.7438) acc 87.5000 (82.0066) lr 2.7103e-04 eta 0:01:02
epoch [21/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.012) loss 0.3701 (0.7384) acc 93.7500 (82.2188) lr 1.9098e-04 eta 0:01:01
epoch [22/25] batch [5/100] time 0.146 (0.392) data 0.000 (0.249) loss 0.7905 (0.6669) acc 84.3750 (85.0000) lr 1.9098e-04 eta 0:02:34
epoch [22/25] batch [10/100] time 0.143 (0.267) data 0.000 (0.125) loss 0.9443 (0.6991) acc 75.0000 (83.1250) lr 1.9098e-04 eta 0:01:44
epoch [22/25] batch [15/100] time 0.144 (0.226) data 0.000 (0.083) loss 0.8379 (0.7417) acc 84.3750 (82.9167) lr 1.9098e-04 eta 0:01:26
epoch [22/25] batch [20/100] time 0.142 (0.205) data 0.000 (0.063) loss 0.5366 (0.7091) acc 84.3750 (83.5938) lr 1.9098e-04 eta 0:01:17
epoch [22/25] batch [25/100] time 0.146 (0.193) data 0.000 (0.050) loss 0.8594 (0.7111) acc 81.2500 (83.1250) lr 1.9098e-04 eta 0:01:12
epoch [22/25] batch [30/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.8413 (0.6934) acc 71.8750 (83.3333) lr 1.9098e-04 eta 0:01:08
epoch [22/25] batch [35/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.6704 (0.6971) acc 84.3750 (83.3036) lr 1.9098e-04 eta 0:01:05
epoch [22/25] batch [40/100] time 0.143 (0.174) data 0.000 (0.032) loss 0.5146 (0.7023) acc 87.5000 (83.1250) lr 1.9098e-04 eta 0:01:02
epoch [22/25] batch [45/100] time 0.142 (0.171) data 0.000 (0.028) loss 0.7593 (0.7129) acc 81.2500 (82.6389) lr 1.9098e-04 eta 0:01:00
epoch [22/25] batch [50/100] time 0.143 (0.168) data 0.001 (0.025) loss 0.6934 (0.7100) acc 84.3750 (83.1250) lr 1.9098e-04 eta 0:00:58
epoch [22/25] batch [55/100] time 0.144 (0.166) data 0.000 (0.023) loss 0.6875 (0.7134) acc 84.3750 (83.2955) lr 1.9098e-04 eta 0:00:57
epoch [22/25] batch [60/100] time 0.141 (0.164) data 0.000 (0.021) loss 0.9209 (0.7211) acc 81.2500 (83.1771) lr 1.9098e-04 eta 0:00:55
epoch [22/25] batch [65/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.0771 (0.7212) acc 78.1250 (83.2692) lr 1.9098e-04 eta 0:00:54
epoch [22/25] batch [70/100] time 0.142 (0.161) data 0.001 (0.018) loss 1.0166 (0.7333) acc 78.1250 (82.9018) lr 1.9098e-04 eta 0:00:53
epoch [22/25] batch [75/100] time 0.143 (0.160) data 0.000 (0.017) loss 1.3428 (0.7509) acc 71.8750 (82.6667) lr 1.9098e-04 eta 0:00:51
epoch [22/25] batch [80/100] time 0.143 (0.159) data 0.000 (0.016) loss 0.4524 (0.7498) acc 93.7500 (82.6562) lr 1.9098e-04 eta 0:00:50
epoch [22/25] batch [85/100] time 0.142 (0.158) data 0.001 (0.015) loss 0.5557 (0.7454) acc 90.6250 (82.8309) lr 1.9098e-04 eta 0:00:49
epoch [22/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.014) loss 0.9438 (0.7491) acc 78.1250 (82.6736) lr 1.9098e-04 eta 0:00:48
epoch [22/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.7944 (0.7473) acc 75.0000 (82.7632) lr 1.9098e-04 eta 0:00:47
epoch [22/25] batch [100/100] time 0.124 (0.155) data 0.000 (0.013) loss 1.0352 (0.7519) acc 62.5000 (82.5625) lr 1.2369e-04 eta 0:00:46
epoch [23/25] batch [5/100] time 0.142 (0.391) data 0.000 (0.248) loss 0.6665 (0.6786) acc 84.3750 (83.7500) lr 1.2369e-04 eta 0:01:55
epoch [23/25] batch [10/100] time 0.142 (0.267) data 0.000 (0.124) loss 0.7944 (0.7192) acc 75.0000 (83.1250) lr 1.2369e-04 eta 0:01:17
epoch [23/25] batch [15/100] time 0.142 (0.225) data 0.000 (0.083) loss 0.8940 (0.7361) acc 81.2500 (83.1250) lr 1.2369e-04 eta 0:01:04
epoch [23/25] batch [20/100] time 0.142 (0.205) data 0.000 (0.062) loss 0.7788 (0.7202) acc 84.3750 (83.9062) lr 1.2369e-04 eta 0:00:57
epoch [23/25] batch [25/100] time 0.147 (0.192) data 0.000 (0.050) loss 0.8291 (0.7053) acc 84.3750 (84.5000) lr 1.2369e-04 eta 0:00:52
epoch [23/25] batch [30/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.9937 (0.6960) acc 65.6250 (84.4792) lr 1.2369e-04 eta 0:00:49
epoch [23/25] batch [35/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.6064 (0.6988) acc 84.3750 (84.4643) lr 1.2369e-04 eta 0:00:47
epoch [23/25] batch [40/100] time 0.142 (0.173) data 0.000 (0.031) loss 0.4656 (0.6940) acc 81.2500 (84.3750) lr 1.2369e-04 eta 0:00:45
epoch [23/25] batch [45/100] time 0.143 (0.170) data 0.000 (0.028) loss 0.4995 (0.6913) acc 93.7500 (84.3056) lr 1.2369e-04 eta 0:00:43
epoch [23/25] batch [50/100] time 0.145 (0.167) data 0.000 (0.025) loss 0.7065 (0.7056) acc 87.5000 (84.1875) lr 1.2369e-04 eta 0:00:41
epoch [23/25] batch [55/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6787 (0.7144) acc 84.3750 (84.0909) lr 1.2369e-04 eta 0:00:40
epoch [23/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.7007 (0.7174) acc 78.1250 (83.8021) lr 1.2369e-04 eta 0:00:39
epoch [23/25] batch [65/100] time 0.143 (0.162) data 0.000 (0.019) loss 0.5986 (0.7091) acc 81.2500 (84.2788) lr 1.2369e-04 eta 0:00:37
epoch [23/25] batch [70/100] time 0.143 (0.160) data 0.000 (0.018) loss 0.9126 (0.7248) acc 81.2500 (83.9286) lr 1.2369e-04 eta 0:00:36
epoch [23/25] batch [75/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.6318 (0.7332) acc 84.3750 (83.6250) lr 1.2369e-04 eta 0:00:35
epoch [23/25] batch [80/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.8813 (0.7308) acc 78.1250 (83.5938) lr 1.2369e-04 eta 0:00:34
epoch [23/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.7354 (0.7319) acc 81.2500 (83.5662) lr 1.2369e-04 eta 0:00:33
epoch [23/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.4128 (0.7320) acc 93.7500 (83.4722) lr 1.2369e-04 eta 0:00:32
epoch [23/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.7114 (0.7319) acc 90.6250 (83.6842) lr 1.2369e-04 eta 0:00:31
epoch [23/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.013) loss 1.0742 (0.7346) acc 75.0000 (83.5000) lr 7.0224e-05 eta 0:00:30
epoch [24/25] batch [5/100] time 0.142 (0.349) data 0.000 (0.207) loss 0.6265 (0.8001) acc 78.1250 (81.8750) lr 7.0224e-05 eta 0:01:08
epoch [24/25] batch [10/100] time 0.142 (0.246) data 0.000 (0.104) loss 1.0527 (0.7761) acc 84.3750 (82.8125) lr 7.0224e-05 eta 0:00:46
epoch [24/25] batch [15/100] time 0.144 (0.211) data 0.000 (0.069) loss 0.5557 (0.7494) acc 90.6250 (83.9583) lr 7.0224e-05 eta 0:00:39
epoch [24/25] batch [20/100] time 0.143 (0.194) data 0.000 (0.052) loss 0.6523 (0.7165) acc 78.1250 (84.3750) lr 7.0224e-05 eta 0:00:34
epoch [24/25] batch [25/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.6465 (0.6916) acc 84.3750 (84.6250) lr 7.0224e-05 eta 0:00:32
epoch [24/25] batch [30/100] time 0.142 (0.177) data 0.000 (0.035) loss 0.5513 (0.7017) acc 87.5000 (84.4792) lr 7.0224e-05 eta 0:00:30
epoch [24/25] batch [35/100] time 0.143 (0.172) data 0.000 (0.030) loss 0.5361 (0.7061) acc 90.6250 (84.1071) lr 7.0224e-05 eta 0:00:28
epoch [24/25] batch [40/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.7456 (0.7041) acc 78.1250 (84.1406) lr 7.0224e-05 eta 0:00:26
epoch [24/25] batch [45/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6465 (0.6991) acc 81.2500 (84.0972) lr 7.0224e-05 eta 0:00:25
epoch [24/25] batch [50/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.7266 (0.7007) acc 81.2500 (84.1875) lr 7.0224e-05 eta 0:00:24
epoch [24/25] batch [55/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.7979 (0.7024) acc 81.2500 (84.0909) lr 7.0224e-05 eta 0:00:23
epoch [24/25] batch [60/100] time 0.142 (0.159) data 0.000 (0.018) loss 0.4507 (0.6970) acc 93.7500 (84.2708) lr 7.0224e-05 eta 0:00:22
epoch [24/25] batch [65/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.8438 (0.7151) acc 78.1250 (83.5096) lr 7.0224e-05 eta 0:00:21
epoch [24/25] batch [70/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.6367 (0.7078) acc 84.3750 (83.7946) lr 7.0224e-05 eta 0:00:20
epoch [24/25] batch [75/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.8042 (0.7190) acc 84.3750 (83.5000) lr 7.0224e-05 eta 0:00:19
epoch [24/25] batch [80/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5410 (0.7218) acc 87.5000 (83.3203) lr 7.0224e-05 eta 0:00:18
epoch [24/25] batch [85/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.7695 (0.7182) acc 87.5000 (83.6765) lr 7.0224e-05 eta 0:00:17
epoch [24/25] batch [90/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.5249 (0.7175) acc 87.5000 (83.6806) lr 7.0224e-05 eta 0:00:16
epoch [24/25] batch [95/100] time 0.142 (0.153) data 0.000 (0.011) loss 0.6465 (0.7170) acc 84.3750 (83.6184) lr 7.0224e-05 eta 0:00:16
epoch [24/25] batch [100/100] time 0.124 (0.152) data 0.000 (0.011) loss 0.6831 (0.7274) acc 75.0000 (83.3125) lr 3.1417e-05 eta 0:00:15
epoch [25/25] batch [5/100] time 0.143 (0.434) data 0.000 (0.290) loss 0.6431 (0.8037) acc 84.3750 (80.0000) lr 3.1417e-05 eta 0:00:41
epoch [25/25] batch [10/100] time 0.146 (0.289) data 0.000 (0.145) loss 0.4497 (0.7120) acc 90.6250 (82.1875) lr 3.1417e-05 eta 0:00:25
epoch [25/25] batch [15/100] time 0.142 (0.240) data 0.000 (0.097) loss 1.0215 (0.7273) acc 75.0000 (82.7083) lr 3.1417e-05 eta 0:00:20
epoch [25/25] batch [20/100] time 0.142 (0.215) data 0.000 (0.073) loss 1.0146 (0.7358) acc 78.1250 (82.9688) lr 3.1417e-05 eta 0:00:17
epoch [25/25] batch [25/100] time 0.143 (0.201) data 0.000 (0.058) loss 0.7881 (0.7350) acc 78.1250 (83.1250) lr 3.1417e-05 eta 0:00:15
epoch [25/25] batch [30/100] time 0.143 (0.191) data 0.000 (0.049) loss 0.7178 (0.7241) acc 81.2500 (83.2292) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [35/100] time 0.143 (0.184) data 0.001 (0.042) loss 0.6499 (0.7173) acc 81.2500 (83.2143) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [40/100] time 0.142 (0.179) data 0.000 (0.037) loss 0.8179 (0.7258) acc 75.0000 (82.4219) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [45/100] time 0.146 (0.175) data 0.000 (0.033) loss 0.6772 (0.7289) acc 78.1250 (82.5694) lr 3.1417e-05 eta 0:00:09
epoch [25/25] batch [50/100] time 0.145 (0.172) data 0.000 (0.029) loss 0.7222 (0.7189) acc 87.5000 (83.1875) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [55/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.5566 (0.7130) acc 84.3750 (83.2386) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [60/100] time 0.144 (0.167) data 0.001 (0.025) loss 0.4966 (0.6998) acc 90.6250 (83.8021) lr 3.1417e-05 eta 0:00:06
epoch [25/25] batch [65/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.5669 (0.6876) acc 78.1250 (84.1827) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [70/100] time 0.143 (0.164) data 0.000 (0.021) loss 0.7529 (0.6916) acc 75.0000 (84.0179) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.6191 (0.6953) acc 87.5000 (84.0833) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [80/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.6738 (0.6921) acc 87.5000 (84.2969) lr 3.1417e-05 eta 0:00:03
epoch [25/25] batch [85/100] time 0.142 (0.160) data 0.000 (0.017) loss 0.5088 (0.6977) acc 90.6250 (84.3750) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [90/100] time 0.142 (0.159) data 0.000 (0.016) loss 0.8804 (0.7009) acc 93.7500 (84.5486) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [95/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.5420 (0.6969) acc 90.6250 (84.5395) lr 3.1417e-05 eta 0:00:00
epoch [25/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.6987 (0.7003) acc 93.7500 (84.5625) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:27<08:35, 27.15s/it] 10%|█         | 2/20 [00:28<03:35, 11.99s/it] 15%|█▌        | 3/20 [00:29<01:55,  6.79s/it] 20%|██        | 4/20 [00:29<01:09,  4.35s/it] 25%|██▌       | 5/20 [00:30<00:44,  3.00s/it] 30%|███       | 6/20 [00:30<00:30,  2.18s/it] 35%|███▌      | 7/20 [00:31<00:21,  1.67s/it] 40%|████      | 8/20 [00:32<00:15,  1.33s/it] 45%|████▌     | 9/20 [00:46<00:58,  5.35s/it] 50%|█████     | 10/20 [00:49<00:46,  4.68s/it] 55%|█████▌    | 11/20 [00:51<00:33,  3.70s/it] 60%|██████    | 12/20 [00:51<00:22,  2.76s/it] 65%|██████▌   | 13/20 [00:52<00:14,  2.11s/it] 70%|███████   | 14/20 [00:52<00:09,  1.65s/it] 75%|███████▌  | 15/20 [00:53<00:06,  1.34s/it] 80%|████████  | 16/20 [00:54<00:04,  1.12s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.46s/it] 90%|█████████ | 18/20 [01:08<00:08,  4.15s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.23s/it]100%|██████████| 20/20 [01:10<00:00,  2.45s/it]100%|██████████| 20/20 [01:10<00:00,  3.53s/it]
=> result
* total: 9,950
* correct: 8,234
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 82.6%
Elapsed: 0:08:02
Run this job and save the output to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      796
# test     9,950
---------  ------
['abbey', 'airplane_cabin', 'airport_terminal', 'alley', 'amphitheater', 'amusement_arcade', 'amusement_park', 'anechoic_chamber', 'outdoor apartment_building', 'indoor apse', 'aquarium', 'aqueduct', 'arch', 'archive', 'outdoor arrival_gate', 'art_gallery', 'art_school', 'art_studio', 'assembly_line', 'outdoor athletic_field', 'public atrium', 'attic', 'auditorium', 'auto_factory', 'badlands', 'indoor badminton_court', 'baggage_claim', 'shop bakery', 'exterior balcony', 'interior balcony', 'ball_pit', 'ballroom', 'bamboo_forest', 'banquet_hall', 'bar', 'barn', 'barndoor', 'baseball_field', 'basement', 'basilica', 'outdoor basketball_court', 'bathroom', 'batters_box', 'bayou', 'indoor bazaar', 'outdoor bazaar', 'beach', 'beauty_salon', 'bedroom', 'berth', 'biology_laboratory', 'indoor bistro', 'boardwalk', 'boat_deck', 'boathouse', 'bookstore', 'indoor booth', 'botanical_garden', 'indoor bow_window', 'outdoor bow_window', 'bowling_alley', 'boxing_ring', 'indoor brewery', 'bridge', 'building_facade', 'bullring', 'burial_chamber', 'bus_interior', 'butchers_shop', 'butte', 'outdoor cabin', 'cafeteria', 'campsite', 'campus', 'natural canal', 'urban canal', 'candy_store', 'canyon', 'backseat car_interior', 'frontseat car_interior', 'carrousel', 'indoor casino', 'castle', 'catacomb', 'indoor cathedral', 'outdoor cathedral', 'indoor cavern', 'cemetery', 'chalet', 'cheese_factory', 'chemistry_lab', 'indoor chicken_coop', 'outdoor chicken_coop', 'childs_room', 'indoor church', 'outdoor church', 'classroom', 'clean_room', 'cliff', 'indoor cloister', 'closet', 'clothing_store', 'coast', 'cockpit', 'coffee_shop', 'computer_room', 'conference_center', 'conference_room', 'construction_site', 'control_room', 'outdoor control_tower', 'corn_field', 'corral', 'corridor', 'cottage_garden', 'courthouse', 'courtroom', 'courtyard', 'exterior covered_bridge', 'creek', 'crevasse', 'crosswalk', 'office cubicle', 'dam', 'delicatessen', 'dentists_office', 'sand desert', 'vegetation desert', 'indoor diner', 'outdoor diner', 'home dinette', 'vehicle dinette', 'dining_car', 'dining_room', 'discotheque', 'dock', 'outdoor doorway', 'dorm_room', 'driveway', 'outdoor driving_range', 'drugstore', 'electrical_substation', 'door elevator', 'interior elevator', 'elevator_shaft', 'engine_room', 'indoor escalator', 'excavation', 'indoor factory', 'fairway', 'fastfood_restaurant', 'cultivated field', 'wild field', 'fire_escape', 'fire_station', 'indoor firing_range', 'fishpond', 'indoor florist_shop', 'food_court', 'broadleaf forest', 'needleleaf forest', 'forest_path', 'forest_road', 'formal_garden', 'fountain', 'galley', 'game_room', 'indoor garage', 'garbage_dump', 'gas_station', 'exterior gazebo', 'indoor general_store', 'outdoor general_store', 'gift_shop', 'golf_course', 'indoor greenhouse', 'outdoor greenhouse', 'indoor gymnasium', 'indoor hangar', 'outdoor hangar', 'harbor', 'hayfield', 'heliport', 'herb_garden', 'highway', 'hill', 'home_office', 'hospital', 'hospital_room', 'hot_spring', 'outdoor hot_tub', 'outdoor hotel', 'hotel_room', 'house', 'outdoor hunting_lodge', 'ice_cream_parlor', 'ice_floe', 'ice_shelf', 'indoor ice_skating_rink']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abbey.', 'X X X X airplane cabin.', 'X X X X airport terminal.', 'X X X X alley.', 'X X X X amphitheater.', 'X X X X amusement arcade.', 'X X X X amusement park.', 'X X X X anechoic chamber.', 'X X X X outdoor apartment building.', 'X X X X indoor apse.', 'X X X X aquarium.', 'X X X X aqueduct.', 'X X X X arch.', 'X X X X archive.', 'X X X X outdoor arrival gate.', 'X X X X art gallery.', 'X X X X art school.', 'X X X X art studio.', 'X X X X assembly line.', 'X X X X outdoor athletic field.', 'X X X X public atrium.', 'X X X X attic.', 'X X X X auditorium.', 'X X X X auto factory.', 'X X X X badlands.', 'X X X X indoor badminton court.', 'X X X X baggage claim.', 'X X X X shop bakery.', 'X X X X exterior balcony.', 'X X X X interior balcony.', 'X X X X ball pit.', 'X X X X ballroom.', 'X X X X bamboo forest.', 'X X X X banquet hall.', 'X X X X bar.', 'X X X X barn.', 'X X X X barndoor.', 'X X X X baseball field.', 'X X X X basement.', 'X X X X basilica.', 'X X X X outdoor basketball court.', 'X X X X bathroom.', 'X X X X batters box.', 'X X X X bayou.', 'X X X X indoor bazaar.', 'X X X X outdoor bazaar.', 'X X X X beach.', 'X X X X beauty salon.', 'X X X X bedroom.', 'X X X X berth.', 'X X X X biology laboratory.', 'X X X X indoor bistro.', 'X X X X boardwalk.', 'X X X X boat deck.', 'X X X X boathouse.', 'X X X X bookstore.', 'X X X X indoor booth.', 'X X X X botanical garden.', 'X X X X indoor bow window.', 'X X X X outdoor bow window.', 'X X X X bowling alley.', 'X X X X boxing ring.', 'X X X X indoor brewery.', 'X X X X bridge.', 'X X X X building facade.', 'X X X X bullring.', 'X X X X burial chamber.', 'X X X X bus interior.', 'X X X X butchers shop.', 'X X X X butte.', 'X X X X outdoor cabin.', 'X X X X cafeteria.', 'X X X X campsite.', 'X X X X campus.', 'X X X X natural canal.', 'X X X X urban canal.', 'X X X X candy store.', 'X X X X canyon.', 'X X X X backseat car interior.', 'X X X X frontseat car interior.', 'X X X X carrousel.', 'X X X X indoor casino.', 'X X X X castle.', 'X X X X catacomb.', 'X X X X indoor cathedral.', 'X X X X outdoor cathedral.', 'X X X X indoor cavern.', 'X X X X cemetery.', 'X X X X chalet.', 'X X X X cheese factory.', 'X X X X chemistry lab.', 'X X X X indoor chicken coop.', 'X X X X outdoor chicken coop.', 'X X X X childs room.', 'X X X X indoor church.', 'X X X X outdoor church.', 'X X X X classroom.', 'X X X X clean room.', 'X X X X cliff.', 'X X X X indoor cloister.', 'X X X X closet.', 'X X X X clothing store.', 'X X X X coast.', 'X X X X cockpit.', 'X X X X coffee shop.', 'X X X X computer room.', 'X X X X conference center.', 'X X X X conference room.', 'X X X X construction site.', 'X X X X control room.', 'X X X X outdoor control tower.', 'X X X X corn field.', 'X X X X corral.', 'X X X X corridor.', 'X X X X cottage garden.', 'X X X X courthouse.', 'X X X X courtroom.', 'X X X X courtyard.', 'X X X X exterior covered bridge.', 'X X X X creek.', 'X X X X crevasse.', 'X X X X crosswalk.', 'X X X X office cubicle.', 'X X X X dam.', 'X X X X delicatessen.', 'X X X X dentists office.', 'X X X X sand desert.', 'X X X X vegetation desert.', 'X X X X indoor diner.', 'X X X X outdoor diner.', 'X X X X home dinette.', 'X X X X vehicle dinette.', 'X X X X dining car.', 'X X X X dining room.', 'X X X X discotheque.', 'X X X X dock.', 'X X X X outdoor doorway.', 'X X X X dorm room.', 'X X X X driveway.', 'X X X X outdoor driving range.', 'X X X X drugstore.', 'X X X X electrical substation.', 'X X X X door elevator.', 'X X X X interior elevator.', 'X X X X elevator shaft.', 'X X X X engine room.', 'X X X X indoor escalator.', 'X X X X excavation.', 'X X X X indoor factory.', 'X X X X fairway.', 'X X X X fastfood restaurant.', 'X X X X cultivated field.', 'X X X X wild field.', 'X X X X fire escape.', 'X X X X fire station.', 'X X X X indoor firing range.', 'X X X X fishpond.', 'X X X X indoor florist shop.', 'X X X X food court.', 'X X X X broadleaf forest.', 'X X X X needleleaf forest.', 'X X X X forest path.', 'X X X X forest road.', 'X X X X formal garden.', 'X X X X fountain.', 'X X X X galley.', 'X X X X game room.', 'X X X X indoor garage.', 'X X X X garbage dump.', 'X X X X gas station.', 'X X X X exterior gazebo.', 'X X X X indoor general store.', 'X X X X outdoor general store.', 'X X X X gift shop.', 'X X X X golf course.', 'X X X X indoor greenhouse.', 'X X X X outdoor greenhouse.', 'X X X X indoor gymnasium.', 'X X X X indoor hangar.', 'X X X X outdoor hangar.', 'X X X X harbor.', 'X X X X hayfield.', 'X X X X heliport.', 'X X X X herb garden.', 'X X X X highway.', 'X X X X hill.', 'X X X X home office.', 'X X X X hospital.', 'X X X X hospital room.', 'X X X X hot spring.', 'X X X X outdoor hot tub.', 'X X X X outdoor hotel.', 'X X X X hotel room.', 'X X X X house.', 'X X X X outdoor hunting lodge.', 'X X X X ice cream parlor.', 'X X X X ice floe.', 'X X X X ice shelf.', 'X X X X indoor ice skating rink.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/25] batch [5/100] time 0.141 (0.635) data 0.000 (0.478) loss 3.5215 (3.2508) acc 50.0000 (55.0000) lr 1.0000e-05 eta 0:26:24
epoch [1/25] batch [10/100] time 0.169 (0.407) data 0.029 (0.259) loss 3.6074 (3.3768) acc 40.6250 (51.2500) lr 1.0000e-05 eta 0:16:53
epoch [1/25] batch [15/100] time 0.139 (0.319) data 0.000 (0.173) loss 3.5586 (3.3862) acc 43.7500 (50.8333) lr 1.0000e-05 eta 0:13:11
epoch [1/25] batch [20/100] time 0.140 (0.277) data 0.000 (0.132) loss 2.5820 (3.2519) acc 68.7500 (53.2812) lr 1.0000e-05 eta 0:11:26
epoch [1/25] batch [25/100] time 0.140 (0.249) data 0.000 (0.106) loss 3.1055 (3.2638) acc 56.2500 (52.5000) lr 1.0000e-05 eta 0:10:17
epoch [1/25] batch [30/100] time 0.141 (0.233) data 0.000 (0.090) loss 2.9941 (3.2332) acc 50.0000 (52.6042) lr 1.0000e-05 eta 0:09:35
epoch [1/25] batch [35/100] time 0.140 (0.220) data 0.000 (0.078) loss 3.0625 (3.1669) acc 68.7500 (54.2857) lr 1.0000e-05 eta 0:09:02
epoch [1/25] batch [40/100] time 0.143 (0.245) data 0.000 (0.102) loss 2.2734 (3.1297) acc 71.8750 (55.0781) lr 1.0000e-05 eta 0:10:01
epoch [1/25] batch [45/100] time 0.139 (0.233) data 0.000 (0.091) loss 2.8438 (3.1086) acc 46.8750 (55.2083) lr 1.0000e-05 eta 0:09:31
epoch [1/25] batch [50/100] time 0.140 (0.228) data 0.000 (0.086) loss 3.0742 (3.1163) acc 53.1250 (55.1875) lr 1.0000e-05 eta 0:09:18
epoch [1/25] batch [55/100] time 0.140 (0.225) data 0.000 (0.083) loss 2.8047 (3.0901) acc 53.1250 (55.1705) lr 1.0000e-05 eta 0:09:09
epoch [1/25] batch [60/100] time 0.382 (0.222) data 0.242 (0.080) loss 2.8438 (3.0616) acc 46.8750 (55.2083) lr 1.0000e-05 eta 0:09:00
epoch [1/25] batch [65/100] time 0.373 (0.219) data 0.233 (0.078) loss 2.9434 (3.0414) acc 62.5000 (55.4808) lr 1.0000e-05 eta 0:08:53
epoch [1/25] batch [70/100] time 0.140 (0.223) data 0.000 (0.082) loss 2.4082 (3.0157) acc 62.5000 (55.6250) lr 1.0000e-05 eta 0:09:01
epoch [1/25] batch [75/100] time 0.142 (0.218) data 0.000 (0.076) loss 3.0000 (2.9928) acc 46.8750 (55.7917) lr 1.0000e-05 eta 0:08:47
epoch [1/25] batch [80/100] time 0.141 (0.217) data 0.000 (0.076) loss 2.1191 (2.9737) acc 78.1250 (55.8984) lr 1.0000e-05 eta 0:08:44
epoch [1/25] batch [85/100] time 0.480 (0.216) data 0.339 (0.075) loss 2.6641 (2.9646) acc 59.3750 (55.9559) lr 1.0000e-05 eta 0:08:42
epoch [1/25] batch [90/100] time 0.140 (0.212) data 0.000 (0.071) loss 2.6289 (2.9480) acc 59.3750 (56.2500) lr 1.0000e-05 eta 0:08:31
epoch [1/25] batch [95/100] time 0.140 (0.214) data 0.000 (0.073) loss 2.9961 (2.9303) acc 56.2500 (56.5132) lr 1.0000e-05 eta 0:08:35
epoch [1/25] batch [100/100] time 0.216 (0.212) data 0.000 (0.070) loss 2.6836 (2.9128) acc 56.2500 (56.6875) lr 2.0000e-03 eta 0:08:27
epoch [2/25] batch [5/100] time 0.140 (0.380) data 0.000 (0.239) loss 1.7998 (2.2139) acc 65.6250 (57.5000) lr 2.0000e-03 eta 0:15:09
epoch [2/25] batch [10/100] time 0.140 (0.260) data 0.000 (0.120) loss 1.5547 (1.9327) acc 65.6250 (61.8750) lr 2.0000e-03 eta 0:10:21
epoch [2/25] batch [15/100] time 0.148 (0.222) data 0.000 (0.080) loss 1.4531 (1.8072) acc 68.7500 (63.7500) lr 2.0000e-03 eta 0:08:49
epoch [2/25] batch [20/100] time 0.141 (0.202) data 0.000 (0.060) loss 2.4863 (1.7891) acc 50.0000 (63.4375) lr 2.0000e-03 eta 0:08:00
epoch [2/25] batch [25/100] time 0.141 (0.190) data 0.000 (0.048) loss 1.2871 (1.7184) acc 71.8750 (63.5000) lr 2.0000e-03 eta 0:07:30
epoch [2/25] batch [30/100] time 0.140 (0.181) data 0.000 (0.040) loss 1.2764 (1.6583) acc 56.2500 (63.6458) lr 2.0000e-03 eta 0:07:09
epoch [2/25] batch [35/100] time 0.140 (0.176) data 0.000 (0.034) loss 1.3369 (1.6152) acc 56.2500 (63.2143) lr 2.0000e-03 eta 0:06:55
epoch [2/25] batch [40/100] time 0.141 (0.171) data 0.000 (0.030) loss 0.9707 (1.5624) acc 81.2500 (64.0625) lr 2.0000e-03 eta 0:06:44
epoch [2/25] batch [45/100] time 0.141 (0.168) data 0.000 (0.027) loss 1.2139 (1.5378) acc 68.7500 (64.3056) lr 2.0000e-03 eta 0:06:35
epoch [2/25] batch [50/100] time 0.140 (0.165) data 0.000 (0.024) loss 1.1016 (1.5079) acc 78.1250 (64.8125) lr 2.0000e-03 eta 0:06:28
epoch [2/25] batch [55/100] time 0.142 (0.163) data 0.000 (0.022) loss 1.1445 (1.4836) acc 62.5000 (65.0000) lr 2.0000e-03 eta 0:06:22
epoch [2/25] batch [60/100] time 0.141 (0.161) data 0.001 (0.020) loss 1.6299 (1.4752) acc 53.1250 (64.9479) lr 2.0000e-03 eta 0:06:17
epoch [2/25] batch [65/100] time 0.141 (0.160) data 0.000 (0.019) loss 1.3760 (1.4582) acc 71.8750 (65.2404) lr 2.0000e-03 eta 0:06:12
epoch [2/25] batch [70/100] time 0.144 (0.158) data 0.000 (0.017) loss 1.3066 (1.4485) acc 68.7500 (65.4911) lr 2.0000e-03 eta 0:06:09
epoch [2/25] batch [75/100] time 0.143 (0.157) data 0.000 (0.016) loss 1.0537 (1.4311) acc 78.1250 (65.9167) lr 2.0000e-03 eta 0:06:05
epoch [2/25] batch [80/100] time 0.141 (0.156) data 0.000 (0.015) loss 0.8770 (1.4100) acc 84.3750 (66.5625) lr 2.0000e-03 eta 0:06:02
epoch [2/25] batch [85/100] time 0.141 (0.155) data 0.000 (0.014) loss 1.2979 (1.3951) acc 65.6250 (66.7647) lr 2.0000e-03 eta 0:05:59
epoch [2/25] batch [90/100] time 0.141 (0.155) data 0.000 (0.014) loss 1.6650 (1.3954) acc 65.6250 (66.5278) lr 2.0000e-03 eta 0:05:56
epoch [2/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.013) loss 1.3008 (1.3882) acc 71.8750 (66.7105) lr 2.0000e-03 eta 0:05:54
epoch [2/25] batch [100/100] time 0.123 (0.153) data 0.000 (0.012) loss 1.2910 (1.3776) acc 68.7500 (67.0000) lr 1.9921e-03 eta 0:05:51
epoch [3/25] batch [5/100] time 0.141 (0.400) data 0.000 (0.259) loss 1.2891 (1.2816) acc 62.5000 (62.5000) lr 1.9921e-03 eta 0:15:17
epoch [3/25] batch [10/100] time 0.141 (0.270) data 0.000 (0.129) loss 1.0527 (1.1780) acc 71.8750 (67.8125) lr 1.9921e-03 eta 0:10:19
epoch [3/25] batch [15/100] time 0.141 (0.227) data 0.000 (0.086) loss 1.1279 (1.1860) acc 71.8750 (68.9583) lr 1.9921e-03 eta 0:08:39
epoch [3/25] batch [20/100] time 0.141 (0.206) data 0.000 (0.065) loss 0.7124 (1.1588) acc 78.1250 (69.8438) lr 1.9921e-03 eta 0:07:49
epoch [3/25] batch [25/100] time 0.141 (0.193) data 0.000 (0.052) loss 1.3213 (1.1709) acc 78.1250 (69.8750) lr 1.9921e-03 eta 0:07:18
epoch [3/25] batch [30/100] time 0.141 (0.184) data 0.000 (0.043) loss 0.8140 (1.1454) acc 75.0000 (70.6250) lr 1.9921e-03 eta 0:06:58
epoch [3/25] batch [35/100] time 0.141 (0.178) data 0.000 (0.037) loss 1.0420 (1.1500) acc 71.8750 (70.6250) lr 1.9921e-03 eta 0:06:43
epoch [3/25] batch [40/100] time 0.141 (0.173) data 0.000 (0.033) loss 1.0801 (1.1473) acc 75.0000 (70.5469) lr 1.9921e-03 eta 0:06:31
epoch [3/25] batch [45/100] time 0.141 (0.170) data 0.000 (0.029) loss 1.0742 (1.1448) acc 65.6250 (70.6250) lr 1.9921e-03 eta 0:06:23
epoch [3/25] batch [50/100] time 0.141 (0.167) data 0.000 (0.026) loss 1.3076 (1.1507) acc 59.3750 (70.5625) lr 1.9921e-03 eta 0:06:15
epoch [3/25] batch [55/100] time 0.142 (0.165) data 0.000 (0.024) loss 0.9648 (1.1413) acc 71.8750 (70.5682) lr 1.9921e-03 eta 0:06:09
epoch [3/25] batch [60/100] time 0.141 (0.163) data 0.000 (0.022) loss 1.3965 (1.1383) acc 65.6250 (70.7292) lr 1.9921e-03 eta 0:06:04
epoch [3/25] batch [65/100] time 0.141 (0.161) data 0.000 (0.020) loss 0.5176 (1.1397) acc 87.5000 (70.5288) lr 1.9921e-03 eta 0:06:00
epoch [3/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.019) loss 0.7891 (1.1354) acc 75.0000 (70.6696) lr 1.9921e-03 eta 0:05:56
epoch [3/25] batch [75/100] time 0.141 (0.159) data 0.000 (0.018) loss 0.8740 (1.1297) acc 71.8750 (70.7917) lr 1.9921e-03 eta 0:05:52
epoch [3/25] batch [80/100] time 0.141 (0.157) data 0.000 (0.016) loss 1.2588 (1.1487) acc 71.8750 (70.4688) lr 1.9921e-03 eta 0:05:49
epoch [3/25] batch [85/100] time 0.141 (0.157) data 0.000 (0.015) loss 1.5469 (1.1513) acc 68.7500 (70.4779) lr 1.9921e-03 eta 0:05:46
epoch [3/25] batch [90/100] time 0.141 (0.156) data 0.000 (0.015) loss 1.2109 (1.1483) acc 71.8750 (70.6250) lr 1.9921e-03 eta 0:05:44
epoch [3/25] batch [95/100] time 0.142 (0.155) data 0.000 (0.014) loss 1.3564 (1.1412) acc 62.5000 (70.7566) lr 1.9921e-03 eta 0:05:41
epoch [3/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.013) loss 1.1445 (1.1323) acc 75.0000 (70.9688) lr 1.9686e-03 eta 0:05:38
epoch [4/25] batch [5/100] time 0.141 (0.388) data 0.000 (0.247) loss 1.3906 (1.1537) acc 68.7500 (70.6250) lr 1.9686e-03 eta 0:14:12
epoch [4/25] batch [10/100] time 0.142 (0.265) data 0.000 (0.124) loss 1.0791 (1.1771) acc 81.2500 (69.6875) lr 1.9686e-03 eta 0:09:41
epoch [4/25] batch [15/100] time 0.141 (0.224) data 0.000 (0.083) loss 0.8301 (1.1412) acc 81.2500 (69.5833) lr 1.9686e-03 eta 0:08:09
epoch [4/25] batch [20/100] time 0.141 (0.203) data 0.000 (0.062) loss 0.9634 (1.0771) acc 71.8750 (72.1875) lr 1.9686e-03 eta 0:07:23
epoch [4/25] batch [25/100] time 0.141 (0.191) data 0.000 (0.050) loss 1.1152 (1.0787) acc 71.8750 (71.6250) lr 1.9686e-03 eta 0:06:55
epoch [4/25] batch [30/100] time 0.141 (0.183) data 0.000 (0.042) loss 1.1699 (1.0833) acc 65.6250 (71.2500) lr 1.9686e-03 eta 0:06:36
epoch [4/25] batch [35/100] time 0.141 (0.177) data 0.000 (0.036) loss 0.6914 (1.1032) acc 75.0000 (71.0714) lr 1.9686e-03 eta 0:06:23
epoch [4/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.031) loss 0.5581 (1.0734) acc 87.5000 (71.7969) lr 1.9686e-03 eta 0:06:12
epoch [4/25] batch [45/100] time 0.141 (0.169) data 0.000 (0.028) loss 1.2109 (1.0749) acc 65.6250 (71.8750) lr 1.9686e-03 eta 0:06:04
epoch [4/25] batch [50/100] time 0.141 (0.166) data 0.000 (0.025) loss 1.4951 (1.0850) acc 46.8750 (71.3125) lr 1.9686e-03 eta 0:05:57
epoch [4/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.023) loss 0.9902 (1.0915) acc 68.7500 (71.4773) lr 1.9686e-03 eta 0:05:52
epoch [4/25] batch [60/100] time 0.141 (0.162) data 0.000 (0.021) loss 0.9839 (1.0980) acc 78.1250 (71.5625) lr 1.9686e-03 eta 0:05:47
epoch [4/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.019) loss 1.3408 (1.0860) acc 71.8750 (71.9231) lr 1.9686e-03 eta 0:05:43
epoch [4/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.018) loss 1.2422 (1.0926) acc 71.8750 (71.6518) lr 1.9686e-03 eta 0:05:39
epoch [4/25] batch [75/100] time 0.142 (0.158) data 0.001 (0.017) loss 0.9473 (1.0891) acc 75.0000 (71.8333) lr 1.9686e-03 eta 0:05:36
epoch [4/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.016) loss 1.1123 (1.0859) acc 75.0000 (71.9531) lr 1.9686e-03 eta 0:05:33
epoch [4/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.015) loss 0.8911 (1.0820) acc 78.1250 (71.9853) lr 1.9686e-03 eta 0:05:30
epoch [4/25] batch [90/100] time 0.141 (0.155) data 0.000 (0.014) loss 1.4023 (1.0886) acc 68.7500 (71.9097) lr 1.9686e-03 eta 0:05:27
epoch [4/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 1.1709 (1.0808) acc 75.0000 (72.2368) lr 1.9686e-03 eta 0:05:25
epoch [4/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.013) loss 1.4082 (1.0795) acc 75.0000 (72.4688) lr 1.9298e-03 eta 0:05:23
epoch [5/25] batch [5/100] time 0.142 (0.410) data 0.000 (0.268) loss 0.8320 (0.8686) acc 78.1250 (78.7500) lr 1.9298e-03 eta 0:14:19
epoch [5/25] batch [10/100] time 0.151 (0.277) data 0.000 (0.134) loss 1.1367 (0.9616) acc 68.7500 (76.5625) lr 1.9298e-03 eta 0:09:39
epoch [5/25] batch [15/100] time 0.142 (0.232) data 0.000 (0.089) loss 0.8906 (0.9859) acc 84.3750 (76.0417) lr 1.9298e-03 eta 0:08:03
epoch [5/25] batch [20/100] time 0.144 (0.209) data 0.000 (0.067) loss 1.1680 (1.0053) acc 68.7500 (75.4688) lr 1.9298e-03 eta 0:07:15
epoch [5/25] batch [25/100] time 0.142 (0.196) data 0.000 (0.054) loss 1.0137 (1.0050) acc 75.0000 (74.6250) lr 1.9298e-03 eta 0:06:46
epoch [5/25] batch [30/100] time 0.142 (0.187) data 0.000 (0.045) loss 0.9360 (1.0279) acc 71.8750 (73.3333) lr 1.9298e-03 eta 0:06:27
epoch [5/25] batch [35/100] time 0.142 (0.181) data 0.000 (0.039) loss 1.1621 (1.0294) acc 71.8750 (73.7500) lr 1.9298e-03 eta 0:06:13
epoch [5/25] batch [40/100] time 0.141 (0.176) data 0.000 (0.034) loss 0.6128 (1.0263) acc 78.1250 (74.0625) lr 1.9298e-03 eta 0:06:02
epoch [5/25] batch [45/100] time 0.151 (0.172) data 0.001 (0.030) loss 0.7930 (1.0297) acc 75.0000 (73.6111) lr 1.9298e-03 eta 0:05:54
epoch [5/25] batch [50/100] time 0.143 (0.169) data 0.000 (0.027) loss 1.0068 (1.0296) acc 71.8750 (73.6250) lr 1.9298e-03 eta 0:05:47
epoch [5/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.7563 (1.0262) acc 81.2500 (73.5227) lr 1.9298e-03 eta 0:05:41
epoch [5/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.9209 (1.0146) acc 71.8750 (73.9583) lr 1.9298e-03 eta 0:05:36
epoch [5/25] batch [65/100] time 0.141 (0.163) data 0.000 (0.021) loss 0.8843 (1.0137) acc 81.2500 (74.0865) lr 1.9298e-03 eta 0:05:31
epoch [5/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.019) loss 0.6919 (1.0111) acc 75.0000 (74.1071) lr 1.9298e-03 eta 0:05:28
epoch [5/25] batch [75/100] time 0.142 (0.160) data 0.000 (0.018) loss 1.0391 (1.0101) acc 62.5000 (74.0000) lr 1.9298e-03 eta 0:05:24
epoch [5/25] batch [80/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.8306 (1.0177) acc 78.1250 (73.8672) lr 1.9298e-03 eta 0:05:21
epoch [5/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9868 (1.0160) acc 75.0000 (73.6765) lr 1.9298e-03 eta 0:05:19
epoch [5/25] batch [90/100] time 0.143 (0.157) data 0.000 (0.015) loss 1.0459 (1.0107) acc 71.8750 (73.7500) lr 1.9298e-03 eta 0:05:16
epoch [5/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.014) loss 1.0684 (1.0115) acc 78.1250 (73.6842) lr 1.9298e-03 eta 0:05:14
epoch [5/25] batch [100/100] time 0.125 (0.156) data 0.000 (0.014) loss 0.9849 (1.0080) acc 68.7500 (73.7812) lr 1.8763e-03 eta 0:05:11
epoch [6/25] batch [5/100] time 0.143 (0.379) data 0.000 (0.236) loss 0.9570 (0.9338) acc 71.8750 (76.8750) lr 1.8763e-03 eta 0:12:36
epoch [6/25] batch [10/100] time 0.145 (0.261) data 0.000 (0.118) loss 0.9985 (0.9079) acc 68.7500 (74.3750) lr 1.8763e-03 eta 0:08:40
epoch [6/25] batch [15/100] time 0.143 (0.222) data 0.000 (0.079) loss 1.4434 (0.9612) acc 68.7500 (73.9583) lr 1.8763e-03 eta 0:07:19
epoch [6/25] batch [20/100] time 0.142 (0.202) data 0.000 (0.059) loss 0.8735 (0.9452) acc 87.5000 (75.0000) lr 1.8763e-03 eta 0:06:39
epoch [6/25] batch [25/100] time 0.142 (0.190) data 0.000 (0.047) loss 0.6792 (0.9253) acc 81.2500 (75.5000) lr 1.8763e-03 eta 0:06:14
epoch [6/25] batch [30/100] time 0.142 (0.182) data 0.000 (0.040) loss 0.8848 (0.9295) acc 75.0000 (75.6250) lr 1.8763e-03 eta 0:05:58
epoch [6/25] batch [35/100] time 0.142 (0.176) data 0.000 (0.034) loss 0.6831 (0.9317) acc 87.5000 (76.3393) lr 1.8763e-03 eta 0:05:46
epoch [6/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 0.9131 (0.9415) acc 62.5000 (76.3281) lr 1.8763e-03 eta 0:05:37
epoch [6/25] batch [45/100] time 0.143 (0.169) data 0.000 (0.027) loss 1.2295 (0.9681) acc 75.0000 (75.7639) lr 1.8763e-03 eta 0:05:30
epoch [6/25] batch [50/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.4810 (0.9588) acc 90.6250 (75.8125) lr 1.8763e-03 eta 0:05:24
epoch [6/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.6680 (0.9590) acc 87.5000 (75.9659) lr 1.8763e-03 eta 0:05:19
epoch [6/25] batch [60/100] time 0.143 (0.162) data 0.000 (0.020) loss 1.0059 (0.9665) acc 68.7500 (75.7292) lr 1.8763e-03 eta 0:05:14
epoch [6/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.7612 (0.9709) acc 81.2500 (75.8173) lr 1.8763e-03 eta 0:05:11
epoch [6/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.9961 (0.9879) acc 71.8750 (75.4018) lr 1.8763e-03 eta 0:05:07
epoch [6/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.4863 (0.9799) acc 65.6250 (75.6667) lr 1.8763e-03 eta 0:05:04
epoch [6/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.1445 (0.9875) acc 68.7500 (75.4688) lr 1.8763e-03 eta 0:05:01
epoch [6/25] batch [85/100] time 0.143 (0.156) data 0.000 (0.014) loss 0.9307 (0.9878) acc 75.0000 (75.2941) lr 1.8763e-03 eta 0:04:59
epoch [6/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.013) loss 1.0391 (0.9913) acc 65.6250 (75.2431) lr 1.8763e-03 eta 0:04:57
epoch [6/25] batch [95/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.8013 (0.9948) acc 84.3750 (75.0329) lr 1.8763e-03 eta 0:04:54
epoch [6/25] batch [100/100] time 0.125 (0.154) data 0.000 (0.012) loss 1.0088 (0.9932) acc 68.7500 (74.9062) lr 1.8090e-03 eta 0:04:52
epoch [7/25] batch [5/100] time 0.142 (0.434) data 0.000 (0.292) loss 0.9380 (1.0262) acc 81.2500 (76.8750) lr 1.8090e-03 eta 0:13:43
epoch [7/25] batch [10/100] time 0.142 (0.289) data 0.000 (0.146) loss 0.9668 (0.9788) acc 78.1250 (75.3125) lr 1.8090e-03 eta 0:09:05
epoch [7/25] batch [15/100] time 0.142 (0.240) data 0.000 (0.098) loss 1.1211 (1.0014) acc 71.8750 (74.5833) lr 1.8090e-03 eta 0:07:31
epoch [7/25] batch [20/100] time 0.142 (0.215) data 0.000 (0.073) loss 1.0840 (1.0192) acc 68.7500 (73.9062) lr 1.8090e-03 eta 0:06:44
epoch [7/25] batch [25/100] time 0.142 (0.200) data 0.000 (0.059) loss 1.2285 (1.0198) acc 75.0000 (74.1250) lr 1.8090e-03 eta 0:06:15
epoch [7/25] batch [30/100] time 0.142 (0.191) data 0.001 (0.049) loss 1.0449 (1.0194) acc 78.1250 (74.3750) lr 1.8090e-03 eta 0:05:56
epoch [7/25] batch [35/100] time 0.142 (0.184) data 0.000 (0.042) loss 1.2588 (1.0337) acc 65.6250 (73.6607) lr 1.8090e-03 eta 0:05:42
epoch [7/25] batch [40/100] time 0.141 (0.178) data 0.000 (0.037) loss 0.7954 (1.0436) acc 75.0000 (73.3594) lr 1.8090e-03 eta 0:05:31
epoch [7/25] batch [45/100] time 0.142 (0.174) data 0.000 (0.033) loss 0.8203 (1.0365) acc 81.2500 (73.4722) lr 1.8090e-03 eta 0:05:23
epoch [7/25] batch [50/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.9214 (1.0174) acc 75.0000 (74.0625) lr 1.8090e-03 eta 0:05:16
epoch [7/25] batch [55/100] time 0.141 (0.169) data 0.000 (0.027) loss 0.7104 (1.0039) acc 81.2500 (74.2614) lr 1.8090e-03 eta 0:05:10
epoch [7/25] batch [60/100] time 0.141 (0.166) data 0.000 (0.025) loss 0.7305 (1.0076) acc 81.2500 (74.1667) lr 1.8090e-03 eta 0:05:05
epoch [7/25] batch [65/100] time 0.141 (0.164) data 0.000 (0.023) loss 0.8350 (1.0097) acc 78.1250 (74.0865) lr 1.8090e-03 eta 0:05:01
epoch [7/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 1.3398 (1.0091) acc 68.7500 (74.4643) lr 1.8090e-03 eta 0:04:57
epoch [7/25] batch [75/100] time 0.142 (0.161) data 0.000 (0.020) loss 1.4580 (1.0103) acc 78.1250 (74.5000) lr 1.8090e-03 eta 0:04:54
epoch [7/25] batch [80/100] time 0.141 (0.160) data 0.000 (0.019) loss 0.7090 (0.9973) acc 87.5000 (74.8047) lr 1.8090e-03 eta 0:04:51
epoch [7/25] batch [85/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.7148 (0.9903) acc 84.3750 (74.9632) lr 1.8090e-03 eta 0:04:48
epoch [7/25] batch [90/100] time 0.143 (0.158) data 0.000 (0.017) loss 1.1387 (0.9917) acc 65.6250 (75.0000) lr 1.8090e-03 eta 0:04:46
epoch [7/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.016) loss 0.8569 (0.9932) acc 81.2500 (75.0000) lr 1.8090e-03 eta 0:04:43
epoch [7/25] batch [100/100] time 0.123 (0.156) data 0.000 (0.015) loss 1.0664 (0.9910) acc 68.7500 (75.0625) lr 1.7290e-03 eta 0:04:41
epoch [8/25] batch [5/100] time 0.143 (0.380) data 0.000 (0.237) loss 1.0850 (0.9404) acc 71.8750 (76.2500) lr 1.7290e-03 eta 0:11:21
epoch [8/25] batch [10/100] time 0.145 (0.261) data 0.000 (0.119) loss 1.4678 (0.9629) acc 68.7500 (76.5625) lr 1.7290e-03 eta 0:07:47
epoch [8/25] batch [15/100] time 0.143 (0.222) data 0.000 (0.079) loss 0.8169 (0.9428) acc 75.0000 (76.4583) lr 1.7290e-03 eta 0:06:36
epoch [8/25] batch [20/100] time 0.141 (0.202) data 0.000 (0.060) loss 0.7939 (0.9812) acc 78.1250 (75.9375) lr 1.7290e-03 eta 0:05:59
epoch [8/25] batch [25/100] time 0.145 (0.190) data 0.001 (0.048) loss 0.9741 (0.9716) acc 71.8750 (76.0000) lr 1.7290e-03 eta 0:05:37
epoch [8/25] batch [30/100] time 0.141 (0.182) data 0.000 (0.040) loss 0.6602 (0.9747) acc 81.2500 (75.8333) lr 1.7290e-03 eta 0:05:22
epoch [8/25] batch [35/100] time 0.143 (0.177) data 0.000 (0.034) loss 0.7246 (0.9531) acc 81.2500 (76.4286) lr 1.7290e-03 eta 0:05:11
epoch [8/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 1.5156 (0.9537) acc 53.1250 (76.4062) lr 1.7290e-03 eta 0:05:02
epoch [8/25] batch [45/100] time 0.142 (0.169) data 0.000 (0.027) loss 1.1377 (0.9661) acc 68.7500 (76.1111) lr 1.7290e-03 eta 0:04:56
epoch [8/25] batch [50/100] time 0.143 (0.166) data 0.000 (0.024) loss 0.4907 (0.9573) acc 84.3750 (76.3125) lr 1.7290e-03 eta 0:04:50
epoch [8/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.2275 (0.9601) acc 62.5000 (75.8523) lr 1.7290e-03 eta 0:04:46
epoch [8/25] batch [60/100] time 0.143 (0.162) data 0.000 (0.020) loss 0.9648 (0.9769) acc 78.1250 (75.7292) lr 1.7290e-03 eta 0:04:42
epoch [8/25] batch [65/100] time 0.145 (0.161) data 0.000 (0.019) loss 0.7666 (0.9680) acc 84.3750 (76.0096) lr 1.7290e-03 eta 0:04:38
epoch [8/25] batch [70/100] time 0.141 (0.159) data 0.000 (0.017) loss 0.7754 (0.9551) acc 81.2500 (76.1161) lr 1.7290e-03 eta 0:04:35
epoch [8/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9287 (0.9478) acc 68.7500 (76.1667) lr 1.7290e-03 eta 0:04:32
epoch [8/25] batch [80/100] time 0.142 (0.157) data 0.001 (0.015) loss 1.3545 (0.9487) acc 62.5000 (76.2891) lr 1.7290e-03 eta 0:04:30
epoch [8/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.0293 (0.9502) acc 71.8750 (76.3603) lr 1.7290e-03 eta 0:04:27
epoch [8/25] batch [90/100] time 0.143 (0.155) data 0.000 (0.014) loss 0.8853 (0.9504) acc 81.2500 (76.2847) lr 1.7290e-03 eta 0:04:25
epoch [8/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.8408 (0.9653) acc 78.1250 (75.9211) lr 1.7290e-03 eta 0:04:23
epoch [8/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.012) loss 0.6982 (0.9652) acc 75.0000 (75.8125) lr 1.6374e-03 eta 0:04:21
epoch [9/25] batch [5/100] time 0.142 (0.395) data 0.000 (0.251) loss 0.9375 (0.7919) acc 71.8750 (79.3750) lr 1.6374e-03 eta 0:11:09
epoch [9/25] batch [10/100] time 0.143 (0.269) data 0.000 (0.126) loss 0.5674 (0.7532) acc 84.3750 (79.0625) lr 1.6374e-03 eta 0:07:34
epoch [9/25] batch [15/100] time 0.143 (0.227) data 0.000 (0.084) loss 0.8848 (0.7890) acc 81.2500 (78.5417) lr 1.6374e-03 eta 0:06:21
epoch [9/25] batch [20/100] time 0.141 (0.205) data 0.000 (0.063) loss 0.7144 (0.8256) acc 84.3750 (77.0312) lr 1.6374e-03 eta 0:05:45
epoch [9/25] batch [25/100] time 0.143 (0.193) data 0.000 (0.051) loss 1.0156 (0.8786) acc 84.3750 (76.3750) lr 1.6374e-03 eta 0:05:22
epoch [9/25] batch [30/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.9702 (0.9243) acc 68.7500 (75.4167) lr 1.6374e-03 eta 0:05:07
epoch [9/25] batch [35/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.7197 (0.9011) acc 75.0000 (76.1607) lr 1.6374e-03 eta 0:04:56
epoch [9/25] batch [40/100] time 0.142 (0.174) data 0.000 (0.032) loss 0.9409 (0.9029) acc 78.1250 (76.7188) lr 1.6374e-03 eta 0:04:48
epoch [9/25] batch [45/100] time 0.142 (0.170) data 0.000 (0.028) loss 0.7598 (0.8906) acc 87.5000 (77.3611) lr 1.6374e-03 eta 0:04:41
epoch [9/25] batch [50/100] time 0.141 (0.167) data 0.000 (0.025) loss 1.0664 (0.8688) acc 71.8750 (78.3125) lr 1.6374e-03 eta 0:04:36
epoch [9/25] batch [55/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.9028 (0.8876) acc 75.0000 (77.6136) lr 1.6374e-03 eta 0:04:31
epoch [9/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.021) loss 1.1191 (0.8802) acc 68.7500 (78.0208) lr 1.6374e-03 eta 0:04:27
epoch [9/25] batch [65/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.7676 (0.8778) acc 68.7500 (77.8846) lr 1.6374e-03 eta 0:04:24
epoch [9/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.018) loss 1.2871 (0.8859) acc 68.7500 (77.7679) lr 1.6374e-03 eta 0:04:21
epoch [9/25] batch [75/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.7642 (0.8906) acc 81.2500 (77.7917) lr 1.6374e-03 eta 0:04:18
epoch [9/25] batch [80/100] time 0.144 (0.158) data 0.000 (0.016) loss 1.1309 (0.8980) acc 59.3750 (77.5781) lr 1.6374e-03 eta 0:04:15
epoch [9/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.0654 (0.9042) acc 68.7500 (77.4265) lr 1.6374e-03 eta 0:04:13
epoch [9/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.0254 (0.9049) acc 59.3750 (77.3611) lr 1.6374e-03 eta 0:04:11
epoch [9/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.014) loss 1.2881 (0.9104) acc 65.6250 (77.3026) lr 1.6374e-03 eta 0:04:09
epoch [9/25] batch [100/100] time 0.123 (0.155) data 0.000 (0.013) loss 1.0762 (0.9115) acc 68.7500 (77.2500) lr 1.5358e-03 eta 0:04:07
epoch [10/25] batch [5/100] time 0.141 (0.350) data 0.000 (0.209) loss 1.0996 (1.0060) acc 75.0000 (78.1250) lr 1.5358e-03 eta 0:09:18
epoch [10/25] batch [10/100] time 0.141 (0.246) data 0.000 (0.104) loss 1.0088 (0.9579) acc 75.0000 (78.1250) lr 1.5358e-03 eta 0:06:31
epoch [10/25] batch [15/100] time 0.142 (0.211) data 0.000 (0.070) loss 1.1475 (0.9805) acc 75.0000 (76.4583) lr 1.5358e-03 eta 0:05:35
epoch [10/25] batch [20/100] time 0.141 (0.194) data 0.000 (0.052) loss 0.9546 (0.9357) acc 71.8750 (77.0312) lr 1.5358e-03 eta 0:05:06
epoch [10/25] batch [25/100] time 0.141 (0.183) data 0.000 (0.042) loss 0.7817 (0.9302) acc 81.2500 (76.8750) lr 1.5358e-03 eta 0:04:48
epoch [10/25] batch [30/100] time 0.145 (0.177) data 0.000 (0.035) loss 1.0898 (0.9131) acc 78.1250 (77.5000) lr 1.5358e-03 eta 0:04:37
epoch [10/25] batch [35/100] time 0.141 (0.172) data 0.000 (0.030) loss 0.7646 (0.9228) acc 84.3750 (77.7679) lr 1.5358e-03 eta 0:04:28
epoch [10/25] batch [40/100] time 0.141 (0.168) data 0.000 (0.026) loss 0.9800 (0.9205) acc 75.0000 (77.5781) lr 1.5358e-03 eta 0:04:21
epoch [10/25] batch [45/100] time 0.142 (0.165) data 0.000 (0.024) loss 0.8008 (0.9258) acc 84.3750 (77.4306) lr 1.5358e-03 eta 0:04:16
epoch [10/25] batch [50/100] time 0.141 (0.163) data 0.000 (0.021) loss 0.7114 (0.9026) acc 84.3750 (78.3125) lr 1.5358e-03 eta 0:04:12
epoch [10/25] batch [55/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.8472 (0.9044) acc 84.3750 (78.2386) lr 1.5358e-03 eta 0:04:08
epoch [10/25] batch [60/100] time 0.141 (0.159) data 0.000 (0.018) loss 1.1943 (0.9050) acc 78.1250 (78.3854) lr 1.5358e-03 eta 0:04:05
epoch [10/25] batch [65/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.6719 (0.9199) acc 84.3750 (78.0288) lr 1.5358e-03 eta 0:04:02
epoch [10/25] batch [70/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.7593 (0.9129) acc 84.3750 (78.3482) lr 1.5358e-03 eta 0:03:59
epoch [10/25] batch [75/100] time 0.143 (0.156) data 0.000 (0.014) loss 0.7632 (0.9112) acc 81.2500 (78.2917) lr 1.5358e-03 eta 0:03:57
epoch [10/25] batch [80/100] time 0.141 (0.155) data 0.000 (0.013) loss 1.0391 (0.9097) acc 68.7500 (78.2422) lr 1.5358e-03 eta 0:03:55
epoch [10/25] batch [85/100] time 0.141 (0.154) data 0.000 (0.013) loss 1.1387 (0.9143) acc 71.8750 (78.1985) lr 1.5358e-03 eta 0:03:53
epoch [10/25] batch [90/100] time 0.141 (0.153) data 0.000 (0.012) loss 0.6333 (0.9074) acc 87.5000 (78.4028) lr 1.5358e-03 eta 0:03:51
epoch [10/25] batch [95/100] time 0.141 (0.153) data 0.000 (0.011) loss 0.7549 (0.9005) acc 81.2500 (78.6513) lr 1.5358e-03 eta 0:03:49
epoch [10/25] batch [100/100] time 0.123 (0.152) data 0.000 (0.011) loss 1.1484 (0.9040) acc 68.7500 (78.4062) lr 1.4258e-03 eta 0:03:47
epoch [11/25] batch [5/100] time 0.142 (0.383) data 0.000 (0.240) loss 0.6392 (0.9073) acc 93.7500 (77.5000) lr 1.4258e-03 eta 0:09:32
epoch [11/25] batch [10/100] time 0.145 (0.263) data 0.001 (0.120) loss 0.9648 (0.8902) acc 81.2500 (80.0000) lr 1.4258e-03 eta 0:06:32
epoch [11/25] batch [15/100] time 0.141 (0.223) data 0.000 (0.080) loss 0.6987 (0.8928) acc 81.2500 (79.3750) lr 1.4258e-03 eta 0:05:31
epoch [11/25] batch [20/100] time 0.141 (0.203) data 0.000 (0.060) loss 0.8877 (0.8807) acc 75.0000 (79.2188) lr 1.4258e-03 eta 0:04:59
epoch [11/25] batch [25/100] time 0.142 (0.191) data 0.000 (0.048) loss 0.7363 (0.8788) acc 84.3750 (79.3750) lr 1.4258e-03 eta 0:04:41
epoch [11/25] batch [30/100] time 0.141 (0.182) data 0.000 (0.040) loss 0.7676 (0.8783) acc 78.1250 (79.3750) lr 1.4258e-03 eta 0:04:27
epoch [11/25] batch [35/100] time 0.141 (0.177) data 0.000 (0.035) loss 0.5537 (0.8819) acc 93.7500 (79.0179) lr 1.4258e-03 eta 0:04:18
epoch [11/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 0.8584 (0.8607) acc 81.2500 (79.5312) lr 1.4258e-03 eta 0:04:11
epoch [11/25] batch [45/100] time 0.143 (0.169) data 0.000 (0.027) loss 1.1797 (0.8893) acc 78.1250 (78.8889) lr 1.4258e-03 eta 0:04:05
epoch [11/25] batch [50/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.7007 (0.8751) acc 81.2500 (79.6875) lr 1.4258e-03 eta 0:04:00
epoch [11/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.4993 (0.8659) acc 84.3750 (79.3182) lr 1.4258e-03 eta 0:03:56
epoch [11/25] batch [60/100] time 0.144 (0.162) data 0.001 (0.020) loss 0.8696 (0.8575) acc 78.1250 (79.6354) lr 1.4258e-03 eta 0:03:53
epoch [11/25] batch [65/100] time 0.144 (0.161) data 0.000 (0.019) loss 0.8149 (0.8681) acc 78.1250 (79.2308) lr 1.4258e-03 eta 0:03:50
epoch [11/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 1.5557 (0.8835) acc 56.2500 (78.9286) lr 1.4258e-03 eta 0:03:47
epoch [11/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9985 (0.8815) acc 71.8750 (78.9583) lr 1.4258e-03 eta 0:03:45
epoch [11/25] batch [80/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.5078 (0.8801) acc 90.6250 (78.9453) lr 1.4258e-03 eta 0:03:42
epoch [11/25] batch [85/100] time 0.141 (0.156) data 0.000 (0.014) loss 0.9746 (0.8788) acc 71.8750 (78.7868) lr 1.4258e-03 eta 0:03:40
epoch [11/25] batch [90/100] time 0.141 (0.155) data 0.000 (0.014) loss 0.8032 (0.8774) acc 81.2500 (78.8889) lr 1.4258e-03 eta 0:03:38
epoch [11/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.9360 (0.8814) acc 75.0000 (78.8158) lr 1.4258e-03 eta 0:03:37
epoch [11/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.012) loss 0.6870 (0.8766) acc 75.0000 (78.7500) lr 1.3090e-03 eta 0:03:35
epoch [12/25] batch [5/100] time 0.142 (0.383) data 0.000 (0.242) loss 0.5400 (0.7612) acc 84.3750 (81.2500) lr 1.3090e-03 eta 0:08:54
epoch [12/25] batch [10/100] time 0.141 (0.263) data 0.000 (0.121) loss 0.7148 (0.7045) acc 81.2500 (83.1250) lr 1.3090e-03 eta 0:06:05
epoch [12/25] batch [15/100] time 0.141 (0.222) data 0.000 (0.081) loss 0.7017 (0.8173) acc 90.6250 (81.6667) lr 1.3090e-03 eta 0:05:08
epoch [12/25] batch [20/100] time 0.141 (0.202) data 0.000 (0.061) loss 0.6392 (0.8087) acc 81.2500 (81.0938) lr 1.3090e-03 eta 0:04:39
epoch [12/25] batch [25/100] time 0.144 (0.190) data 0.000 (0.049) loss 0.8550 (0.8386) acc 68.7500 (79.5000) lr 1.3090e-03 eta 0:04:21
epoch [12/25] batch [30/100] time 0.141 (0.182) data 0.000 (0.040) loss 0.9692 (0.8371) acc 71.8750 (79.5833) lr 1.3090e-03 eta 0:04:09
epoch [12/25] batch [35/100] time 0.144 (0.176) data 0.000 (0.035) loss 0.9390 (0.8561) acc 75.0000 (78.8393) lr 1.3090e-03 eta 0:04:00
epoch [12/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 1.2012 (0.8711) acc 71.8750 (78.6719) lr 1.3090e-03 eta 0:03:53
epoch [12/25] batch [45/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.8921 (0.8682) acc 81.2500 (78.6806) lr 1.3090e-03 eta 0:03:48
epoch [12/25] batch [50/100] time 0.142 (0.166) data 0.000 (0.024) loss 1.0273 (0.8824) acc 68.7500 (78.0625) lr 1.3090e-03 eta 0:03:43
epoch [12/25] batch [55/100] time 0.141 (0.164) data 0.000 (0.022) loss 1.0039 (0.8805) acc 78.1250 (78.2386) lr 1.3090e-03 eta 0:03:40
epoch [12/25] batch [60/100] time 0.141 (0.162) data 0.000 (0.020) loss 0.7822 (0.8884) acc 81.2500 (78.0208) lr 1.3090e-03 eta 0:03:36
epoch [12/25] batch [65/100] time 0.141 (0.160) data 0.000 (0.019) loss 0.9238 (0.8895) acc 78.1250 (78.0769) lr 1.3090e-03 eta 0:03:33
epoch [12/25] batch [70/100] time 0.141 (0.159) data 0.000 (0.018) loss 1.1260 (0.8956) acc 68.7500 (77.6786) lr 1.3090e-03 eta 0:03:31
epoch [12/25] batch [75/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.9780 (0.8923) acc 78.1250 (77.7500) lr 1.3090e-03 eta 0:03:29
epoch [12/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.4932 (0.8856) acc 84.3750 (77.9688) lr 1.3090e-03 eta 0:03:26
epoch [12/25] batch [85/100] time 0.141 (0.156) data 0.000 (0.015) loss 0.9414 (0.8831) acc 75.0000 (77.9412) lr 1.3090e-03 eta 0:03:24
epoch [12/25] batch [90/100] time 0.141 (0.155) data 0.000 (0.014) loss 1.0049 (0.8832) acc 71.8750 (77.9167) lr 1.3090e-03 eta 0:03:23
epoch [12/25] batch [95/100] time 0.142 (0.154) data 0.000 (0.013) loss 1.2588 (0.8975) acc 59.3750 (77.4671) lr 1.3090e-03 eta 0:03:21
epoch [12/25] batch [100/100] time 0.123 (0.153) data 0.000 (0.012) loss 0.5713 (0.8860) acc 75.0000 (77.5625) lr 1.1874e-03 eta 0:03:19
epoch [13/25] batch [5/100] time 0.146 (0.380) data 0.000 (0.237) loss 0.6670 (0.7890) acc 81.2500 (81.2500) lr 1.1874e-03 eta 0:08:12
epoch [13/25] batch [10/100] time 0.142 (0.262) data 0.000 (0.119) loss 0.6694 (0.8249) acc 84.3750 (79.6875) lr 1.1874e-03 eta 0:05:37
epoch [13/25] batch [15/100] time 0.141 (0.222) data 0.000 (0.079) loss 0.7939 (0.7938) acc 84.3750 (81.0417) lr 1.1874e-03 eta 0:04:44
epoch [13/25] batch [20/100] time 0.145 (0.202) data 0.004 (0.060) loss 1.1396 (0.8100) acc 65.6250 (80.1562) lr 1.1874e-03 eta 0:04:18
epoch [13/25] batch [25/100] time 0.142 (0.190) data 0.000 (0.048) loss 1.3789 (0.8168) acc 59.3750 (79.3750) lr 1.1874e-03 eta 0:04:02
epoch [13/25] batch [30/100] time 0.141 (0.182) data 0.000 (0.040) loss 0.5820 (0.8438) acc 87.5000 (78.6458) lr 1.1874e-03 eta 0:03:51
epoch [13/25] batch [35/100] time 0.142 (0.176) data 0.001 (0.034) loss 0.5156 (0.8065) acc 93.7500 (80.0893) lr 1.1874e-03 eta 0:03:42
epoch [13/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 0.6646 (0.7916) acc 84.3750 (80.3125) lr 1.1874e-03 eta 0:03:36
epoch [13/25] batch [45/100] time 0.141 (0.168) data 0.000 (0.027) loss 1.5371 (0.7976) acc 62.5000 (80.3472) lr 1.1874e-03 eta 0:03:31
epoch [13/25] batch [50/100] time 0.141 (0.166) data 0.000 (0.024) loss 0.7979 (0.8112) acc 68.7500 (79.8750) lr 1.1874e-03 eta 0:03:27
epoch [13/25] batch [55/100] time 0.143 (0.164) data 0.000 (0.022) loss 0.7817 (0.8085) acc 78.1250 (79.7727) lr 1.1874e-03 eta 0:03:23
epoch [13/25] batch [60/100] time 0.141 (0.162) data 0.001 (0.020) loss 0.5322 (0.8061) acc 87.5000 (79.8958) lr 1.1874e-03 eta 0:03:20
epoch [13/25] batch [65/100] time 0.141 (0.160) data 0.000 (0.019) loss 0.9707 (0.8093) acc 78.1250 (79.9038) lr 1.1874e-03 eta 0:03:18
epoch [13/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.7207 (0.8081) acc 81.2500 (79.7768) lr 1.1874e-03 eta 0:03:15
epoch [13/25] batch [75/100] time 0.143 (0.158) data 0.000 (0.016) loss 0.6968 (0.8092) acc 78.1250 (79.8333) lr 1.1874e-03 eta 0:03:13
epoch [13/25] batch [80/100] time 0.143 (0.157) data 0.000 (0.015) loss 1.3135 (0.8165) acc 65.6250 (79.7266) lr 1.1874e-03 eta 0:03:11
epoch [13/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 1.0938 (0.8170) acc 71.8750 (79.9632) lr 1.1874e-03 eta 0:03:09
epoch [13/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5762 (0.8193) acc 90.6250 (79.8958) lr 1.1874e-03 eta 0:03:07
epoch [13/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.013) loss 0.7969 (0.8197) acc 81.2500 (79.8355) lr 1.1874e-03 eta 0:03:06
epoch [13/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.012) loss 0.8706 (0.8225) acc 87.5000 (79.5938) lr 1.0628e-03 eta 0:03:04
epoch [14/25] batch [5/100] time 0.142 (0.387) data 0.000 (0.244) loss 1.3408 (0.9960) acc 71.8750 (76.8750) lr 1.0628e-03 eta 0:07:42
epoch [14/25] batch [10/100] time 0.149 (0.265) data 0.001 (0.122) loss 0.8540 (0.8529) acc 84.3750 (80.6250) lr 1.0628e-03 eta 0:05:15
epoch [14/25] batch [15/100] time 0.142 (0.224) data 0.000 (0.082) loss 0.8057 (0.8247) acc 81.2500 (81.0417) lr 1.0628e-03 eta 0:04:26
epoch [14/25] batch [20/100] time 0.141 (0.204) data 0.000 (0.061) loss 0.9526 (0.8116) acc 81.2500 (81.5625) lr 1.0628e-03 eta 0:04:00
epoch [14/25] batch [25/100] time 0.141 (0.191) data 0.000 (0.049) loss 0.6719 (0.8257) acc 84.3750 (80.8750) lr 1.0628e-03 eta 0:03:44
epoch [14/25] batch [30/100] time 0.142 (0.183) data 0.000 (0.041) loss 0.8516 (0.8292) acc 81.2500 (80.9375) lr 1.0628e-03 eta 0:03:34
epoch [14/25] batch [35/100] time 0.141 (0.177) data 0.000 (0.035) loss 0.6440 (0.8161) acc 87.5000 (80.8036) lr 1.0628e-03 eta 0:03:26
epoch [14/25] batch [40/100] time 0.141 (0.173) data 0.000 (0.031) loss 0.7876 (0.8209) acc 78.1250 (80.7031) lr 1.0628e-03 eta 0:03:20
epoch [14/25] batch [45/100] time 0.142 (0.170) data 0.000 (0.027) loss 0.7598 (0.8199) acc 71.8750 (80.6944) lr 1.0628e-03 eta 0:03:15
epoch [14/25] batch [50/100] time 0.141 (0.167) data 0.000 (0.025) loss 1.2832 (0.8228) acc 68.7500 (80.4375) lr 1.0628e-03 eta 0:03:11
epoch [14/25] batch [55/100] time 0.144 (0.164) data 0.000 (0.023) loss 0.7500 (0.8228) acc 78.1250 (80.2841) lr 1.0628e-03 eta 0:03:08
epoch [14/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.5371 (0.8278) acc 96.8750 (80.2604) lr 1.0628e-03 eta 0:03:05
epoch [14/25] batch [65/100] time 0.143 (0.161) data 0.000 (0.019) loss 0.9395 (0.8357) acc 75.0000 (79.8558) lr 1.0628e-03 eta 0:03:02
epoch [14/25] batch [70/100] time 0.143 (0.160) data 0.000 (0.018) loss 0.4941 (0.8215) acc 93.7500 (80.4018) lr 1.0628e-03 eta 0:03:00
epoch [14/25] batch [75/100] time 0.141 (0.158) data 0.000 (0.017) loss 0.9033 (0.8212) acc 81.2500 (80.3333) lr 1.0628e-03 eta 0:02:58
epoch [14/25] batch [80/100] time 0.145 (0.157) data 0.000 (0.016) loss 0.5542 (0.8254) acc 81.2500 (80.3125) lr 1.0628e-03 eta 0:02:56
epoch [14/25] batch [85/100] time 0.141 (0.156) data 0.000 (0.015) loss 1.2002 (0.8348) acc 59.3750 (79.9265) lr 1.0628e-03 eta 0:02:54
epoch [14/25] batch [90/100] time 0.141 (0.156) data 0.000 (0.014) loss 0.6216 (0.8284) acc 87.5000 (80.1389) lr 1.0628e-03 eta 0:02:52
epoch [14/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.7256 (0.8347) acc 81.2500 (80.0658) lr 1.0628e-03 eta 0:02:51
epoch [14/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.013) loss 0.8154 (0.8337) acc 81.2500 (80.2812) lr 9.3721e-04 eta 0:02:49
epoch [15/25] batch [5/100] time 0.141 (0.428) data 0.000 (0.287) loss 1.2070 (0.7567) acc 81.2500 (84.3750) lr 9.3721e-04 eta 0:07:49
epoch [15/25] batch [10/100] time 0.142 (0.285) data 0.000 (0.144) loss 1.2783 (0.8143) acc 68.7500 (81.5625) lr 9.3721e-04 eta 0:05:10
epoch [15/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.096) loss 1.1689 (0.8417) acc 68.7500 (81.2500) lr 9.3721e-04 eta 0:04:17
epoch [15/25] batch [20/100] time 0.141 (0.214) data 0.000 (0.072) loss 1.0762 (0.8066) acc 75.0000 (82.0312) lr 9.3721e-04 eta 0:03:50
epoch [15/25] batch [25/100] time 0.142 (0.199) data 0.000 (0.058) loss 1.2773 (0.8164) acc 68.7500 (81.7500) lr 9.3721e-04 eta 0:03:34
epoch [15/25] batch [30/100] time 0.145 (0.190) data 0.000 (0.048) loss 0.8408 (0.8201) acc 81.2500 (81.4583) lr 9.3721e-04 eta 0:03:23
epoch [15/25] batch [35/100] time 0.142 (0.183) data 0.000 (0.041) loss 0.5596 (0.8368) acc 93.7500 (80.8929) lr 9.3721e-04 eta 0:03:14
epoch [15/25] batch [40/100] time 0.141 (0.178) data 0.000 (0.036) loss 0.4622 (0.8264) acc 84.3750 (80.8594) lr 9.3721e-04 eta 0:03:08
epoch [15/25] batch [45/100] time 0.144 (0.174) data 0.000 (0.032) loss 0.9863 (0.8426) acc 71.8750 (80.1389) lr 9.3721e-04 eta 0:03:03
epoch [15/25] batch [50/100] time 0.143 (0.171) data 0.000 (0.029) loss 0.8242 (0.8369) acc 75.0000 (80.1875) lr 9.3721e-04 eta 0:02:59
epoch [15/25] batch [55/100] time 0.141 (0.168) data 0.000 (0.026) loss 0.9570 (0.8336) acc 71.8750 (80.0000) lr 9.3721e-04 eta 0:02:55
epoch [15/25] batch [60/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.9727 (0.8366) acc 68.7500 (79.6354) lr 9.3721e-04 eta 0:02:52
epoch [15/25] batch [65/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.5508 (0.8227) acc 87.5000 (79.8558) lr 9.3721e-04 eta 0:02:49
epoch [15/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8135 (0.8239) acc 78.1250 (79.6875) lr 9.3721e-04 eta 0:02:47
epoch [15/25] batch [75/100] time 0.143 (0.161) data 0.000 (0.019) loss 0.7866 (0.8319) acc 84.3750 (79.5000) lr 9.3721e-04 eta 0:02:45
epoch [15/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.8433 (0.8252) acc 68.7500 (79.7656) lr 9.3721e-04 eta 0:02:43
epoch [15/25] batch [85/100] time 0.141 (0.159) data 0.000 (0.017) loss 0.7700 (0.8225) acc 81.2500 (80.0735) lr 9.3721e-04 eta 0:02:41
epoch [15/25] batch [90/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.8511 (0.8236) acc 84.3750 (80.0347) lr 9.3721e-04 eta 0:02:39
epoch [15/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.9653 (0.8262) acc 84.3750 (80.0987) lr 9.3721e-04 eta 0:02:37
epoch [15/25] batch [100/100] time 0.123 (0.156) data 0.000 (0.015) loss 0.3743 (0.8179) acc 100.0000 (80.3125) lr 8.1262e-04 eta 0:02:36
epoch [16/25] batch [5/100] time 0.144 (0.373) data 0.000 (0.230) loss 0.7817 (0.7310) acc 90.6250 (88.1250) lr 8.1262e-04 eta 0:06:11
epoch [16/25] batch [10/100] time 0.144 (0.258) data 0.000 (0.115) loss 0.6733 (0.7288) acc 81.2500 (86.2500) lr 8.1262e-04 eta 0:04:15
epoch [16/25] batch [15/100] time 0.142 (0.220) data 0.000 (0.077) loss 0.6875 (0.7636) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:03:36
epoch [16/25] batch [20/100] time 0.141 (0.200) data 0.000 (0.058) loss 0.9531 (0.8049) acc 68.7500 (82.6562) lr 8.1262e-04 eta 0:03:16
epoch [16/25] batch [25/100] time 0.141 (0.188) data 0.000 (0.046) loss 1.0273 (0.8147) acc 65.6250 (80.7500) lr 8.1262e-04 eta 0:03:03
epoch [16/25] batch [30/100] time 0.141 (0.181) data 0.000 (0.039) loss 0.6348 (0.7995) acc 87.5000 (80.9375) lr 8.1262e-04 eta 0:02:55
epoch [16/25] batch [35/100] time 0.142 (0.175) data 0.000 (0.033) loss 0.7729 (0.7949) acc 71.8750 (80.9821) lr 8.1262e-04 eta 0:02:49
epoch [16/25] batch [40/100] time 0.141 (0.171) data 0.000 (0.029) loss 0.8442 (0.7798) acc 84.3750 (81.5625) lr 8.1262e-04 eta 0:02:44
epoch [16/25] batch [45/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.5576 (0.7638) acc 90.6250 (81.9444) lr 8.1262e-04 eta 0:02:40
epoch [16/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.6523 (0.7720) acc 78.1250 (81.6875) lr 8.1262e-04 eta 0:02:36
epoch [16/25] batch [55/100] time 0.141 (0.163) data 0.000 (0.021) loss 1.1396 (0.7818) acc 71.8750 (81.4205) lr 8.1262e-04 eta 0:02:34
epoch [16/25] batch [60/100] time 0.141 (0.161) data 0.000 (0.020) loss 0.9297 (0.7822) acc 75.0000 (81.3021) lr 8.1262e-04 eta 0:02:31
epoch [16/25] batch [65/100] time 0.141 (0.160) data 0.000 (0.018) loss 0.8320 (0.7830) acc 75.0000 (81.2019) lr 8.1262e-04 eta 0:02:29
epoch [16/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.8057 (0.7809) acc 81.2500 (81.2946) lr 8.1262e-04 eta 0:02:27
epoch [16/25] batch [75/100] time 0.142 (0.157) data 0.000 (0.016) loss 0.7295 (0.7811) acc 81.2500 (81.4583) lr 8.1262e-04 eta 0:02:25
epoch [16/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.5308 (0.7771) acc 90.6250 (81.4453) lr 8.1262e-04 eta 0:02:23
epoch [16/25] batch [85/100] time 0.141 (0.156) data 0.000 (0.014) loss 0.9082 (0.7789) acc 71.8750 (81.2500) lr 8.1262e-04 eta 0:02:22
epoch [16/25] batch [90/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.6553 (0.7869) acc 87.5000 (80.7986) lr 8.1262e-04 eta 0:02:20
epoch [16/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.6973 (0.7913) acc 84.3750 (80.5592) lr 8.1262e-04 eta 0:02:19
epoch [16/25] batch [100/100] time 0.123 (0.153) data 0.000 (0.012) loss 0.8174 (0.7941) acc 87.5000 (80.5938) lr 6.9098e-04 eta 0:02:18
epoch [17/25] batch [5/100] time 0.142 (0.415) data 0.000 (0.271) loss 0.7983 (0.8143) acc 87.5000 (85.6250) lr 6.9098e-04 eta 0:06:11
epoch [17/25] batch [10/100] time 0.143 (0.279) data 0.000 (0.136) loss 0.6909 (0.7762) acc 84.3750 (84.3750) lr 6.9098e-04 eta 0:04:08
epoch [17/25] batch [15/100] time 0.145 (0.233) data 0.000 (0.091) loss 0.8584 (0.8202) acc 78.1250 (81.6667) lr 6.9098e-04 eta 0:03:26
epoch [17/25] batch [20/100] time 0.142 (0.211) data 0.000 (0.068) loss 0.9160 (0.8222) acc 71.8750 (80.9375) lr 6.9098e-04 eta 0:03:05
epoch [17/25] batch [25/100] time 0.141 (0.197) data 0.000 (0.055) loss 0.7832 (0.7975) acc 78.1250 (81.3750) lr 6.9098e-04 eta 0:02:52
epoch [17/25] batch [30/100] time 0.143 (0.188) data 0.001 (0.046) loss 0.6807 (0.8038) acc 87.5000 (81.3542) lr 6.9098e-04 eta 0:02:43
epoch [17/25] batch [35/100] time 0.141 (0.181) data 0.000 (0.039) loss 0.9707 (0.8134) acc 71.8750 (80.8036) lr 6.9098e-04 eta 0:02:36
epoch [17/25] batch [40/100] time 0.142 (0.177) data 0.000 (0.034) loss 0.6216 (0.7952) acc 84.3750 (81.0938) lr 6.9098e-04 eta 0:02:31
epoch [17/25] batch [45/100] time 0.142 (0.173) data 0.000 (0.031) loss 0.8486 (0.8096) acc 78.1250 (80.4861) lr 6.9098e-04 eta 0:02:27
epoch [17/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.027) loss 0.5503 (0.8098) acc 87.5000 (80.3125) lr 6.9098e-04 eta 0:02:24
epoch [17/25] batch [55/100] time 0.142 (0.167) data 0.001 (0.025) loss 0.5747 (0.8093) acc 81.2500 (80.5682) lr 6.9098e-04 eta 0:02:21
epoch [17/25] batch [60/100] time 0.142 (0.165) data 0.001 (0.023) loss 0.5850 (0.7968) acc 81.2500 (81.0938) lr 6.9098e-04 eta 0:02:18
epoch [17/25] batch [65/100] time 0.143 (0.164) data 0.000 (0.021) loss 0.9492 (0.7992) acc 75.0000 (80.8654) lr 6.9098e-04 eta 0:02:16
epoch [17/25] batch [70/100] time 0.143 (0.162) data 0.001 (0.020) loss 0.8154 (0.7935) acc 84.3750 (81.0714) lr 6.9098e-04 eta 0:02:14
epoch [17/25] batch [75/100] time 0.143 (0.161) data 0.001 (0.018) loss 1.0234 (0.7925) acc 75.0000 (81.1250) lr 6.9098e-04 eta 0:02:12
epoch [17/25] batch [80/100] time 0.144 (0.160) data 0.000 (0.017) loss 1.2002 (0.7962) acc 71.8750 (81.0547) lr 6.9098e-04 eta 0:02:10
epoch [17/25] batch [85/100] time 0.141 (0.159) data 0.000 (0.016) loss 0.6948 (0.7947) acc 84.3750 (81.1029) lr 6.9098e-04 eta 0:02:09
epoch [17/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.5205 (0.7893) acc 93.7500 (81.2847) lr 6.9098e-04 eta 0:02:07
epoch [17/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.6572 (0.7965) acc 84.3750 (81.1513) lr 6.9098e-04 eta 0:02:06
epoch [17/25] batch [100/100] time 0.123 (0.156) data 0.000 (0.014) loss 0.8516 (0.7986) acc 75.0000 (80.9375) lr 5.7422e-04 eta 0:02:04
epoch [18/25] batch [5/100] time 0.141 (0.402) data 0.000 (0.261) loss 0.6899 (0.7097) acc 84.3750 (83.1250) lr 5.7422e-04 eta 0:05:19
epoch [18/25] batch [10/100] time 0.141 (0.272) data 0.000 (0.130) loss 0.7915 (0.7650) acc 75.0000 (80.0000) lr 5.7422e-04 eta 0:03:34
epoch [18/25] batch [15/100] time 0.142 (0.229) data 0.000 (0.087) loss 0.7515 (0.7895) acc 87.5000 (79.5833) lr 5.7422e-04 eta 0:02:59
epoch [18/25] batch [20/100] time 0.141 (0.207) data 0.000 (0.065) loss 0.8394 (0.8041) acc 75.0000 (78.9062) lr 5.7422e-04 eta 0:02:41
epoch [18/25] batch [25/100] time 0.142 (0.194) data 0.000 (0.052) loss 0.7612 (0.7893) acc 84.3750 (80.0000) lr 5.7422e-04 eta 0:02:30
epoch [18/25] batch [30/100] time 0.141 (0.185) data 0.000 (0.044) loss 0.7378 (0.7746) acc 84.3750 (80.3125) lr 5.7422e-04 eta 0:02:22
epoch [18/25] batch [35/100] time 0.142 (0.179) data 0.000 (0.038) loss 0.9609 (0.7919) acc 78.1250 (80.1786) lr 5.7422e-04 eta 0:02:17
epoch [18/25] batch [40/100] time 0.142 (0.174) data 0.000 (0.033) loss 0.9321 (0.7879) acc 78.1250 (80.7812) lr 5.7422e-04 eta 0:02:12
epoch [18/25] batch [45/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.7666 (0.7905) acc 84.3750 (80.8333) lr 5.7422e-04 eta 0:02:08
epoch [18/25] batch [50/100] time 0.141 (0.168) data 0.000 (0.026) loss 0.7861 (0.7796) acc 75.0000 (81.0625) lr 5.7422e-04 eta 0:02:05
epoch [18/25] batch [55/100] time 0.141 (0.166) data 0.000 (0.024) loss 0.6504 (0.7821) acc 84.3750 (80.6250) lr 5.7422e-04 eta 0:02:03
epoch [18/25] batch [60/100] time 0.142 (0.164) data 0.001 (0.022) loss 0.8335 (0.7761) acc 81.2500 (80.8854) lr 5.7422e-04 eta 0:02:01
epoch [18/25] batch [65/100] time 0.141 (0.162) data 0.000 (0.020) loss 0.6978 (0.7793) acc 84.3750 (80.7212) lr 5.7422e-04 eta 0:01:59
epoch [18/25] batch [70/100] time 0.148 (0.161) data 0.000 (0.019) loss 0.7510 (0.7717) acc 84.3750 (81.0714) lr 5.7422e-04 eta 0:01:57
epoch [18/25] batch [75/100] time 0.142 (0.159) data 0.000 (0.018) loss 0.5884 (0.7742) acc 87.5000 (81.0417) lr 5.7422e-04 eta 0:01:55
epoch [18/25] batch [80/100] time 0.141 (0.158) data 0.000 (0.017) loss 0.6318 (0.7753) acc 84.3750 (81.0938) lr 5.7422e-04 eta 0:01:53
epoch [18/25] batch [85/100] time 0.143 (0.157) data 0.000 (0.016) loss 0.6826 (0.7837) acc 90.6250 (80.9191) lr 5.7422e-04 eta 0:01:52
epoch [18/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.015) loss 0.6836 (0.7830) acc 84.3750 (81.0764) lr 5.7422e-04 eta 0:01:51
epoch [18/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.7153 (0.7837) acc 81.2500 (81.0526) lr 5.7422e-04 eta 0:01:49
epoch [18/25] batch [100/100] time 0.123 (0.155) data 0.000 (0.013) loss 0.9683 (0.7883) acc 68.7500 (80.9375) lr 4.6417e-04 eta 0:01:48
epoch [19/25] batch [5/100] time 0.142 (0.394) data 0.000 (0.252) loss 0.7466 (0.7171) acc 87.5000 (83.7500) lr 4.6417e-04 eta 0:04:33
epoch [19/25] batch [10/100] time 0.142 (0.268) data 0.000 (0.126) loss 0.3320 (0.6798) acc 96.8750 (85.0000) lr 4.6417e-04 eta 0:03:04
epoch [19/25] batch [15/100] time 0.141 (0.226) data 0.000 (0.084) loss 0.7822 (0.6631) acc 84.3750 (85.6250) lr 4.6417e-04 eta 0:02:34
epoch [19/25] batch [20/100] time 0.141 (0.205) data 0.000 (0.063) loss 0.6016 (0.6723) acc 87.5000 (85.3125) lr 4.6417e-04 eta 0:02:19
epoch [19/25] batch [25/100] time 0.141 (0.192) data 0.000 (0.051) loss 0.8965 (0.7057) acc 84.3750 (84.6250) lr 4.6417e-04 eta 0:02:09
epoch [19/25] batch [30/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.7183 (0.7046) acc 87.5000 (84.5833) lr 4.6417e-04 eta 0:02:03
epoch [19/25] batch [35/100] time 0.143 (0.178) data 0.000 (0.036) loss 0.6733 (0.7074) acc 87.5000 (84.1071) lr 4.6417e-04 eta 0:01:58
epoch [19/25] batch [40/100] time 0.141 (0.173) data 0.000 (0.032) loss 0.6055 (0.7263) acc 84.3750 (82.9688) lr 4.6417e-04 eta 0:01:54
epoch [19/25] batch [45/100] time 0.141 (0.170) data 0.000 (0.028) loss 0.8892 (0.7358) acc 78.1250 (82.7778) lr 4.6417e-04 eta 0:01:51
epoch [19/25] batch [50/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.7739 (0.7382) acc 87.5000 (83.1875) lr 4.6417e-04 eta 0:01:48
epoch [19/25] batch [55/100] time 0.146 (0.165) data 0.000 (0.023) loss 1.1660 (0.7539) acc 75.0000 (82.5000) lr 4.6417e-04 eta 0:01:46
epoch [19/25] batch [60/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.7422 (0.7568) acc 87.5000 (82.5521) lr 4.6417e-04 eta 0:01:44
epoch [19/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.020) loss 1.1709 (0.7687) acc 78.1250 (82.3558) lr 4.6417e-04 eta 0:01:42
epoch [19/25] batch [70/100] time 0.145 (0.160) data 0.000 (0.018) loss 1.1162 (0.7736) acc 75.0000 (82.3214) lr 4.6417e-04 eta 0:01:40
epoch [19/25] batch [75/100] time 0.142 (0.159) data 0.000 (0.017) loss 1.1191 (0.7813) acc 71.8750 (82.3333) lr 4.6417e-04 eta 0:01:39
epoch [19/25] batch [80/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.2402 (0.7835) acc 62.5000 (82.2266) lr 4.6417e-04 eta 0:01:37
epoch [19/25] batch [85/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.4126 (0.7817) acc 93.7500 (82.2059) lr 4.6417e-04 eta 0:01:36
epoch [19/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.5640 (0.7857) acc 84.3750 (81.9444) lr 4.6417e-04 eta 0:01:35
epoch [19/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.014) loss 0.8540 (0.7819) acc 90.6250 (82.1382) lr 4.6417e-04 eta 0:01:33
epoch [19/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.013) loss 0.5820 (0.7869) acc 87.5000 (81.8750) lr 3.6258e-04 eta 0:01:32
epoch [20/25] batch [5/100] time 0.142 (0.362) data 0.000 (0.220) loss 0.6890 (0.7733) acc 90.6250 (83.7500) lr 3.6258e-04 eta 0:03:35
epoch [20/25] batch [10/100] time 0.143 (0.253) data 0.000 (0.110) loss 0.5405 (0.7972) acc 87.5000 (82.1875) lr 3.6258e-04 eta 0:02:28
epoch [20/25] batch [15/100] time 0.141 (0.216) data 0.000 (0.074) loss 0.9131 (0.7587) acc 81.2500 (82.9167) lr 3.6258e-04 eta 0:02:06
epoch [20/25] batch [20/100] time 0.141 (0.197) data 0.000 (0.055) loss 0.4480 (0.7225) acc 96.8750 (83.4375) lr 3.6258e-04 eta 0:01:54
epoch [20/25] batch [25/100] time 0.146 (0.187) data 0.000 (0.044) loss 0.5332 (0.7245) acc 90.6250 (83.2500) lr 3.6258e-04 eta 0:01:47
epoch [20/25] batch [30/100] time 0.144 (0.179) data 0.001 (0.037) loss 1.3252 (0.7491) acc 71.8750 (83.0208) lr 3.6258e-04 eta 0:01:42
epoch [20/25] batch [35/100] time 0.142 (0.174) data 0.001 (0.032) loss 0.3774 (0.7253) acc 96.8750 (83.6607) lr 3.6258e-04 eta 0:01:38
epoch [20/25] batch [40/100] time 0.141 (0.170) data 0.000 (0.028) loss 0.6646 (0.7346) acc 84.3750 (83.6719) lr 3.6258e-04 eta 0:01:35
epoch [20/25] batch [45/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.8472 (0.7435) acc 71.8750 (83.2639) lr 3.6258e-04 eta 0:01:32
epoch [20/25] batch [50/100] time 0.141 (0.164) data 0.000 (0.022) loss 0.7549 (0.7447) acc 84.3750 (83.1875) lr 3.6258e-04 eta 0:01:30
epoch [20/25] batch [55/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.7021 (0.7598) acc 84.3750 (82.6136) lr 3.6258e-04 eta 0:01:28
epoch [20/25] batch [60/100] time 0.142 (0.161) data 0.001 (0.019) loss 0.6143 (0.7630) acc 90.6250 (82.4479) lr 3.6258e-04 eta 0:01:26
epoch [20/25] batch [65/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.8828 (0.7651) acc 78.1250 (82.2115) lr 3.6258e-04 eta 0:01:25
epoch [20/25] batch [70/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.6162 (0.7646) acc 84.3750 (82.1875) lr 3.6258e-04 eta 0:01:23
epoch [20/25] batch [75/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.7383 (0.7805) acc 78.1250 (81.6250) lr 3.6258e-04 eta 0:01:22
epoch [20/25] batch [80/100] time 0.144 (0.156) data 0.000 (0.014) loss 0.8110 (0.7837) acc 78.1250 (81.5234) lr 3.6258e-04 eta 0:01:21
epoch [20/25] batch [85/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5898 (0.7893) acc 87.5000 (81.2500) lr 3.6258e-04 eta 0:01:19
epoch [20/25] batch [90/100] time 0.141 (0.154) data 0.000 (0.013) loss 0.9697 (0.7793) acc 84.3750 (81.7361) lr 3.6258e-04 eta 0:01:18
epoch [20/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.8135 (0.7769) acc 71.8750 (81.6447) lr 3.6258e-04 eta 0:01:17
epoch [20/25] batch [100/100] time 0.123 (0.153) data 0.000 (0.011) loss 0.4619 (0.7731) acc 87.5000 (81.7500) lr 2.7103e-04 eta 0:01:16
epoch [21/25] batch [5/100] time 0.142 (0.358) data 0.000 (0.215) loss 0.6553 (0.6101) acc 87.5000 (87.5000) lr 2.7103e-04 eta 0:02:57
epoch [21/25] batch [10/100] time 0.141 (0.251) data 0.000 (0.108) loss 0.8979 (0.6782) acc 78.1250 (84.6875) lr 2.7103e-04 eta 0:02:02
epoch [21/25] batch [15/100] time 0.142 (0.215) data 0.000 (0.072) loss 0.6099 (0.7201) acc 90.6250 (83.1250) lr 2.7103e-04 eta 0:01:44
epoch [21/25] batch [20/100] time 0.142 (0.196) data 0.000 (0.054) loss 0.6279 (0.7155) acc 90.6250 (83.1250) lr 2.7103e-04 eta 0:01:34
epoch [21/25] batch [25/100] time 0.141 (0.185) data 0.000 (0.043) loss 0.7217 (0.6936) acc 87.5000 (84.1250) lr 2.7103e-04 eta 0:01:28
epoch [21/25] batch [30/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.6411 (0.6915) acc 81.2500 (83.6458) lr 2.7103e-04 eta 0:01:23
epoch [21/25] batch [35/100] time 0.141 (0.173) data 0.000 (0.031) loss 0.6260 (0.7035) acc 90.6250 (83.5714) lr 2.7103e-04 eta 0:01:20
epoch [21/25] batch [40/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.7856 (0.7186) acc 81.2500 (82.9688) lr 2.7103e-04 eta 0:01:17
epoch [21/25] batch [45/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.8018 (0.7147) acc 78.1250 (83.1250) lr 2.7103e-04 eta 0:01:15
epoch [21/25] batch [50/100] time 0.141 (0.164) data 0.000 (0.022) loss 0.7524 (0.7256) acc 81.2500 (82.6875) lr 2.7103e-04 eta 0:01:13
epoch [21/25] batch [55/100] time 0.141 (0.162) data 0.000 (0.020) loss 0.8047 (0.7381) acc 87.5000 (82.3864) lr 2.7103e-04 eta 0:01:11
epoch [21/25] batch [60/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.6802 (0.7360) acc 84.3750 (82.3958) lr 2.7103e-04 eta 0:01:10
epoch [21/25] batch [65/100] time 0.141 (0.159) data 0.000 (0.017) loss 0.7642 (0.7297) acc 71.8750 (82.5962) lr 2.7103e-04 eta 0:01:09
epoch [21/25] batch [70/100] time 0.141 (0.157) data 0.000 (0.016) loss 0.8794 (0.7289) acc 84.3750 (82.9464) lr 2.7103e-04 eta 0:01:07
epoch [21/25] batch [75/100] time 0.142 (0.156) data 0.000 (0.015) loss 0.8677 (0.7218) acc 75.0000 (83.1667) lr 2.7103e-04 eta 0:01:06
epoch [21/25] batch [80/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9473 (0.7192) acc 78.1250 (83.0859) lr 2.7103e-04 eta 0:01:05
epoch [21/25] batch [85/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.6123 (0.7113) acc 87.5000 (83.4926) lr 2.7103e-04 eta 0:01:04
epoch [21/25] batch [90/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.5996 (0.7120) acc 87.5000 (83.4028) lr 2.7103e-04 eta 0:01:03
epoch [21/25] batch [95/100] time 0.142 (0.153) data 0.000 (0.012) loss 1.0059 (0.7219) acc 65.6250 (83.0921) lr 2.7103e-04 eta 0:01:02
epoch [21/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.011) loss 1.0264 (0.7249) acc 81.2500 (83.1562) lr 1.9098e-04 eta 0:01:01
epoch [22/25] batch [5/100] time 0.143 (0.425) data 0.000 (0.282) loss 0.7749 (0.7935) acc 87.5000 (81.8750) lr 1.9098e-04 eta 0:02:47
epoch [22/25] batch [10/100] time 0.143 (0.284) data 0.001 (0.141) loss 0.5664 (0.7329) acc 87.5000 (83.7500) lr 1.9098e-04 eta 0:01:50
epoch [22/25] batch [15/100] time 0.142 (0.237) data 0.000 (0.094) loss 0.6465 (0.7355) acc 90.6250 (83.5417) lr 1.9098e-04 eta 0:01:31
epoch [22/25] batch [20/100] time 0.142 (0.213) data 0.000 (0.071) loss 1.1113 (0.7849) acc 71.8750 (82.0312) lr 1.9098e-04 eta 0:01:21
epoch [22/25] batch [25/100] time 0.143 (0.199) data 0.000 (0.057) loss 0.6533 (0.7611) acc 78.1250 (82.3750) lr 1.9098e-04 eta 0:01:14
epoch [22/25] batch [30/100] time 0.142 (0.190) data 0.000 (0.047) loss 0.4258 (0.7316) acc 93.7500 (83.1250) lr 1.9098e-04 eta 0:01:10
epoch [22/25] batch [35/100] time 0.142 (0.183) data 0.000 (0.041) loss 0.9375 (0.7336) acc 75.0000 (82.9464) lr 1.9098e-04 eta 0:01:06
epoch [22/25] batch [40/100] time 0.143 (0.178) data 0.000 (0.036) loss 0.9106 (0.7248) acc 71.8750 (83.2812) lr 1.9098e-04 eta 0:01:04
epoch [22/25] batch [45/100] time 0.142 (0.174) data 0.000 (0.032) loss 1.1123 (0.7304) acc 78.1250 (83.2639) lr 1.9098e-04 eta 0:01:01
epoch [22/25] batch [50/100] time 0.144 (0.171) data 0.000 (0.029) loss 0.6851 (0.7308) acc 87.5000 (83.5000) lr 1.9098e-04 eta 0:00:59
epoch [22/25] batch [55/100] time 0.143 (0.168) data 0.000 (0.026) loss 0.8057 (0.7383) acc 81.2500 (83.2386) lr 1.9098e-04 eta 0:00:58
epoch [22/25] batch [60/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.8218 (0.7311) acc 87.5000 (83.8021) lr 1.9098e-04 eta 0:00:56
epoch [22/25] batch [65/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.1367 (0.7351) acc 71.8750 (83.7019) lr 1.9098e-04 eta 0:00:55
epoch [22/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 1.1982 (0.7409) acc 68.7500 (83.5714) lr 1.9098e-04 eta 0:00:53
epoch [22/25] batch [75/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.4441 (0.7386) acc 93.7500 (83.5833) lr 1.9098e-04 eta 0:00:52
epoch [22/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.7188 (0.7285) acc 84.3750 (83.7891) lr 1.9098e-04 eta 0:00:51
epoch [22/25] batch [85/100] time 0.141 (0.159) data 0.000 (0.017) loss 0.6772 (0.7317) acc 87.5000 (83.6765) lr 1.9098e-04 eta 0:00:50
epoch [22/25] batch [90/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.9429 (0.7358) acc 84.3750 (83.6111) lr 1.9098e-04 eta 0:00:49
epoch [22/25] batch [95/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.5435 (0.7290) acc 87.5000 (83.8158) lr 1.9098e-04 eta 0:00:47
epoch [22/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 1.1221 (0.7333) acc 62.5000 (83.5312) lr 1.2369e-04 eta 0:00:46
epoch [23/25] batch [5/100] time 0.142 (0.418) data 0.000 (0.277) loss 0.9341 (0.8635) acc 81.2500 (78.7500) lr 1.2369e-04 eta 0:02:03
epoch [23/25] batch [10/100] time 0.142 (0.281) data 0.000 (0.138) loss 0.5947 (0.7694) acc 87.5000 (82.5000) lr 1.2369e-04 eta 0:01:21
epoch [23/25] batch [15/100] time 0.142 (0.235) data 0.000 (0.092) loss 0.6357 (0.8116) acc 78.1250 (82.0833) lr 1.2369e-04 eta 0:01:06
epoch [23/25] batch [20/100] time 0.142 (0.212) data 0.000 (0.069) loss 0.7124 (0.7781) acc 78.1250 (82.6562) lr 1.2369e-04 eta 0:00:59
epoch [23/25] batch [25/100] time 0.142 (0.198) data 0.000 (0.056) loss 0.6123 (0.7524) acc 87.5000 (83.6250) lr 1.2369e-04 eta 0:00:54
epoch [23/25] batch [30/100] time 0.142 (0.188) data 0.000 (0.046) loss 0.8467 (0.7441) acc 84.3750 (83.9583) lr 1.2369e-04 eta 0:00:50
epoch [23/25] batch [35/100] time 0.144 (0.182) data 0.000 (0.040) loss 0.6851 (0.7441) acc 84.3750 (83.4821) lr 1.2369e-04 eta 0:00:48
epoch [23/25] batch [40/100] time 0.143 (0.177) data 0.000 (0.035) loss 0.7202 (0.7479) acc 75.0000 (83.2812) lr 1.2369e-04 eta 0:00:45
epoch [23/25] batch [45/100] time 0.141 (0.173) data 0.000 (0.031) loss 0.7437 (0.7609) acc 84.3750 (82.7778) lr 1.2369e-04 eta 0:00:44
epoch [23/25] batch [50/100] time 0.144 (0.170) data 0.000 (0.028) loss 0.4556 (0.7548) acc 93.7500 (82.8125) lr 1.2369e-04 eta 0:00:42
epoch [23/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.8252 (0.7594) acc 84.3750 (82.8977) lr 1.2369e-04 eta 0:00:40
epoch [23/25] batch [60/100] time 0.141 (0.165) data 0.000 (0.023) loss 0.5752 (0.7595) acc 84.3750 (83.0208) lr 1.2369e-04 eta 0:00:39
epoch [23/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.022) loss 0.7549 (0.7561) acc 78.1250 (82.9808) lr 1.2369e-04 eta 0:00:38
epoch [23/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.0947 (0.7626) acc 68.7500 (82.8571) lr 1.2369e-04 eta 0:00:37
epoch [23/25] batch [75/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.7256 (0.7690) acc 81.2500 (82.5000) lr 1.2369e-04 eta 0:00:36
epoch [23/25] batch [80/100] time 0.142 (0.159) data 0.000 (0.018) loss 0.9858 (0.7629) acc 78.1250 (82.7344) lr 1.2369e-04 eta 0:00:35
epoch [23/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.017) loss 0.5469 (0.7546) acc 90.6250 (83.0882) lr 1.2369e-04 eta 0:00:34
epoch [23/25] batch [90/100] time 0.141 (0.158) data 0.000 (0.016) loss 0.5527 (0.7432) acc 84.3750 (83.1597) lr 1.2369e-04 eta 0:00:33
epoch [23/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.8682 (0.7435) acc 75.0000 (83.0921) lr 1.2369e-04 eta 0:00:32
epoch [23/25] batch [100/100] time 0.123 (0.156) data 0.000 (0.014) loss 1.0527 (0.7507) acc 81.2500 (83.1250) lr 7.0224e-05 eta 0:00:31
epoch [24/25] batch [5/100] time 0.142 (0.367) data 0.000 (0.223) loss 0.5801 (0.6382) acc 84.3750 (85.0000) lr 7.0224e-05 eta 0:01:11
epoch [24/25] batch [10/100] time 0.142 (0.255) data 0.001 (0.112) loss 0.7524 (0.7142) acc 84.3750 (81.5625) lr 7.0224e-05 eta 0:00:48
epoch [24/25] batch [15/100] time 0.146 (0.218) data 0.000 (0.075) loss 1.1309 (0.7300) acc 75.0000 (82.2917) lr 7.0224e-05 eta 0:00:40
epoch [24/25] batch [20/100] time 0.142 (0.199) data 0.000 (0.056) loss 0.6748 (0.7532) acc 84.3750 (81.8750) lr 7.0224e-05 eta 0:00:35
epoch [24/25] batch [25/100] time 0.142 (0.188) data 0.000 (0.045) loss 0.7686 (0.7580) acc 84.3750 (82.1250) lr 7.0224e-05 eta 0:00:32
epoch [24/25] batch [30/100] time 0.141 (0.180) data 0.000 (0.038) loss 0.8472 (0.7424) acc 81.2500 (82.9167) lr 7.0224e-05 eta 0:00:30
epoch [24/25] batch [35/100] time 0.141 (0.174) data 0.000 (0.032) loss 0.6499 (0.7367) acc 87.5000 (83.3929) lr 7.0224e-05 eta 0:00:28
epoch [24/25] batch [40/100] time 0.142 (0.170) data 0.000 (0.028) loss 0.8071 (0.7505) acc 78.1250 (82.9688) lr 7.0224e-05 eta 0:00:27
epoch [24/25] batch [45/100] time 0.143 (0.167) data 0.000 (0.025) loss 0.9199 (0.7438) acc 65.6250 (82.8472) lr 7.0224e-05 eta 0:00:25
epoch [24/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.8013 (0.7469) acc 81.2500 (82.7500) lr 7.0224e-05 eta 0:00:24
epoch [24/25] batch [55/100] time 0.142 (0.163) data 0.000 (0.021) loss 1.0391 (0.7536) acc 65.6250 (82.3864) lr 7.0224e-05 eta 0:00:23
epoch [24/25] batch [60/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.5557 (0.7440) acc 87.5000 (82.7083) lr 7.0224e-05 eta 0:00:22
epoch [24/25] batch [65/100] time 0.142 (0.160) data 0.001 (0.017) loss 0.9438 (0.7551) acc 71.8750 (82.0673) lr 7.0224e-05 eta 0:00:21
epoch [24/25] batch [70/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.7549 (0.7529) acc 78.1250 (81.8304) lr 7.0224e-05 eta 0:00:20
epoch [24/25] batch [75/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.6875 (0.7441) acc 90.6250 (82.2500) lr 7.0224e-05 eta 0:00:19
epoch [24/25] batch [80/100] time 0.141 (0.156) data 0.000 (0.014) loss 0.5830 (0.7370) acc 93.7500 (82.6562) lr 7.0224e-05 eta 0:00:18
epoch [24/25] batch [85/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5625 (0.7307) acc 90.6250 (82.9044) lr 7.0224e-05 eta 0:00:17
epoch [24/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.6538 (0.7311) acc 84.3750 (83.0556) lr 7.0224e-05 eta 0:00:17
epoch [24/25] batch [95/100] time 0.141 (0.154) data 0.000 (0.012) loss 0.5186 (0.7257) acc 87.5000 (83.1908) lr 7.0224e-05 eta 0:00:16
epoch [24/25] batch [100/100] time 0.123 (0.153) data 0.000 (0.011) loss 1.1650 (0.7306) acc 81.2500 (83.1875) lr 3.1417e-05 eta 0:00:15
epoch [25/25] batch [5/100] time 0.142 (0.421) data 0.000 (0.279) loss 0.5532 (0.6484) acc 93.7500 (86.2500) lr 3.1417e-05 eta 0:00:40
epoch [25/25] batch [10/100] time 0.141 (0.281) data 0.000 (0.140) loss 0.5977 (0.6695) acc 81.2500 (85.3125) lr 3.1417e-05 eta 0:00:25
epoch [25/25] batch [15/100] time 0.142 (0.235) data 0.000 (0.093) loss 0.7402 (0.6695) acc 87.5000 (85.4167) lr 3.1417e-05 eta 0:00:19
epoch [25/25] batch [20/100] time 0.143 (0.212) data 0.000 (0.070) loss 0.9683 (0.6730) acc 78.1250 (85.3125) lr 3.1417e-05 eta 0:00:16
epoch [25/25] batch [25/100] time 0.142 (0.198) data 0.000 (0.056) loss 0.6826 (0.6698) acc 90.6250 (85.8750) lr 3.1417e-05 eta 0:00:14
epoch [25/25] batch [30/100] time 0.142 (0.189) data 0.000 (0.047) loss 0.3821 (0.6873) acc 93.7500 (85.2083) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [35/100] time 0.141 (0.182) data 0.000 (0.040) loss 0.6855 (0.6943) acc 78.1250 (84.7321) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [40/100] time 0.142 (0.177) data 0.000 (0.035) loss 0.5278 (0.6844) acc 87.5000 (84.5312) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [45/100] time 0.142 (0.173) data 0.000 (0.031) loss 0.7505 (0.6887) acc 87.5000 (84.4444) lr 3.1417e-05 eta 0:00:09
epoch [25/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.028) loss 0.8501 (0.6914) acc 84.3750 (84.6875) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [55/100] time 0.141 (0.167) data 0.000 (0.026) loss 0.3894 (0.7025) acc 96.8750 (84.4886) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [60/100] time 0.142 (0.165) data 0.001 (0.024) loss 0.5029 (0.7048) acc 87.5000 (84.5833) lr 3.1417e-05 eta 0:00:06
epoch [25/25] batch [65/100] time 0.141 (0.164) data 0.000 (0.022) loss 0.6108 (0.6987) acc 87.5000 (84.6635) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.7095 (0.6934) acc 84.3750 (84.7768) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [75/100] time 0.144 (0.161) data 0.000 (0.019) loss 0.7686 (0.6901) acc 84.3750 (84.9167) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.5430 (0.6868) acc 87.5000 (85.1562) lr 3.1417e-05 eta 0:00:03
epoch [25/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.7041 (0.6871) acc 84.3750 (85.1471) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.7808 (0.6896) acc 84.3750 (84.8611) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.5381 (0.6897) acc 90.6250 (84.6382) lr 3.1417e-05 eta 0:00:00
epoch [25/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.6089 (0.6864) acc 81.2500 (84.6875) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:14<04:37, 14.60s/it] 10%|█         | 2/20 [00:17<02:17,  7.62s/it] 15%|█▌        | 3/20 [00:17<01:15,  4.42s/it] 20%|██        | 4/20 [00:18<00:46,  2.91s/it] 25%|██▌       | 5/20 [00:19<00:31,  2.08s/it] 30%|███       | 6/20 [00:19<00:22,  1.58s/it] 35%|███▌      | 7/20 [00:20<00:16,  1.26s/it] 40%|████      | 8/20 [00:20<00:12,  1.05s/it] 45%|████▌     | 9/20 [00:23<00:15,  1.44s/it] 50%|█████     | 10/20 [00:28<00:26,  2.70s/it] 55%|█████▌    | 11/20 [00:29<00:18,  2.06s/it] 60%|██████    | 12/20 [00:29<00:12,  1.61s/it] 65%|██████▌   | 13/20 [00:30<00:09,  1.31s/it] 70%|███████   | 14/20 [00:31<00:06,  1.10s/it] 75%|███████▌  | 15/20 [00:31<00:04,  1.05it/s] 80%|████████  | 16/20 [00:32<00:03,  1.18it/s] 85%|████████▌ | 17/20 [00:34<00:03,  1.26s/it] 90%|█████████ | 18/20 [00:43<00:07,  3.53s/it] 95%|█████████▌| 19/20 [00:44<00:02,  2.65s/it]100%|██████████| 20/20 [00:44<00:00,  2.04s/it]100%|██████████| 20/20 [00:44<00:00,  2.24s/it]
=> result
* total: 9,950
* correct: 8,211
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 82.3%
Elapsed: 0:07:20
Run this job and save the output to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      796
# test     9,950
---------  ------
['abbey', 'airplane_cabin', 'airport_terminal', 'alley', 'amphitheater', 'amusement_arcade', 'amusement_park', 'anechoic_chamber', 'outdoor apartment_building', 'indoor apse', 'aquarium', 'aqueduct', 'arch', 'archive', 'outdoor arrival_gate', 'art_gallery', 'art_school', 'art_studio', 'assembly_line', 'outdoor athletic_field', 'public atrium', 'attic', 'auditorium', 'auto_factory', 'badlands', 'indoor badminton_court', 'baggage_claim', 'shop bakery', 'exterior balcony', 'interior balcony', 'ball_pit', 'ballroom', 'bamboo_forest', 'banquet_hall', 'bar', 'barn', 'barndoor', 'baseball_field', 'basement', 'basilica', 'outdoor basketball_court', 'bathroom', 'batters_box', 'bayou', 'indoor bazaar', 'outdoor bazaar', 'beach', 'beauty_salon', 'bedroom', 'berth', 'biology_laboratory', 'indoor bistro', 'boardwalk', 'boat_deck', 'boathouse', 'bookstore', 'indoor booth', 'botanical_garden', 'indoor bow_window', 'outdoor bow_window', 'bowling_alley', 'boxing_ring', 'indoor brewery', 'bridge', 'building_facade', 'bullring', 'burial_chamber', 'bus_interior', 'butchers_shop', 'butte', 'outdoor cabin', 'cafeteria', 'campsite', 'campus', 'natural canal', 'urban canal', 'candy_store', 'canyon', 'backseat car_interior', 'frontseat car_interior', 'carrousel', 'indoor casino', 'castle', 'catacomb', 'indoor cathedral', 'outdoor cathedral', 'indoor cavern', 'cemetery', 'chalet', 'cheese_factory', 'chemistry_lab', 'indoor chicken_coop', 'outdoor chicken_coop', 'childs_room', 'indoor church', 'outdoor church', 'classroom', 'clean_room', 'cliff', 'indoor cloister', 'closet', 'clothing_store', 'coast', 'cockpit', 'coffee_shop', 'computer_room', 'conference_center', 'conference_room', 'construction_site', 'control_room', 'outdoor control_tower', 'corn_field', 'corral', 'corridor', 'cottage_garden', 'courthouse', 'courtroom', 'courtyard', 'exterior covered_bridge', 'creek', 'crevasse', 'crosswalk', 'office cubicle', 'dam', 'delicatessen', 'dentists_office', 'sand desert', 'vegetation desert', 'indoor diner', 'outdoor diner', 'home dinette', 'vehicle dinette', 'dining_car', 'dining_room', 'discotheque', 'dock', 'outdoor doorway', 'dorm_room', 'driveway', 'outdoor driving_range', 'drugstore', 'electrical_substation', 'door elevator', 'interior elevator', 'elevator_shaft', 'engine_room', 'indoor escalator', 'excavation', 'indoor factory', 'fairway', 'fastfood_restaurant', 'cultivated field', 'wild field', 'fire_escape', 'fire_station', 'indoor firing_range', 'fishpond', 'indoor florist_shop', 'food_court', 'broadleaf forest', 'needleleaf forest', 'forest_path', 'forest_road', 'formal_garden', 'fountain', 'galley', 'game_room', 'indoor garage', 'garbage_dump', 'gas_station', 'exterior gazebo', 'indoor general_store', 'outdoor general_store', 'gift_shop', 'golf_course', 'indoor greenhouse', 'outdoor greenhouse', 'indoor gymnasium', 'indoor hangar', 'outdoor hangar', 'harbor', 'hayfield', 'heliport', 'herb_garden', 'highway', 'hill', 'home_office', 'hospital', 'hospital_room', 'hot_spring', 'outdoor hot_tub', 'outdoor hotel', 'hotel_room', 'house', 'outdoor hunting_lodge', 'ice_cream_parlor', 'ice_floe', 'ice_shelf', 'indoor ice_skating_rink']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abbey.', 'X X X X airplane cabin.', 'X X X X airport terminal.', 'X X X X alley.', 'X X X X amphitheater.', 'X X X X amusement arcade.', 'X X X X amusement park.', 'X X X X anechoic chamber.', 'X X X X outdoor apartment building.', 'X X X X indoor apse.', 'X X X X aquarium.', 'X X X X aqueduct.', 'X X X X arch.', 'X X X X archive.', 'X X X X outdoor arrival gate.', 'X X X X art gallery.', 'X X X X art school.', 'X X X X art studio.', 'X X X X assembly line.', 'X X X X outdoor athletic field.', 'X X X X public atrium.', 'X X X X attic.', 'X X X X auditorium.', 'X X X X auto factory.', 'X X X X badlands.', 'X X X X indoor badminton court.', 'X X X X baggage claim.', 'X X X X shop bakery.', 'X X X X exterior balcony.', 'X X X X interior balcony.', 'X X X X ball pit.', 'X X X X ballroom.', 'X X X X bamboo forest.', 'X X X X banquet hall.', 'X X X X bar.', 'X X X X barn.', 'X X X X barndoor.', 'X X X X baseball field.', 'X X X X basement.', 'X X X X basilica.', 'X X X X outdoor basketball court.', 'X X X X bathroom.', 'X X X X batters box.', 'X X X X bayou.', 'X X X X indoor bazaar.', 'X X X X outdoor bazaar.', 'X X X X beach.', 'X X X X beauty salon.', 'X X X X bedroom.', 'X X X X berth.', 'X X X X biology laboratory.', 'X X X X indoor bistro.', 'X X X X boardwalk.', 'X X X X boat deck.', 'X X X X boathouse.', 'X X X X bookstore.', 'X X X X indoor booth.', 'X X X X botanical garden.', 'X X X X indoor bow window.', 'X X X X outdoor bow window.', 'X X X X bowling alley.', 'X X X X boxing ring.', 'X X X X indoor brewery.', 'X X X X bridge.', 'X X X X building facade.', 'X X X X bullring.', 'X X X X burial chamber.', 'X X X X bus interior.', 'X X X X butchers shop.', 'X X X X butte.', 'X X X X outdoor cabin.', 'X X X X cafeteria.', 'X X X X campsite.', 'X X X X campus.', 'X X X X natural canal.', 'X X X X urban canal.', 'X X X X candy store.', 'X X X X canyon.', 'X X X X backseat car interior.', 'X X X X frontseat car interior.', 'X X X X carrousel.', 'X X X X indoor casino.', 'X X X X castle.', 'X X X X catacomb.', 'X X X X indoor cathedral.', 'X X X X outdoor cathedral.', 'X X X X indoor cavern.', 'X X X X cemetery.', 'X X X X chalet.', 'X X X X cheese factory.', 'X X X X chemistry lab.', 'X X X X indoor chicken coop.', 'X X X X outdoor chicken coop.', 'X X X X childs room.', 'X X X X indoor church.', 'X X X X outdoor church.', 'X X X X classroom.', 'X X X X clean room.', 'X X X X cliff.', 'X X X X indoor cloister.', 'X X X X closet.', 'X X X X clothing store.', 'X X X X coast.', 'X X X X cockpit.', 'X X X X coffee shop.', 'X X X X computer room.', 'X X X X conference center.', 'X X X X conference room.', 'X X X X construction site.', 'X X X X control room.', 'X X X X outdoor control tower.', 'X X X X corn field.', 'X X X X corral.', 'X X X X corridor.', 'X X X X cottage garden.', 'X X X X courthouse.', 'X X X X courtroom.', 'X X X X courtyard.', 'X X X X exterior covered bridge.', 'X X X X creek.', 'X X X X crevasse.', 'X X X X crosswalk.', 'X X X X office cubicle.', 'X X X X dam.', 'X X X X delicatessen.', 'X X X X dentists office.', 'X X X X sand desert.', 'X X X X vegetation desert.', 'X X X X indoor diner.', 'X X X X outdoor diner.', 'X X X X home dinette.', 'X X X X vehicle dinette.', 'X X X X dining car.', 'X X X X dining room.', 'X X X X discotheque.', 'X X X X dock.', 'X X X X outdoor doorway.', 'X X X X dorm room.', 'X X X X driveway.', 'X X X X outdoor driving range.', 'X X X X drugstore.', 'X X X X electrical substation.', 'X X X X door elevator.', 'X X X X interior elevator.', 'X X X X elevator shaft.', 'X X X X engine room.', 'X X X X indoor escalator.', 'X X X X excavation.', 'X X X X indoor factory.', 'X X X X fairway.', 'X X X X fastfood restaurant.', 'X X X X cultivated field.', 'X X X X wild field.', 'X X X X fire escape.', 'X X X X fire station.', 'X X X X indoor firing range.', 'X X X X fishpond.', 'X X X X indoor florist shop.', 'X X X X food court.', 'X X X X broadleaf forest.', 'X X X X needleleaf forest.', 'X X X X forest path.', 'X X X X forest road.', 'X X X X formal garden.', 'X X X X fountain.', 'X X X X galley.', 'X X X X game room.', 'X X X X indoor garage.', 'X X X X garbage dump.', 'X X X X gas station.', 'X X X X exterior gazebo.', 'X X X X indoor general store.', 'X X X X outdoor general store.', 'X X X X gift shop.', 'X X X X golf course.', 'X X X X indoor greenhouse.', 'X X X X outdoor greenhouse.', 'X X X X indoor gymnasium.', 'X X X X indoor hangar.', 'X X X X outdoor hangar.', 'X X X X harbor.', 'X X X X hayfield.', 'X X X X heliport.', 'X X X X herb garden.', 'X X X X highway.', 'X X X X hill.', 'X X X X home office.', 'X X X X hospital.', 'X X X X hospital room.', 'X X X X hot spring.', 'X X X X outdoor hot tub.', 'X X X X outdoor hotel.', 'X X X X hotel room.', 'X X X X house.', 'X X X X outdoor hunting lodge.', 'X X X X ice cream parlor.', 'X X X X ice floe.', 'X X X X ice shelf.', 'X X X X indoor ice skating rink.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/25] batch [5/100] time 0.140 (0.513) data 0.001 (0.354) loss 3.3984 (3.3926) acc 56.2500 (51.2500) lr 1.0000e-05 eta 0:21:19
epoch [1/25] batch [10/100] time 0.141 (0.327) data 0.000 (0.177) loss 3.5547 (3.3547) acc 59.3750 (54.0625) lr 1.0000e-05 eta 0:13:33
epoch [1/25] batch [15/100] time 0.140 (0.265) data 0.000 (0.118) loss 3.1484 (3.3212) acc 62.5000 (53.3333) lr 1.0000e-05 eta 0:10:59
epoch [1/25] batch [20/100] time 0.141 (0.234) data 0.000 (0.089) loss 2.9238 (3.3059) acc 56.2500 (53.5938) lr 1.0000e-05 eta 0:09:41
epoch [1/25] batch [25/100] time 0.141 (0.216) data 0.000 (0.071) loss 3.0781 (3.2696) acc 56.2500 (53.6250) lr 1.0000e-05 eta 0:08:54
epoch [1/25] batch [30/100] time 0.140 (0.204) data 0.000 (0.059) loss 3.8105 (3.2777) acc 53.1250 (54.4792) lr 1.0000e-05 eta 0:08:22
epoch [1/25] batch [35/100] time 0.142 (0.195) data 0.001 (0.051) loss 2.6562 (3.2529) acc 62.5000 (54.3750) lr 1.0000e-05 eta 0:07:59
epoch [1/25] batch [40/100] time 0.140 (0.188) data 0.000 (0.045) loss 2.8633 (3.2401) acc 50.0000 (53.9062) lr 1.0000e-05 eta 0:07:42
epoch [1/25] batch [45/100] time 0.140 (0.183) data 0.000 (0.040) loss 3.0820 (3.2296) acc 56.2500 (53.8889) lr 1.0000e-05 eta 0:07:28
epoch [1/25] batch [50/100] time 0.140 (0.178) data 0.000 (0.036) loss 3.2812 (3.2229) acc 50.0000 (53.6250) lr 1.0000e-05 eta 0:07:17
epoch [1/25] batch [55/100] time 0.140 (0.175) data 0.000 (0.033) loss 2.5508 (3.1803) acc 71.8750 (54.4318) lr 1.0000e-05 eta 0:07:07
epoch [1/25] batch [60/100] time 0.141 (0.172) data 0.000 (0.030) loss 3.0664 (3.1707) acc 53.1250 (54.1146) lr 1.0000e-05 eta 0:07:00
epoch [1/25] batch [65/100] time 0.140 (0.170) data 0.000 (0.028) loss 2.8203 (3.1550) acc 53.1250 (53.7500) lr 1.0000e-05 eta 0:06:53
epoch [1/25] batch [70/100] time 0.142 (0.168) data 0.001 (0.026) loss 2.7734 (3.1314) acc 71.8750 (54.2411) lr 1.0000e-05 eta 0:06:47
epoch [1/25] batch [75/100] time 0.141 (0.166) data 0.000 (0.024) loss 2.9219 (3.1101) acc 59.3750 (54.5000) lr 1.0000e-05 eta 0:06:42
epoch [1/25] batch [80/100] time 0.141 (0.164) data 0.000 (0.022) loss 2.9277 (3.0917) acc 59.3750 (54.8438) lr 1.0000e-05 eta 0:06:37
epoch [1/25] batch [85/100] time 0.141 (0.163) data 0.000 (0.021) loss 2.7246 (3.0740) acc 56.2500 (55.0000) lr 1.0000e-05 eta 0:06:33
epoch [1/25] batch [90/100] time 0.141 (0.162) data 0.000 (0.020) loss 2.3555 (3.0548) acc 71.8750 (55.3125) lr 1.0000e-05 eta 0:06:30
epoch [1/25] batch [95/100] time 0.141 (0.161) data 0.000 (0.019) loss 2.6523 (3.0360) acc 53.1250 (55.4605) lr 1.0000e-05 eta 0:06:26
epoch [1/25] batch [100/100] time 0.222 (0.161) data 0.000 (0.018) loss 2.2461 (3.0149) acc 62.5000 (55.6250) lr 2.0000e-03 eta 0:06:25
epoch [2/25] batch [5/100] time 0.143 (0.377) data 0.001 (0.234) loss 1.9209 (2.2584) acc 56.2500 (55.6250) lr 2.0000e-03 eta 0:15:03
epoch [2/25] batch [10/100] time 0.142 (0.260) data 0.001 (0.118) loss 1.6064 (2.0129) acc 68.7500 (60.0000) lr 2.0000e-03 eta 0:10:22
epoch [2/25] batch [15/100] time 0.142 (0.221) data 0.000 (0.078) loss 1.7598 (1.9023) acc 62.5000 (62.5000) lr 2.0000e-03 eta 0:08:46
epoch [2/25] batch [20/100] time 0.142 (0.201) data 0.000 (0.059) loss 1.8486 (1.7579) acc 56.2500 (63.7500) lr 2.0000e-03 eta 0:07:58
epoch [2/25] batch [25/100] time 0.155 (0.190) data 0.000 (0.047) loss 1.9170 (1.7052) acc 50.0000 (63.8750) lr 2.0000e-03 eta 0:07:30
epoch [2/25] batch [30/100] time 0.143 (0.182) data 0.000 (0.039) loss 1.3301 (1.6444) acc 75.0000 (64.7917) lr 2.0000e-03 eta 0:07:11
epoch [2/25] batch [35/100] time 0.142 (0.176) data 0.000 (0.034) loss 1.1406 (1.6258) acc 68.7500 (64.2857) lr 2.0000e-03 eta 0:06:57
epoch [2/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 1.4814 (1.5903) acc 71.8750 (64.6875) lr 2.0000e-03 eta 0:06:46
epoch [2/25] batch [45/100] time 0.142 (0.169) data 0.001 (0.026) loss 1.5693 (1.5543) acc 62.5000 (65.2083) lr 2.0000e-03 eta 0:06:37
epoch [2/25] batch [50/100] time 0.144 (0.166) data 0.000 (0.024) loss 1.4346 (1.5292) acc 50.0000 (65.4375) lr 2.0000e-03 eta 0:06:30
epoch [2/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.6045 (1.5145) acc 62.5000 (65.7386) lr 2.0000e-03 eta 0:06:24
epoch [2/25] batch [60/100] time 0.141 (0.162) data 0.000 (0.020) loss 0.8452 (1.4855) acc 78.1250 (66.2500) lr 2.0000e-03 eta 0:06:19
epoch [2/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.018) loss 1.3135 (1.4709) acc 78.1250 (66.7308) lr 2.0000e-03 eta 0:06:14
epoch [2/25] batch [70/100] time 0.142 (0.159) data 0.001 (0.017) loss 1.0547 (1.4437) acc 78.1250 (67.1429) lr 2.0000e-03 eta 0:06:10
epoch [2/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 1.0586 (1.4316) acc 71.8750 (67.4583) lr 2.0000e-03 eta 0:06:07
epoch [2/25] batch [80/100] time 0.145 (0.157) data 0.000 (0.015) loss 1.0254 (1.4128) acc 68.7500 (67.5000) lr 2.0000e-03 eta 0:06:04
epoch [2/25] batch [85/100] time 0.143 (0.156) data 0.000 (0.014) loss 1.7734 (1.4076) acc 50.0000 (67.4632) lr 2.0000e-03 eta 0:06:01
epoch [2/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 1.2314 (1.3939) acc 62.5000 (67.6389) lr 2.0000e-03 eta 0:05:58
epoch [2/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.013) loss 0.8164 (1.3813) acc 81.2500 (67.7303) lr 2.0000e-03 eta 0:05:56
epoch [2/25] batch [100/100] time 0.123 (0.154) data 0.000 (0.012) loss 1.1172 (1.3623) acc 75.0000 (68.1250) lr 1.9921e-03 eta 0:05:53
epoch [3/25] batch [5/100] time 0.142 (0.404) data 0.000 (0.261) loss 1.1943 (0.9685) acc 59.3750 (73.7500) lr 1.9921e-03 eta 0:15:26
epoch [3/25] batch [10/100] time 0.142 (0.273) data 0.000 (0.131) loss 1.1240 (1.0600) acc 71.8750 (71.2500) lr 1.9921e-03 eta 0:10:24
epoch [3/25] batch [15/100] time 0.141 (0.229) data 0.000 (0.088) loss 1.0879 (1.1413) acc 75.0000 (70.6250) lr 1.9921e-03 eta 0:08:43
epoch [3/25] batch [20/100] time 0.142 (0.207) data 0.000 (0.066) loss 1.3613 (1.1509) acc 65.6250 (70.4688) lr 1.9921e-03 eta 0:07:52
epoch [3/25] batch [25/100] time 0.141 (0.194) data 0.000 (0.053) loss 0.7847 (1.1222) acc 81.2500 (71.1250) lr 1.9921e-03 eta 0:07:21
epoch [3/25] batch [30/100] time 0.142 (0.185) data 0.000 (0.044) loss 1.3291 (1.1375) acc 71.8750 (71.4583) lr 1.9921e-03 eta 0:07:01
epoch [3/25] batch [35/100] time 0.141 (0.179) data 0.000 (0.038) loss 1.2119 (1.1244) acc 75.0000 (71.9643) lr 1.9921e-03 eta 0:06:45
epoch [3/25] batch [40/100] time 0.142 (0.174) data 0.000 (0.033) loss 1.2529 (1.1476) acc 68.7500 (71.3281) lr 1.9921e-03 eta 0:06:34
epoch [3/25] batch [45/100] time 0.141 (0.171) data 0.000 (0.029) loss 1.1348 (1.1608) acc 68.7500 (70.7639) lr 1.9921e-03 eta 0:06:25
epoch [3/25] batch [50/100] time 0.141 (0.168) data 0.000 (0.026) loss 1.1895 (1.1525) acc 71.8750 (70.8125) lr 1.9921e-03 eta 0:06:17
epoch [3/25] batch [55/100] time 0.142 (0.166) data 0.000 (0.024) loss 1.2607 (1.1478) acc 71.8750 (71.0795) lr 1.9921e-03 eta 0:06:11
epoch [3/25] batch [60/100] time 0.142 (0.164) data 0.000 (0.022) loss 1.2773 (1.1600) acc 65.6250 (70.7292) lr 1.9921e-03 eta 0:06:06
epoch [3/25] batch [65/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.8877 (1.1560) acc 81.2500 (70.8173) lr 1.9921e-03 eta 0:06:02
epoch [3/25] batch [70/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.8979 (1.1509) acc 81.2500 (71.1161) lr 1.9921e-03 eta 0:05:58
epoch [3/25] batch [75/100] time 0.142 (0.159) data 0.000 (0.018) loss 0.9341 (1.1429) acc 75.0000 (71.3750) lr 1.9921e-03 eta 0:05:54
epoch [3/25] batch [80/100] time 0.141 (0.158) data 0.000 (0.017) loss 0.9922 (1.1302) acc 71.8750 (71.6797) lr 1.9921e-03 eta 0:05:51
epoch [3/25] batch [85/100] time 0.141 (0.157) data 0.000 (0.016) loss 1.0293 (1.1191) acc 81.2500 (72.1324) lr 1.9921e-03 eta 0:05:48
epoch [3/25] batch [90/100] time 0.141 (0.157) data 0.000 (0.015) loss 0.8677 (1.1077) acc 84.3750 (72.7083) lr 1.9921e-03 eta 0:05:45
epoch [3/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.8818 (1.0980) acc 81.2500 (72.8289) lr 1.9921e-03 eta 0:05:43
epoch [3/25] batch [100/100] time 0.124 (0.155) data 0.000 (0.013) loss 0.8784 (1.0962) acc 68.7500 (72.6250) lr 1.9686e-03 eta 0:05:40
epoch [4/25] batch [5/100] time 0.142 (0.403) data 0.000 (0.261) loss 1.0811 (1.0473) acc 78.1250 (71.8750) lr 1.9686e-03 eta 0:14:44
epoch [4/25] batch [10/100] time 0.141 (0.272) data 0.000 (0.131) loss 1.0215 (1.0788) acc 68.7500 (72.1875) lr 1.9686e-03 eta 0:09:55
epoch [4/25] batch [15/100] time 0.142 (0.228) data 0.000 (0.087) loss 1.0596 (1.0869) acc 65.6250 (72.2917) lr 1.9686e-03 eta 0:08:19
epoch [4/25] batch [20/100] time 0.141 (0.207) data 0.000 (0.066) loss 1.2217 (1.0942) acc 65.6250 (72.9688) lr 1.9686e-03 eta 0:07:30
epoch [4/25] batch [25/100] time 0.141 (0.194) data 0.000 (0.052) loss 0.5830 (1.0629) acc 84.3750 (73.8750) lr 1.9686e-03 eta 0:07:01
epoch [4/25] batch [30/100] time 0.141 (0.185) data 0.000 (0.044) loss 1.4883 (1.0566) acc 62.5000 (73.9583) lr 1.9686e-03 eta 0:06:41
epoch [4/25] batch [35/100] time 0.141 (0.179) data 0.000 (0.038) loss 1.1729 (1.0454) acc 71.8750 (73.6607) lr 1.9686e-03 eta 0:06:26
epoch [4/25] batch [40/100] time 0.142 (0.174) data 0.000 (0.033) loss 0.9033 (1.0430) acc 78.1250 (73.7500) lr 1.9686e-03 eta 0:06:15
epoch [4/25] batch [45/100] time 0.141 (0.170) data 0.000 (0.029) loss 1.3184 (1.0591) acc 68.7500 (73.2639) lr 1.9686e-03 eta 0:06:07
epoch [4/25] batch [50/100] time 0.141 (0.168) data 0.000 (0.026) loss 1.0156 (1.0366) acc 81.2500 (73.9375) lr 1.9686e-03 eta 0:06:00
epoch [4/25] batch [55/100] time 0.142 (0.165) data 0.000 (0.024) loss 0.6875 (1.0411) acc 84.3750 (73.7500) lr 1.9686e-03 eta 0:05:54
epoch [4/25] batch [60/100] time 0.141 (0.163) data 0.000 (0.022) loss 0.9766 (1.0390) acc 84.3750 (74.1146) lr 1.9686e-03 eta 0:05:49
epoch [4/25] batch [65/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.2852 (1.0434) acc 62.5000 (73.9423) lr 1.9686e-03 eta 0:05:45
epoch [4/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.019) loss 0.7520 (1.0323) acc 84.3750 (74.3750) lr 1.9686e-03 eta 0:05:41
epoch [4/25] batch [75/100] time 0.144 (0.159) data 0.000 (0.018) loss 0.8994 (1.0380) acc 71.8750 (74.1667) lr 1.9686e-03 eta 0:05:38
epoch [4/25] batch [80/100] time 0.142 (0.158) data 0.000 (0.017) loss 1.1885 (1.0459) acc 53.1250 (73.5156) lr 1.9686e-03 eta 0:05:34
epoch [4/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.016) loss 0.9307 (1.0427) acc 75.0000 (73.5294) lr 1.9686e-03 eta 0:05:32
epoch [4/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.015) loss 1.0410 (1.0404) acc 78.1250 (73.6806) lr 1.9686e-03 eta 0:05:29
epoch [4/25] batch [95/100] time 0.141 (0.155) data 0.000 (0.014) loss 0.7656 (1.0346) acc 87.5000 (73.8158) lr 1.9686e-03 eta 0:05:27
epoch [4/25] batch [100/100] time 0.123 (0.155) data 0.000 (0.013) loss 1.0996 (1.0399) acc 56.2500 (73.5000) lr 1.9298e-03 eta 0:05:24
epoch [5/25] batch [5/100] time 0.141 (0.439) data 0.000 (0.297) loss 1.0420 (0.8446) acc 71.8750 (77.5000) lr 1.9298e-03 eta 0:15:19
epoch [5/25] batch [10/100] time 0.143 (0.291) data 0.000 (0.149) loss 1.2646 (1.0112) acc 71.8750 (73.1250) lr 1.9298e-03 eta 0:10:07
epoch [5/25] batch [15/100] time 0.143 (0.241) data 0.000 (0.099) loss 0.8159 (1.0115) acc 78.1250 (73.9583) lr 1.9298e-03 eta 0:08:22
epoch [5/25] batch [20/100] time 0.146 (0.217) data 0.000 (0.075) loss 1.2266 (1.0104) acc 65.6250 (74.2188) lr 1.9298e-03 eta 0:07:30
epoch [5/25] batch [25/100] time 0.142 (0.202) data 0.000 (0.060) loss 0.6719 (0.9977) acc 78.1250 (75.1250) lr 1.9298e-03 eta 0:06:58
epoch [5/25] batch [30/100] time 0.144 (0.192) data 0.000 (0.050) loss 1.1484 (1.0176) acc 71.8750 (74.0625) lr 1.9298e-03 eta 0:06:36
epoch [5/25] batch [35/100] time 0.142 (0.185) data 0.000 (0.043) loss 1.0410 (1.0252) acc 68.7500 (73.3929) lr 1.9298e-03 eta 0:06:21
epoch [5/25] batch [40/100] time 0.142 (0.179) data 0.000 (0.037) loss 1.3662 (1.0256) acc 62.5000 (73.1250) lr 1.9298e-03 eta 0:06:09
epoch [5/25] batch [45/100] time 0.141 (0.175) data 0.000 (0.033) loss 0.8130 (1.0150) acc 78.1250 (73.9583) lr 1.9298e-03 eta 0:05:59
epoch [5/25] batch [50/100] time 0.142 (0.172) data 0.000 (0.030) loss 1.1250 (1.0181) acc 71.8750 (73.5625) lr 1.9298e-03 eta 0:05:52
epoch [5/25] batch [55/100] time 0.146 (0.169) data 0.000 (0.027) loss 0.8442 (1.0213) acc 78.1250 (73.6932) lr 1.9298e-03 eta 0:05:46
epoch [5/25] batch [60/100] time 0.145 (0.167) data 0.000 (0.025) loss 0.9614 (1.0054) acc 75.0000 (74.1146) lr 1.9298e-03 eta 0:05:40
epoch [5/25] batch [65/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.8242 (1.0222) acc 81.2500 (73.9423) lr 1.9298e-03 eta 0:05:35
epoch [5/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8481 (1.0086) acc 81.2500 (74.2411) lr 1.9298e-03 eta 0:05:31
epoch [5/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.020) loss 1.3721 (1.0062) acc 68.7500 (74.5417) lr 1.9298e-03 eta 0:05:28
epoch [5/25] batch [80/100] time 0.142 (0.161) data 0.000 (0.019) loss 1.5684 (1.0115) acc 65.6250 (74.6484) lr 1.9298e-03 eta 0:05:24
epoch [5/25] batch [85/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.8618 (1.0177) acc 65.6250 (74.6324) lr 1.9298e-03 eta 0:05:21
epoch [5/25] batch [90/100] time 0.142 (0.159) data 0.000 (0.017) loss 1.2520 (1.0174) acc 71.8750 (74.5833) lr 1.9298e-03 eta 0:05:18
epoch [5/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9019 (1.0113) acc 84.3750 (74.7039) lr 1.9298e-03 eta 0:05:16
epoch [5/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.8511 (1.0167) acc 81.2500 (74.5000) lr 1.8763e-03 eta 0:05:13
epoch [6/25] batch [5/100] time 0.141 (0.422) data 0.000 (0.279) loss 1.2988 (1.1571) acc 65.6250 (70.6250) lr 1.8763e-03 eta 0:14:02
epoch [6/25] batch [10/100] time 0.146 (0.283) data 0.001 (0.140) loss 0.8511 (1.0602) acc 75.0000 (72.5000) lr 1.8763e-03 eta 0:09:23
epoch [6/25] batch [15/100] time 0.144 (0.237) data 0.000 (0.093) loss 1.0996 (1.0335) acc 68.7500 (73.1250) lr 1.8763e-03 eta 0:07:50
epoch [6/25] batch [20/100] time 0.142 (0.213) data 0.000 (0.070) loss 1.2988 (1.0267) acc 65.6250 (74.2188) lr 1.8763e-03 eta 0:07:02
epoch [6/25] batch [25/100] time 0.142 (0.199) data 0.000 (0.056) loss 0.7056 (0.9906) acc 84.3750 (75.5000) lr 1.8763e-03 eta 0:06:33
epoch [6/25] batch [30/100] time 0.142 (0.190) data 0.000 (0.047) loss 0.9932 (0.9814) acc 78.1250 (75.7292) lr 1.8763e-03 eta 0:06:13
epoch [6/25] batch [35/100] time 0.142 (0.183) data 0.000 (0.040) loss 0.8564 (0.9794) acc 81.2500 (75.1786) lr 1.8763e-03 eta 0:05:59
epoch [6/25] batch [40/100] time 0.147 (0.178) data 0.000 (0.035) loss 1.0146 (1.0018) acc 75.0000 (74.6094) lr 1.8763e-03 eta 0:05:49
epoch [6/25] batch [45/100] time 0.143 (0.174) data 0.001 (0.031) loss 1.0039 (1.0099) acc 65.6250 (73.8889) lr 1.8763e-03 eta 0:05:40
epoch [6/25] batch [50/100] time 0.143 (0.171) data 0.000 (0.028) loss 0.9380 (1.0070) acc 78.1250 (74.2500) lr 1.8763e-03 eta 0:05:33
epoch [6/25] batch [55/100] time 0.142 (0.169) data 0.000 (0.026) loss 1.0176 (1.0170) acc 71.8750 (73.9205) lr 1.8763e-03 eta 0:05:28
epoch [6/25] batch [60/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.6470 (1.0034) acc 84.3750 (74.2708) lr 1.8763e-03 eta 0:05:23
epoch [6/25] batch [65/100] time 0.142 (0.165) data 0.000 (0.022) loss 1.1816 (1.0028) acc 71.8750 (74.3269) lr 1.8763e-03 eta 0:05:18
epoch [6/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.020) loss 0.9526 (1.0024) acc 78.1250 (74.2857) lr 1.8763e-03 eta 0:05:14
epoch [6/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.019) loss 1.0195 (1.0004) acc 75.0000 (74.3750) lr 1.8763e-03 eta 0:05:11
epoch [6/25] batch [80/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.7017 (0.9980) acc 84.3750 (74.4531) lr 1.8763e-03 eta 0:05:08
epoch [6/25] batch [85/100] time 0.144 (0.160) data 0.000 (0.017) loss 0.9619 (0.9945) acc 71.8750 (74.3750) lr 1.8763e-03 eta 0:05:05
epoch [6/25] batch [90/100] time 0.142 (0.159) data 0.000 (0.016) loss 1.3789 (0.9983) acc 62.5000 (74.3056) lr 1.8763e-03 eta 0:05:03
epoch [6/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.9053 (0.9958) acc 68.7500 (74.4079) lr 1.8763e-03 eta 0:05:00
epoch [6/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.014) loss 0.9497 (0.9988) acc 75.0000 (74.0625) lr 1.8090e-03 eta 0:04:58
epoch [7/25] batch [5/100] time 0.142 (0.411) data 0.000 (0.268) loss 0.8101 (0.9817) acc 84.3750 (73.7500) lr 1.8090e-03 eta 0:12:58
epoch [7/25] batch [10/100] time 0.142 (0.277) data 0.000 (0.134) loss 1.2188 (0.9988) acc 81.2500 (74.0625) lr 1.8090e-03 eta 0:08:43
epoch [7/25] batch [15/100] time 0.142 (0.232) data 0.000 (0.090) loss 1.0430 (0.9514) acc 81.2500 (76.0417) lr 1.8090e-03 eta 0:07:17
epoch [7/25] batch [20/100] time 0.142 (0.210) data 0.000 (0.067) loss 1.2646 (0.9634) acc 71.8750 (75.6250) lr 1.8090e-03 eta 0:06:34
epoch [7/25] batch [25/100] time 0.143 (0.196) data 0.000 (0.054) loss 1.2686 (0.9498) acc 71.8750 (75.6250) lr 1.8090e-03 eta 0:06:08
epoch [7/25] batch [30/100] time 0.142 (0.187) data 0.000 (0.045) loss 0.9165 (0.9492) acc 81.2500 (75.6250) lr 1.8090e-03 eta 0:05:50
epoch [7/25] batch [35/100] time 0.142 (0.181) data 0.000 (0.039) loss 0.7651 (0.9572) acc 87.5000 (75.9821) lr 1.8090e-03 eta 0:05:37
epoch [7/25] batch [40/100] time 0.143 (0.176) data 0.001 (0.034) loss 0.9751 (0.9743) acc 71.8750 (75.3906) lr 1.8090e-03 eta 0:05:27
epoch [7/25] batch [45/100] time 0.144 (0.172) data 0.000 (0.030) loss 0.9458 (0.9682) acc 75.0000 (75.6944) lr 1.8090e-03 eta 0:05:19
epoch [7/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.027) loss 0.9165 (0.9925) acc 81.2500 (75.2500) lr 1.8090e-03 eta 0:05:13
epoch [7/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.8442 (0.9716) acc 75.0000 (75.8523) lr 1.8090e-03 eta 0:05:08
epoch [7/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.023) loss 1.1787 (0.9686) acc 78.1250 (76.1979) lr 1.8090e-03 eta 0:05:03
epoch [7/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8208 (0.9699) acc 84.3750 (76.2019) lr 1.8090e-03 eta 0:04:59
epoch [7/25] batch [70/100] time 0.143 (0.162) data 0.000 (0.019) loss 0.8931 (0.9648) acc 75.0000 (76.2946) lr 1.8090e-03 eta 0:04:56
epoch [7/25] batch [75/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.9399 (0.9685) acc 81.2500 (76.0833) lr 1.8090e-03 eta 0:04:52
epoch [7/25] batch [80/100] time 0.142 (0.159) data 0.001 (0.017) loss 0.8774 (0.9629) acc 71.8750 (76.2500) lr 1.8090e-03 eta 0:04:50
epoch [7/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.8545 (0.9593) acc 75.0000 (76.2132) lr 1.8090e-03 eta 0:04:47
epoch [7/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.4648 (0.9717) acc 53.1250 (75.7292) lr 1.8090e-03 eta 0:04:45
epoch [7/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.014) loss 1.1357 (0.9719) acc 78.1250 (75.7566) lr 1.8090e-03 eta 0:04:42
epoch [7/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 1.0430 (0.9730) acc 68.7500 (75.7188) lr 1.7290e-03 eta 0:04:40
epoch [8/25] batch [5/100] time 0.145 (0.407) data 0.000 (0.265) loss 0.8877 (0.9897) acc 75.0000 (78.1250) lr 1.7290e-03 eta 0:12:11
epoch [8/25] batch [10/100] time 0.142 (0.275) data 0.000 (0.133) loss 0.8936 (0.8969) acc 84.3750 (80.0000) lr 1.7290e-03 eta 0:08:12
epoch [8/25] batch [15/100] time 0.143 (0.231) data 0.000 (0.088) loss 0.7471 (0.8963) acc 87.5000 (80.2083) lr 1.7290e-03 eta 0:06:52
epoch [8/25] batch [20/100] time 0.142 (0.209) data 0.000 (0.066) loss 0.6597 (0.8504) acc 81.2500 (80.7812) lr 1.7290e-03 eta 0:06:11
epoch [8/25] batch [25/100] time 0.142 (0.196) data 0.000 (0.053) loss 0.9243 (0.8557) acc 78.1250 (80.2500) lr 1.7290e-03 eta 0:05:47
epoch [8/25] batch [30/100] time 0.142 (0.187) data 0.000 (0.044) loss 1.1475 (0.8574) acc 68.7500 (79.7917) lr 1.7290e-03 eta 0:05:30
epoch [8/25] batch [35/100] time 0.142 (0.180) data 0.000 (0.038) loss 1.1191 (0.8757) acc 68.7500 (79.2857) lr 1.7290e-03 eta 0:05:18
epoch [8/25] batch [40/100] time 0.142 (0.176) data 0.000 (0.033) loss 1.2334 (0.8798) acc 65.6250 (78.9062) lr 1.7290e-03 eta 0:05:09
epoch [8/25] batch [45/100] time 0.142 (0.172) data 0.000 (0.030) loss 0.5615 (0.8834) acc 90.6250 (78.7500) lr 1.7290e-03 eta 0:05:01
epoch [8/25] batch [50/100] time 0.143 (0.169) data 0.000 (0.027) loss 0.9517 (0.8908) acc 78.1250 (78.6875) lr 1.7290e-03 eta 0:04:55
epoch [8/25] batch [55/100] time 0.144 (0.167) data 0.000 (0.024) loss 0.7876 (0.8801) acc 84.3750 (78.9205) lr 1.7290e-03 eta 0:04:50
epoch [8/25] batch [60/100] time 0.143 (0.165) data 0.000 (0.022) loss 1.1562 (0.8848) acc 56.2500 (78.4896) lr 1.7290e-03 eta 0:04:46
epoch [8/25] batch [65/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.9746 (0.8786) acc 78.1250 (78.3654) lr 1.7290e-03 eta 0:04:42
epoch [8/25] batch [70/100] time 0.143 (0.161) data 0.000 (0.019) loss 0.7407 (0.8795) acc 78.1250 (77.9911) lr 1.7290e-03 eta 0:04:39
epoch [8/25] batch [75/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.8691 (0.8762) acc 81.2500 (78.1250) lr 1.7290e-03 eta 0:04:36
epoch [8/25] batch [80/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.8921 (0.8720) acc 87.5000 (78.3203) lr 1.7290e-03 eta 0:04:33
epoch [8/25] batch [85/100] time 0.146 (0.158) data 0.004 (0.016) loss 0.9175 (0.8652) acc 71.8750 (78.5662) lr 1.7290e-03 eta 0:04:31
epoch [8/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.015) loss 1.2207 (0.8776) acc 68.7500 (78.2292) lr 1.7290e-03 eta 0:04:28
epoch [8/25] batch [95/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9907 (0.8786) acc 81.2500 (78.3224) lr 1.7290e-03 eta 0:04:26
epoch [8/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.9263 (0.8815) acc 75.0000 (78.1562) lr 1.6374e-03 eta 0:04:24
epoch [9/25] batch [5/100] time 0.142 (0.365) data 0.000 (0.222) loss 1.2891 (0.9383) acc 59.3750 (72.5000) lr 1.6374e-03 eta 0:10:18
epoch [9/25] batch [10/100] time 0.142 (0.254) data 0.000 (0.111) loss 1.2227 (0.9137) acc 68.7500 (75.9375) lr 1.6374e-03 eta 0:07:08
epoch [9/25] batch [15/100] time 0.142 (0.217) data 0.000 (0.075) loss 0.5693 (0.8359) acc 90.6250 (77.9167) lr 1.6374e-03 eta 0:06:05
epoch [9/25] batch [20/100] time 0.142 (0.198) data 0.000 (0.056) loss 0.7300 (0.8317) acc 78.1250 (77.8125) lr 1.6374e-03 eta 0:05:32
epoch [9/25] batch [25/100] time 0.142 (0.187) data 0.000 (0.045) loss 0.8008 (0.8464) acc 87.5000 (78.2500) lr 1.6374e-03 eta 0:05:13
epoch [9/25] batch [30/100] time 0.143 (0.180) data 0.000 (0.037) loss 0.8628 (0.8567) acc 71.8750 (77.7083) lr 1.6374e-03 eta 0:04:59
epoch [9/25] batch [35/100] time 0.142 (0.174) data 0.000 (0.032) loss 1.0801 (0.8676) acc 68.7500 (77.3214) lr 1.6374e-03 eta 0:04:50
epoch [9/25] batch [40/100] time 0.142 (0.170) data 0.000 (0.028) loss 0.9902 (0.8731) acc 75.0000 (78.2031) lr 1.6374e-03 eta 0:04:42
epoch [9/25] batch [45/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.7686 (0.8576) acc 87.5000 (78.7500) lr 1.6374e-03 eta 0:04:36
epoch [9/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.023) loss 1.1016 (0.8781) acc 62.5000 (78.1250) lr 1.6374e-03 eta 0:04:31
epoch [9/25] batch [55/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8506 (0.8783) acc 75.0000 (78.1818) lr 1.6374e-03 eta 0:04:27
epoch [9/25] batch [60/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.8369 (0.8810) acc 78.1250 (77.8646) lr 1.6374e-03 eta 0:04:24
epoch [9/25] batch [65/100] time 0.143 (0.160) data 0.000 (0.017) loss 0.6509 (0.8851) acc 84.3750 (77.8846) lr 1.6374e-03 eta 0:04:20
epoch [9/25] batch [70/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.5264 (0.8877) acc 87.5000 (77.9911) lr 1.6374e-03 eta 0:04:18
epoch [9/25] batch [75/100] time 0.143 (0.157) data 0.000 (0.015) loss 0.9878 (0.8814) acc 75.0000 (78.2917) lr 1.6374e-03 eta 0:04:15
epoch [9/25] batch [80/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.7188 (0.8741) acc 81.2500 (78.3203) lr 1.6374e-03 eta 0:04:13
epoch [9/25] batch [85/100] time 0.143 (0.156) data 0.000 (0.013) loss 0.5010 (0.8775) acc 93.7500 (78.1250) lr 1.6374e-03 eta 0:04:11
epoch [9/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5439 (0.8805) acc 90.6250 (78.2986) lr 1.6374e-03 eta 0:04:09
epoch [9/25] batch [95/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.7456 (0.8834) acc 87.5000 (78.2895) lr 1.6374e-03 eta 0:04:07
epoch [9/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.011) loss 1.0293 (0.8852) acc 81.2500 (78.2500) lr 1.5358e-03 eta 0:04:05
epoch [10/25] batch [5/100] time 0.143 (0.378) data 0.000 (0.235) loss 0.9761 (1.0532) acc 71.8750 (70.0000) lr 1.5358e-03 eta 0:10:02
epoch [10/25] batch [10/100] time 0.143 (0.260) data 0.000 (0.117) loss 0.9238 (0.9146) acc 78.1250 (76.5625) lr 1.5358e-03 eta 0:06:53
epoch [10/25] batch [15/100] time 0.142 (0.221) data 0.000 (0.078) loss 0.7827 (0.9053) acc 75.0000 (77.2917) lr 1.5358e-03 eta 0:05:50
epoch [10/25] batch [20/100] time 0.143 (0.201) data 0.000 (0.059) loss 1.2021 (0.9048) acc 78.1250 (77.5000) lr 1.5358e-03 eta 0:05:18
epoch [10/25] batch [25/100] time 0.143 (0.190) data 0.000 (0.047) loss 1.0273 (0.8974) acc 68.7500 (77.2500) lr 1.5358e-03 eta 0:04:58
epoch [10/25] batch [30/100] time 0.147 (0.182) data 0.004 (0.039) loss 0.8022 (0.8963) acc 87.5000 (78.1250) lr 1.5358e-03 eta 0:04:45
epoch [10/25] batch [35/100] time 0.144 (0.176) data 0.001 (0.034) loss 1.0195 (0.8795) acc 78.1250 (78.7500) lr 1.5358e-03 eta 0:04:35
epoch [10/25] batch [40/100] time 0.143 (0.172) data 0.000 (0.030) loss 0.6055 (0.8757) acc 90.6250 (78.9062) lr 1.5358e-03 eta 0:04:28
epoch [10/25] batch [45/100] time 0.143 (0.169) data 0.000 (0.026) loss 0.6597 (0.8660) acc 84.3750 (79.0972) lr 1.5358e-03 eta 0:04:22
epoch [10/25] batch [50/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.5625 (0.8734) acc 87.5000 (78.5625) lr 1.5358e-03 eta 0:04:17
epoch [10/25] batch [55/100] time 0.143 (0.164) data 0.000 (0.022) loss 0.8521 (0.8679) acc 87.5000 (78.8068) lr 1.5358e-03 eta 0:04:13
epoch [10/25] batch [60/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.7441 (0.8581) acc 71.8750 (78.8542) lr 1.5358e-03 eta 0:04:10
epoch [10/25] batch [65/100] time 0.143 (0.161) data 0.000 (0.018) loss 0.4761 (0.8628) acc 90.6250 (78.7500) lr 1.5358e-03 eta 0:04:07
epoch [10/25] batch [70/100] time 0.142 (0.160) data 0.000 (0.017) loss 0.7017 (0.8659) acc 90.6250 (78.7054) lr 1.5358e-03 eta 0:04:04
epoch [10/25] batch [75/100] time 0.143 (0.159) data 0.000 (0.016) loss 1.2100 (0.8622) acc 71.8750 (78.9583) lr 1.5358e-03 eta 0:04:01
epoch [10/25] batch [80/100] time 0.143 (0.158) data 0.000 (0.015) loss 0.7812 (0.8654) acc 78.1250 (78.5938) lr 1.5358e-03 eta 0:03:59
epoch [10/25] batch [85/100] time 0.142 (0.157) data 0.000 (0.014) loss 0.6680 (0.8790) acc 84.3750 (78.2353) lr 1.5358e-03 eta 0:03:57
epoch [10/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.013) loss 0.7271 (0.8740) acc 75.0000 (78.2639) lr 1.5358e-03 eta 0:03:55
epoch [10/25] batch [95/100] time 0.143 (0.155) data 0.000 (0.013) loss 0.6279 (0.8608) acc 78.1250 (78.4539) lr 1.5358e-03 eta 0:03:53
epoch [10/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.012) loss 0.7041 (0.8703) acc 81.2500 (78.3125) lr 1.4258e-03 eta 0:03:51
epoch [11/25] batch [5/100] time 0.142 (0.363) data 0.000 (0.219) loss 0.5850 (0.8270) acc 90.6250 (81.2500) lr 1.4258e-03 eta 0:09:02
epoch [11/25] batch [10/100] time 0.142 (0.253) data 0.000 (0.110) loss 1.0381 (0.8467) acc 75.0000 (79.3750) lr 1.4258e-03 eta 0:06:17
epoch [11/25] batch [15/100] time 0.142 (0.216) data 0.000 (0.073) loss 0.4038 (0.8326) acc 96.8750 (79.7917) lr 1.4258e-03 eta 0:05:21
epoch [11/25] batch [20/100] time 0.142 (0.198) data 0.000 (0.055) loss 1.0020 (0.8663) acc 78.1250 (79.5312) lr 1.4258e-03 eta 0:04:52
epoch [11/25] batch [25/100] time 0.143 (0.187) data 0.000 (0.044) loss 0.7974 (0.8694) acc 84.3750 (79.1250) lr 1.4258e-03 eta 0:04:35
epoch [11/25] batch [30/100] time 0.142 (0.180) data 0.000 (0.037) loss 0.7988 (0.8734) acc 78.1250 (79.2708) lr 1.4258e-03 eta 0:04:24
epoch [11/25] batch [35/100] time 0.143 (0.174) data 0.000 (0.032) loss 0.8174 (0.8917) acc 78.1250 (78.9286) lr 1.4258e-03 eta 0:04:15
epoch [11/25] batch [40/100] time 0.142 (0.170) data 0.000 (0.028) loss 0.7817 (0.8772) acc 78.1250 (79.2969) lr 1.4258e-03 eta 0:04:08
epoch [11/25] batch [45/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.8179 (0.8604) acc 71.8750 (79.5139) lr 1.4258e-03 eta 0:04:03
epoch [11/25] batch [50/100] time 0.142 (0.165) data 0.000 (0.022) loss 0.7793 (0.8662) acc 78.1250 (79.1875) lr 1.4258e-03 eta 0:03:59
epoch [11/25] batch [55/100] time 0.145 (0.163) data 0.000 (0.020) loss 0.7373 (0.8635) acc 93.7500 (79.4318) lr 1.4258e-03 eta 0:03:55
epoch [11/25] batch [60/100] time 0.143 (0.161) data 0.000 (0.019) loss 1.0703 (0.8658) acc 71.8750 (79.3750) lr 1.4258e-03 eta 0:03:52
epoch [11/25] batch [65/100] time 0.143 (0.160) data 0.000 (0.017) loss 0.7837 (0.8657) acc 87.5000 (79.3269) lr 1.4258e-03 eta 0:03:49
epoch [11/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.016) loss 1.0674 (0.8609) acc 68.7500 (79.5536) lr 1.4258e-03 eta 0:03:46
epoch [11/25] batch [75/100] time 0.147 (0.158) data 0.004 (0.015) loss 0.8896 (0.8666) acc 71.8750 (79.4167) lr 1.4258e-03 eta 0:03:44
epoch [11/25] batch [80/100] time 0.142 (0.157) data 0.000 (0.014) loss 1.0098 (0.8688) acc 71.8750 (79.3359) lr 1.4258e-03 eta 0:03:42
epoch [11/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.013) loss 0.8784 (0.8745) acc 71.8750 (79.2279) lr 1.4258e-03 eta 0:03:40
epoch [11/25] batch [90/100] time 0.142 (0.155) data 0.000 (0.012) loss 0.5059 (0.8709) acc 90.6250 (79.3750) lr 1.4258e-03 eta 0:03:38
epoch [11/25] batch [95/100] time 0.142 (0.154) data 0.000 (0.012) loss 1.4258 (0.8687) acc 56.2500 (79.3421) lr 1.4258e-03 eta 0:03:36
epoch [11/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.011) loss 0.9849 (0.8668) acc 62.5000 (79.1250) lr 1.3090e-03 eta 0:03:34
epoch [12/25] batch [5/100] time 0.142 (0.413) data 0.000 (0.271) loss 0.6157 (0.7450) acc 84.3750 (81.2500) lr 1.3090e-03 eta 0:09:35
epoch [12/25] batch [10/100] time 0.142 (0.278) data 0.000 (0.135) loss 1.1787 (0.8116) acc 65.6250 (80.0000) lr 1.3090e-03 eta 0:06:25
epoch [12/25] batch [15/100] time 0.144 (0.232) data 0.000 (0.090) loss 0.9033 (0.7959) acc 81.2500 (80.6250) lr 1.3090e-03 eta 0:05:21
epoch [12/25] batch [20/100] time 0.143 (0.210) data 0.000 (0.068) loss 0.8228 (0.7929) acc 81.2500 (80.4688) lr 1.3090e-03 eta 0:04:49
epoch [12/25] batch [25/100] time 0.142 (0.196) data 0.000 (0.054) loss 0.6616 (0.7964) acc 84.3750 (80.5000) lr 1.3090e-03 eta 0:04:30
epoch [12/25] batch [30/100] time 0.146 (0.188) data 0.000 (0.045) loss 1.1035 (0.7823) acc 71.8750 (81.1458) lr 1.3090e-03 eta 0:04:17
epoch [12/25] batch [35/100] time 0.143 (0.181) data 0.000 (0.039) loss 0.9385 (0.7881) acc 78.1250 (81.2500) lr 1.3090e-03 eta 0:04:07
epoch [12/25] batch [40/100] time 0.142 (0.176) data 0.001 (0.034) loss 0.9902 (0.7878) acc 78.1250 (81.4062) lr 1.3090e-03 eta 0:03:59
epoch [12/25] batch [45/100] time 0.146 (0.173) data 0.000 (0.030) loss 0.5723 (0.7743) acc 87.5000 (81.8056) lr 1.3090e-03 eta 0:03:54
epoch [12/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.027) loss 1.0762 (0.7890) acc 75.0000 (81.1875) lr 1.3090e-03 eta 0:03:49
epoch [12/25] batch [55/100] time 0.143 (0.167) data 0.000 (0.025) loss 0.7031 (0.7816) acc 81.2500 (81.2500) lr 1.3090e-03 eta 0:03:44
epoch [12/25] batch [60/100] time 0.147 (0.165) data 0.000 (0.023) loss 0.8550 (0.7954) acc 81.2500 (80.7812) lr 1.3090e-03 eta 0:03:41
epoch [12/25] batch [65/100] time 0.143 (0.164) data 0.000 (0.021) loss 0.8071 (0.7921) acc 84.3750 (80.8654) lr 1.3090e-03 eta 0:03:38
epoch [12/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.8906 (0.7982) acc 81.2500 (80.4911) lr 1.3090e-03 eta 0:03:35
epoch [12/25] batch [75/100] time 0.143 (0.161) data 0.000 (0.018) loss 0.9814 (0.8026) acc 75.0000 (80.3333) lr 1.3090e-03 eta 0:03:33
epoch [12/25] batch [80/100] time 0.144 (0.160) data 0.001 (0.017) loss 0.5947 (0.8041) acc 78.1250 (80.2344) lr 1.3090e-03 eta 0:03:30
epoch [12/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.016) loss 0.5771 (0.8114) acc 90.6250 (80.1103) lr 1.3090e-03 eta 0:03:28
epoch [12/25] batch [90/100] time 0.143 (0.158) data 0.000 (0.015) loss 0.7881 (0.8169) acc 81.2500 (79.8958) lr 1.3090e-03 eta 0:03:26
epoch [12/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.7261 (0.8158) acc 78.1250 (79.8684) lr 1.3090e-03 eta 0:03:24
epoch [12/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.7168 (0.8147) acc 87.5000 (79.8438) lr 1.1874e-03 eta 0:03:22
epoch [13/25] batch [5/100] time 0.143 (0.426) data 0.000 (0.283) loss 0.9756 (0.9170) acc 81.2500 (80.6250) lr 1.1874e-03 eta 0:09:11
epoch [13/25] batch [10/100] time 0.143 (0.285) data 0.000 (0.142) loss 0.7310 (0.8638) acc 84.3750 (80.9375) lr 1.1874e-03 eta 0:06:07
epoch [13/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.094) loss 0.7358 (0.8509) acc 78.1250 (80.4167) lr 1.1874e-03 eta 0:05:05
epoch [13/25] batch [20/100] time 0.142 (0.214) data 0.000 (0.071) loss 0.8120 (0.8297) acc 78.1250 (80.4688) lr 1.1874e-03 eta 0:04:33
epoch [13/25] batch [25/100] time 0.144 (0.200) data 0.000 (0.057) loss 0.9077 (0.8293) acc 78.1250 (80.5000) lr 1.1874e-03 eta 0:04:14
epoch [13/25] batch [30/100] time 0.142 (0.190) data 0.000 (0.047) loss 0.9585 (0.8322) acc 75.0000 (80.5208) lr 1.1874e-03 eta 0:04:01
epoch [13/25] batch [35/100] time 0.142 (0.183) data 0.000 (0.041) loss 0.8613 (0.8322) acc 81.2500 (80.5357) lr 1.1874e-03 eta 0:03:52
epoch [13/25] batch [40/100] time 0.143 (0.178) data 0.000 (0.036) loss 1.0527 (0.8493) acc 78.1250 (80.1562) lr 1.1874e-03 eta 0:03:44
epoch [13/25] batch [45/100] time 0.144 (0.174) data 0.000 (0.032) loss 0.6323 (0.8329) acc 93.7500 (80.3472) lr 1.1874e-03 eta 0:03:38
epoch [13/25] batch [50/100] time 0.144 (0.171) data 0.000 (0.029) loss 0.9009 (0.8214) acc 87.5000 (80.9375) lr 1.1874e-03 eta 0:03:34
epoch [13/25] batch [55/100] time 0.143 (0.169) data 0.001 (0.026) loss 1.0059 (0.8344) acc 71.8750 (80.3977) lr 1.1874e-03 eta 0:03:30
epoch [13/25] batch [60/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.9092 (0.8537) acc 75.0000 (80.0000) lr 1.1874e-03 eta 0:03:26
epoch [13/25] batch [65/100] time 0.143 (0.165) data 0.000 (0.022) loss 0.7959 (0.8538) acc 81.2500 (79.9519) lr 1.1874e-03 eta 0:03:23
epoch [13/25] batch [70/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.9038 (0.8667) acc 81.2500 (79.7321) lr 1.1874e-03 eta 0:03:20
epoch [13/25] batch [75/100] time 0.143 (0.162) data 0.000 (0.019) loss 0.8955 (0.8643) acc 81.2500 (79.7083) lr 1.1874e-03 eta 0:03:18
epoch [13/25] batch [80/100] time 0.143 (0.161) data 0.000 (0.018) loss 0.9761 (0.8643) acc 68.7500 (79.7266) lr 1.1874e-03 eta 0:03:16
epoch [13/25] batch [85/100] time 0.148 (0.160) data 0.000 (0.017) loss 0.6802 (0.8516) acc 87.5000 (80.0368) lr 1.1874e-03 eta 0:03:14
epoch [13/25] batch [90/100] time 0.144 (0.159) data 0.000 (0.016) loss 0.6064 (0.8444) acc 87.5000 (80.2431) lr 1.1874e-03 eta 0:03:12
epoch [13/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.7666 (0.8435) acc 71.8750 (80.2303) lr 1.1874e-03 eta 0:03:10
epoch [13/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.014) loss 0.7461 (0.8319) acc 68.7500 (80.3438) lr 1.0628e-03 eta 0:03:08
epoch [14/25] batch [5/100] time 0.143 (0.429) data 0.000 (0.285) loss 0.5630 (0.8401) acc 93.7500 (82.5000) lr 1.0628e-03 eta 0:08:32
epoch [14/25] batch [10/100] time 0.143 (0.286) data 0.000 (0.143) loss 0.8154 (0.8233) acc 81.2500 (82.8125) lr 1.0628e-03 eta 0:05:40
epoch [14/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.095) loss 0.4854 (0.7831) acc 90.6250 (82.9167) lr 1.0628e-03 eta 0:04:42
epoch [14/25] batch [20/100] time 0.142 (0.214) data 0.000 (0.071) loss 0.6553 (0.7401) acc 87.5000 (83.7500) lr 1.0628e-03 eta 0:04:13
epoch [14/25] batch [25/100] time 0.143 (0.200) data 0.000 (0.057) loss 0.6538 (0.7386) acc 84.3750 (83.1250) lr 1.0628e-03 eta 0:03:55
epoch [14/25] batch [30/100] time 0.143 (0.191) data 0.000 (0.048) loss 0.7148 (0.7252) acc 87.5000 (83.4375) lr 1.0628e-03 eta 0:03:43
epoch [14/25] batch [35/100] time 0.143 (0.184) data 0.000 (0.041) loss 0.6230 (0.7172) acc 84.3750 (83.2143) lr 1.0628e-03 eta 0:03:34
epoch [14/25] batch [40/100] time 0.143 (0.179) data 0.000 (0.036) loss 0.8813 (0.7384) acc 71.8750 (82.1875) lr 1.0628e-03 eta 0:03:27
epoch [14/25] batch [45/100] time 0.143 (0.175) data 0.000 (0.032) loss 0.7930 (0.7574) acc 78.1250 (81.7361) lr 1.0628e-03 eta 0:03:22
epoch [14/25] batch [50/100] time 0.144 (0.172) data 0.000 (0.029) loss 0.5410 (0.7626) acc 93.7500 (81.6250) lr 1.0628e-03 eta 0:03:17
epoch [14/25] batch [55/100] time 0.144 (0.169) data 0.001 (0.026) loss 0.6968 (0.7647) acc 78.1250 (81.5909) lr 1.0628e-03 eta 0:03:13
epoch [14/25] batch [60/100] time 0.142 (0.167) data 0.000 (0.024) loss 1.3691 (0.7764) acc 71.8750 (81.3542) lr 1.0628e-03 eta 0:03:10
epoch [14/25] batch [65/100] time 0.144 (0.165) data 0.000 (0.022) loss 1.1650 (0.7844) acc 68.7500 (81.2981) lr 1.0628e-03 eta 0:03:07
epoch [14/25] batch [70/100] time 0.144 (0.164) data 0.000 (0.021) loss 1.0234 (0.7870) acc 78.1250 (81.2054) lr 1.0628e-03 eta 0:03:05
epoch [14/25] batch [75/100] time 0.146 (0.162) data 0.000 (0.019) loss 0.6577 (0.7881) acc 84.3750 (81.0833) lr 1.0628e-03 eta 0:03:02
epoch [14/25] batch [80/100] time 0.143 (0.161) data 0.000 (0.018) loss 0.6504 (0.7872) acc 84.3750 (81.0938) lr 1.0628e-03 eta 0:03:00
epoch [14/25] batch [85/100] time 0.142 (0.160) data 0.000 (0.017) loss 0.9253 (0.7910) acc 75.0000 (80.9926) lr 1.0628e-03 eta 0:02:58
epoch [14/25] batch [90/100] time 0.143 (0.159) data 0.000 (0.016) loss 0.9741 (0.7949) acc 78.1250 (81.0417) lr 1.0628e-03 eta 0:02:56
epoch [14/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.7671 (0.7982) acc 78.1250 (80.9211) lr 1.0628e-03 eta 0:02:54
epoch [14/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.6655 (0.8074) acc 75.0000 (80.6562) lr 9.3721e-04 eta 0:02:53
epoch [15/25] batch [5/100] time 0.143 (0.348) data 0.000 (0.205) loss 0.7324 (0.7310) acc 84.3750 (81.2500) lr 9.3721e-04 eta 0:06:21
epoch [15/25] batch [10/100] time 0.142 (0.246) data 0.000 (0.103) loss 0.7383 (0.7545) acc 81.2500 (80.0000) lr 9.3721e-04 eta 0:04:27
epoch [15/25] batch [15/100] time 0.143 (0.212) data 0.000 (0.069) loss 0.9302 (0.7961) acc 81.2500 (80.0000) lr 9.3721e-04 eta 0:03:49
epoch [15/25] batch [20/100] time 0.144 (0.195) data 0.000 (0.052) loss 0.8579 (0.8127) acc 90.6250 (80.3125) lr 9.3721e-04 eta 0:03:30
epoch [15/25] batch [25/100] time 0.144 (0.184) data 0.000 (0.041) loss 0.8154 (0.7984) acc 84.3750 (81.0000) lr 9.3721e-04 eta 0:03:18
epoch [15/25] batch [30/100] time 0.143 (0.178) data 0.001 (0.035) loss 0.6348 (0.7819) acc 93.7500 (81.8750) lr 9.3721e-04 eta 0:03:09
epoch [15/25] batch [35/100] time 0.142 (0.173) data 0.000 (0.030) loss 0.9502 (0.7962) acc 75.0000 (80.8036) lr 9.3721e-04 eta 0:03:03
epoch [15/25] batch [40/100] time 0.142 (0.169) data 0.000 (0.026) loss 0.6016 (0.8050) acc 87.5000 (80.7031) lr 9.3721e-04 eta 0:02:58
epoch [15/25] batch [45/100] time 0.143 (0.166) data 0.000 (0.023) loss 0.7539 (0.7991) acc 84.3750 (80.9722) lr 9.3721e-04 eta 0:02:55
epoch [15/25] batch [50/100] time 0.143 (0.164) data 0.000 (0.021) loss 0.7939 (0.7923) acc 75.0000 (81.1250) lr 9.3721e-04 eta 0:02:51
epoch [15/25] batch [55/100] time 0.144 (0.162) data 0.000 (0.019) loss 0.7041 (0.8033) acc 90.6250 (80.6250) lr 9.3721e-04 eta 0:02:49
epoch [15/25] batch [60/100] time 0.143 (0.160) data 0.000 (0.018) loss 1.0117 (0.8081) acc 71.8750 (80.6250) lr 9.3721e-04 eta 0:02:46
epoch [15/25] batch [65/100] time 0.142 (0.159) data 0.000 (0.016) loss 0.7593 (0.8120) acc 81.2500 (80.5288) lr 9.3721e-04 eta 0:02:44
epoch [15/25] batch [70/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.8164 (0.8114) acc 71.8750 (80.2232) lr 9.3721e-04 eta 0:02:42
epoch [15/25] batch [75/100] time 0.142 (0.157) data 0.000 (0.014) loss 0.5205 (0.8018) acc 84.3750 (80.5833) lr 9.3721e-04 eta 0:02:40
epoch [15/25] batch [80/100] time 0.143 (0.156) data 0.000 (0.013) loss 0.7871 (0.8004) acc 81.2500 (80.6641) lr 9.3721e-04 eta 0:02:39
epoch [15/25] batch [85/100] time 0.142 (0.155) data 0.000 (0.012) loss 0.9741 (0.7995) acc 71.8750 (80.7353) lr 9.3721e-04 eta 0:02:37
epoch [15/25] batch [90/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.9326 (0.7972) acc 75.0000 (80.6250) lr 9.3721e-04 eta 0:02:35
epoch [15/25] batch [95/100] time 0.142 (0.154) data 0.000 (0.011) loss 0.8345 (0.7923) acc 78.1250 (80.5263) lr 9.3721e-04 eta 0:02:34
epoch [15/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.011) loss 0.7354 (0.7949) acc 81.2500 (80.4375) lr 8.1262e-04 eta 0:02:32
epoch [16/25] batch [5/100] time 0.142 (0.410) data 0.000 (0.267) loss 0.8916 (0.6960) acc 84.3750 (83.7500) lr 8.1262e-04 eta 0:06:47
epoch [16/25] batch [10/100] time 0.142 (0.276) data 0.000 (0.134) loss 1.0479 (0.7628) acc 71.8750 (80.9375) lr 8.1262e-04 eta 0:04:33
epoch [16/25] batch [15/100] time 0.142 (0.231) data 0.000 (0.089) loss 0.8325 (0.7830) acc 78.1250 (80.4167) lr 8.1262e-04 eta 0:03:47
epoch [16/25] batch [20/100] time 0.142 (0.209) data 0.000 (0.067) loss 0.9702 (0.7967) acc 68.7500 (80.3125) lr 8.1262e-04 eta 0:03:24
epoch [16/25] batch [25/100] time 0.143 (0.196) data 0.000 (0.054) loss 0.8193 (0.7653) acc 84.3750 (81.6250) lr 8.1262e-04 eta 0:03:10
epoch [16/25] batch [30/100] time 0.143 (0.187) data 0.000 (0.045) loss 0.5308 (0.7517) acc 84.3750 (81.5625) lr 8.1262e-04 eta 0:03:01
epoch [16/25] batch [35/100] time 0.146 (0.181) data 0.000 (0.038) loss 0.8091 (0.7579) acc 87.5000 (81.5179) lr 8.1262e-04 eta 0:02:54
epoch [16/25] batch [40/100] time 0.143 (0.176) data 0.000 (0.034) loss 0.6084 (0.7470) acc 87.5000 (81.8750) lr 8.1262e-04 eta 0:02:49
epoch [16/25] batch [45/100] time 0.143 (0.173) data 0.000 (0.030) loss 0.9712 (0.7479) acc 71.8750 (81.6667) lr 8.1262e-04 eta 0:02:44
epoch [16/25] batch [50/100] time 0.142 (0.170) data 0.000 (0.027) loss 0.7271 (0.7450) acc 78.1250 (81.6250) lr 8.1262e-04 eta 0:02:41
epoch [16/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.025) loss 0.5020 (0.7420) acc 93.7500 (81.7045) lr 8.1262e-04 eta 0:02:37
epoch [16/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.023) loss 1.2656 (0.7490) acc 71.8750 (81.8229) lr 8.1262e-04 eta 0:02:35
epoch [16/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8232 (0.7478) acc 78.1250 (81.8750) lr 8.1262e-04 eta 0:02:32
epoch [16/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.019) loss 0.5811 (0.7581) acc 87.5000 (81.7411) lr 8.1262e-04 eta 0:02:30
epoch [16/25] batch [75/100] time 0.144 (0.161) data 0.000 (0.018) loss 0.9155 (0.7652) acc 81.2500 (81.5417) lr 8.1262e-04 eta 0:02:28
epoch [16/25] batch [80/100] time 0.144 (0.159) data 0.000 (0.017) loss 0.8867 (0.7712) acc 78.1250 (81.4844) lr 8.1262e-04 eta 0:02:26
epoch [16/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.016) loss 0.5225 (0.7702) acc 96.8750 (81.8015) lr 8.1262e-04 eta 0:02:25
epoch [16/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.5801 (0.7680) acc 87.5000 (81.9444) lr 8.1262e-04 eta 0:02:23
epoch [16/25] batch [95/100] time 0.143 (0.157) data 0.000 (0.014) loss 0.7627 (0.7708) acc 87.5000 (81.9737) lr 8.1262e-04 eta 0:02:21
epoch [16/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.4939 (0.7701) acc 87.5000 (81.7812) lr 6.9098e-04 eta 0:02:20
epoch [17/25] batch [5/100] time 0.143 (0.383) data 0.000 (0.240) loss 0.7451 (0.6821) acc 81.2500 (82.5000) lr 6.9098e-04 eta 0:05:42
epoch [17/25] batch [10/100] time 0.142 (0.263) data 0.000 (0.120) loss 0.8921 (0.7454) acc 78.1250 (81.8750) lr 6.9098e-04 eta 0:03:53
epoch [17/25] batch [15/100] time 0.142 (0.222) data 0.000 (0.080) loss 0.7563 (0.7745) acc 81.2500 (82.2917) lr 6.9098e-04 eta 0:03:16
epoch [17/25] batch [20/100] time 0.142 (0.203) data 0.000 (0.060) loss 0.8604 (0.7492) acc 75.0000 (83.1250) lr 6.9098e-04 eta 0:02:58
epoch [17/25] batch [25/100] time 0.143 (0.191) data 0.000 (0.048) loss 0.8931 (0.7435) acc 75.0000 (82.8750) lr 6.9098e-04 eta 0:02:46
epoch [17/25] batch [30/100] time 0.142 (0.183) data 0.000 (0.040) loss 0.4656 (0.7335) acc 87.5000 (83.0208) lr 6.9098e-04 eta 0:02:38
epoch [17/25] batch [35/100] time 0.144 (0.177) data 0.000 (0.035) loss 0.8564 (0.7428) acc 84.3750 (83.2143) lr 6.9098e-04 eta 0:02:33
epoch [17/25] batch [40/100] time 0.143 (0.173) data 0.000 (0.030) loss 1.0938 (0.7630) acc 75.0000 (82.7344) lr 6.9098e-04 eta 0:02:28
epoch [17/25] batch [45/100] time 0.143 (0.169) data 0.000 (0.027) loss 0.7549 (0.7481) acc 81.2500 (82.9861) lr 6.9098e-04 eta 0:02:24
epoch [17/25] batch [50/100] time 0.143 (0.167) data 0.000 (0.024) loss 0.7075 (0.7621) acc 84.3750 (82.6875) lr 6.9098e-04 eta 0:02:21
epoch [17/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.9434 (0.7649) acc 81.2500 (82.7273) lr 6.9098e-04 eta 0:02:18
epoch [17/25] batch [60/100] time 0.143 (0.163) data 0.000 (0.020) loss 0.8228 (0.7632) acc 84.3750 (82.6562) lr 6.9098e-04 eta 0:02:16
epoch [17/25] batch [65/100] time 0.142 (0.161) data 0.000 (0.019) loss 0.9976 (0.7615) acc 71.8750 (82.6923) lr 6.9098e-04 eta 0:02:14
epoch [17/25] batch [70/100] time 0.146 (0.160) data 0.000 (0.017) loss 0.5767 (0.7590) acc 84.3750 (82.6786) lr 6.9098e-04 eta 0:02:12
epoch [17/25] batch [75/100] time 0.143 (0.159) data 0.000 (0.016) loss 0.7632 (0.7576) acc 81.2500 (82.8333) lr 6.9098e-04 eta 0:02:10
epoch [17/25] batch [80/100] time 0.144 (0.158) data 0.000 (0.015) loss 0.8262 (0.7685) acc 81.2500 (82.3438) lr 6.9098e-04 eta 0:02:09
epoch [17/25] batch [85/100] time 0.146 (0.157) data 0.000 (0.014) loss 0.9463 (0.7683) acc 71.8750 (82.4265) lr 6.9098e-04 eta 0:02:07
epoch [17/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.6914 (0.7677) acc 81.2500 (82.2569) lr 6.9098e-04 eta 0:02:06
epoch [17/25] batch [95/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.5498 (0.7640) acc 90.6250 (82.3355) lr 6.9098e-04 eta 0:02:05
epoch [17/25] batch [100/100] time 0.124 (0.155) data 0.000 (0.012) loss 1.0391 (0.7684) acc 75.0000 (82.2188) lr 5.7422e-04 eta 0:02:03
epoch [18/25] batch [5/100] time 0.144 (0.394) data 0.000 (0.252) loss 0.7549 (0.7315) acc 87.5000 (86.2500) lr 5.7422e-04 eta 0:05:13
epoch [18/25] batch [10/100] time 0.202 (0.274) data 0.058 (0.132) loss 0.6021 (0.7391) acc 84.3750 (85.0000) lr 5.7422e-04 eta 0:03:36
epoch [18/25] batch [15/100] time 0.142 (0.231) data 0.000 (0.088) loss 0.6230 (0.6733) acc 87.5000 (86.8750) lr 5.7422e-04 eta 0:03:00
epoch [18/25] batch [20/100] time 0.142 (0.209) data 0.000 (0.066) loss 0.7788 (0.7014) acc 81.2500 (84.3750) lr 5.7422e-04 eta 0:02:42
epoch [18/25] batch [25/100] time 0.146 (0.196) data 0.000 (0.053) loss 0.7202 (0.7182) acc 87.5000 (84.2500) lr 5.7422e-04 eta 0:02:31
epoch [18/25] batch [30/100] time 0.142 (0.187) data 0.000 (0.044) loss 0.8179 (0.7356) acc 87.5000 (83.8542) lr 5.7422e-04 eta 0:02:23
epoch [18/25] batch [35/100] time 0.142 (0.181) data 0.000 (0.038) loss 0.6494 (0.7550) acc 81.2500 (83.0357) lr 5.7422e-04 eta 0:02:18
epoch [18/25] batch [40/100] time 0.143 (0.176) data 0.000 (0.033) loss 0.8071 (0.7436) acc 78.1250 (83.2031) lr 5.7422e-04 eta 0:02:13
epoch [18/25] batch [45/100] time 0.145 (0.172) data 0.001 (0.030) loss 0.7358 (0.7346) acc 78.1250 (83.6111) lr 5.7422e-04 eta 0:02:09
epoch [18/25] batch [50/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.8389 (0.7517) acc 81.2500 (83.3125) lr 5.7422e-04 eta 0:02:06
epoch [18/25] batch [55/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.6133 (0.7491) acc 90.6250 (83.4659) lr 5.7422e-04 eta 0:02:04
epoch [18/25] batch [60/100] time 0.144 (0.165) data 0.000 (0.022) loss 0.4006 (0.7511) acc 96.8750 (83.3854) lr 5.7422e-04 eta 0:02:01
epoch [18/25] batch [65/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.8140 (0.7582) acc 78.1250 (83.0769) lr 5.7422e-04 eta 0:01:59
epoch [18/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.019) loss 0.5713 (0.7613) acc 87.5000 (82.9464) lr 5.7422e-04 eta 0:01:57
epoch [18/25] batch [75/100] time 0.143 (0.160) data 0.000 (0.018) loss 0.9551 (0.7612) acc 84.3750 (83.0000) lr 5.7422e-04 eta 0:01:56
epoch [18/25] batch [80/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.5908 (0.7594) acc 87.5000 (83.0859) lr 5.7422e-04 eta 0:01:54
epoch [18/25] batch [85/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.6040 (0.7575) acc 84.3750 (82.9412) lr 5.7422e-04 eta 0:01:53
epoch [18/25] batch [90/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.4475 (0.7645) acc 87.5000 (82.6736) lr 5.7422e-04 eta 0:01:51
epoch [18/25] batch [95/100] time 0.143 (0.157) data 0.000 (0.014) loss 0.9165 (0.7625) acc 75.0000 (82.6645) lr 5.7422e-04 eta 0:01:50
epoch [18/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.013) loss 0.8242 (0.7597) acc 75.0000 (82.5625) lr 4.6417e-04 eta 0:01:48
epoch [19/25] batch [5/100] time 0.142 (0.432) data 0.000 (0.290) loss 0.6445 (0.6524) acc 93.7500 (86.8750) lr 4.6417e-04 eta 0:04:59
epoch [19/25] batch [10/100] time 0.143 (0.287) data 0.000 (0.145) loss 1.3145 (0.7661) acc 75.0000 (82.5000) lr 4.6417e-04 eta 0:03:18
epoch [19/25] batch [15/100] time 0.144 (0.239) data 0.000 (0.097) loss 0.5225 (0.7301) acc 84.3750 (83.1250) lr 4.6417e-04 eta 0:02:43
epoch [19/25] batch [20/100] time 0.142 (0.215) data 0.000 (0.073) loss 0.7295 (0.7518) acc 84.3750 (81.5625) lr 4.6417e-04 eta 0:02:26
epoch [19/25] batch [25/100] time 0.142 (0.200) data 0.000 (0.058) loss 0.6196 (0.7646) acc 93.7500 (81.1250) lr 4.6417e-04 eta 0:02:15
epoch [19/25] batch [30/100] time 0.142 (0.191) data 0.000 (0.048) loss 0.9985 (0.7653) acc 75.0000 (80.8333) lr 4.6417e-04 eta 0:02:07
epoch [19/25] batch [35/100] time 0.142 (0.184) data 0.000 (0.042) loss 1.0322 (0.7720) acc 84.3750 (80.8036) lr 4.6417e-04 eta 0:02:02
epoch [19/25] batch [40/100] time 0.142 (0.179) data 0.000 (0.036) loss 0.7354 (0.7579) acc 81.2500 (81.4062) lr 4.6417e-04 eta 0:01:57
epoch [19/25] batch [45/100] time 0.142 (0.175) data 0.000 (0.033) loss 0.7490 (0.7628) acc 81.2500 (80.6944) lr 4.6417e-04 eta 0:01:54
epoch [19/25] batch [50/100] time 0.142 (0.172) data 0.000 (0.029) loss 0.6353 (0.7625) acc 84.3750 (80.8125) lr 4.6417e-04 eta 0:01:51
epoch [19/25] batch [55/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.3909 (0.7516) acc 93.7500 (81.2500) lr 4.6417e-04 eta 0:01:48
epoch [19/25] batch [60/100] time 0.142 (0.167) data 0.000 (0.024) loss 0.9023 (0.7463) acc 71.8750 (81.5104) lr 4.6417e-04 eta 0:01:46
epoch [19/25] batch [65/100] time 0.142 (0.165) data 0.000 (0.023) loss 0.7256 (0.7442) acc 75.0000 (81.6346) lr 4.6417e-04 eta 0:01:44
epoch [19/25] batch [70/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.6113 (0.7503) acc 90.6250 (81.4732) lr 4.6417e-04 eta 0:01:42
epoch [19/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.6528 (0.7480) acc 75.0000 (81.4583) lr 4.6417e-04 eta 0:01:41
epoch [19/25] batch [80/100] time 0.142 (0.161) data 0.000 (0.018) loss 0.6172 (0.7484) acc 87.5000 (81.5234) lr 4.6417e-04 eta 0:01:39
epoch [19/25] batch [85/100] time 0.142 (0.160) data 0.000 (0.017) loss 0.8013 (0.7490) acc 81.2500 (81.5074) lr 4.6417e-04 eta 0:01:38
epoch [19/25] batch [90/100] time 0.144 (0.159) data 0.000 (0.016) loss 0.7266 (0.7517) acc 84.3750 (81.4236) lr 4.6417e-04 eta 0:01:36
epoch [19/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.8330 (0.7477) acc 78.1250 (81.5461) lr 4.6417e-04 eta 0:01:35
epoch [19/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.6221 (0.7504) acc 68.7500 (81.3750) lr 3.6258e-04 eta 0:01:34
epoch [20/25] batch [5/100] time 0.142 (0.429) data 0.000 (0.287) loss 0.6499 (0.6584) acc 84.3750 (86.2500) lr 3.6258e-04 eta 0:04:15
epoch [20/25] batch [10/100] time 0.142 (0.286) data 0.000 (0.144) loss 0.5547 (0.6649) acc 90.6250 (85.9375) lr 3.6258e-04 eta 0:02:48
epoch [20/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.096) loss 0.5889 (0.6542) acc 87.5000 (85.6250) lr 3.6258e-04 eta 0:02:19
epoch [20/25] batch [20/100] time 0.142 (0.214) data 0.000 (0.072) loss 0.6934 (0.7084) acc 90.6250 (84.6875) lr 3.6258e-04 eta 0:02:04
epoch [20/25] batch [25/100] time 0.142 (0.200) data 0.000 (0.058) loss 0.5645 (0.7151) acc 81.2500 (84.1250) lr 3.6258e-04 eta 0:01:54
epoch [20/25] batch [30/100] time 0.145 (0.190) data 0.000 (0.048) loss 0.6055 (0.7101) acc 78.1250 (83.6458) lr 3.6258e-04 eta 0:01:48
epoch [20/25] batch [35/100] time 0.144 (0.183) data 0.000 (0.041) loss 0.6074 (0.7234) acc 87.5000 (83.7500) lr 3.6258e-04 eta 0:01:43
epoch [20/25] batch [40/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.6826 (0.7380) acc 84.3750 (83.4375) lr 3.6258e-04 eta 0:01:39
epoch [20/25] batch [45/100] time 0.142 (0.174) data 0.000 (0.032) loss 0.8960 (0.7408) acc 81.2500 (83.5417) lr 3.6258e-04 eta 0:01:36
epoch [20/25] batch [50/100] time 0.142 (0.171) data 0.000 (0.029) loss 0.4775 (0.7328) acc 87.5000 (83.6875) lr 3.6258e-04 eta 0:01:34
epoch [20/25] batch [55/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.9238 (0.7320) acc 81.2500 (83.6364) lr 3.6258e-04 eta 0:01:31
epoch [20/25] batch [60/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.5781 (0.7224) acc 87.5000 (84.0625) lr 3.6258e-04 eta 0:01:29
epoch [20/25] batch [65/100] time 0.143 (0.164) data 0.000 (0.022) loss 0.6260 (0.7193) acc 87.5000 (84.2788) lr 3.6258e-04 eta 0:01:27
epoch [20/25] batch [70/100] time 0.143 (0.163) data 0.000 (0.021) loss 0.8096 (0.7124) acc 78.1250 (84.4643) lr 3.6258e-04 eta 0:01:26
epoch [20/25] batch [75/100] time 0.142 (0.162) data 0.000 (0.019) loss 0.4861 (0.7064) acc 90.6250 (84.6667) lr 3.6258e-04 eta 0:01:24
epoch [20/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.7500 (0.7086) acc 87.5000 (84.6484) lr 3.6258e-04 eta 0:01:23
epoch [20/25] batch [85/100] time 0.144 (0.159) data 0.000 (0.017) loss 0.7441 (0.7162) acc 90.6250 (84.4118) lr 3.6258e-04 eta 0:01:22
epoch [20/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.6299 (0.7153) acc 84.3750 (84.3403) lr 3.6258e-04 eta 0:01:20
epoch [20/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.4551 (0.7147) acc 87.5000 (84.2763) lr 3.6258e-04 eta 0:01:19
epoch [20/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.6904 (0.7175) acc 87.5000 (84.2188) lr 2.7103e-04 eta 0:01:18
epoch [21/25] batch [5/100] time 0.142 (0.422) data 0.000 (0.281) loss 0.8022 (0.7953) acc 87.5000 (81.8750) lr 2.7103e-04 eta 0:03:29
epoch [21/25] batch [10/100] time 0.142 (0.282) data 0.000 (0.140) loss 0.7954 (0.7781) acc 81.2500 (82.1875) lr 2.7103e-04 eta 0:02:18
epoch [21/25] batch [15/100] time 0.142 (0.236) data 0.000 (0.094) loss 0.6133 (0.7436) acc 90.6250 (83.1250) lr 2.7103e-04 eta 0:01:54
epoch [21/25] batch [20/100] time 0.142 (0.212) data 0.000 (0.070) loss 0.7314 (0.7137) acc 93.7500 (84.8438) lr 2.7103e-04 eta 0:01:41
epoch [21/25] batch [25/100] time 0.141 (0.198) data 0.000 (0.056) loss 0.3691 (0.6928) acc 96.8750 (85.5000) lr 2.7103e-04 eta 0:01:34
epoch [21/25] batch [30/100] time 0.142 (0.189) data 0.000 (0.047) loss 0.5723 (0.6929) acc 87.5000 (85.6250) lr 2.7103e-04 eta 0:01:28
epoch [21/25] batch [35/100] time 0.142 (0.182) data 0.000 (0.040) loss 0.7661 (0.6951) acc 84.3750 (85.2679) lr 2.7103e-04 eta 0:01:24
epoch [21/25] batch [40/100] time 0.142 (0.177) data 0.000 (0.035) loss 0.8247 (0.6976) acc 84.3750 (85.1562) lr 2.7103e-04 eta 0:01:21
epoch [21/25] batch [45/100] time 0.142 (0.173) data 0.000 (0.031) loss 0.5137 (0.7038) acc 87.5000 (84.9306) lr 2.7103e-04 eta 0:01:18
epoch [21/25] batch [50/100] time 0.143 (0.170) data 0.000 (0.028) loss 0.8662 (0.7049) acc 81.2500 (84.5625) lr 2.7103e-04 eta 0:01:16
epoch [21/25] batch [55/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.8545 (0.7001) acc 78.1250 (84.7159) lr 2.7103e-04 eta 0:01:14
epoch [21/25] batch [60/100] time 0.142 (0.165) data 0.000 (0.024) loss 0.5645 (0.6966) acc 93.7500 (84.8438) lr 2.7103e-04 eta 0:01:12
epoch [21/25] batch [65/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.7710 (0.7001) acc 81.2500 (84.5192) lr 2.7103e-04 eta 0:01:11
epoch [21/25] batch [70/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.8892 (0.6985) acc 78.1250 (84.5536) lr 2.7103e-04 eta 0:01:09
epoch [21/25] batch [75/100] time 0.144 (0.161) data 0.000 (0.019) loss 0.7598 (0.7006) acc 84.3750 (84.4167) lr 2.7103e-04 eta 0:01:08
epoch [21/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.7207 (0.6974) acc 78.1250 (84.4141) lr 2.7103e-04 eta 0:01:07
epoch [21/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.4993 (0.6911) acc 93.7500 (84.7059) lr 2.7103e-04 eta 0:01:05
epoch [21/25] batch [90/100] time 0.143 (0.158) data 0.000 (0.016) loss 0.7861 (0.6971) acc 81.2500 (84.5486) lr 2.7103e-04 eta 0:01:04
epoch [21/25] batch [95/100] time 0.142 (0.157) data 0.000 (0.015) loss 0.8311 (0.6979) acc 71.8750 (84.4408) lr 2.7103e-04 eta 0:01:03
epoch [21/25] batch [100/100] time 0.124 (0.156) data 0.000 (0.014) loss 0.4170 (0.6935) acc 93.7500 (84.7188) lr 1.9098e-04 eta 0:01:02
epoch [22/25] batch [5/100] time 0.142 (0.430) data 0.000 (0.286) loss 0.7056 (0.6668) acc 87.5000 (85.0000) lr 1.9098e-04 eta 0:02:49
epoch [22/25] batch [10/100] time 0.142 (0.286) data 0.000 (0.143) loss 0.5596 (0.7021) acc 93.7500 (85.0000) lr 1.9098e-04 eta 0:01:51
epoch [22/25] batch [15/100] time 0.142 (0.238) data 0.000 (0.096) loss 1.0723 (0.7181) acc 84.3750 (85.0000) lr 1.9098e-04 eta 0:01:31
epoch [22/25] batch [20/100] time 0.142 (0.214) data 0.000 (0.072) loss 0.6807 (0.7016) acc 78.1250 (85.0000) lr 1.9098e-04 eta 0:01:21
epoch [22/25] batch [25/100] time 0.142 (0.200) data 0.000 (0.057) loss 0.5034 (0.6846) acc 90.6250 (85.2500) lr 1.9098e-04 eta 0:01:14
epoch [22/25] batch [30/100] time 0.142 (0.190) data 0.000 (0.048) loss 0.5283 (0.6780) acc 90.6250 (85.6250) lr 1.9098e-04 eta 0:01:10
epoch [22/25] batch [35/100] time 0.142 (0.183) data 0.000 (0.041) loss 0.8350 (0.6941) acc 75.0000 (84.9107) lr 1.9098e-04 eta 0:01:06
epoch [22/25] batch [40/100] time 0.142 (0.178) data 0.000 (0.036) loss 0.7300 (0.6915) acc 87.5000 (84.9219) lr 1.9098e-04 eta 0:01:04
epoch [22/25] batch [45/100] time 0.143 (0.174) data 0.000 (0.032) loss 0.3936 (0.6775) acc 93.7500 (85.2778) lr 1.9098e-04 eta 0:01:01
epoch [22/25] batch [50/100] time 0.142 (0.171) data 0.000 (0.029) loss 1.1572 (0.6900) acc 68.7500 (84.8125) lr 1.9098e-04 eta 0:00:59
epoch [22/25] batch [55/100] time 0.142 (0.168) data 0.000 (0.026) loss 0.6611 (0.6826) acc 84.3750 (85.0568) lr 1.9098e-04 eta 0:00:58
epoch [22/25] batch [60/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.8149 (0.6893) acc 84.3750 (84.8958) lr 1.9098e-04 eta 0:00:56
epoch [22/25] batch [65/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.3809 (0.6748) acc 96.8750 (85.2404) lr 1.9098e-04 eta 0:00:55
epoch [22/25] batch [70/100] time 0.145 (0.163) data 0.000 (0.021) loss 0.6211 (0.6730) acc 87.5000 (85.0446) lr 1.9098e-04 eta 0:00:53
epoch [22/25] batch [75/100] time 0.144 (0.162) data 0.000 (0.019) loss 0.5986 (0.6746) acc 87.5000 (85.1250) lr 1.9098e-04 eta 0:00:52
epoch [22/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.4150 (0.6766) acc 96.8750 (85.1562) lr 1.9098e-04 eta 0:00:51
epoch [22/25] batch [85/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.7739 (0.6728) acc 78.1250 (85.3309) lr 1.9098e-04 eta 0:00:50
epoch [22/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.4055 (0.6702) acc 87.5000 (85.3125) lr 1.9098e-04 eta 0:00:49
epoch [22/25] batch [95/100] time 0.143 (0.158) data 0.000 (0.015) loss 0.6104 (0.6743) acc 87.5000 (85.1316) lr 1.9098e-04 eta 0:00:48
epoch [22/25] batch [100/100] time 0.124 (0.157) data 0.000 (0.015) loss 0.5195 (0.6790) acc 87.5000 (84.8750) lr 1.2369e-04 eta 0:00:46
epoch [23/25] batch [5/100] time 0.142 (0.380) data 0.000 (0.238) loss 0.6670 (0.6610) acc 84.3750 (86.8750) lr 1.2369e-04 eta 0:01:52
epoch [23/25] batch [10/100] time 0.142 (0.261) data 0.000 (0.119) loss 1.0107 (0.6989) acc 78.1250 (85.9375) lr 1.2369e-04 eta 0:01:15
epoch [23/25] batch [15/100] time 0.145 (0.222) data 0.000 (0.080) loss 0.6309 (0.7121) acc 90.6250 (85.4167) lr 1.2369e-04 eta 0:01:03
epoch [23/25] batch [20/100] time 0.146 (0.202) data 0.000 (0.060) loss 0.7183 (0.7169) acc 84.3750 (85.3125) lr 1.2369e-04 eta 0:00:56
epoch [23/25] batch [25/100] time 0.141 (0.190) data 0.000 (0.048) loss 0.4453 (0.7091) acc 90.6250 (84.7500) lr 1.2369e-04 eta 0:00:52
epoch [23/25] batch [30/100] time 0.142 (0.182) data 0.000 (0.040) loss 0.7695 (0.7176) acc 68.7500 (83.6458) lr 1.2369e-04 eta 0:00:49
epoch [23/25] batch [35/100] time 0.145 (0.177) data 0.003 (0.034) loss 0.5664 (0.7166) acc 87.5000 (83.8393) lr 1.2369e-04 eta 0:00:46
epoch [23/25] batch [40/100] time 0.142 (0.172) data 0.000 (0.030) loss 0.7383 (0.7138) acc 75.0000 (83.9062) lr 1.2369e-04 eta 0:00:44
epoch [23/25] batch [45/100] time 0.142 (0.169) data 0.000 (0.027) loss 0.8975 (0.7198) acc 75.0000 (83.9583) lr 1.2369e-04 eta 0:00:43
epoch [23/25] batch [50/100] time 0.142 (0.166) data 0.000 (0.024) loss 1.1943 (0.7272) acc 68.7500 (83.4375) lr 1.2369e-04 eta 0:00:41
epoch [23/25] batch [55/100] time 0.142 (0.164) data 0.000 (0.022) loss 0.9312 (0.7254) acc 68.7500 (83.5227) lr 1.2369e-04 eta 0:00:40
epoch [23/25] batch [60/100] time 0.142 (0.162) data 0.000 (0.020) loss 0.7998 (0.7188) acc 84.3750 (83.8021) lr 1.2369e-04 eta 0:00:38
epoch [23/25] batch [65/100] time 0.141 (0.161) data 0.000 (0.019) loss 0.5005 (0.7051) acc 84.3750 (84.1346) lr 1.2369e-04 eta 0:00:37
epoch [23/25] batch [70/100] time 0.142 (0.159) data 0.000 (0.017) loss 0.4561 (0.7091) acc 87.5000 (83.8393) lr 1.2369e-04 eta 0:00:36
epoch [23/25] batch [75/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.4446 (0.7059) acc 87.5000 (83.7917) lr 1.2369e-04 eta 0:00:35
epoch [23/25] batch [80/100] time 0.143 (0.157) data 0.000 (0.015) loss 0.5254 (0.6989) acc 90.6250 (83.9844) lr 1.2369e-04 eta 0:00:34
epoch [23/25] batch [85/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.6577 (0.6975) acc 84.3750 (84.1912) lr 1.2369e-04 eta 0:00:33
epoch [23/25] batch [90/100] time 0.142 (0.156) data 0.000 (0.014) loss 0.9336 (0.6966) acc 71.8750 (84.1319) lr 1.2369e-04 eta 0:00:32
epoch [23/25] batch [95/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.6279 (0.6912) acc 84.3750 (84.3092) lr 1.2369e-04 eta 0:00:31
epoch [23/25] batch [100/100] time 0.124 (0.154) data 0.000 (0.012) loss 0.9614 (0.6940) acc 75.0000 (84.3125) lr 7.0224e-05 eta 0:00:30
epoch [24/25] batch [5/100] time 0.142 (0.350) data 0.000 (0.208) loss 0.6426 (0.6917) acc 78.1250 (81.8750) lr 7.0224e-05 eta 0:01:08
epoch [24/25] batch [10/100] time 0.142 (0.247) data 0.000 (0.104) loss 0.5918 (0.6868) acc 84.3750 (82.5000) lr 7.0224e-05 eta 0:00:46
epoch [24/25] batch [15/100] time 0.142 (0.212) data 0.000 (0.070) loss 0.6270 (0.6836) acc 87.5000 (83.5417) lr 7.0224e-05 eta 0:00:39
epoch [24/25] batch [20/100] time 0.142 (0.195) data 0.000 (0.052) loss 0.7490 (0.6818) acc 87.5000 (83.9062) lr 7.0224e-05 eta 0:00:35
epoch [24/25] batch [25/100] time 0.142 (0.184) data 0.000 (0.042) loss 0.4109 (0.6719) acc 93.7500 (84.6250) lr 7.0224e-05 eta 0:00:32
epoch [24/25] batch [30/100] time 0.142 (0.177) data 0.000 (0.035) loss 0.8457 (0.6773) acc 84.3750 (84.4792) lr 7.0224e-05 eta 0:00:30
epoch [24/25] batch [35/100] time 0.143 (0.172) data 0.000 (0.030) loss 0.4968 (0.6612) acc 90.6250 (85.3571) lr 7.0224e-05 eta 0:00:28
epoch [24/25] batch [40/100] time 0.143 (0.169) data 0.000 (0.026) loss 0.8540 (0.6569) acc 78.1250 (85.1562) lr 7.0224e-05 eta 0:00:26
epoch [24/25] batch [45/100] time 0.143 (0.166) data 0.000 (0.023) loss 0.7598 (0.6599) acc 84.3750 (84.9306) lr 7.0224e-05 eta 0:00:25
epoch [24/25] batch [50/100] time 0.142 (0.163) data 0.000 (0.021) loss 0.7852 (0.6661) acc 84.3750 (84.9375) lr 7.0224e-05 eta 0:00:24
epoch [24/25] batch [55/100] time 0.143 (0.162) data 0.000 (0.019) loss 0.6338 (0.6731) acc 87.5000 (84.6023) lr 7.0224e-05 eta 0:00:23
epoch [24/25] batch [60/100] time 0.144 (0.160) data 0.000 (0.018) loss 0.5498 (0.6692) acc 90.6250 (84.7396) lr 7.0224e-05 eta 0:00:22
epoch [24/25] batch [65/100] time 0.143 (0.159) data 0.000 (0.016) loss 0.6919 (0.6684) acc 87.5000 (84.8077) lr 7.0224e-05 eta 0:00:21
epoch [24/25] batch [70/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.7827 (0.6637) acc 81.2500 (85.2232) lr 7.0224e-05 eta 0:00:20
epoch [24/25] batch [75/100] time 0.143 (0.157) data 0.000 (0.014) loss 0.5439 (0.6608) acc 90.6250 (85.2917) lr 7.0224e-05 eta 0:00:19
epoch [24/25] batch [80/100] time 0.142 (0.156) data 0.000 (0.013) loss 0.4487 (0.6597) acc 96.8750 (85.5469) lr 7.0224e-05 eta 0:00:18
epoch [24/25] batch [85/100] time 0.142 (0.155) data 0.000 (0.013) loss 0.7036 (0.6605) acc 81.2500 (85.6250) lr 7.0224e-05 eta 0:00:17
epoch [24/25] batch [90/100] time 0.142 (0.154) data 0.000 (0.012) loss 0.7305 (0.6604) acc 84.3750 (85.4514) lr 7.0224e-05 eta 0:00:16
epoch [24/25] batch [95/100] time 0.142 (0.153) data 0.000 (0.011) loss 0.5684 (0.6582) acc 87.5000 (85.5592) lr 7.0224e-05 eta 0:00:16
epoch [24/25] batch [100/100] time 0.124 (0.153) data 0.000 (0.011) loss 0.9292 (0.6580) acc 81.2500 (85.6562) lr 3.1417e-05 eta 0:00:15
epoch [25/25] batch [5/100] time 0.145 (0.424) data 0.000 (0.280) loss 0.9780 (0.6587) acc 71.8750 (86.2500) lr 3.1417e-05 eta 0:00:40
epoch [25/25] batch [10/100] time 0.142 (0.283) data 0.000 (0.140) loss 0.6367 (0.6905) acc 87.5000 (86.8750) lr 3.1417e-05 eta 0:00:25
epoch [25/25] batch [15/100] time 0.144 (0.236) data 0.001 (0.094) loss 0.8936 (0.6741) acc 81.2500 (87.0833) lr 3.1417e-05 eta 0:00:20
epoch [25/25] batch [20/100] time 0.143 (0.213) data 0.000 (0.070) loss 0.6938 (0.6598) acc 81.2500 (87.8125) lr 3.1417e-05 eta 0:00:17
epoch [25/25] batch [25/100] time 0.143 (0.199) data 0.001 (0.056) loss 0.4331 (0.6377) acc 90.6250 (88.0000) lr 3.1417e-05 eta 0:00:14
epoch [25/25] batch [30/100] time 0.143 (0.190) data 0.000 (0.047) loss 0.9116 (0.6380) acc 81.2500 (87.5000) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [35/100] time 0.143 (0.183) data 0.000 (0.040) loss 0.6084 (0.6386) acc 90.6250 (87.7679) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [40/100] time 0.147 (0.178) data 0.000 (0.035) loss 0.6963 (0.6586) acc 87.5000 (87.1094) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [45/100] time 0.142 (0.174) data 0.000 (0.031) loss 0.5693 (0.6489) acc 87.5000 (87.4306) lr 3.1417e-05 eta 0:00:09
epoch [25/25] batch [50/100] time 0.142 (0.171) data 0.000 (0.028) loss 0.7656 (0.6633) acc 87.5000 (87.3125) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [55/100] time 0.142 (0.169) data 0.000 (0.026) loss 0.8853 (0.6601) acc 84.3750 (87.3295) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [60/100] time 0.142 (0.166) data 0.000 (0.024) loss 0.6548 (0.6684) acc 84.3750 (86.5625) lr 3.1417e-05 eta 0:00:06
epoch [25/25] batch [65/100] time 0.142 (0.165) data 0.000 (0.022) loss 0.5410 (0.6728) acc 93.7500 (86.1058) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [70/100] time 0.142 (0.163) data 0.000 (0.020) loss 0.4905 (0.6726) acc 93.7500 (85.9375) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [75/100] time 0.144 (0.162) data 0.001 (0.019) loss 0.6157 (0.6789) acc 90.6250 (85.8750) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [80/100] time 0.142 (0.160) data 0.000 (0.018) loss 0.6128 (0.6799) acc 90.6250 (86.0938) lr 3.1417e-05 eta 0:00:03
epoch [25/25] batch [85/100] time 0.143 (0.159) data 0.000 (0.017) loss 0.6484 (0.6816) acc 87.5000 (85.9926) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [90/100] time 0.142 (0.158) data 0.000 (0.016) loss 0.9014 (0.6840) acc 75.0000 (85.8333) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [95/100] time 0.142 (0.158) data 0.000 (0.015) loss 0.7202 (0.6799) acc 81.2500 (85.8224) lr 3.1417e-05 eta 0:00:00
epoch [25/25] batch [100/100] time 0.123 (0.157) data 0.000 (0.014) loss 0.5361 (0.6808) acc 87.5000 (85.7812) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:14<04:41, 14.84s/it] 10%|█         | 2/20 [00:17<02:14,  7.49s/it] 15%|█▌        | 3/20 [00:17<01:13,  4.35s/it] 20%|██        | 4/20 [00:18<00:45,  2.87s/it] 25%|██▌       | 5/20 [00:19<00:30,  2.06s/it] 30%|███       | 6/20 [00:19<00:21,  1.56s/it] 35%|███▌      | 7/20 [00:20<00:16,  1.25s/it] 40%|████      | 8/20 [00:20<00:12,  1.05s/it] 45%|████▌     | 9/20 [00:23<00:18,  1.66s/it] 50%|█████     | 10/20 [00:28<00:25,  2.52s/it] 55%|█████▌    | 11/20 [00:28<00:17,  1.94s/it] 60%|██████    | 12/20 [00:29<00:12,  1.53s/it] 65%|██████▌   | 13/20 [00:30<00:08,  1.25s/it] 70%|███████   | 14/20 [00:30<00:06,  1.06s/it] 75%|███████▌  | 15/20 [00:31<00:04,  1.09it/s] 80%|████████  | 16/20 [00:31<00:03,  1.21it/s] 85%|████████▌ | 17/20 [00:35<00:04,  1.59s/it] 90%|█████████ | 18/20 [00:42<00:06,  3.30s/it] 95%|█████████▌| 19/20 [00:43<00:02,  2.49s/it]100%|██████████| 20/20 [00:43<00:00,  1.93s/it]100%|██████████| 20/20 [00:43<00:00,  2.20s/it]
=> result
* total: 9,950
* correct: 8,226
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 82.5%
Elapsed: 0:07:16
Run this job and save the output to output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      792
# test     9,900
---------  ------
['outdoor ice_skating_rink', 'iceberg', 'igloo', 'industrial_area', 'outdoor inn', 'islet', 'indoor jacuzzi', 'indoor jail', 'jail_cell', 'jewelry_shop', 'kasbah', 'indoor kennel', 'outdoor kennel', 'kindergarden_classroom', 'kitchen', 'kitchenette', 'outdoor labyrinth', 'natural lake', 'landfill', 'landing_deck', 'laundromat', 'lecture_room', 'indoor library', 'outdoor library', 'outdoor lido_deck', 'lift_bridge', 'lighthouse', 'limousine_interior', 'living_room', 'lobby', 'lock_chamber', 'locker_room', 'mansion', 'manufactured_home', 'indoor market', 'outdoor market', 'marsh', 'martial_arts_gym', 'mausoleum', 'medina', 'water moat', 'outdoor monastery', 'indoor mosque', 'outdoor mosque', 'motel', 'mountain', 'mountain_snowy', 'indoor movie_theater', 'indoor museum', 'music_store', 'music_studio', 'outdoor nuclear_power_plant', 'nursery', 'oast_house', 'outdoor observatory', 'ocean', 'office', 'office_building', 'outdoor oil_refinery', 'oilrig', 'operating_room', 'orchard', 'outdoor outhouse', 'pagoda', 'palace', 'pantry', 'park', 'indoor parking_garage', 'outdoor parking_garage', 'parking_lot', 'parlor', 'pasture', 'patio', 'pavilion', 'pharmacy', 'phone_booth', 'physics_laboratory', 'picnic_area', 'indoor pilothouse', 'outdoor planetarium', 'playground', 'playroom', 'plaza', 'indoor podium', 'outdoor podium', 'pond', 'establishment poolroom', 'home poolroom', 'outdoor power_plant', 'promenade_deck', 'indoor pub', 'pulpit', 'putting_green', 'racecourse', 'raceway', 'raft', 'railroad_track', 'rainforest', 'reception', 'recreation_room', 'residential_neighborhood', 'restaurant', 'restaurant_kitchen', 'restaurant_patio', 'rice_paddy', 'riding_arena', 'river', 'rock_arch', 'rope_bridge', 'ruin', 'runway', 'sandbar', 'sandbox', 'sauna', 'schoolhouse', 'sea_cliff', 'server_room', 'shed', 'shoe_shop', 'shopfront', 'indoor shopping_mall', 'shower', 'skatepark', 'ski_lodge', 'ski_resort', 'ski_slope', 'sky', 'skyscraper', 'slum', 'snowfield', 'squash_court', 'stable', 'baseball stadium', 'football stadium', 'indoor stage', 'staircase', 'street', 'subway_interior', 'platform subway_station', 'supermarket', 'sushi_bar', 'swamp', 'indoor swimming_pool', 'outdoor swimming_pool', 'indoor synagogue', 'outdoor synagogue', 'television_studio', 'east_asia temple', 'south_asia temple', 'indoor tennis_court', 'outdoor tennis_court', 'outdoor tent', 'indoor_procenium theater', 'indoor_seats theater', 'thriftshop', 'throne_room', 'ticket_booth', 'toll_plaza', 'topiary_garden', 'tower', 'toyshop', 'outdoor track', 'train_railway', 'platform train_station', 'tree_farm', 'tree_house', 'trench', 'coral_reef underwater', 'utility_room', 'valley', 'van_interior', 'vegetable_garden', 'veranda', 'veterinarians_office', 'viaduct', 'videostore', 'village', 'vineyard', 'volcano', 'indoor volleyball_court', 'outdoor volleyball_court', 'waiting_room', 'indoor warehouse', 'water_tower', 'block waterfall', 'fan waterfall', 'plunge waterfall', 'watering_hole', 'wave', 'wet_bar', 'wheat_field', 'wind_farm', 'windmill', 'barrel_storage wine_cellar', 'bottle_storage wine_cellar', 'indoor wrestling_ring', 'yard', 'youth_hostel']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X outdoor ice skating rink.', 'X X X X iceberg.', 'X X X X igloo.', 'X X X X industrial area.', 'X X X X outdoor inn.', 'X X X X islet.', 'X X X X indoor jacuzzi.', 'X X X X indoor jail.', 'X X X X jail cell.', 'X X X X jewelry shop.', 'X X X X kasbah.', 'X X X X indoor kennel.', 'X X X X outdoor kennel.', 'X X X X kindergarden classroom.', 'X X X X kitchen.', 'X X X X kitchenette.', 'X X X X outdoor labyrinth.', 'X X X X natural lake.', 'X X X X landfill.', 'X X X X landing deck.', 'X X X X laundromat.', 'X X X X lecture room.', 'X X X X indoor library.', 'X X X X outdoor library.', 'X X X X outdoor lido deck.', 'X X X X lift bridge.', 'X X X X lighthouse.', 'X X X X limousine interior.', 'X X X X living room.', 'X X X X lobby.', 'X X X X lock chamber.', 'X X X X locker room.', 'X X X X mansion.', 'X X X X manufactured home.', 'X X X X indoor market.', 'X X X X outdoor market.', 'X X X X marsh.', 'X X X X martial arts gym.', 'X X X X mausoleum.', 'X X X X medina.', 'X X X X water moat.', 'X X X X outdoor monastery.', 'X X X X indoor mosque.', 'X X X X outdoor mosque.', 'X X X X motel.', 'X X X X mountain.', 'X X X X mountain snowy.', 'X X X X indoor movie theater.', 'X X X X indoor museum.', 'X X X X music store.', 'X X X X music studio.', 'X X X X outdoor nuclear power plant.', 'X X X X nursery.', 'X X X X oast house.', 'X X X X outdoor observatory.', 'X X X X ocean.', 'X X X X office.', 'X X X X office building.', 'X X X X outdoor oil refinery.', 'X X X X oilrig.', 'X X X X operating room.', 'X X X X orchard.', 'X X X X outdoor outhouse.', 'X X X X pagoda.', 'X X X X palace.', 'X X X X pantry.', 'X X X X park.', 'X X X X indoor parking garage.', 'X X X X outdoor parking garage.', 'X X X X parking lot.', 'X X X X parlor.', 'X X X X pasture.', 'X X X X patio.', 'X X X X pavilion.', 'X X X X pharmacy.', 'X X X X phone booth.', 'X X X X physics laboratory.', 'X X X X picnic area.', 'X X X X indoor pilothouse.', 'X X X X outdoor planetarium.', 'X X X X playground.', 'X X X X playroom.', 'X X X X plaza.', 'X X X X indoor podium.', 'X X X X outdoor podium.', 'X X X X pond.', 'X X X X establishment poolroom.', 'X X X X home poolroom.', 'X X X X outdoor power plant.', 'X X X X promenade deck.', 'X X X X indoor pub.', 'X X X X pulpit.', 'X X X X putting green.', 'X X X X racecourse.', 'X X X X raceway.', 'X X X X raft.', 'X X X X railroad track.', 'X X X X rainforest.', 'X X X X reception.', 'X X X X recreation room.', 'X X X X residential neighborhood.', 'X X X X restaurant.', 'X X X X restaurant kitchen.', 'X X X X restaurant patio.', 'X X X X rice paddy.', 'X X X X riding arena.', 'X X X X river.', 'X X X X rock arch.', 'X X X X rope bridge.', 'X X X X ruin.', 'X X X X runway.', 'X X X X sandbar.', 'X X X X sandbox.', 'X X X X sauna.', 'X X X X schoolhouse.', 'X X X X sea cliff.', 'X X X X server room.', 'X X X X shed.', 'X X X X shoe shop.', 'X X X X shopfront.', 'X X X X indoor shopping mall.', 'X X X X shower.', 'X X X X skatepark.', 'X X X X ski lodge.', 'X X X X ski resort.', 'X X X X ski slope.', 'X X X X sky.', 'X X X X skyscraper.', 'X X X X slum.', 'X X X X snowfield.', 'X X X X squash court.', 'X X X X stable.', 'X X X X baseball stadium.', 'X X X X football stadium.', 'X X X X indoor stage.', 'X X X X staircase.', 'X X X X street.', 'X X X X subway interior.', 'X X X X platform subway station.', 'X X X X supermarket.', 'X X X X sushi bar.', 'X X X X swamp.', 'X X X X indoor swimming pool.', 'X X X X outdoor swimming pool.', 'X X X X indoor synagogue.', 'X X X X outdoor synagogue.', 'X X X X television studio.', 'X X X X east asia temple.', 'X X X X south asia temple.', 'X X X X indoor tennis court.', 'X X X X outdoor tennis court.', 'X X X X outdoor tent.', 'X X X X indoor procenium theater.', 'X X X X indoor seats theater.', 'X X X X thriftshop.', 'X X X X throne room.', 'X X X X ticket booth.', 'X X X X toll plaza.', 'X X X X topiary garden.', 'X X X X tower.', 'X X X X toyshop.', 'X X X X outdoor track.', 'X X X X train railway.', 'X X X X platform train station.', 'X X X X tree farm.', 'X X X X tree house.', 'X X X X trench.', 'X X X X coral reef underwater.', 'X X X X utility room.', 'X X X X valley.', 'X X X X van interior.', 'X X X X vegetable garden.', 'X X X X veranda.', 'X X X X veterinarians office.', 'X X X X viaduct.', 'X X X X videostore.', 'X X X X village.', 'X X X X vineyard.', 'X X X X volcano.', 'X X X X indoor volleyball court.', 'X X X X outdoor volleyball court.', 'X X X X waiting room.', 'X X X X indoor warehouse.', 'X X X X water tower.', 'X X X X block waterfall.', 'X X X X fan waterfall.', 'X X X X plunge waterfall.', 'X X X X watering hole.', 'X X X X wave.', 'X X X X wet bar.', 'X X X X wheat field.', 'X X X X wind farm.', 'X X X X windmill.', 'X X X X barrel storage wine cellar.', 'X X X X bottle storage wine cellar.', 'X X X X indoor wrestling ring.', 'X X X X yard.', 'X X X X youth hostel.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:19<06:12, 19.58s/it] 10%|█         | 2/20 [00:27<03:53, 12.99s/it] 15%|█▌        | 3/20 [00:28<02:04,  7.33s/it] 20%|██        | 4/20 [00:29<01:14,  4.68s/it] 25%|██▌       | 5/20 [00:29<00:48,  3.21s/it] 30%|███       | 6/20 [00:30<00:32,  2.32s/it] 35%|███▌      | 7/20 [00:30<00:22,  1.76s/it] 40%|████      | 8/20 [00:31<00:16,  1.39s/it] 45%|████▌     | 9/20 [00:41<00:45,  4.16s/it] 50%|█████     | 10/20 [00:45<00:38,  3.86s/it] 55%|█████▌    | 11/20 [00:45<00:25,  2.86s/it] 60%|██████    | 12/20 [00:50<00:28,  3.54s/it] 65%|██████▌   | 13/20 [00:51<00:18,  2.65s/it] 70%|███████   | 14/20 [00:51<00:12,  2.03s/it] 75%|███████▌  | 15/20 [00:52<00:08,  1.60s/it] 80%|████████  | 16/20 [00:53<00:05,  1.30s/it] 85%|████████▌ | 17/20 [00:56<00:05,  1.99s/it] 90%|█████████ | 18/20 [01:01<00:05,  2.74s/it] 95%|█████████▌| 19/20 [01:01<00:02,  2.10s/it]100%|██████████| 20/20 [01:05<00:00,  2.55s/it]100%|██████████| 20/20 [01:05<00:00,  3.28s/it]
=> result
* total: 9,900
* correct: 7,771
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 77.7%
Run this job and save the output to output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      792
# test     9,900
---------  ------
['outdoor ice_skating_rink', 'iceberg', 'igloo', 'industrial_area', 'outdoor inn', 'islet', 'indoor jacuzzi', 'indoor jail', 'jail_cell', 'jewelry_shop', 'kasbah', 'indoor kennel', 'outdoor kennel', 'kindergarden_classroom', 'kitchen', 'kitchenette', 'outdoor labyrinth', 'natural lake', 'landfill', 'landing_deck', 'laundromat', 'lecture_room', 'indoor library', 'outdoor library', 'outdoor lido_deck', 'lift_bridge', 'lighthouse', 'limousine_interior', 'living_room', 'lobby', 'lock_chamber', 'locker_room', 'mansion', 'manufactured_home', 'indoor market', 'outdoor market', 'marsh', 'martial_arts_gym', 'mausoleum', 'medina', 'water moat', 'outdoor monastery', 'indoor mosque', 'outdoor mosque', 'motel', 'mountain', 'mountain_snowy', 'indoor movie_theater', 'indoor museum', 'music_store', 'music_studio', 'outdoor nuclear_power_plant', 'nursery', 'oast_house', 'outdoor observatory', 'ocean', 'office', 'office_building', 'outdoor oil_refinery', 'oilrig', 'operating_room', 'orchard', 'outdoor outhouse', 'pagoda', 'palace', 'pantry', 'park', 'indoor parking_garage', 'outdoor parking_garage', 'parking_lot', 'parlor', 'pasture', 'patio', 'pavilion', 'pharmacy', 'phone_booth', 'physics_laboratory', 'picnic_area', 'indoor pilothouse', 'outdoor planetarium', 'playground', 'playroom', 'plaza', 'indoor podium', 'outdoor podium', 'pond', 'establishment poolroom', 'home poolroom', 'outdoor power_plant', 'promenade_deck', 'indoor pub', 'pulpit', 'putting_green', 'racecourse', 'raceway', 'raft', 'railroad_track', 'rainforest', 'reception', 'recreation_room', 'residential_neighborhood', 'restaurant', 'restaurant_kitchen', 'restaurant_patio', 'rice_paddy', 'riding_arena', 'river', 'rock_arch', 'rope_bridge', 'ruin', 'runway', 'sandbar', 'sandbox', 'sauna', 'schoolhouse', 'sea_cliff', 'server_room', 'shed', 'shoe_shop', 'shopfront', 'indoor shopping_mall', 'shower', 'skatepark', 'ski_lodge', 'ski_resort', 'ski_slope', 'sky', 'skyscraper', 'slum', 'snowfield', 'squash_court', 'stable', 'baseball stadium', 'football stadium', 'indoor stage', 'staircase', 'street', 'subway_interior', 'platform subway_station', 'supermarket', 'sushi_bar', 'swamp', 'indoor swimming_pool', 'outdoor swimming_pool', 'indoor synagogue', 'outdoor synagogue', 'television_studio', 'east_asia temple', 'south_asia temple', 'indoor tennis_court', 'outdoor tennis_court', 'outdoor tent', 'indoor_procenium theater', 'indoor_seats theater', 'thriftshop', 'throne_room', 'ticket_booth', 'toll_plaza', 'topiary_garden', 'tower', 'toyshop', 'outdoor track', 'train_railway', 'platform train_station', 'tree_farm', 'tree_house', 'trench', 'coral_reef underwater', 'utility_room', 'valley', 'van_interior', 'vegetable_garden', 'veranda', 'veterinarians_office', 'viaduct', 'videostore', 'village', 'vineyard', 'volcano', 'indoor volleyball_court', 'outdoor volleyball_court', 'waiting_room', 'indoor warehouse', 'water_tower', 'block waterfall', 'fan waterfall', 'plunge waterfall', 'watering_hole', 'wave', 'wet_bar', 'wheat_field', 'wind_farm', 'windmill', 'barrel_storage wine_cellar', 'bottle_storage wine_cellar', 'indoor wrestling_ring', 'yard', 'youth_hostel']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X outdoor ice skating rink.', 'X X X X iceberg.', 'X X X X igloo.', 'X X X X industrial area.', 'X X X X outdoor inn.', 'X X X X islet.', 'X X X X indoor jacuzzi.', 'X X X X indoor jail.', 'X X X X jail cell.', 'X X X X jewelry shop.', 'X X X X kasbah.', 'X X X X indoor kennel.', 'X X X X outdoor kennel.', 'X X X X kindergarden classroom.', 'X X X X kitchen.', 'X X X X kitchenette.', 'X X X X outdoor labyrinth.', 'X X X X natural lake.', 'X X X X landfill.', 'X X X X landing deck.', 'X X X X laundromat.', 'X X X X lecture room.', 'X X X X indoor library.', 'X X X X outdoor library.', 'X X X X outdoor lido deck.', 'X X X X lift bridge.', 'X X X X lighthouse.', 'X X X X limousine interior.', 'X X X X living room.', 'X X X X lobby.', 'X X X X lock chamber.', 'X X X X locker room.', 'X X X X mansion.', 'X X X X manufactured home.', 'X X X X indoor market.', 'X X X X outdoor market.', 'X X X X marsh.', 'X X X X martial arts gym.', 'X X X X mausoleum.', 'X X X X medina.', 'X X X X water moat.', 'X X X X outdoor monastery.', 'X X X X indoor mosque.', 'X X X X outdoor mosque.', 'X X X X motel.', 'X X X X mountain.', 'X X X X mountain snowy.', 'X X X X indoor movie theater.', 'X X X X indoor museum.', 'X X X X music store.', 'X X X X music studio.', 'X X X X outdoor nuclear power plant.', 'X X X X nursery.', 'X X X X oast house.', 'X X X X outdoor observatory.', 'X X X X ocean.', 'X X X X office.', 'X X X X office building.', 'X X X X outdoor oil refinery.', 'X X X X oilrig.', 'X X X X operating room.', 'X X X X orchard.', 'X X X X outdoor outhouse.', 'X X X X pagoda.', 'X X X X palace.', 'X X X X pantry.', 'X X X X park.', 'X X X X indoor parking garage.', 'X X X X outdoor parking garage.', 'X X X X parking lot.', 'X X X X parlor.', 'X X X X pasture.', 'X X X X patio.', 'X X X X pavilion.', 'X X X X pharmacy.', 'X X X X phone booth.', 'X X X X physics laboratory.', 'X X X X picnic area.', 'X X X X indoor pilothouse.', 'X X X X outdoor planetarium.', 'X X X X playground.', 'X X X X playroom.', 'X X X X plaza.', 'X X X X indoor podium.', 'X X X X outdoor podium.', 'X X X X pond.', 'X X X X establishment poolroom.', 'X X X X home poolroom.', 'X X X X outdoor power plant.', 'X X X X promenade deck.', 'X X X X indoor pub.', 'X X X X pulpit.', 'X X X X putting green.', 'X X X X racecourse.', 'X X X X raceway.', 'X X X X raft.', 'X X X X railroad track.', 'X X X X rainforest.', 'X X X X reception.', 'X X X X recreation room.', 'X X X X residential neighborhood.', 'X X X X restaurant.', 'X X X X restaurant kitchen.', 'X X X X restaurant patio.', 'X X X X rice paddy.', 'X X X X riding arena.', 'X X X X river.', 'X X X X rock arch.', 'X X X X rope bridge.', 'X X X X ruin.', 'X X X X runway.', 'X X X X sandbar.', 'X X X X sandbox.', 'X X X X sauna.', 'X X X X schoolhouse.', 'X X X X sea cliff.', 'X X X X server room.', 'X X X X shed.', 'X X X X shoe shop.', 'X X X X shopfront.', 'X X X X indoor shopping mall.', 'X X X X shower.', 'X X X X skatepark.', 'X X X X ski lodge.', 'X X X X ski resort.', 'X X X X ski slope.', 'X X X X sky.', 'X X X X skyscraper.', 'X X X X slum.', 'X X X X snowfield.', 'X X X X squash court.', 'X X X X stable.', 'X X X X baseball stadium.', 'X X X X football stadium.', 'X X X X indoor stage.', 'X X X X staircase.', 'X X X X street.', 'X X X X subway interior.', 'X X X X platform subway station.', 'X X X X supermarket.', 'X X X X sushi bar.', 'X X X X swamp.', 'X X X X indoor swimming pool.', 'X X X X outdoor swimming pool.', 'X X X X indoor synagogue.', 'X X X X outdoor synagogue.', 'X X X X television studio.', 'X X X X east asia temple.', 'X X X X south asia temple.', 'X X X X indoor tennis court.', 'X X X X outdoor tennis court.', 'X X X X outdoor tent.', 'X X X X indoor procenium theater.', 'X X X X indoor seats theater.', 'X X X X thriftshop.', 'X X X X throne room.', 'X X X X ticket booth.', 'X X X X toll plaza.', 'X X X X topiary garden.', 'X X X X tower.', 'X X X X toyshop.', 'X X X X outdoor track.', 'X X X X train railway.', 'X X X X platform train station.', 'X X X X tree farm.', 'X X X X tree house.', 'X X X X trench.', 'X X X X coral reef underwater.', 'X X X X utility room.', 'X X X X valley.', 'X X X X van interior.', 'X X X X vegetable garden.', 'X X X X veranda.', 'X X X X veterinarians office.', 'X X X X viaduct.', 'X X X X videostore.', 'X X X X village.', 'X X X X vineyard.', 'X X X X volcano.', 'X X X X indoor volleyball court.', 'X X X X outdoor volleyball court.', 'X X X X waiting room.', 'X X X X indoor warehouse.', 'X X X X water tower.', 'X X X X block waterfall.', 'X X X X fan waterfall.', 'X X X X plunge waterfall.', 'X X X X watering hole.', 'X X X X wave.', 'X X X X wet bar.', 'X X X X wheat field.', 'X X X X wind farm.', 'X X X X windmill.', 'X X X X barrel storage wine cellar.', 'X X X X bottle storage wine cellar.', 'X X X X indoor wrestling ring.', 'X X X X yard.', 'X X X X youth hostel.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:10<03:27, 10.92s/it] 10%|█         | 2/20 [00:16<02:20,  7.78s/it] 15%|█▌        | 3/20 [00:17<01:16,  4.50s/it] 20%|██        | 4/20 [00:17<00:47,  2.96s/it] 25%|██▌       | 5/20 [00:18<00:31,  2.11s/it] 30%|███       | 6/20 [00:18<00:22,  1.60s/it] 35%|███▌      | 7/20 [00:19<00:16,  1.27s/it] 40%|████      | 8/20 [00:20<00:12,  1.06s/it] 45%|████▌     | 9/20 [00:23<00:20,  1.90s/it] 50%|█████     | 10/20 [00:26<00:21,  2.17s/it] 55%|█████▌    | 11/20 [00:27<00:15,  1.69s/it] 60%|██████    | 12/20 [00:29<00:15,  1.92s/it] 65%|██████▌   | 13/20 [00:30<00:10,  1.52s/it] 70%|███████   | 14/20 [00:30<00:07,  1.24s/it] 75%|███████▌  | 15/20 [00:31<00:05,  1.05s/it] 80%|████████  | 16/20 [00:32<00:03,  1.09it/s] 85%|████████▌ | 17/20 [00:33<00:02,  1.05it/s] 90%|█████████ | 18/20 [00:37<00:04,  2.03s/it] 95%|█████████▌| 19/20 [00:38<00:01,  1.60s/it]100%|██████████| 20/20 [00:39<00:00,  1.51s/it]100%|██████████| 20/20 [00:39<00:00,  1.99s/it]
=> result
* total: 9,900
* correct: 7,765
* accuracy: 78.4%
* error: 21.6%
* macro_f1: 77.6%
Run this job and save the output to output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: SUN397
Reading split from /data/yht/data/cl/data/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/sun397/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      792
# test     9,900
---------  ------
['outdoor ice_skating_rink', 'iceberg', 'igloo', 'industrial_area', 'outdoor inn', 'islet', 'indoor jacuzzi', 'indoor jail', 'jail_cell', 'jewelry_shop', 'kasbah', 'indoor kennel', 'outdoor kennel', 'kindergarden_classroom', 'kitchen', 'kitchenette', 'outdoor labyrinth', 'natural lake', 'landfill', 'landing_deck', 'laundromat', 'lecture_room', 'indoor library', 'outdoor library', 'outdoor lido_deck', 'lift_bridge', 'lighthouse', 'limousine_interior', 'living_room', 'lobby', 'lock_chamber', 'locker_room', 'mansion', 'manufactured_home', 'indoor market', 'outdoor market', 'marsh', 'martial_arts_gym', 'mausoleum', 'medina', 'water moat', 'outdoor monastery', 'indoor mosque', 'outdoor mosque', 'motel', 'mountain', 'mountain_snowy', 'indoor movie_theater', 'indoor museum', 'music_store', 'music_studio', 'outdoor nuclear_power_plant', 'nursery', 'oast_house', 'outdoor observatory', 'ocean', 'office', 'office_building', 'outdoor oil_refinery', 'oilrig', 'operating_room', 'orchard', 'outdoor outhouse', 'pagoda', 'palace', 'pantry', 'park', 'indoor parking_garage', 'outdoor parking_garage', 'parking_lot', 'parlor', 'pasture', 'patio', 'pavilion', 'pharmacy', 'phone_booth', 'physics_laboratory', 'picnic_area', 'indoor pilothouse', 'outdoor planetarium', 'playground', 'playroom', 'plaza', 'indoor podium', 'outdoor podium', 'pond', 'establishment poolroom', 'home poolroom', 'outdoor power_plant', 'promenade_deck', 'indoor pub', 'pulpit', 'putting_green', 'racecourse', 'raceway', 'raft', 'railroad_track', 'rainforest', 'reception', 'recreation_room', 'residential_neighborhood', 'restaurant', 'restaurant_kitchen', 'restaurant_patio', 'rice_paddy', 'riding_arena', 'river', 'rock_arch', 'rope_bridge', 'ruin', 'runway', 'sandbar', 'sandbox', 'sauna', 'schoolhouse', 'sea_cliff', 'server_room', 'shed', 'shoe_shop', 'shopfront', 'indoor shopping_mall', 'shower', 'skatepark', 'ski_lodge', 'ski_resort', 'ski_slope', 'sky', 'skyscraper', 'slum', 'snowfield', 'squash_court', 'stable', 'baseball stadium', 'football stadium', 'indoor stage', 'staircase', 'street', 'subway_interior', 'platform subway_station', 'supermarket', 'sushi_bar', 'swamp', 'indoor swimming_pool', 'outdoor swimming_pool', 'indoor synagogue', 'outdoor synagogue', 'television_studio', 'east_asia temple', 'south_asia temple', 'indoor tennis_court', 'outdoor tennis_court', 'outdoor tent', 'indoor_procenium theater', 'indoor_seats theater', 'thriftshop', 'throne_room', 'ticket_booth', 'toll_plaza', 'topiary_garden', 'tower', 'toyshop', 'outdoor track', 'train_railway', 'platform train_station', 'tree_farm', 'tree_house', 'trench', 'coral_reef underwater', 'utility_room', 'valley', 'van_interior', 'vegetable_garden', 'veranda', 'veterinarians_office', 'viaduct', 'videostore', 'village', 'vineyard', 'volcano', 'indoor volleyball_court', 'outdoor volleyball_court', 'waiting_room', 'indoor warehouse', 'water_tower', 'block waterfall', 'fan waterfall', 'plunge waterfall', 'watering_hole', 'wave', 'wet_bar', 'wheat_field', 'wind_farm', 'windmill', 'barrel_storage wine_cellar', 'bottle_storage wine_cellar', 'indoor wrestling_ring', 'yard', 'youth_hostel']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X outdoor ice skating rink.', 'X X X X iceberg.', 'X X X X igloo.', 'X X X X industrial area.', 'X X X X outdoor inn.', 'X X X X islet.', 'X X X X indoor jacuzzi.', 'X X X X indoor jail.', 'X X X X jail cell.', 'X X X X jewelry shop.', 'X X X X kasbah.', 'X X X X indoor kennel.', 'X X X X outdoor kennel.', 'X X X X kindergarden classroom.', 'X X X X kitchen.', 'X X X X kitchenette.', 'X X X X outdoor labyrinth.', 'X X X X natural lake.', 'X X X X landfill.', 'X X X X landing deck.', 'X X X X laundromat.', 'X X X X lecture room.', 'X X X X indoor library.', 'X X X X outdoor library.', 'X X X X outdoor lido deck.', 'X X X X lift bridge.', 'X X X X lighthouse.', 'X X X X limousine interior.', 'X X X X living room.', 'X X X X lobby.', 'X X X X lock chamber.', 'X X X X locker room.', 'X X X X mansion.', 'X X X X manufactured home.', 'X X X X indoor market.', 'X X X X outdoor market.', 'X X X X marsh.', 'X X X X martial arts gym.', 'X X X X mausoleum.', 'X X X X medina.', 'X X X X water moat.', 'X X X X outdoor monastery.', 'X X X X indoor mosque.', 'X X X X outdoor mosque.', 'X X X X motel.', 'X X X X mountain.', 'X X X X mountain snowy.', 'X X X X indoor movie theater.', 'X X X X indoor museum.', 'X X X X music store.', 'X X X X music studio.', 'X X X X outdoor nuclear power plant.', 'X X X X nursery.', 'X X X X oast house.', 'X X X X outdoor observatory.', 'X X X X ocean.', 'X X X X office.', 'X X X X office building.', 'X X X X outdoor oil refinery.', 'X X X X oilrig.', 'X X X X operating room.', 'X X X X orchard.', 'X X X X outdoor outhouse.', 'X X X X pagoda.', 'X X X X palace.', 'X X X X pantry.', 'X X X X park.', 'X X X X indoor parking garage.', 'X X X X outdoor parking garage.', 'X X X X parking lot.', 'X X X X parlor.', 'X X X X pasture.', 'X X X X patio.', 'X X X X pavilion.', 'X X X X pharmacy.', 'X X X X phone booth.', 'X X X X physics laboratory.', 'X X X X picnic area.', 'X X X X indoor pilothouse.', 'X X X X outdoor planetarium.', 'X X X X playground.', 'X X X X playroom.', 'X X X X plaza.', 'X X X X indoor podium.', 'X X X X outdoor podium.', 'X X X X pond.', 'X X X X establishment poolroom.', 'X X X X home poolroom.', 'X X X X outdoor power plant.', 'X X X X promenade deck.', 'X X X X indoor pub.', 'X X X X pulpit.', 'X X X X putting green.', 'X X X X racecourse.', 'X X X X raceway.', 'X X X X raft.', 'X X X X railroad track.', 'X X X X rainforest.', 'X X X X reception.', 'X X X X recreation room.', 'X X X X residential neighborhood.', 'X X X X restaurant.', 'X X X X restaurant kitchen.', 'X X X X restaurant patio.', 'X X X X rice paddy.', 'X X X X riding arena.', 'X X X X river.', 'X X X X rock arch.', 'X X X X rope bridge.', 'X X X X ruin.', 'X X X X runway.', 'X X X X sandbar.', 'X X X X sandbox.', 'X X X X sauna.', 'X X X X schoolhouse.', 'X X X X sea cliff.', 'X X X X server room.', 'X X X X shed.', 'X X X X shoe shop.', 'X X X X shopfront.', 'X X X X indoor shopping mall.', 'X X X X shower.', 'X X X X skatepark.', 'X X X X ski lodge.', 'X X X X ski resort.', 'X X X X ski slope.', 'X X X X sky.', 'X X X X skyscraper.', 'X X X X slum.', 'X X X X snowfield.', 'X X X X squash court.', 'X X X X stable.', 'X X X X baseball stadium.', 'X X X X football stadium.', 'X X X X indoor stage.', 'X X X X staircase.', 'X X X X street.', 'X X X X subway interior.', 'X X X X platform subway station.', 'X X X X supermarket.', 'X X X X sushi bar.', 'X X X X swamp.', 'X X X X indoor swimming pool.', 'X X X X outdoor swimming pool.', 'X X X X indoor synagogue.', 'X X X X outdoor synagogue.', 'X X X X television studio.', 'X X X X east asia temple.', 'X X X X south asia temple.', 'X X X X indoor tennis court.', 'X X X X outdoor tennis court.', 'X X X X outdoor tent.', 'X X X X indoor procenium theater.', 'X X X X indoor seats theater.', 'X X X X thriftshop.', 'X X X X throne room.', 'X X X X ticket booth.', 'X X X X toll plaza.', 'X X X X topiary garden.', 'X X X X tower.', 'X X X X toyshop.', 'X X X X outdoor track.', 'X X X X train railway.', 'X X X X platform train station.', 'X X X X tree farm.', 'X X X X tree house.', 'X X X X trench.', 'X X X X coral reef underwater.', 'X X X X utility room.', 'X X X X valley.', 'X X X X van interior.', 'X X X X vegetable garden.', 'X X X X veranda.', 'X X X X veterinarians office.', 'X X X X viaduct.', 'X X X X videostore.', 'X X X X village.', 'X X X X vineyard.', 'X X X X volcano.', 'X X X X indoor volleyball court.', 'X X X X outdoor volleyball court.', 'X X X X waiting room.', 'X X X X indoor warehouse.', 'X X X X water tower.', 'X X X X block waterfall.', 'X X X X fan waterfall.', 'X X X X plunge waterfall.', 'X X X X watering hole.', 'X X X X wave.', 'X X X X wet bar.', 'X X X X wheat field.', 'X X X X wind farm.', 'X X X X windmill.', 'X X X X barrel storage wine cellar.', 'X X X X bottle storage wine cellar.', 'X X X X indoor wrestling ring.', 'X X X X yard.', 'X X X X youth hostel.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/sun397/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:10<03:27, 10.91s/it] 10%|█         | 2/20 [00:16<02:15,  7.51s/it] 15%|█▌        | 3/20 [00:16<01:14,  4.35s/it] 20%|██        | 4/20 [00:17<00:45,  2.87s/it] 25%|██▌       | 5/20 [00:17<00:30,  2.05s/it] 30%|███       | 6/20 [00:18<00:21,  1.56s/it] 35%|███▌      | 7/20 [00:19<00:16,  1.25s/it] 40%|████      | 8/20 [00:19<00:12,  1.04s/it] 45%|████▌     | 9/20 [00:24<00:25,  2.29s/it] 50%|█████     | 10/20 [00:26<00:19,  1.99s/it] 55%|█████▌    | 11/20 [00:26<00:14,  1.57s/it] 60%|██████    | 12/20 [00:28<00:13,  1.74s/it] 65%|██████▌   | 13/20 [00:29<00:09,  1.40s/it] 70%|███████   | 14/20 [00:29<00:06,  1.16s/it] 75%|███████▌  | 15/20 [00:30<00:04,  1.01it/s] 80%|████████  | 16/20 [00:31<00:03,  1.14it/s] 85%|████████▌ | 17/20 [00:34<00:04,  1.51s/it] 90%|█████████ | 18/20 [00:36<00:03,  1.81s/it] 95%|█████████▌| 19/20 [00:37<00:01,  1.45s/it]100%|██████████| 20/20 [00:38<00:00,  1.36s/it]100%|██████████| 20/20 [00:38<00:00,  1.93s/it]
=> result
* total: 9,900
* correct: 7,691
* accuracy: 77.7%
* error: 22.3%
* macro_f1: 76.7%
Run this job and save the output to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'rooster', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'American robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'American dipper', 'kite (bird of prey)', 'bald eagle', 'vulture', 'great grey owl', 'fire salamander', 'smooth newt', 'newt', 'spotted salamander', 'axolotl', 'American bullfrog', 'tree frog', 'tailed frog', 'loggerhead sea turtle', 'leatherback sea turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'green iguana', 'Carolina anole', 'desert grassland whiptail lizard', 'agama', 'frilled-necked lizard', 'alligator lizard', 'Gila monster', 'European green lizard', 'chameleon', 'Komodo dragon', 'Nile crocodile', 'American alligator', 'triceratops', 'worm snake', 'ring-necked snake', 'eastern hog-nosed snake', 'smooth green snake', 'kingsnake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'African rock python', 'Indian cobra', 'green mamba', 'sea snake', 'Saharan horned viper', 'eastern diamondback rattlesnake', 'sidewinder rattlesnake', 'trilobite', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie grouse', 'peafowl', 'quail', 'partridge', 'african grey parrot', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'duck', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'red king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'great egret', 'bittern bird', 'crane bird', 'limpkin', 'common gallinule', 'American coot', 'bustard', 'ruddy turnstone', 'dunlin', 'common redshank', 'dowitcher', 'oystercatcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'Papillon', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Otterhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'English Setter', 'Irish Setter', 'Gordon Setter', 'Brittany dog', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniel', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael dog', 'Malinois', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Shetland Sheepdog', 'collie', 'Border Collie', 'Bouvier des Flandres dog', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland dog', 'Great Pyrenees dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Keeshond', 'brussels griffon', 'Pembroke Welsh Corgi', 'Cardigan Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog (xoloitzcuintli)', 'grey wolf', 'Alaskan tundra wolf', 'red wolf or maned wolf', 'coyote', 'dingo', 'dhole', 'African wild dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'polar bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ladybug', 'ground beetle', 'longhorn beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket insect', 'stick insect', 'cockroach', 'praying mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'red admiral butterfly', 'ringlet butterfly', 'monarch butterfly', 'small white butterfly', 'sulphur butterfly', 'gossamer-winged butterfly', 'starfish', 'sea urchin', 'sea cucumber', 'cottontail rabbit', 'hare', 'Angora rabbit', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'common sorrel horse', 'zebra', 'pig', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram (adult male sheep)', 'bighorn sheep', 'Alpine ibex', 'hartebeest', 'impala (antelope)', 'gazelle', 'arabian camel', 'llama', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas monkey', 'baboon', 'macaque', 'langur', 'black-and-white colobus', 'proboscis monkey', 'marmoset', 'white-headed capuchin', 'howler monkey', 'titi monkey', "Geoffroy's spider monkey", 'common squirrel monkey', 'ring-tailed lemur', 'indri', 'Asian elephant', 'African bush elephant', 'red panda', 'giant panda', 'snoek fish', 'eel', 'silver salmon', 'rock beauty fish', 'clownfish', 'sturgeon', 'gar fish', 'lionfish', 'pufferfish', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'altar', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'apron', 'trash can', 'assault rifle', 'backpack', 'bakery', 'balance beam', 'balloon', 'ballpoint pen', 'Band-Aid', 'banjo', 'baluster / handrail', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military hat (bearskin or shako)', 'beer bottle', 'beer glass', 'bell tower', 'baby bib', 'tandem bicycle', 'bikini', 'ring binder', 'binoculars', 'birdhouse', 'boathouse', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bottle cap', 'hunting bow', 'bow tie', 'brass memorial plaque', 'bra', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'canoe', 'can opener', 'cardigan', 'car mirror', 'carousel', 'tool kit', 'cardboard box / carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'mobile phone', 'chain', 'chain-link fence', 'chain mail', 'chainsaw', 'storage chest', 'chiffonier', 'bell or wind chime', 'china cabinet', 'Christmas stocking', 'church', 'movie theater', 'cleaver']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a tench.', 'a photo of a goldfish.', 'a photo of a great white shark.', 'a photo of a tiger shark.', 'a photo of a hammerhead shark.', 'a photo of a electric ray.', 'a photo of a stingray.', 'a photo of a rooster.', 'a photo of a hen.', 'a photo of a ostrich.', 'a photo of a brambling.', 'a photo of a goldfinch.', 'a photo of a house finch.', 'a photo of a junco.', 'a photo of a indigo bunting.', 'a photo of a American robin.', 'a photo of a bulbul.', 'a photo of a jay.', 'a photo of a magpie.', 'a photo of a chickadee.', 'a photo of a American dipper.', 'a photo of a kite (bird of prey).', 'a photo of a bald eagle.', 'a photo of a vulture.', 'a photo of a great grey owl.', 'a photo of a fire salamander.', 'a photo of a smooth newt.', 'a photo of a newt.', 'a photo of a spotted salamander.', 'a photo of a axolotl.', 'a photo of a American bullfrog.', 'a photo of a tree frog.', 'a photo of a tailed frog.', 'a photo of a loggerhead sea turtle.', 'a photo of a leatherback sea turtle.', 'a photo of a mud turtle.', 'a photo of a terrapin.', 'a photo of a box turtle.', 'a photo of a banded gecko.', 'a photo of a green iguana.', 'a photo of a Carolina anole.', 'a photo of a desert grassland whiptail lizard.', 'a photo of a agama.', 'a photo of a frilled-necked lizard.', 'a photo of a alligator lizard.', 'a photo of a Gila monster.', 'a photo of a European green lizard.', 'a photo of a chameleon.', 'a photo of a Komodo dragon.', 'a photo of a Nile crocodile.', 'a photo of a American alligator.', 'a photo of a triceratops.', 'a photo of a worm snake.', 'a photo of a ring-necked snake.', 'a photo of a eastern hog-nosed snake.', 'a photo of a smooth green snake.', 'a photo of a kingsnake.', 'a photo of a garter snake.', 'a photo of a water snake.', 'a photo of a vine snake.', 'a photo of a night snake.', 'a photo of a boa constrictor.', 'a photo of a African rock python.', 'a photo of a Indian cobra.', 'a photo of a green mamba.', 'a photo of a sea snake.', 'a photo of a Saharan horned viper.', 'a photo of a eastern diamondback rattlesnake.', 'a photo of a sidewinder rattlesnake.', 'a photo of a trilobite.', 'a photo of a harvestman.', 'a photo of a scorpion.', 'a photo of a yellow garden spider.', 'a photo of a barn spider.', 'a photo of a European garden spider.', 'a photo of a southern black widow.', 'a photo of a tarantula.', 'a photo of a wolf spider.', 'a photo of a tick.', 'a photo of a centipede.', 'a photo of a black grouse.', 'a photo of a ptarmigan.', 'a photo of a ruffed grouse.', 'a photo of a prairie grouse.', 'a photo of a peafowl.', 'a photo of a quail.', 'a photo of a partridge.', 'a photo of a african grey parrot.', 'a photo of a macaw.', 'a photo of a sulphur-crested cockatoo.', 'a photo of a lorikeet.', 'a photo of a coucal.', 'a photo of a bee eater.', 'a photo of a hornbill.', 'a photo of a hummingbird.', 'a photo of a jacamar.', 'a photo of a toucan.', 'a photo of a duck.', 'a photo of a red-breasted merganser.', 'a photo of a goose.', 'a photo of a black swan.', 'a photo of a tusker.', 'a photo of a echidna.', 'a photo of a platypus.', 'a photo of a wallaby.', 'a photo of a koala.', 'a photo of a wombat.', 'a photo of a jellyfish.', 'a photo of a sea anemone.', 'a photo of a brain coral.', 'a photo of a flatworm.', 'a photo of a nematode.', 'a photo of a conch.', 'a photo of a snail.', 'a photo of a slug.', 'a photo of a sea slug.', 'a photo of a chiton.', 'a photo of a chambered nautilus.', 'a photo of a Dungeness crab.', 'a photo of a rock crab.', 'a photo of a fiddler crab.', 'a photo of a red king crab.', 'a photo of a American lobster.', 'a photo of a spiny lobster.', 'a photo of a crayfish.', 'a photo of a hermit crab.', 'a photo of a isopod.', 'a photo of a white stork.', 'a photo of a black stork.', 'a photo of a spoonbill.', 'a photo of a flamingo.', 'a photo of a little blue heron.', 'a photo of a great egret.', 'a photo of a bittern bird.', 'a photo of a crane bird.', 'a photo of a limpkin.', 'a photo of a common gallinule.', 'a photo of a American coot.', 'a photo of a bustard.', 'a photo of a ruddy turnstone.', 'a photo of a dunlin.', 'a photo of a common redshank.', 'a photo of a dowitcher.', 'a photo of a oystercatcher.', 'a photo of a pelican.', 'a photo of a king penguin.', 'a photo of a albatross.', 'a photo of a grey whale.', 'a photo of a killer whale.', 'a photo of a dugong.', 'a photo of a sea lion.', 'a photo of a Chihuahua.', 'a photo of a Japanese Chin.', 'a photo of a Maltese.', 'a photo of a Pekingese.', 'a photo of a Shih Tzu.', 'a photo of a King Charles Spaniel.', 'a photo of a Papillon.', 'a photo of a toy terrier.', 'a photo of a Rhodesian Ridgeback.', 'a photo of a Afghan Hound.', 'a photo of a Basset Hound.', 'a photo of a Beagle.', 'a photo of a Bloodhound.', 'a photo of a Bluetick Coonhound.', 'a photo of a Black and Tan Coonhound.', 'a photo of a Treeing Walker Coonhound.', 'a photo of a English foxhound.', 'a photo of a Redbone Coonhound.', 'a photo of a borzoi.', 'a photo of a Irish Wolfhound.', 'a photo of a Italian Greyhound.', 'a photo of a Whippet.', 'a photo of a Ibizan Hound.', 'a photo of a Norwegian Elkhound.', 'a photo of a Otterhound.', 'a photo of a Saluki.', 'a photo of a Scottish Deerhound.', 'a photo of a Weimaraner.', 'a photo of a Staffordshire Bull Terrier.', 'a photo of a American Staffordshire Terrier.', 'a photo of a Bedlington Terrier.', 'a photo of a Border Terrier.', 'a photo of a Kerry Blue Terrier.', 'a photo of a Irish Terrier.', 'a photo of a Norfolk Terrier.', 'a photo of a Norwich Terrier.', 'a photo of a Yorkshire Terrier.', 'a photo of a Wire Fox Terrier.', 'a photo of a Lakeland Terrier.', 'a photo of a Sealyham Terrier.', 'a photo of a Airedale Terrier.', 'a photo of a Cairn Terrier.', 'a photo of a Australian Terrier.', 'a photo of a Dandie Dinmont Terrier.', 'a photo of a Boston Terrier.', 'a photo of a Miniature Schnauzer.', 'a photo of a Giant Schnauzer.', 'a photo of a Standard Schnauzer.', 'a photo of a Scottish Terrier.', 'a photo of a Tibetan Terrier.', 'a photo of a Australian Silky Terrier.', 'a photo of a Soft-coated Wheaten Terrier.', 'a photo of a West Highland White Terrier.', 'a photo of a Lhasa Apso.', 'a photo of a Flat-Coated Retriever.', 'a photo of a Curly-coated Retriever.', 'a photo of a Golden Retriever.', 'a photo of a Labrador Retriever.', 'a photo of a Chesapeake Bay Retriever.', 'a photo of a German Shorthaired Pointer.', 'a photo of a Vizsla.', 'a photo of a English Setter.', 'a photo of a Irish Setter.', 'a photo of a Gordon Setter.', 'a photo of a Brittany dog.', 'a photo of a Clumber Spaniel.', 'a photo of a English Springer Spaniel.', 'a photo of a Welsh Springer Spaniel.', 'a photo of a Cocker Spaniel.', 'a photo of a Sussex Spaniel.', 'a photo of a Irish Water Spaniel.', 'a photo of a Kuvasz.', 'a photo of a Schipperke.', 'a photo of a Groenendael dog.', 'a photo of a Malinois.', 'a photo of a Briard.', 'a photo of a Australian Kelpie.', 'a photo of a Komondor.', 'a photo of a Old English Sheepdog.', 'a photo of a Shetland Sheepdog.', 'a photo of a collie.', 'a photo of a Border Collie.', 'a photo of a Bouvier des Flandres dog.', 'a photo of a Rottweiler.', 'a photo of a German Shepherd Dog.', 'a photo of a Dobermann.', 'a photo of a Miniature Pinscher.', 'a photo of a Greater Swiss Mountain Dog.', 'a photo of a Bernese Mountain Dog.', 'a photo of a Appenzeller Sennenhund.', 'a photo of a Entlebucher Sennenhund.', 'a photo of a Boxer.', 'a photo of a Bullmastiff.', 'a photo of a Tibetan Mastiff.', 'a photo of a French Bulldog.', 'a photo of a Great Dane.', 'a photo of a St. Bernard.', 'a photo of a husky.', 'a photo of a Alaskan Malamute.', 'a photo of a Siberian Husky.', 'a photo of a Dalmatian.', 'a photo of a Affenpinscher.', 'a photo of a Basenji.', 'a photo of a pug.', 'a photo of a Leonberger.', 'a photo of a Newfoundland dog.', 'a photo of a Great Pyrenees dog.', 'a photo of a Samoyed.', 'a photo of a Pomeranian.', 'a photo of a Chow Chow.', 'a photo of a Keeshond.', 'a photo of a brussels griffon.', 'a photo of a Pembroke Welsh Corgi.', 'a photo of a Cardigan Welsh Corgi.', 'a photo of a Toy Poodle.', 'a photo of a Miniature Poodle.', 'a photo of a Standard Poodle.', 'a photo of a Mexican hairless dog (xoloitzcuintli).', 'a photo of a grey wolf.', 'a photo of a Alaskan tundra wolf.', 'a photo of a red wolf or maned wolf.', 'a photo of a coyote.', 'a photo of a dingo.', 'a photo of a dhole.', 'a photo of a African wild dog.', 'a photo of a hyena.', 'a photo of a red fox.', 'a photo of a kit fox.', 'a photo of a Arctic fox.', 'a photo of a grey fox.', 'a photo of a tabby cat.', 'a photo of a tiger cat.', 'a photo of a Persian cat.', 'a photo of a Siamese cat.', 'a photo of a Egyptian Mau.', 'a photo of a cougar.', 'a photo of a lynx.', 'a photo of a leopard.', 'a photo of a snow leopard.', 'a photo of a jaguar.', 'a photo of a lion.', 'a photo of a tiger.', 'a photo of a cheetah.', 'a photo of a brown bear.', 'a photo of a American black bear.', 'a photo of a polar bear.', 'a photo of a sloth bear.', 'a photo of a mongoose.', 'a photo of a meerkat.', 'a photo of a tiger beetle.', 'a photo of a ladybug.', 'a photo of a ground beetle.', 'a photo of a longhorn beetle.', 'a photo of a leaf beetle.', 'a photo of a dung beetle.', 'a photo of a rhinoceros beetle.', 'a photo of a weevil.', 'a photo of a fly.', 'a photo of a bee.', 'a photo of a ant.', 'a photo of a grasshopper.', 'a photo of a cricket insect.', 'a photo of a stick insect.', 'a photo of a cockroach.', 'a photo of a praying mantis.', 'a photo of a cicada.', 'a photo of a leafhopper.', 'a photo of a lacewing.', 'a photo of a dragonfly.', 'a photo of a damselfly.', 'a photo of a red admiral butterfly.', 'a photo of a ringlet butterfly.', 'a photo of a monarch butterfly.', 'a photo of a small white butterfly.', 'a photo of a sulphur butterfly.', 'a photo of a gossamer-winged butterfly.', 'a photo of a starfish.', 'a photo of a sea urchin.', 'a photo of a sea cucumber.', 'a photo of a cottontail rabbit.', 'a photo of a hare.', 'a photo of a Angora rabbit.', 'a photo of a hamster.', 'a photo of a porcupine.', 'a photo of a fox squirrel.', 'a photo of a marmot.', 'a photo of a beaver.', 'a photo of a guinea pig.', 'a photo of a common sorrel horse.', 'a photo of a zebra.', 'a photo of a pig.', 'a photo of a wild boar.', 'a photo of a warthog.', 'a photo of a hippopotamus.', 'a photo of a ox.', 'a photo of a water buffalo.', 'a photo of a bison.', 'a photo of a ram (adult male sheep).', 'a photo of a bighorn sheep.', 'a photo of a Alpine ibex.', 'a photo of a hartebeest.', 'a photo of a impala (antelope).', 'a photo of a gazelle.', 'a photo of a arabian camel.', 'a photo of a llama.', 'a photo of a weasel.', 'a photo of a mink.', 'a photo of a European polecat.', 'a photo of a black-footed ferret.', 'a photo of a otter.', 'a photo of a skunk.', 'a photo of a badger.', 'a photo of a armadillo.', 'a photo of a three-toed sloth.', 'a photo of a orangutan.', 'a photo of a gorilla.', 'a photo of a chimpanzee.', 'a photo of a gibbon.', 'a photo of a siamang.', 'a photo of a guenon.', 'a photo of a patas monkey.', 'a photo of a baboon.', 'a photo of a macaque.', 'a photo of a langur.', 'a photo of a black-and-white colobus.', 'a photo of a proboscis monkey.', 'a photo of a marmoset.', 'a photo of a white-headed capuchin.', 'a photo of a howler monkey.', 'a photo of a titi monkey.', "a photo of a Geoffroy's spider monkey.", 'a photo of a common squirrel monkey.', 'a photo of a ring-tailed lemur.', 'a photo of a indri.', 'a photo of a Asian elephant.', 'a photo of a African bush elephant.', 'a photo of a red panda.', 'a photo of a giant panda.', 'a photo of a snoek fish.', 'a photo of a eel.', 'a photo of a silver salmon.', 'a photo of a rock beauty fish.', 'a photo of a clownfish.', 'a photo of a sturgeon.', 'a photo of a gar fish.', 'a photo of a lionfish.', 'a photo of a pufferfish.', 'a photo of a abacus.', 'a photo of a abaya.', 'a photo of a academic gown.', 'a photo of a accordion.', 'a photo of a acoustic guitar.', 'a photo of a aircraft carrier.', 'a photo of a airliner.', 'a photo of a airship.', 'a photo of a altar.', 'a photo of a ambulance.', 'a photo of a amphibious vehicle.', 'a photo of a analog clock.', 'a photo of a apiary.', 'a photo of a apron.', 'a photo of a trash can.', 'a photo of a assault rifle.', 'a photo of a backpack.', 'a photo of a bakery.', 'a photo of a balance beam.', 'a photo of a balloon.', 'a photo of a ballpoint pen.', 'a photo of a Band-Aid.', 'a photo of a banjo.', 'a photo of a baluster / handrail.', 'a photo of a barbell.', 'a photo of a barber chair.', 'a photo of a barbershop.', 'a photo of a barn.', 'a photo of a barometer.', 'a photo of a barrel.', 'a photo of a wheelbarrow.', 'a photo of a baseball.', 'a photo of a basketball.', 'a photo of a bassinet.', 'a photo of a bassoon.', 'a photo of a swimming cap.', 'a photo of a bath towel.', 'a photo of a bathtub.', 'a photo of a station wagon.', 'a photo of a lighthouse.', 'a photo of a beaker.', 'a photo of a military hat (bearskin or shako).', 'a photo of a beer bottle.', 'a photo of a beer glass.', 'a photo of a bell tower.', 'a photo of a baby bib.', 'a photo of a tandem bicycle.', 'a photo of a bikini.', 'a photo of a ring binder.', 'a photo of a binoculars.', 'a photo of a birdhouse.', 'a photo of a boathouse.', 'a photo of a bobsleigh.', 'a photo of a bolo tie.', 'a photo of a poke bonnet.', 'a photo of a bookcase.', 'a photo of a bookstore.', 'a photo of a bottle cap.', 'a photo of a hunting bow.', 'a photo of a bow tie.', 'a photo of a brass memorial plaque.', 'a photo of a bra.', 'a photo of a breakwater.', 'a photo of a breastplate.', 'a photo of a broom.', 'a photo of a bucket.', 'a photo of a buckle.', 'a photo of a bulletproof vest.', 'a photo of a high-speed train.', 'a photo of a butcher shop.', 'a photo of a taxicab.', 'a photo of a cauldron.', 'a photo of a candle.', 'a photo of a cannon.', 'a photo of a canoe.', 'a photo of a can opener.', 'a photo of a cardigan.', 'a photo of a car mirror.', 'a photo of a carousel.', 'a photo of a tool kit.', 'a photo of a cardboard box / carton.', 'a photo of a car wheel.', 'a photo of a automated teller machine.', 'a photo of a cassette.', 'a photo of a cassette player.', 'a photo of a castle.', 'a photo of a catamaran.', 'a photo of a CD player.', 'a photo of a cello.', 'a photo of a mobile phone.', 'a photo of a chain.', 'a photo of a chain-link fence.', 'a photo of a chain mail.', 'a photo of a chainsaw.', 'a photo of a storage chest.', 'a photo of a chiffonier.', 'a photo of a bell or wind chime.', 'a photo of a china cabinet.', 'a photo of a Christmas stocking.', 'a photo of a church.', 'a photo of a movie theater.', 'a photo of a cleaver.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/25] batch [5/250] time 0.284 (0.488) data 0.000 (0.176) loss 3.9492 (4.1488) acc 53.1250 (50.0000) lr 1.0000e-05 eta 0:50:49
epoch [1/25] batch [10/250] time 0.285 (0.387) data 0.000 (0.088) loss 4.2148 (3.9549) acc 43.7500 (51.5625) lr 1.0000e-05 eta 0:40:14
epoch [1/25] batch [15/250] time 0.286 (0.353) data 0.000 (0.059) loss 4.3516 (3.9259) acc 34.3750 (50.2083) lr 1.0000e-05 eta 0:36:41
epoch [1/25] batch [20/250] time 0.286 (0.336) data 0.000 (0.044) loss 3.1426 (3.8536) acc 65.6250 (51.5625) lr 1.0000e-05 eta 0:34:54
epoch [1/25] batch [25/250] time 0.286 (0.326) data 0.000 (0.035) loss 3.9863 (3.8586) acc 46.8750 (51.0000) lr 1.0000e-05 eta 0:33:49
epoch [1/25] batch [30/250] time 0.285 (0.319) data 0.000 (0.030) loss 3.6914 (3.8245) acc 50.0000 (51.3542) lr 1.0000e-05 eta 0:33:06
epoch [1/25] batch [35/250] time 0.286 (0.315) data 0.000 (0.025) loss 3.5547 (3.7781) acc 56.2500 (51.6964) lr 1.0000e-05 eta 0:32:35
epoch [1/25] batch [40/250] time 0.286 (0.311) data 0.000 (0.022) loss 3.8887 (3.7651) acc 53.1250 (51.7188) lr 1.0000e-05 eta 0:32:11
epoch [1/25] batch [45/250] time 0.286 (0.308) data 0.000 (0.020) loss 3.2617 (3.7057) acc 65.6250 (52.5694) lr 1.0000e-05 eta 0:31:52
epoch [1/25] batch [50/250] time 0.286 (0.306) data 0.000 (0.018) loss 3.2383 (3.6720) acc 71.8750 (53.1250) lr 1.0000e-05 eta 0:31:37
epoch [1/25] batch [55/250] time 0.286 (0.304) data 0.000 (0.016) loss 4.0820 (3.6660) acc 34.3750 (52.5568) lr 1.0000e-05 eta 0:31:24
epoch [1/25] batch [60/250] time 0.287 (0.303) data 0.001 (0.015) loss 2.9629 (3.6184) acc 59.3750 (52.9688) lr 1.0000e-05 eta 0:31:14
epoch [1/25] batch [65/250] time 0.287 (0.302) data 0.000 (0.014) loss 3.2676 (3.5979) acc 53.1250 (52.9327) lr 1.0000e-05 eta 0:31:04
epoch [1/25] batch [70/250] time 0.287 (0.300) data 0.000 (0.013) loss 2.8516 (3.5802) acc 62.5000 (52.9911) lr 1.0000e-05 eta 0:30:56
epoch [1/25] batch [75/250] time 0.287 (0.300) data 0.000 (0.012) loss 3.0195 (3.5611) acc 59.3750 (53.0417) lr 1.0000e-05 eta 0:30:49
epoch [1/25] batch [80/250] time 0.286 (0.299) data 0.000 (0.011) loss 3.2773 (3.5442) acc 59.3750 (53.0469) lr 1.0000e-05 eta 0:30:43
epoch [1/25] batch [85/250] time 0.287 (0.298) data 0.000 (0.011) loss 3.6406 (3.5395) acc 53.1250 (52.9044) lr 1.0000e-05 eta 0:30:37
epoch [1/25] batch [90/250] time 0.287 (0.297) data 0.000 (0.010) loss 3.5449 (3.5127) acc 50.0000 (53.1944) lr 1.0000e-05 eta 0:30:32
epoch [1/25] batch [95/250] time 0.287 (0.297) data 0.000 (0.010) loss 3.6582 (3.5023) acc 40.6250 (53.0592) lr 1.0000e-05 eta 0:30:27
epoch [1/25] batch [100/250] time 0.287 (0.296) data 0.000 (0.009) loss 2.8184 (3.4788) acc 59.3750 (53.4375) lr 1.0000e-05 eta 0:30:22
epoch [1/25] batch [105/250] time 0.287 (0.296) data 0.000 (0.009) loss 3.1367 (3.4602) acc 59.3750 (53.6310) lr 1.0000e-05 eta 0:30:18
epoch [1/25] batch [110/250] time 0.287 (0.296) data 0.000 (0.008) loss 2.6836 (3.4449) acc 56.2500 (53.7784) lr 1.0000e-05 eta 0:30:14
epoch [1/25] batch [115/250] time 0.287 (0.295) data 0.000 (0.008) loss 3.0195 (3.4299) acc 59.3750 (53.7500) lr 1.0000e-05 eta 0:30:11
epoch [1/25] batch [120/250] time 0.288 (0.295) data 0.000 (0.008) loss 3.1953 (3.4188) acc 50.0000 (53.5938) lr 1.0000e-05 eta 0:30:07
epoch [1/25] batch [125/250] time 0.287 (0.295) data 0.000 (0.007) loss 2.8438 (3.4009) acc 56.2500 (53.7000) lr 1.0000e-05 eta 0:30:04
epoch [1/25] batch [130/250] time 0.288 (0.294) data 0.000 (0.007) loss 2.8535 (3.3802) acc 62.5000 (53.9183) lr 1.0000e-05 eta 0:30:01
epoch [1/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.007) loss 3.1758 (3.3655) acc 50.0000 (54.0278) lr 1.0000e-05 eta 0:29:58
epoch [1/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.007) loss 3.7070 (3.3572) acc 53.1250 (54.1295) lr 1.0000e-05 eta 0:29:55
epoch [1/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.8516 (3.3465) acc 56.2500 (54.2026) lr 1.0000e-05 eta 0:29:53
epoch [1/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.8242 (3.3297) acc 68.7500 (54.4375) lr 1.0000e-05 eta 0:29:50
epoch [1/25] batch [155/250] time 0.288 (0.293) data 0.000 (0.006) loss 2.5586 (3.3079) acc 50.0000 (54.5363) lr 1.0000e-05 eta 0:29:48
epoch [1/25] batch [160/250] time 0.288 (0.293) data 0.000 (0.006) loss 2.4102 (3.2976) acc 62.5000 (54.3750) lr 1.0000e-05 eta 0:29:45
epoch [1/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.006) loss 3.2715 (3.2918) acc 56.2500 (54.3371) lr 1.0000e-05 eta 0:29:43
epoch [1/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.4512 (3.2752) acc 65.6250 (54.4669) lr 1.0000e-05 eta 0:29:41
epoch [1/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.1914 (3.2552) acc 78.1250 (54.8036) lr 1.0000e-05 eta 0:29:39
epoch [1/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.5195 (3.2370) acc 50.0000 (54.9306) lr 1.0000e-05 eta 0:29:37
epoch [1/25] batch [185/250] time 0.290 (0.293) data 0.000 (0.005) loss 3.3887 (3.2345) acc 53.1250 (54.8818) lr 1.0000e-05 eta 0:29:34
epoch [1/25] batch [190/250] time 0.288 (0.293) data 0.000 (0.005) loss 3.4863 (3.2230) acc 37.5000 (54.8026) lr 1.0000e-05 eta 0:29:32
epoch [1/25] batch [195/250] time 0.289 (0.292) data 0.001 (0.005) loss 2.4082 (3.2122) acc 75.0000 (54.9359) lr 1.0000e-05 eta 0:29:30
epoch [1/25] batch [200/250] time 0.289 (0.292) data 0.000 (0.005) loss 2.0078 (3.1992) acc 71.8750 (54.9844) lr 1.0000e-05 eta 0:29:28
epoch [1/25] batch [205/250] time 0.288 (0.292) data 0.000 (0.005) loss 2.6484 (3.1846) acc 59.3750 (55.1220) lr 1.0000e-05 eta 0:29:26
epoch [1/25] batch [210/250] time 0.288 (0.292) data 0.000 (0.004) loss 2.6699 (3.1679) acc 56.2500 (55.2827) lr 1.0000e-05 eta 0:29:24
epoch [1/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 2.4922 (3.1537) acc 62.5000 (55.4506) lr 1.0000e-05 eta 0:29:22
epoch [1/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 2.8594 (3.1456) acc 56.2500 (55.5114) lr 1.0000e-05 eta 0:29:20
epoch [1/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 2.2969 (3.1332) acc 71.8750 (55.6389) lr 1.0000e-05 eta 0:29:19
epoch [1/25] batch [230/250] time 0.290 (0.292) data 0.000 (0.004) loss 2.7227 (3.1174) acc 46.8750 (55.6929) lr 1.0000e-05 eta 0:29:17
epoch [1/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.004) loss 2.6816 (3.1129) acc 50.0000 (55.7048) lr 1.0000e-05 eta 0:29:15
epoch [1/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 2.1875 (3.1015) acc 71.8750 (55.7552) lr 1.0000e-05 eta 0:29:13
epoch [1/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.004) loss 2.4688 (3.0908) acc 65.6250 (55.8929) lr 1.0000e-05 eta 0:29:11
epoch [1/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.004) loss 2.3105 (3.0826) acc 50.0000 (55.7875) lr 2.0000e-03 eta 0:29:10
epoch [2/25] batch [5/250] time 0.289 (0.443) data 0.000 (0.153) loss 1.9004 (2.2260) acc 56.2500 (58.7500) lr 2.0000e-03 eta 0:44:14
epoch [2/25] batch [10/250] time 0.290 (0.367) data 0.000 (0.077) loss 1.4287 (2.1118) acc 68.7500 (58.1250) lr 2.0000e-03 eta 0:36:36
epoch [2/25] batch [15/250] time 0.289 (0.341) data 0.000 (0.051) loss 1.3174 (1.8630) acc 71.8750 (62.2917) lr 2.0000e-03 eta 0:33:59
epoch [2/25] batch [20/250] time 0.290 (0.328) data 0.000 (0.039) loss 1.3760 (1.7759) acc 71.8750 (63.4375) lr 2.0000e-03 eta 0:32:40
epoch [2/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 1.7207 (1.7127) acc 59.3750 (64.5000) lr 2.0000e-03 eta 0:31:53
epoch [2/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.026) loss 1.3838 (1.6442) acc 65.6250 (65.1042) lr 2.0000e-03 eta 0:31:21
epoch [2/25] batch [35/250] time 0.290 (0.311) data 0.000 (0.022) loss 1.1826 (1.5934) acc 62.5000 (65.8036) lr 2.0000e-03 eta 0:30:58
epoch [2/25] batch [40/250] time 0.290 (0.309) data 0.000 (0.020) loss 1.5947 (1.6158) acc 65.6250 (65.3125) lr 2.0000e-03 eta 0:30:40
epoch [2/25] batch [45/250] time 0.288 (0.307) data 0.000 (0.017) loss 1.7949 (1.6232) acc 59.3750 (65.2778) lr 2.0000e-03 eta 0:30:26
epoch [2/25] batch [50/250] time 0.289 (0.305) data 0.001 (0.016) loss 1.2090 (1.6244) acc 75.0000 (65.1250) lr 2.0000e-03 eta 0:30:14
epoch [2/25] batch [55/250] time 0.289 (0.304) data 0.001 (0.014) loss 1.6348 (1.6123) acc 59.3750 (65.1136) lr 2.0000e-03 eta 0:30:04
epoch [2/25] batch [60/250] time 0.292 (0.302) data 0.001 (0.013) loss 1.1377 (1.5910) acc 68.7500 (64.8958) lr 2.0000e-03 eta 0:29:55
epoch [2/25] batch [65/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.1660 (1.5865) acc 65.6250 (64.5673) lr 2.0000e-03 eta 0:29:48
epoch [2/25] batch [70/250] time 0.290 (0.301) data 0.001 (0.011) loss 1.9756 (1.5853) acc 53.1250 (64.2411) lr 2.0000e-03 eta 0:29:42
epoch [2/25] batch [75/250] time 0.291 (0.300) data 0.000 (0.011) loss 1.7529 (1.5706) acc 56.2500 (64.4167) lr 2.0000e-03 eta 0:29:36
epoch [2/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1670 (1.5608) acc 75.0000 (64.5703) lr 2.0000e-03 eta 0:29:31
epoch [2/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.3047 (1.5553) acc 75.0000 (64.4853) lr 2.0000e-03 eta 0:29:26
epoch [2/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2031 (1.5415) acc 68.7500 (64.4097) lr 2.0000e-03 eta 0:29:21
epoch [2/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.2461 (1.5286) acc 56.2500 (64.3421) lr 2.0000e-03 eta 0:29:17
epoch [2/25] batch [100/250] time 0.290 (0.297) data 0.001 (0.008) loss 1.2910 (1.5143) acc 59.3750 (64.4062) lr 2.0000e-03 eta 0:29:13
epoch [2/25] batch [105/250] time 0.289 (0.297) data 0.001 (0.008) loss 1.9180 (1.5140) acc 56.2500 (64.2857) lr 2.0000e-03 eta 0:29:10
epoch [2/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.8018 (1.5184) acc 62.5000 (64.0057) lr 2.0000e-03 eta 0:29:06
epoch [2/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7871 (1.5226) acc 59.3750 (63.9674) lr 2.0000e-03 eta 0:29:03
epoch [2/25] batch [120/250] time 0.290 (0.296) data 0.001 (0.007) loss 1.1943 (1.5236) acc 68.7500 (63.9062) lr 2.0000e-03 eta 0:28:59
epoch [2/25] batch [125/250] time 0.288 (0.296) data 0.000 (0.007) loss 1.6494 (1.5318) acc 62.5000 (63.8000) lr 2.0000e-03 eta 0:28:57
epoch [2/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3516 (1.5293) acc 75.0000 (63.8942) lr 2.0000e-03 eta 0:28:54
epoch [2/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.1816 (1.5309) acc 56.2500 (63.8889) lr 2.0000e-03 eta 0:28:51
epoch [2/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6807 (1.5279) acc 62.5000 (63.9509) lr 2.0000e-03 eta 0:28:48
epoch [2/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3984 (1.5266) acc 71.8750 (63.9440) lr 2.0000e-03 eta 0:28:45
epoch [2/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1758 (1.5202) acc 68.7500 (63.9375) lr 2.0000e-03 eta 0:28:43
epoch [2/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0605 (1.5092) acc 71.8750 (64.0927) lr 2.0000e-03 eta 0:28:40
epoch [2/25] batch [160/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.7891 (1.5084) acc 59.3750 (64.0625) lr 2.0000e-03 eta 0:28:38
epoch [2/25] batch [165/250] time 0.288 (0.294) data 0.000 (0.005) loss 0.9609 (1.5070) acc 75.0000 (64.1477) lr 2.0000e-03 eta 0:28:35
epoch [2/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7959 (1.5121) acc 56.2500 (64.0257) lr 2.0000e-03 eta 0:28:33
epoch [2/25] batch [175/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.1865 (1.5119) acc 71.8750 (64.0536) lr 2.0000e-03 eta 0:28:30
epoch [2/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7686 (1.5102) acc 56.2500 (64.0451) lr 2.0000e-03 eta 0:28:28
epoch [2/25] batch [185/250] time 0.288 (0.293) data 0.000 (0.005) loss 2.0117 (1.5050) acc 56.2500 (64.1723) lr 2.0000e-03 eta 0:28:26
epoch [2/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.7842 (1.5038) acc 50.0000 (64.1612) lr 2.0000e-03 eta 0:28:24
epoch [2/25] batch [195/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.2119 (1.5053) acc 71.8750 (64.1667) lr 2.0000e-03 eta 0:28:22
epoch [2/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5059 (1.5105) acc 68.7500 (64.1562) lr 2.0000e-03 eta 0:28:20
epoch [2/25] batch [205/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.3613 (1.5068) acc 65.6250 (64.1463) lr 2.0000e-03 eta 0:28:17
epoch [2/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5488 (1.5031) acc 65.6250 (64.2113) lr 2.0000e-03 eta 0:28:15
epoch [2/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3574 (1.5007) acc 59.3750 (64.2733) lr 2.0000e-03 eta 0:28:13
epoch [2/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2422 (1.5022) acc 65.6250 (64.2472) lr 2.0000e-03 eta 0:28:11
epoch [2/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2559 (1.4976) acc 59.3750 (64.1667) lr 2.0000e-03 eta 0:28:10
epoch [2/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8857 (1.4962) acc 62.5000 (64.2527) lr 2.0000e-03 eta 0:28:08
epoch [2/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.6250 (1.4931) acc 68.7500 (64.3883) lr 2.0000e-03 eta 0:28:06
epoch [2/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.4502 (1.4917) acc 56.2500 (64.2839) lr 2.0000e-03 eta 0:28:04
epoch [2/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2295 (1.4919) acc 62.5000 (64.2347) lr 2.0000e-03 eta 0:28:02
epoch [2/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3213 (1.4944) acc 68.7500 (64.2750) lr 1.9921e-03 eta 0:28:00
epoch [3/25] batch [5/250] time 0.291 (0.456) data 0.001 (0.167) loss 1.2402 (1.3502) acc 59.3750 (63.1250) lr 1.9921e-03 eta 0:43:40
epoch [3/25] batch [10/250] time 0.289 (0.373) data 0.000 (0.084) loss 1.5586 (1.3174) acc 65.6250 (64.0625) lr 1.9921e-03 eta 0:35:40
epoch [3/25] batch [15/250] time 0.290 (0.345) data 0.000 (0.056) loss 1.4814 (1.3308) acc 59.3750 (64.5833) lr 1.9921e-03 eta 0:32:58
epoch [3/25] batch [20/250] time 0.288 (0.331) data 0.000 (0.042) loss 1.5400 (1.3890) acc 71.8750 (65.7812) lr 1.9921e-03 eta 0:31:37
epoch [3/25] batch [25/250] time 0.289 (0.323) data 0.000 (0.034) loss 1.5957 (1.4122) acc 62.5000 (65.0000) lr 1.9921e-03 eta 0:30:47
epoch [3/25] batch [30/250] time 0.289 (0.317) data 0.000 (0.028) loss 1.1953 (1.4120) acc 65.6250 (64.1667) lr 1.9921e-03 eta 0:30:14
epoch [3/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.1084 (1.4314) acc 78.1250 (64.6429) lr 1.9921e-03 eta 0:29:49
epoch [3/25] batch [40/250] time 0.290 (0.310) data 0.001 (0.021) loss 1.1611 (1.4314) acc 56.2500 (63.8281) lr 1.9921e-03 eta 0:29:31
epoch [3/25] batch [45/250] time 0.290 (0.308) data 0.000 (0.019) loss 1.3555 (1.4583) acc 62.5000 (63.6111) lr 1.9921e-03 eta 0:29:16
epoch [3/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.0078 (1.4479) acc 75.0000 (63.6875) lr 1.9921e-03 eta 0:29:04
epoch [3/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.016) loss 1.4326 (1.4469) acc 68.7500 (63.6932) lr 1.9921e-03 eta 0:28:53
epoch [3/25] batch [60/250] time 0.288 (0.303) data 0.001 (0.014) loss 1.1416 (1.4386) acc 56.2500 (64.1146) lr 1.9921e-03 eta 0:28:45
epoch [3/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.0332 (1.4344) acc 71.8750 (64.0385) lr 1.9921e-03 eta 0:28:37
epoch [3/25] batch [70/250] time 0.290 (0.301) data 0.001 (0.012) loss 1.1396 (1.4174) acc 65.6250 (64.3304) lr 1.9921e-03 eta 0:28:31
epoch [3/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.3379 (1.4142) acc 75.0000 (64.4167) lr 1.9921e-03 eta 0:28:24
epoch [3/25] batch [80/250] time 0.288 (0.300) data 0.000 (0.011) loss 1.5703 (1.4224) acc 56.2500 (64.2188) lr 1.9921e-03 eta 0:28:19
epoch [3/25] batch [85/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.4922 (1.4249) acc 50.0000 (64.1912) lr 1.9921e-03 eta 0:28:14
epoch [3/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.2529 (1.4171) acc 71.8750 (64.4097) lr 1.9921e-03 eta 0:28:09
epoch [3/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.5488 (1.4160) acc 65.6250 (64.6382) lr 1.9921e-03 eta 0:28:05
epoch [3/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.6660 (1.4152) acc 56.2500 (64.7188) lr 1.9921e-03 eta 0:28:01
epoch [3/25] batch [105/250] time 0.293 (0.297) data 0.000 (0.008) loss 1.7139 (1.4108) acc 56.2500 (65.0000) lr 1.9921e-03 eta 0:27:58
epoch [3/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.9766 (1.4161) acc 50.0000 (64.8011) lr 1.9921e-03 eta 0:27:54
epoch [3/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2334 (1.4159) acc 62.5000 (64.6739) lr 1.9921e-03 eta 0:27:50
epoch [3/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2881 (1.4128) acc 71.8750 (64.7656) lr 1.9921e-03 eta 0:27:47
epoch [3/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7529 (1.4237) acc 65.6250 (64.5750) lr 1.9921e-03 eta 0:27:44
epoch [3/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0713 (1.4147) acc 78.1250 (64.7356) lr 1.9921e-03 eta 0:27:41
epoch [3/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.3721 (1.4269) acc 68.7500 (64.3519) lr 1.9921e-03 eta 0:27:38
epoch [3/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.9258 (1.4260) acc 62.5000 (64.2857) lr 1.9921e-03 eta 0:27:36
epoch [3/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2803 (1.4263) acc 68.7500 (64.2888) lr 1.9921e-03 eta 0:27:33
epoch [3/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1943 (1.4216) acc 75.0000 (64.4167) lr 1.9921e-03 eta 0:27:30
epoch [3/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2490 (1.4248) acc 56.2500 (64.3548) lr 1.9921e-03 eta 0:27:28
epoch [3/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4307 (1.4271) acc 59.3750 (64.2773) lr 1.9921e-03 eta 0:27:25
epoch [3/25] batch [165/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.9199 (1.4351) acc 50.0000 (64.2045) lr 1.9921e-03 eta 0:27:23
epoch [3/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3887 (1.4337) acc 62.5000 (64.3750) lr 1.9921e-03 eta 0:27:20
epoch [3/25] batch [175/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.4678 (1.4377) acc 59.3750 (64.2857) lr 1.9921e-03 eta 0:27:18
epoch [3/25] batch [180/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.1729 (1.4355) acc 65.6250 (64.3576) lr 1.9921e-03 eta 0:27:16
epoch [3/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0273 (1.4339) acc 46.8750 (64.3919) lr 1.9921e-03 eta 0:27:14
epoch [3/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3350 (1.4316) acc 68.7500 (64.3750) lr 1.9921e-03 eta 0:27:11
epoch [3/25] batch [195/250] time 0.289 (0.293) data 0.001 (0.005) loss 1.3115 (1.4248) acc 62.5000 (64.4231) lr 1.9921e-03 eta 0:27:09
epoch [3/25] batch [200/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.3086 (1.4178) acc 62.5000 (64.5781) lr 1.9921e-03 eta 0:27:07
epoch [3/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9619 (1.4090) acc 71.8750 (64.7409) lr 1.9921e-03 eta 0:27:05
epoch [3/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0625 (1.3995) acc 75.0000 (64.9405) lr 1.9921e-03 eta 0:27:03
epoch [3/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0078 (1.3984) acc 68.7500 (65.0145) lr 1.9921e-03 eta 0:27:01
epoch [3/25] batch [220/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.2168 (1.3954) acc 68.7500 (64.9858) lr 1.9921e-03 eta 0:26:59
epoch [3/25] batch [225/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.7852 (1.3992) acc 53.1250 (64.9306) lr 1.9921e-03 eta 0:26:57
epoch [3/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0889 (1.3987) acc 68.7500 (64.9457) lr 1.9921e-03 eta 0:26:55
epoch [3/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 0.7622 (1.3916) acc 84.3750 (65.1197) lr 1.9921e-03 eta 0:26:53
epoch [3/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2432 (1.3904) acc 68.7500 (65.1823) lr 1.9921e-03 eta 0:26:51
epoch [3/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.7012 (1.3908) acc 46.8750 (65.1403) lr 1.9921e-03 eta 0:26:49
epoch [3/25] batch [250/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.6191 (1.3932) acc 62.5000 (65.0750) lr 1.9686e-03 eta 0:26:47
epoch [4/25] batch [5/250] time 0.289 (0.434) data 0.000 (0.144) loss 1.6211 (1.3313) acc 68.7500 (68.1250) lr 1.9686e-03 eta 0:39:44
epoch [4/25] batch [10/250] time 0.290 (0.362) data 0.001 (0.072) loss 1.2061 (1.2642) acc 68.7500 (70.6250) lr 1.9686e-03 eta 0:33:07
epoch [4/25] batch [15/250] time 0.288 (0.338) data 0.000 (0.048) loss 1.2285 (1.3044) acc 68.7500 (69.5833) lr 1.9686e-03 eta 0:30:53
epoch [4/25] batch [20/250] time 0.290 (0.326) data 0.000 (0.036) loss 1.5166 (1.3185) acc 65.6250 (67.9688) lr 1.9686e-03 eta 0:29:44
epoch [4/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.5645 (1.3061) acc 56.2500 (66.8750) lr 1.9686e-03 eta 0:29:03
epoch [4/25] batch [30/250] time 0.287 (0.313) data 0.000 (0.024) loss 1.3760 (1.3285) acc 56.2500 (66.1458) lr 1.9686e-03 eta 0:28:34
epoch [4/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.9844 (1.3551) acc 75.0000 (66.0714) lr 1.9686e-03 eta 0:28:13
epoch [4/25] batch [40/250] time 0.290 (0.307) data 0.001 (0.018) loss 1.1025 (1.3330) acc 75.0000 (66.5625) lr 1.9686e-03 eta 0:27:58
epoch [4/25] batch [45/250] time 0.290 (0.305) data 0.001 (0.016) loss 1.6797 (1.3288) acc 50.0000 (66.5278) lr 1.9686e-03 eta 0:27:45
epoch [4/25] batch [50/250] time 0.292 (0.304) data 0.001 (0.015) loss 1.0693 (1.3192) acc 71.8750 (66.6250) lr 1.9686e-03 eta 0:27:35
epoch [4/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.014) loss 1.1670 (1.3508) acc 62.5000 (66.0795) lr 1.9686e-03 eta 0:27:26
epoch [4/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.8311 (1.3722) acc 59.3750 (65.9375) lr 1.9686e-03 eta 0:27:18
epoch [4/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.7793 (1.3696) acc 53.1250 (65.7212) lr 1.9686e-03 eta 0:27:12
epoch [4/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.4932 (1.3771) acc 59.3750 (65.6696) lr 1.9686e-03 eta 0:27:06
epoch [4/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3740 (1.3712) acc 62.5000 (65.7500) lr 1.9686e-03 eta 0:27:01
epoch [4/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3809 (1.3669) acc 68.7500 (66.0547) lr 1.9686e-03 eta 0:26:56
epoch [4/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7568 (1.3792) acc 50.0000 (65.5882) lr 1.9686e-03 eta 0:26:52
epoch [4/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.8564 (1.3765) acc 56.2500 (65.7986) lr 1.9686e-03 eta 0:26:47
epoch [4/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0273 (1.3808) acc 71.8750 (65.6579) lr 1.9686e-03 eta 0:26:43
epoch [4/25] batch [100/250] time 0.290 (0.296) data 0.001 (0.008) loss 0.9521 (1.3789) acc 71.8750 (65.7188) lr 1.9686e-03 eta 0:26:40
epoch [4/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.6846 (1.3749) acc 53.1250 (65.7440) lr 1.9686e-03 eta 0:26:36
epoch [4/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3584 (1.3787) acc 65.6250 (65.6250) lr 1.9686e-03 eta 0:26:33
epoch [4/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.1025 (1.3778) acc 68.7500 (65.7609) lr 1.9686e-03 eta 0:26:30
epoch [4/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8057 (1.3819) acc 53.1250 (65.5469) lr 1.9686e-03 eta 0:26:27
epoch [4/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2080 (1.3766) acc 78.1250 (65.7000) lr 1.9686e-03 eta 0:26:24
epoch [4/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6738 (1.3782) acc 65.6250 (65.7212) lr 1.9686e-03 eta 0:26:22
epoch [4/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.9053 (1.3821) acc 50.0000 (65.6250) lr 1.9686e-03 eta 0:26:19
epoch [4/25] batch [140/250] time 0.288 (0.294) data 0.000 (0.006) loss 1.3730 (1.3801) acc 68.7500 (65.6250) lr 1.9686e-03 eta 0:26:16
epoch [4/25] batch [145/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.3818 (1.3780) acc 65.6250 (65.6466) lr 1.9686e-03 eta 0:26:14
epoch [4/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5020 (1.3828) acc 59.3750 (65.5625) lr 1.9686e-03 eta 0:26:11
epoch [4/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6182 (1.3861) acc 62.5000 (65.2218) lr 1.9686e-03 eta 0:26:09
epoch [4/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.4268 (1.3867) acc 59.3750 (65.1367) lr 1.9686e-03 eta 0:26:07
epoch [4/25] batch [165/250] time 0.290 (0.293) data 0.000 (0.005) loss 1.4307 (1.3944) acc 68.7500 (65.0947) lr 1.9686e-03 eta 0:26:05
epoch [4/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.7832 (1.3977) acc 65.6250 (65.1654) lr 1.9686e-03 eta 0:26:02
epoch [4/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.0000 (1.4005) acc 46.8750 (65.1607) lr 1.9686e-03 eta 0:26:00
epoch [4/25] batch [180/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.3350 (1.4013) acc 75.0000 (65.1736) lr 1.9686e-03 eta 0:25:58
epoch [4/25] batch [185/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.6084 (1.4037) acc 62.5000 (65.1689) lr 1.9686e-03 eta 0:25:56
epoch [4/25] batch [190/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.4014 (1.3986) acc 68.7500 (65.2632) lr 1.9686e-03 eta 0:25:54
epoch [4/25] batch [195/250] time 0.288 (0.293) data 0.001 (0.004) loss 1.2432 (1.4007) acc 68.7500 (65.2083) lr 1.9686e-03 eta 0:25:52
epoch [4/25] batch [200/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.5518 (1.3990) acc 65.6250 (65.2656) lr 1.9686e-03 eta 0:25:50
epoch [4/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7510 (1.3974) acc 59.3750 (65.3354) lr 1.9686e-03 eta 0:25:48
epoch [4/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0479 (1.3964) acc 78.1250 (65.4018) lr 1.9686e-03 eta 0:25:46
epoch [4/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2969 (1.3989) acc 65.6250 (65.3198) lr 1.9686e-03 eta 0:25:45
epoch [4/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.1846 (1.4004) acc 68.7500 (65.2273) lr 1.9686e-03 eta 0:25:43
epoch [4/25] batch [225/250] time 0.290 (0.292) data 0.001 (0.004) loss 1.0625 (1.4027) acc 71.8750 (65.1944) lr 1.9686e-03 eta 0:25:41
epoch [4/25] batch [230/250] time 0.289 (0.292) data 0.001 (0.004) loss 1.5518 (1.3991) acc 65.6250 (65.2310) lr 1.9686e-03 eta 0:25:39
epoch [4/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.2520 (1.4008) acc 62.5000 (65.1729) lr 1.9686e-03 eta 0:25:37
epoch [4/25] batch [240/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.2529 (1.3998) acc 62.5000 (65.2214) lr 1.9686e-03 eta 0:25:36
epoch [4/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4023 (1.3976) acc 71.8750 (65.2806) lr 1.9686e-03 eta 0:25:34
epoch [4/25] batch [250/250] time 0.288 (0.292) data 0.000 (0.003) loss 0.9307 (1.3940) acc 78.1250 (65.3500) lr 1.9298e-03 eta 0:25:32
epoch [5/25] batch [5/250] time 0.289 (0.433) data 0.000 (0.143) loss 1.4805 (1.0973) acc 65.6250 (75.0000) lr 1.9298e-03 eta 0:37:50
epoch [5/25] batch [10/250] time 0.289 (0.361) data 0.000 (0.072) loss 1.7090 (1.2789) acc 62.5000 (70.0000) lr 1.9298e-03 eta 0:31:31
epoch [5/25] batch [15/250] time 0.290 (0.337) data 0.000 (0.048) loss 1.0859 (1.2710) acc 65.6250 (68.9583) lr 1.9298e-03 eta 0:29:25
epoch [5/25] batch [20/250] time 0.291 (0.325) data 0.000 (0.036) loss 1.1924 (1.3001) acc 65.6250 (67.9688) lr 1.9298e-03 eta 0:28:21
epoch [5/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.1123 (1.2818) acc 75.0000 (68.1250) lr 1.9298e-03 eta 0:27:42
epoch [5/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.1465 (1.2912) acc 71.8750 (68.3333) lr 1.9298e-03 eta 0:27:15
epoch [5/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.4395 (1.2968) acc 62.5000 (67.6786) lr 1.9298e-03 eta 0:26:56
epoch [5/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.4629 (1.2867) acc 50.0000 (67.8125) lr 1.9298e-03 eta 0:26:41
epoch [5/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 2.0859 (1.2999) acc 59.3750 (67.2222) lr 1.9298e-03 eta 0:26:29
epoch [5/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.6182 (1.3092) acc 59.3750 (66.8750) lr 1.9298e-03 eta 0:26:19
epoch [5/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1709 (1.3023) acc 71.8750 (67.2159) lr 1.9298e-03 eta 0:26:10
epoch [5/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3574 (1.3081) acc 62.5000 (67.3438) lr 1.9298e-03 eta 0:26:03
epoch [5/25] batch [65/250] time 0.288 (0.300) data 0.000 (0.011) loss 1.4395 (1.3284) acc 53.1250 (66.5385) lr 1.9298e-03 eta 0:25:56
epoch [5/25] batch [70/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.6113 (1.3335) acc 59.3750 (66.2054) lr 1.9298e-03 eta 0:25:50
epoch [5/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.7998 (1.3301) acc 78.1250 (66.5000) lr 1.9298e-03 eta 0:25:45
epoch [5/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2842 (1.3321) acc 68.7500 (66.6797) lr 1.9298e-03 eta 0:25:40
epoch [5/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2188 (1.3261) acc 75.0000 (66.7279) lr 1.9298e-03 eta 0:25:36
epoch [5/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9390 (1.3192) acc 71.8750 (66.7708) lr 1.9298e-03 eta 0:25:32
epoch [5/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5693 (1.3101) acc 56.2500 (66.8092) lr 1.9298e-03 eta 0:25:29
epoch [5/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0703 (1.2970) acc 81.2500 (67.1562) lr 1.9298e-03 eta 0:25:25
epoch [5/25] batch [105/250] time 0.288 (0.296) data 0.000 (0.007) loss 1.0469 (1.2963) acc 71.8750 (66.9643) lr 1.9298e-03 eta 0:25:22
epoch [5/25] batch [110/250] time 0.288 (0.296) data 0.000 (0.007) loss 1.5439 (1.2978) acc 50.0000 (66.7614) lr 1.9298e-03 eta 0:25:19
epoch [5/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7041 (1.2974) acc 50.0000 (66.7663) lr 1.9298e-03 eta 0:25:16
epoch [5/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.9941 (1.3012) acc 56.2500 (66.8229) lr 1.9298e-03 eta 0:25:13
epoch [5/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2051 (1.3009) acc 65.6250 (66.8250) lr 1.9298e-03 eta 0:25:10
epoch [5/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8657 (1.2995) acc 75.0000 (66.8750) lr 1.9298e-03 eta 0:25:07
epoch [5/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.2559 (1.3031) acc 62.5000 (66.7593) lr 1.9298e-03 eta 0:25:06
epoch [5/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1162 (1.3026) acc 75.0000 (66.8973) lr 1.9298e-03 eta 0:25:03
epoch [5/25] batch [145/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.1367 (1.3099) acc 75.0000 (66.8966) lr 1.9298e-03 eta 0:25:01
epoch [5/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5469 (1.3065) acc 59.3750 (67.0000) lr 1.9298e-03 eta 0:24:58
epoch [5/25] batch [155/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.8535 (1.3100) acc 65.6250 (66.9960) lr 1.9298e-03 eta 0:24:56
epoch [5/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0488 (1.3086) acc 71.8750 (67.0508) lr 1.9298e-03 eta 0:24:54
epoch [5/25] batch [165/250] time 0.290 (0.293) data 0.000 (0.005) loss 1.8584 (1.3175) acc 53.1250 (66.8561) lr 1.9298e-03 eta 0:24:52
epoch [5/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.0703 (1.3146) acc 68.7500 (66.7279) lr 1.9298e-03 eta 0:24:49
epoch [5/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9448 (1.3197) acc 68.7500 (66.6786) lr 1.9298e-03 eta 0:24:47
epoch [5/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2939 (1.3210) acc 53.1250 (66.5799) lr 1.9298e-03 eta 0:24:45
epoch [5/25] batch [185/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0723 (1.3251) acc 71.8750 (66.5203) lr 1.9298e-03 eta 0:24:43
epoch [5/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5703 (1.3216) acc 65.6250 (66.5625) lr 1.9298e-03 eta 0:24:41
epoch [5/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2627 (1.3232) acc 65.6250 (66.5385) lr 1.9298e-03 eta 0:24:39
epoch [5/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0254 (1.3227) acc 71.8750 (66.5312) lr 1.9298e-03 eta 0:24:37
epoch [5/25] batch [205/250] time 0.288 (0.293) data 0.000 (0.004) loss 0.8657 (1.3244) acc 59.3750 (66.3567) lr 1.9298e-03 eta 0:24:36
epoch [5/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.4170 (1.3255) acc 65.6250 (66.3839) lr 1.9298e-03 eta 0:24:34
epoch [5/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.5029 (1.3326) acc 65.6250 (66.1628) lr 1.9298e-03 eta 0:24:32
epoch [5/25] batch [220/250] time 0.290 (0.292) data 0.000 (0.004) loss 1.5625 (1.3365) acc 62.5000 (66.0795) lr 1.9298e-03 eta 0:24:30
epoch [5/25] batch [225/250] time 0.290 (0.292) data 0.000 (0.004) loss 0.9468 (1.3320) acc 87.5000 (66.2917) lr 1.9298e-03 eta 0:24:28
epoch [5/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.8555 (1.3307) acc 59.3750 (66.3587) lr 1.9298e-03 eta 0:24:26
epoch [5/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.4775 (1.3315) acc 65.6250 (66.3298) lr 1.9298e-03 eta 0:24:25
epoch [5/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3975 (1.3331) acc 59.3750 (66.3151) lr 1.9298e-03 eta 0:24:23
epoch [5/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3623 (1.3339) acc 71.8750 (66.3010) lr 1.9298e-03 eta 0:24:21
epoch [5/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.1113 (1.3363) acc 68.7500 (66.3000) lr 1.8763e-03 eta 0:24:19
epoch [6/25] batch [5/250] time 0.289 (0.427) data 0.000 (0.138) loss 1.5635 (1.4557) acc 62.5000 (63.7500) lr 1.8763e-03 eta 0:35:31
epoch [6/25] batch [10/250] time 0.289 (0.358) data 0.000 (0.069) loss 0.9238 (1.4124) acc 78.1250 (64.0625) lr 1.8763e-03 eta 0:29:46
epoch [6/25] batch [15/250] time 0.291 (0.335) data 0.000 (0.046) loss 1.0918 (1.3867) acc 65.6250 (63.7500) lr 1.8763e-03 eta 0:27:50
epoch [6/25] batch [20/250] time 0.289 (0.324) data 0.000 (0.035) loss 1.4316 (1.3907) acc 62.5000 (63.9062) lr 1.8763e-03 eta 0:26:51
epoch [6/25] batch [25/250] time 0.289 (0.317) data 0.000 (0.028) loss 1.5557 (1.3687) acc 71.8750 (64.7500) lr 1.8763e-03 eta 0:26:15
epoch [6/25] batch [30/250] time 0.289 (0.312) data 0.000 (0.023) loss 1.7520 (1.4027) acc 65.6250 (63.9583) lr 1.8763e-03 eta 0:25:50
epoch [6/25] batch [35/250] time 0.288 (0.309) data 0.000 (0.020) loss 1.5996 (1.3908) acc 62.5000 (64.6429) lr 1.8763e-03 eta 0:25:32
epoch [6/25] batch [40/250] time 0.288 (0.306) data 0.000 (0.017) loss 1.6318 (1.3997) acc 65.6250 (64.4531) lr 1.8763e-03 eta 0:25:17
epoch [6/25] batch [45/250] time 0.290 (0.304) data 0.000 (0.016) loss 1.7520 (1.3928) acc 56.2500 (64.6528) lr 1.8763e-03 eta 0:25:07
epoch [6/25] batch [50/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.3926 (1.3897) acc 68.7500 (65.1250) lr 1.8763e-03 eta 0:24:58
epoch [6/25] batch [55/250] time 0.289 (0.301) data 0.000 (0.013) loss 1.2197 (1.4098) acc 71.8750 (64.7159) lr 1.8763e-03 eta 0:24:50
epoch [6/25] batch [60/250] time 0.290 (0.300) data 0.000 (0.012) loss 1.0400 (1.4052) acc 62.5000 (64.4792) lr 1.8763e-03 eta 0:24:44
epoch [6/25] batch [65/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.4414 (1.4060) acc 71.8750 (64.5673) lr 1.8763e-03 eta 0:24:38
epoch [6/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1133 (1.4006) acc 68.7500 (64.6429) lr 1.8763e-03 eta 0:24:33
epoch [6/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0801 (1.3848) acc 68.7500 (64.7500) lr 1.8763e-03 eta 0:24:28
epoch [6/25] batch [80/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.0107 (1.3699) acc 65.6250 (65.1172) lr 1.8763e-03 eta 0:24:24
epoch [6/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.6973 (1.3676) acc 65.6250 (65.3676) lr 1.8763e-03 eta 0:24:20
epoch [6/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2930 (1.3714) acc 75.0000 (65.4514) lr 1.8763e-03 eta 0:24:16
epoch [6/25] batch [95/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.7998 (1.3571) acc 53.1250 (65.8882) lr 1.8763e-03 eta 0:24:13
epoch [6/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.9512 (1.3608) acc 62.5000 (65.9688) lr 1.8763e-03 eta 0:24:10
epoch [6/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8535 (1.3660) acc 56.2500 (65.8333) lr 1.8763e-03 eta 0:24:07
epoch [6/25] batch [110/250] time 0.290 (0.295) data 0.000 (0.007) loss 1.2256 (1.3638) acc 68.7500 (65.7955) lr 1.8763e-03 eta 0:24:04
epoch [6/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2168 (1.3671) acc 71.8750 (65.7880) lr 1.8763e-03 eta 0:24:01
epoch [6/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.9463 (1.3693) acc 50.0000 (65.7292) lr 1.8763e-03 eta 0:23:58
epoch [6/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9941 (1.3560) acc 81.2500 (66.0500) lr 1.8763e-03 eta 0:23:55
epoch [6/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.6152 (1.3551) acc 68.7500 (66.1058) lr 1.8763e-03 eta 0:23:53
epoch [6/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3174 (1.3501) acc 59.3750 (66.2963) lr 1.8763e-03 eta 0:23:51
epoch [6/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1543 (1.3535) acc 65.6250 (66.1161) lr 1.8763e-03 eta 0:23:48
epoch [6/25] batch [145/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.4434 (1.3526) acc 62.5000 (66.2069) lr 1.8763e-03 eta 0:23:46
epoch [6/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0508 (1.3492) acc 78.1250 (66.2917) lr 1.8763e-03 eta 0:23:44
epoch [6/25] batch [155/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.0156 (1.3487) acc 68.7500 (66.0887) lr 1.8763e-03 eta 0:23:42
epoch [6/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.0664 (1.3453) acc 71.8750 (66.2109) lr 1.8763e-03 eta 0:23:39
epoch [6/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5996 (1.3490) acc 62.5000 (66.1742) lr 1.8763e-03 eta 0:23:37
epoch [6/25] batch [170/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6221 (1.3446) acc 62.5000 (66.2684) lr 1.8763e-03 eta 0:23:35
epoch [6/25] batch [175/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2295 (1.3492) acc 71.8750 (66.1429) lr 1.8763e-03 eta 0:23:33
epoch [6/25] batch [180/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6982 (1.3506) acc 56.2500 (66.0764) lr 1.8763e-03 eta 0:23:31
epoch [6/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1094 (1.3449) acc 68.7500 (66.2162) lr 1.8763e-03 eta 0:23:29
epoch [6/25] batch [190/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2217 (1.3406) acc 68.7500 (66.1842) lr 1.8763e-03 eta 0:23:28
epoch [6/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9541 (1.3390) acc 68.7500 (66.1859) lr 1.8763e-03 eta 0:23:26
epoch [6/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3525 (1.3464) acc 68.7500 (65.9844) lr 1.8763e-03 eta 0:23:24
epoch [6/25] batch [205/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3848 (1.3493) acc 71.8750 (65.8994) lr 1.8763e-03 eta 0:23:22
epoch [6/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3955 (1.3512) acc 56.2500 (65.8780) lr 1.8763e-03 eta 0:23:20
epoch [6/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.7295 (1.3507) acc 56.2500 (65.9884) lr 1.8763e-03 eta 0:23:18
epoch [6/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4551 (1.3502) acc 75.0000 (66.0227) lr 1.8763e-03 eta 0:23:17
epoch [6/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6738 (1.3525) acc 59.3750 (66.0139) lr 1.8763e-03 eta 0:23:15
epoch [6/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1201 (1.3521) acc 78.1250 (66.0598) lr 1.8763e-03 eta 0:23:13
epoch [6/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.003) loss 1.3906 (1.3548) acc 71.8750 (66.0239) lr 1.8763e-03 eta 0:23:11
epoch [6/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0439 (1.3541) acc 68.7500 (66.0286) lr 1.8763e-03 eta 0:23:09
epoch [6/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3096 (1.3537) acc 71.8750 (66.1097) lr 1.8763e-03 eta 0:23:08
epoch [6/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2920 (1.3588) acc 62.5000 (66.0625) lr 1.8090e-03 eta 0:23:06
epoch [7/25] batch [5/250] time 0.289 (0.440) data 0.000 (0.151) loss 1.5381 (1.3664) acc 50.0000 (61.8750) lr 1.8090e-03 eta 0:34:48
epoch [7/25] batch [10/250] time 0.289 (0.364) data 0.000 (0.076) loss 1.3037 (1.3667) acc 65.6250 (63.7500) lr 1.8090e-03 eta 0:28:47
epoch [7/25] batch [15/250] time 0.288 (0.339) data 0.000 (0.051) loss 1.3145 (1.3634) acc 71.8750 (65.2083) lr 1.8090e-03 eta 0:26:45
epoch [7/25] batch [20/250] time 0.288 (0.326) data 0.000 (0.038) loss 1.9570 (1.3698) acc 53.1250 (65.4688) lr 1.8090e-03 eta 0:25:43
epoch [7/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 1.6133 (1.3335) acc 68.7500 (67.1250) lr 1.8090e-03 eta 0:25:07
epoch [7/25] batch [30/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.3428 (1.3021) acc 68.7500 (67.8125) lr 1.8090e-03 eta 0:24:42
epoch [7/25] batch [35/250] time 0.288 (0.310) data 0.000 (0.022) loss 1.5039 (1.3373) acc 68.7500 (66.8750) lr 1.8090e-03 eta 0:24:23
epoch [7/25] batch [40/250] time 0.291 (0.308) data 0.001 (0.019) loss 1.6943 (1.3523) acc 65.6250 (66.5625) lr 1.8090e-03 eta 0:24:09
epoch [7/25] batch [45/250] time 0.288 (0.306) data 0.000 (0.017) loss 1.5400 (1.3532) acc 62.5000 (66.2500) lr 1.8090e-03 eta 0:23:58
epoch [7/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.6113 (1.3537) acc 78.1250 (66.4375) lr 1.8090e-03 eta 0:23:48
epoch [7/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.1387 (1.3488) acc 81.2500 (66.7045) lr 1.8090e-03 eta 0:23:40
epoch [7/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.013) loss 1.2832 (1.3599) acc 62.5000 (66.3021) lr 1.8090e-03 eta 0:23:33
epoch [7/25] batch [65/250] time 0.289 (0.301) data 0.001 (0.012) loss 0.8667 (1.3636) acc 78.1250 (66.2500) lr 1.8090e-03 eta 0:23:27
epoch [7/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.8188 (1.3611) acc 75.0000 (66.2500) lr 1.8090e-03 eta 0:23:22
epoch [7/25] batch [75/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.5781 (1.3646) acc 62.5000 (65.8333) lr 1.8090e-03 eta 0:23:17
epoch [7/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.010) loss 1.7764 (1.3631) acc 59.3750 (65.9766) lr 1.8090e-03 eta 0:23:13
epoch [7/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2471 (1.3540) acc 68.7500 (66.0662) lr 1.8090e-03 eta 0:23:09
epoch [7/25] batch [90/250] time 0.290 (0.297) data 0.000 (0.009) loss 0.7847 (1.3433) acc 84.3750 (66.2153) lr 1.8090e-03 eta 0:23:05
epoch [7/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 2.0801 (1.3515) acc 50.0000 (66.1513) lr 1.8090e-03 eta 0:23:02
epoch [7/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1367 (1.3449) acc 68.7500 (66.0625) lr 1.8090e-03 eta 0:22:58
epoch [7/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.5234 (1.3447) acc 53.1250 (65.9226) lr 1.8090e-03 eta 0:22:55
epoch [7/25] batch [110/250] time 0.288 (0.296) data 0.000 (0.007) loss 1.1152 (1.3376) acc 75.0000 (66.1932) lr 1.8090e-03 eta 0:22:52
epoch [7/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0215 (1.3388) acc 75.0000 (66.3043) lr 1.8090e-03 eta 0:22:50
epoch [7/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.0078 (1.3368) acc 62.5000 (66.2500) lr 1.8090e-03 eta 0:22:47
epoch [7/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3145 (1.3282) acc 68.7500 (66.4000) lr 1.8090e-03 eta 0:22:44
epoch [7/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1670 (1.3278) acc 78.1250 (66.3221) lr 1.8090e-03 eta 0:22:42
epoch [7/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6680 (1.3286) acc 56.2500 (66.2269) lr 1.8090e-03 eta 0:22:39
epoch [7/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.2852 (1.3314) acc 59.3750 (66.4062) lr 1.8090e-03 eta 0:22:37
epoch [7/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 0.9634 (1.3185) acc 68.7500 (66.6164) lr 1.8090e-03 eta 0:22:34
epoch [7/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0996 (1.3267) acc 68.7500 (66.4792) lr 1.8090e-03 eta 0:22:32
epoch [7/25] batch [155/250] time 0.288 (0.294) data 0.000 (0.005) loss 0.7148 (1.3247) acc 81.2500 (66.5121) lr 1.8090e-03 eta 0:22:30
epoch [7/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1289 (1.3249) acc 68.7500 (66.5039) lr 1.8090e-03 eta 0:22:28
epoch [7/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9399 (1.3284) acc 75.0000 (66.4962) lr 1.8090e-03 eta 0:22:26
epoch [7/25] batch [170/250] time 0.290 (0.293) data 0.000 (0.005) loss 1.5332 (1.3364) acc 65.6250 (66.3787) lr 1.8090e-03 eta 0:22:24
epoch [7/25] batch [175/250] time 0.290 (0.293) data 0.001 (0.005) loss 1.3379 (1.3384) acc 71.8750 (66.3929) lr 1.8090e-03 eta 0:22:22
epoch [7/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3945 (1.3345) acc 68.7500 (66.4757) lr 1.8090e-03 eta 0:22:20
epoch [7/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5195 (1.3493) acc 75.0000 (66.2162) lr 1.8090e-03 eta 0:22:18
epoch [7/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9492 (1.3538) acc 78.1250 (66.1184) lr 1.8090e-03 eta 0:22:16
epoch [7/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6680 (1.3560) acc 59.3750 (66.1058) lr 1.8090e-03 eta 0:22:14
epoch [7/25] batch [200/250] time 0.292 (0.293) data 0.000 (0.004) loss 1.8789 (1.3573) acc 62.5000 (66.0625) lr 1.8090e-03 eta 0:22:12
epoch [7/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2754 (1.3562) acc 68.7500 (66.0061) lr 1.8090e-03 eta 0:22:10
epoch [7/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9146 (1.3497) acc 75.0000 (66.1310) lr 1.8090e-03 eta 0:22:08
epoch [7/25] batch [215/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.2266 (1.3510) acc 68.7500 (66.0610) lr 1.8090e-03 eta 0:22:06
epoch [7/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3721 (1.3466) acc 68.7500 (66.2074) lr 1.8090e-03 eta 0:22:05
epoch [7/25] batch [225/250] time 0.290 (0.292) data 0.001 (0.004) loss 1.9307 (1.3450) acc 56.2500 (66.3056) lr 1.8090e-03 eta 0:22:03
epoch [7/25] batch [230/250] time 0.292 (0.292) data 0.000 (0.004) loss 0.7041 (1.3455) acc 87.5000 (66.3587) lr 1.8090e-03 eta 0:22:01
epoch [7/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.004) loss 1.4268 (1.3438) acc 62.5000 (66.3298) lr 1.8090e-03 eta 0:21:59
epoch [7/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2734 (1.3433) acc 62.5000 (66.3932) lr 1.8090e-03 eta 0:21:57
epoch [7/25] batch [245/250] time 0.288 (0.292) data 0.000 (0.003) loss 1.0156 (1.3443) acc 68.7500 (66.3393) lr 1.8090e-03 eta 0:21:56
epoch [7/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3867 (1.3451) acc 68.7500 (66.2625) lr 1.7290e-03 eta 0:21:54
epoch [8/25] batch [5/250] time 0.289 (0.425) data 0.000 (0.136) loss 1.1797 (1.3803) acc 65.6250 (66.8750) lr 1.7290e-03 eta 0:31:50
epoch [8/25] batch [10/250] time 0.290 (0.357) data 0.001 (0.068) loss 0.9741 (1.2841) acc 75.0000 (69.0625) lr 1.7290e-03 eta 0:26:43
epoch [8/25] batch [15/250] time 0.288 (0.334) data 0.000 (0.046) loss 1.2646 (1.2589) acc 53.1250 (68.3333) lr 1.7290e-03 eta 0:24:59
epoch [8/25] batch [20/250] time 0.290 (0.323) data 0.000 (0.034) loss 1.4043 (1.2700) acc 68.7500 (67.6562) lr 1.7290e-03 eta 0:24:07
epoch [8/25] batch [25/250] time 0.289 (0.316) data 0.000 (0.028) loss 1.3105 (1.2350) acc 65.6250 (68.2500) lr 1.7290e-03 eta 0:23:35
epoch [8/25] batch [30/250] time 0.290 (0.312) data 0.000 (0.023) loss 1.5000 (1.2117) acc 59.3750 (68.9583) lr 1.7290e-03 eta 0:23:13
epoch [8/25] batch [35/250] time 0.289 (0.308) data 0.000 (0.020) loss 2.0840 (1.2349) acc 50.0000 (68.0357) lr 1.7290e-03 eta 0:22:57
epoch [8/25] batch [40/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.0352 (1.2518) acc 78.1250 (67.9688) lr 1.7290e-03 eta 0:22:45
epoch [8/25] batch [45/250] time 0.289 (0.304) data 0.000 (0.016) loss 0.7109 (1.2436) acc 84.3750 (68.2639) lr 1.7290e-03 eta 0:22:35
epoch [8/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.2471 (1.2459) acc 71.8750 (68.1875) lr 1.7290e-03 eta 0:22:26
epoch [8/25] batch [55/250] time 0.289 (0.301) data 0.000 (0.013) loss 1.3906 (1.2675) acc 68.7500 (67.7841) lr 1.7290e-03 eta 0:22:19
epoch [8/25] batch [60/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.3125 (1.2675) acc 62.5000 (67.7604) lr 1.7290e-03 eta 0:22:13
epoch [8/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.0811 (1.2736) acc 75.0000 (67.5481) lr 1.7290e-03 eta 0:22:08
epoch [8/25] batch [70/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.3672 (1.2816) acc 65.6250 (67.2321) lr 1.7290e-03 eta 0:22:03
epoch [8/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3750 (1.2906) acc 59.3750 (66.7917) lr 1.7290e-03 eta 0:21:59
epoch [8/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0420 (1.2812) acc 71.8750 (67.1094) lr 1.7290e-03 eta 0:21:55
epoch [8/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5391 (1.2829) acc 56.2500 (66.9853) lr 1.7290e-03 eta 0:21:51
epoch [8/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.7090 (1.2811) acc 59.3750 (67.2569) lr 1.7290e-03 eta 0:21:48
epoch [8/25] batch [95/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.3271 (1.2912) acc 65.6250 (67.0066) lr 1.7290e-03 eta 0:21:44
epoch [8/25] batch [100/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.4082 (1.3012) acc 59.3750 (66.6250) lr 1.7290e-03 eta 0:21:41
epoch [8/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9688 (1.3001) acc 71.8750 (66.4286) lr 1.7290e-03 eta 0:21:39
epoch [8/25] batch [110/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.1211 (1.2976) acc 65.6250 (66.2784) lr 1.7290e-03 eta 0:21:36
epoch [8/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6465 (1.2947) acc 59.3750 (66.5217) lr 1.7290e-03 eta 0:21:33
epoch [8/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4678 (1.2970) acc 65.6250 (66.5625) lr 1.7290e-03 eta 0:21:31
epoch [8/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1123 (1.2999) acc 81.2500 (66.6000) lr 1.7290e-03 eta 0:21:28
epoch [8/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.0986 (1.3028) acc 75.0000 (66.5144) lr 1.7290e-03 eta 0:21:26
epoch [8/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0479 (1.2981) acc 78.1250 (66.7130) lr 1.7290e-03 eta 0:21:24
epoch [8/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2490 (1.3000) acc 62.5000 (66.7411) lr 1.7290e-03 eta 0:21:21
epoch [8/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4229 (1.3141) acc 65.6250 (66.5302) lr 1.7290e-03 eta 0:21:19
epoch [8/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6768 (1.3212) acc 68.7500 (66.4792) lr 1.7290e-03 eta 0:21:17
epoch [8/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0176 (1.3191) acc 78.1250 (66.6129) lr 1.7290e-03 eta 0:21:15
epoch [8/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.8843 (1.3250) acc 75.0000 (66.3867) lr 1.7290e-03 eta 0:21:13
epoch [8/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6758 (1.3293) acc 56.2500 (66.4015) lr 1.7290e-03 eta 0:21:11
epoch [8/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3730 (1.3285) acc 65.6250 (66.3419) lr 1.7290e-03 eta 0:21:09
epoch [8/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4336 (1.3314) acc 59.3750 (66.2500) lr 1.7290e-03 eta 0:21:07
epoch [8/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2529 (1.3281) acc 78.1250 (66.5278) lr 1.7290e-03 eta 0:21:05
epoch [8/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6504 (1.3335) acc 62.5000 (66.4020) lr 1.7290e-03 eta 0:21:03
epoch [8/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2686 (1.3344) acc 75.0000 (66.3980) lr 1.7290e-03 eta 0:21:01
epoch [8/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8779 (1.3323) acc 75.0000 (66.3462) lr 1.7290e-03 eta 0:20:59
epoch [8/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9697 (1.3344) acc 84.3750 (66.4531) lr 1.7290e-03 eta 0:20:57
epoch [8/25] batch [205/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0430 (1.3388) acc 75.0000 (66.3567) lr 1.7290e-03 eta 0:20:56
epoch [8/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.9146 (1.3303) acc 81.2500 (66.5625) lr 1.7290e-03 eta 0:20:54
epoch [8/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3008 (1.3338) acc 68.7500 (66.4390) lr 1.7290e-03 eta 0:20:52
epoch [8/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2354 (1.3331) acc 65.6250 (66.4631) lr 1.7290e-03 eta 0:20:50
epoch [8/25] batch [225/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.0791 (1.3349) acc 71.8750 (66.4167) lr 1.7290e-03 eta 0:20:49
epoch [8/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3076 (1.3353) acc 75.0000 (66.4130) lr 1.7290e-03 eta 0:20:47
epoch [8/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.7734 (1.3389) acc 56.2500 (66.2899) lr 1.7290e-03 eta 0:20:45
epoch [8/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4609 (1.3340) acc 53.1250 (66.3411) lr 1.7290e-03 eta 0:20:43
epoch [8/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2861 (1.3351) acc 78.1250 (66.3776) lr 1.7290e-03 eta 0:20:42
epoch [8/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.8652 (1.3381) acc 65.6250 (66.3375) lr 1.6374e-03 eta 0:20:40
epoch [9/25] batch [5/250] time 0.289 (0.451) data 0.000 (0.161) loss 1.1895 (1.2479) acc 75.0000 (71.2500) lr 1.6374e-03 eta 0:31:54
epoch [9/25] batch [10/250] time 0.290 (0.370) data 0.000 (0.081) loss 0.9214 (1.2521) acc 75.0000 (69.0625) lr 1.6374e-03 eta 0:26:09
epoch [9/25] batch [15/250] time 0.290 (0.343) data 0.000 (0.054) loss 1.4932 (1.2310) acc 56.2500 (68.1250) lr 1.6374e-03 eta 0:24:13
epoch [9/25] batch [20/250] time 0.289 (0.330) data 0.000 (0.041) loss 1.3555 (1.3002) acc 71.8750 (67.8125) lr 1.6374e-03 eta 0:23:15
epoch [9/25] batch [25/250] time 0.289 (0.322) data 0.000 (0.032) loss 1.6475 (1.3209) acc 68.7500 (67.6250) lr 1.6374e-03 eta 0:22:39
epoch [9/25] batch [30/250] time 0.289 (0.316) data 0.000 (0.027) loss 1.1650 (1.2954) acc 68.7500 (68.3333) lr 1.6374e-03 eta 0:22:14
epoch [9/25] batch [35/250] time 0.290 (0.312) data 0.001 (0.023) loss 1.5840 (1.2947) acc 62.5000 (68.3036) lr 1.6374e-03 eta 0:21:56
epoch [9/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.020) loss 1.6992 (1.2905) acc 59.3750 (68.1250) lr 1.6374e-03 eta 0:21:43
epoch [9/25] batch [45/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.3906 (1.3240) acc 56.2500 (66.8750) lr 1.6374e-03 eta 0:21:32
epoch [9/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.016) loss 1.0039 (1.3321) acc 68.7500 (66.8750) lr 1.6374e-03 eta 0:21:23
epoch [9/25] batch [55/250] time 0.290 (0.304) data 0.000 (0.015) loss 1.5303 (1.3348) acc 62.5000 (66.6477) lr 1.6374e-03 eta 0:21:15
epoch [9/25] batch [60/250] time 0.289 (0.303) data 0.001 (0.014) loss 1.2197 (1.3340) acc 81.2500 (66.7708) lr 1.6374e-03 eta 0:21:08
epoch [9/25] batch [65/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.1221 (1.3501) acc 68.7500 (66.3462) lr 1.6374e-03 eta 0:21:03
epoch [9/25] batch [70/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.5811 (1.3444) acc 59.3750 (66.4286) lr 1.6374e-03 eta 0:20:57
epoch [9/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3906 (1.3398) acc 71.8750 (66.9167) lr 1.6374e-03 eta 0:20:53
epoch [9/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4434 (1.3346) acc 68.7500 (67.1484) lr 1.6374e-03 eta 0:20:48
epoch [9/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.6758 (1.3365) acc 56.2500 (66.8382) lr 1.6374e-03 eta 0:20:44
epoch [9/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.3037 (1.3330) acc 59.3750 (66.8403) lr 1.6374e-03 eta 0:20:40
epoch [9/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.8887 (1.3422) acc 50.0000 (66.6447) lr 1.6374e-03 eta 0:20:37
epoch [9/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.8760 (1.3496) acc 56.2500 (66.4688) lr 1.6374e-03 eta 0:20:34
epoch [9/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.6475 (1.3517) acc 65.6250 (66.4583) lr 1.6374e-03 eta 0:20:30
epoch [9/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2998 (1.3577) acc 53.1250 (65.9943) lr 1.6374e-03 eta 0:20:27
epoch [9/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3301 (1.3517) acc 68.7500 (66.1413) lr 1.6374e-03 eta 0:20:25
epoch [9/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.6250 (1.3556) acc 65.6250 (66.1719) lr 1.6374e-03 eta 0:20:22
epoch [9/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3291 (1.3505) acc 65.6250 (66.3500) lr 1.6374e-03 eta 0:20:19
epoch [9/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.7227 (1.3533) acc 65.6250 (66.4904) lr 1.6374e-03 eta 0:20:17
epoch [9/25] batch [135/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.5508 (1.3543) acc 65.6250 (66.5046) lr 1.6374e-03 eta 0:20:15
epoch [9/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3145 (1.3537) acc 71.8750 (66.6295) lr 1.6374e-03 eta 0:20:12
epoch [9/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1768 (1.3559) acc 75.0000 (66.6379) lr 1.6374e-03 eta 0:20:10
epoch [9/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.1680 (1.3537) acc 75.0000 (66.5833) lr 1.6374e-03 eta 0:20:08
epoch [9/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1309 (1.3569) acc 71.8750 (66.5524) lr 1.6374e-03 eta 0:20:05
epoch [9/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.7334 (1.3597) acc 56.2500 (66.4258) lr 1.6374e-03 eta 0:20:03
epoch [9/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2295 (1.3627) acc 65.6250 (66.3826) lr 1.6374e-03 eta 0:20:01
epoch [9/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1836 (1.3600) acc 71.8750 (66.4890) lr 1.6374e-03 eta 0:19:59
epoch [9/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1133 (1.3576) acc 68.7500 (66.5179) lr 1.6374e-03 eta 0:19:57
epoch [9/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2344 (1.3579) acc 65.6250 (66.3889) lr 1.6374e-03 eta 0:19:55
epoch [9/25] batch [185/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.5566 (1.3574) acc 62.5000 (66.3514) lr 1.6374e-03 eta 0:19:53
epoch [9/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4707 (1.3585) acc 59.3750 (66.2829) lr 1.6374e-03 eta 0:19:51
epoch [9/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9507 (1.3494) acc 78.1250 (66.4423) lr 1.6374e-03 eta 0:19:49
epoch [9/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2061 (1.3451) acc 68.7500 (66.4844) lr 1.6374e-03 eta 0:19:48
epoch [9/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3799 (1.3522) acc 56.2500 (66.3110) lr 1.6374e-03 eta 0:19:46
epoch [9/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1973 (1.3523) acc 59.3750 (66.2500) lr 1.6374e-03 eta 0:19:44
epoch [9/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7148 (1.3541) acc 62.5000 (66.2209) lr 1.6374e-03 eta 0:19:42
epoch [9/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1270 (1.3587) acc 62.5000 (66.0653) lr 1.6374e-03 eta 0:19:40
epoch [9/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6250 (1.3625) acc 50.0000 (65.9028) lr 1.6374e-03 eta 0:19:38
epoch [9/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2812 (1.3583) acc 62.5000 (65.9103) lr 1.6374e-03 eta 0:19:37
epoch [9/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.1797 (1.3579) acc 59.3750 (65.9309) lr 1.6374e-03 eta 0:19:35
epoch [9/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9312 (1.3591) acc 75.0000 (65.9505) lr 1.6374e-03 eta 0:19:33
epoch [9/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3252 (1.3591) acc 71.8750 (65.9694) lr 1.6374e-03 eta 0:19:31
epoch [9/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2832 (1.3582) acc 68.7500 (66.0125) lr 1.5358e-03 eta 0:19:30
epoch [10/25] batch [5/250] time 0.289 (0.431) data 0.000 (0.141) loss 1.2842 (1.1063) acc 68.7500 (75.0000) lr 1.5358e-03 eta 0:28:41
epoch [10/25] batch [10/250] time 0.289 (0.360) data 0.000 (0.071) loss 1.1074 (1.2142) acc 68.7500 (71.5625) lr 1.5358e-03 eta 0:23:56
epoch [10/25] batch [15/250] time 0.289 (0.336) data 0.000 (0.047) loss 1.3008 (1.2146) acc 65.6250 (71.6667) lr 1.5358e-03 eta 0:22:20
epoch [10/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.036) loss 1.1953 (1.2152) acc 68.7500 (71.5625) lr 1.5358e-03 eta 0:21:32
epoch [10/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.028) loss 1.4131 (1.2224) acc 71.8750 (71.2500) lr 1.5358e-03 eta 0:21:02
epoch [10/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.1240 (1.2323) acc 75.0000 (71.3542) lr 1.5358e-03 eta 0:20:41
epoch [10/25] batch [35/250] time 0.289 (0.309) data 0.000 (0.020) loss 1.3350 (1.2087) acc 71.8750 (71.7857) lr 1.5358e-03 eta 0:20:26
epoch [10/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.3076 (1.2227) acc 62.5000 (70.9375) lr 1.5358e-03 eta 0:20:15
epoch [10/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.0059 (1.2324) acc 71.8750 (70.6944) lr 1.5358e-03 eta 0:20:06
epoch [10/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.8066 (1.2217) acc 62.5000 (71.0625) lr 1.5358e-03 eta 0:19:58
epoch [10/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.0703 (1.2311) acc 71.8750 (70.7386) lr 1.5358e-03 eta 0:19:51
epoch [10/25] batch [60/250] time 0.288 (0.301) data 0.000 (0.012) loss 1.6494 (1.2372) acc 59.3750 (70.5729) lr 1.5358e-03 eta 0:19:45
epoch [10/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.7119 (1.2437) acc 84.3750 (70.5769) lr 1.5358e-03 eta 0:19:40
epoch [10/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 2.1172 (1.2505) acc 62.5000 (70.6250) lr 1.5358e-03 eta 0:19:35
epoch [10/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.010) loss 1.1631 (1.2534) acc 68.7500 (70.1250) lr 1.5358e-03 eta 0:19:31
epoch [10/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1084 (1.2455) acc 68.7500 (70.1953) lr 1.5358e-03 eta 0:19:27
epoch [10/25] batch [85/250] time 0.290 (0.297) data 0.000 (0.009) loss 1.1260 (1.2434) acc 62.5000 (69.9265) lr 1.5358e-03 eta 0:19:24
epoch [10/25] batch [90/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.3291 (1.2479) acc 75.0000 (69.7569) lr 1.5358e-03 eta 0:19:21
epoch [10/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3252 (1.2465) acc 65.6250 (69.5395) lr 1.5358e-03 eta 0:19:18
epoch [10/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.7803 (1.2374) acc 75.0000 (69.7500) lr 1.5358e-03 eta 0:19:15
epoch [10/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1895 (1.2370) acc 71.8750 (69.7917) lr 1.5358e-03 eta 0:19:12
epoch [10/25] batch [110/250] time 0.290 (0.296) data 0.000 (0.007) loss 2.1445 (1.2479) acc 50.0000 (69.5170) lr 1.5358e-03 eta 0:19:09
epoch [10/25] batch [115/250] time 0.288 (0.295) data 0.000 (0.006) loss 1.6074 (1.2523) acc 59.3750 (69.3478) lr 1.5358e-03 eta 0:19:07
epoch [10/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4521 (1.2606) acc 68.7500 (69.1406) lr 1.5358e-03 eta 0:19:04
epoch [10/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5684 (1.2678) acc 62.5000 (68.8500) lr 1.5358e-03 eta 0:19:02
epoch [10/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5703 (1.2803) acc 65.6250 (68.5096) lr 1.5358e-03 eta 0:18:59
epoch [10/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3711 (1.2908) acc 59.3750 (68.1713) lr 1.5358e-03 eta 0:18:57
epoch [10/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0986 (1.2946) acc 68.7500 (68.2143) lr 1.5358e-03 eta 0:18:55
epoch [10/25] batch [145/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.9775 (1.3014) acc 46.8750 (67.9741) lr 1.5358e-03 eta 0:18:53
epoch [10/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5811 (1.3090) acc 59.3750 (67.7917) lr 1.5358e-03 eta 0:18:51
epoch [10/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5547 (1.3066) acc 59.3750 (67.8629) lr 1.5358e-03 eta 0:18:49
epoch [10/25] batch [160/250] time 0.291 (0.294) data 0.003 (0.005) loss 1.2695 (1.3110) acc 65.6250 (67.7734) lr 1.5358e-03 eta 0:18:47
epoch [10/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3428 (1.3114) acc 65.6250 (67.6515) lr 1.5358e-03 eta 0:18:45
epoch [10/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3066 (1.3154) acc 59.3750 (67.5000) lr 1.5358e-03 eta 0:18:43
epoch [10/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6270 (1.3111) acc 78.1250 (67.5714) lr 1.5358e-03 eta 0:18:41
epoch [10/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3408 (1.3152) acc 65.6250 (67.4653) lr 1.5358e-03 eta 0:18:39
epoch [10/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5078 (1.3122) acc 46.8750 (67.3649) lr 1.5358e-03 eta 0:18:37
epoch [10/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9951 (1.3114) acc 71.8750 (67.2697) lr 1.5358e-03 eta 0:18:35
epoch [10/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9800 (1.3035) acc 68.7500 (67.3558) lr 1.5358e-03 eta 0:18:33
epoch [10/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9927 (1.3053) acc 78.1250 (67.3594) lr 1.5358e-03 eta 0:18:32
epoch [10/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5195 (1.3070) acc 65.6250 (67.3323) lr 1.5358e-03 eta 0:18:30
epoch [10/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9214 (1.3088) acc 78.1250 (67.2917) lr 1.5358e-03 eta 0:18:28
epoch [10/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.8950 (1.3065) acc 84.3750 (67.4273) lr 1.5358e-03 eta 0:18:26
epoch [10/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1436 (1.3021) acc 78.1250 (67.5426) lr 1.5358e-03 eta 0:18:25
epoch [10/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0156 (1.2998) acc 71.8750 (67.5972) lr 1.5358e-03 eta 0:18:23
epoch [10/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4922 (1.3046) acc 56.2500 (67.4321) lr 1.5358e-03 eta 0:18:21
epoch [10/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.003) loss 1.0527 (1.3019) acc 68.7500 (67.4734) lr 1.5358e-03 eta 0:18:19
epoch [10/25] batch [240/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.7939 (1.3030) acc 53.1250 (67.4479) lr 1.5358e-03 eta 0:18:18
epoch [10/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.4131 (1.3074) acc 62.5000 (67.2194) lr 1.5358e-03 eta 0:18:16
epoch [10/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2363 (1.3092) acc 62.5000 (67.1625) lr 1.4258e-03 eta 0:18:15
epoch [11/25] batch [5/250] time 0.289 (0.434) data 0.000 (0.145) loss 1.7266 (1.4984) acc 56.2500 (62.5000) lr 1.4258e-03 eta 0:27:05
epoch [11/25] batch [10/250] time 0.288 (0.361) data 0.000 (0.073) loss 1.4922 (1.5408) acc 65.6250 (60.9375) lr 1.4258e-03 eta 0:22:31
epoch [11/25] batch [15/250] time 0.288 (0.337) data 0.000 (0.049) loss 1.5059 (1.4437) acc 65.6250 (64.1667) lr 1.4258e-03 eta 0:20:58
epoch [11/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.036) loss 1.2920 (1.3402) acc 62.5000 (66.2500) lr 1.4258e-03 eta 0:20:11
epoch [11/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.5947 (1.3351) acc 62.5000 (66.7500) lr 1.4258e-03 eta 0:19:43
epoch [11/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.9634 (1.3412) acc 71.8750 (67.2917) lr 1.4258e-03 eta 0:19:24
epoch [11/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.6240 (1.3529) acc 65.6250 (66.9643) lr 1.4258e-03 eta 0:19:10
epoch [11/25] batch [40/250] time 0.290 (0.307) data 0.000 (0.018) loss 0.8130 (1.3423) acc 71.8750 (66.7969) lr 1.4258e-03 eta 0:18:59
epoch [11/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.8701 (1.3325) acc 75.0000 (66.9444) lr 1.4258e-03 eta 0:18:50
epoch [11/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.015) loss 1.6357 (1.3228) acc 53.1250 (66.9375) lr 1.4258e-03 eta 0:18:42
epoch [11/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.0967 (1.3142) acc 78.1250 (67.2727) lr 1.4258e-03 eta 0:18:36
epoch [11/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.4424 (1.3333) acc 59.3750 (67.0833) lr 1.4258e-03 eta 0:18:31
epoch [11/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2197 (1.3440) acc 68.7500 (66.9231) lr 1.4258e-03 eta 0:18:26
epoch [11/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.011) loss 1.5068 (1.3538) acc 62.5000 (66.9196) lr 1.4258e-03 eta 0:18:22
epoch [11/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4893 (1.3643) acc 56.2500 (66.5417) lr 1.4258e-03 eta 0:18:18
epoch [11/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3623 (1.3702) acc 75.0000 (66.5625) lr 1.4258e-03 eta 0:18:14
epoch [11/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7158 (1.3554) acc 84.3750 (67.1324) lr 1.4258e-03 eta 0:18:11
epoch [11/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2852 (1.3523) acc 56.2500 (66.9792) lr 1.4258e-03 eta 0:18:07
epoch [11/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.7925 (1.3411) acc 75.0000 (67.3026) lr 1.4258e-03 eta 0:18:04
epoch [11/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.0410 (1.3507) acc 75.0000 (66.7812) lr 1.4258e-03 eta 0:18:01
epoch [11/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1836 (1.3389) acc 71.8750 (66.9940) lr 1.4258e-03 eta 0:17:59
epoch [11/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5322 (1.3375) acc 59.3750 (66.9602) lr 1.4258e-03 eta 0:17:56
epoch [11/25] batch [115/250] time 0.290 (0.295) data 0.000 (0.007) loss 1.3467 (1.3342) acc 71.8750 (66.9837) lr 1.4258e-03 eta 0:17:53
epoch [11/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7051 (1.3306) acc 59.3750 (67.1615) lr 1.4258e-03 eta 0:17:51
epoch [11/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4824 (1.3290) acc 71.8750 (67.3500) lr 1.4258e-03 eta 0:17:49
epoch [11/25] batch [130/250] time 0.288 (0.295) data 0.000 (0.006) loss 1.6553 (1.3326) acc 62.5000 (67.3077) lr 1.4258e-03 eta 0:17:46
epoch [11/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.5957 (1.3318) acc 68.7500 (67.3611) lr 1.4258e-03 eta 0:17:44
epoch [11/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8394 (1.3273) acc 84.3750 (67.5893) lr 1.4258e-03 eta 0:17:42
epoch [11/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4844 (1.3306) acc 62.5000 (67.3707) lr 1.4258e-03 eta 0:17:40
epoch [11/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1865 (1.3293) acc 62.5000 (67.3750) lr 1.4258e-03 eta 0:17:38
epoch [11/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9551 (1.3217) acc 78.1250 (67.5605) lr 1.4258e-03 eta 0:17:36
epoch [11/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9697 (1.3201) acc 81.2500 (67.6562) lr 1.4258e-03 eta 0:17:34
epoch [11/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.8755 (1.3190) acc 75.0000 (67.5379) lr 1.4258e-03 eta 0:17:32
epoch [11/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3604 (1.3133) acc 65.6250 (67.5735) lr 1.4258e-03 eta 0:17:30
epoch [11/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6875 (1.3132) acc 65.6250 (67.5714) lr 1.4258e-03 eta 0:17:28
epoch [11/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1475 (1.3123) acc 71.8750 (67.5868) lr 1.4258e-03 eta 0:17:26
epoch [11/25] batch [185/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4736 (1.3090) acc 62.5000 (67.6520) lr 1.4258e-03 eta 0:17:24
epoch [11/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0762 (1.3111) acc 62.5000 (67.5658) lr 1.4258e-03 eta 0:17:22
epoch [11/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1689 (1.3108) acc 71.8750 (67.5801) lr 1.4258e-03 eta 0:17:21
epoch [11/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7461 (1.3108) acc 53.1250 (67.6406) lr 1.4258e-03 eta 0:17:19
epoch [11/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9189 (1.3157) acc 78.1250 (67.5305) lr 1.4258e-03 eta 0:17:17
epoch [11/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3262 (1.3150) acc 68.7500 (67.5149) lr 1.4258e-03 eta 0:17:15
epoch [11/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0986 (1.3119) acc 68.7500 (67.5291) lr 1.4258e-03 eta 0:17:13
epoch [11/25] batch [220/250] time 0.290 (0.292) data 0.000 (0.004) loss 0.9580 (1.3092) acc 65.6250 (67.4716) lr 1.4258e-03 eta 0:17:12
epoch [11/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5703 (1.3078) acc 62.5000 (67.5139) lr 1.4258e-03 eta 0:17:10
epoch [11/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.9956 (1.3081) acc 81.2500 (67.5951) lr 1.4258e-03 eta 0:17:08
epoch [11/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.003) loss 1.3506 (1.3074) acc 68.7500 (67.6463) lr 1.4258e-03 eta 0:17:07
epoch [11/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6475 (1.3075) acc 59.3750 (67.6562) lr 1.4258e-03 eta 0:17:05
epoch [11/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6143 (1.3075) acc 56.2500 (67.6148) lr 1.4258e-03 eta 0:17:03
epoch [11/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4844 (1.3107) acc 59.3750 (67.6000) lr 1.3090e-03 eta 0:17:02
epoch [12/25] batch [5/250] time 0.288 (0.423) data 0.000 (0.133) loss 0.7456 (1.0555) acc 71.8750 (71.2500) lr 1.3090e-03 eta 0:24:36
epoch [12/25] batch [10/250] time 0.289 (0.356) data 0.000 (0.067) loss 1.6104 (1.1692) acc 59.3750 (68.4375) lr 1.3090e-03 eta 0:20:41
epoch [12/25] batch [15/250] time 0.289 (0.333) data 0.000 (0.045) loss 1.2217 (1.1333) acc 78.1250 (70.4167) lr 1.3090e-03 eta 0:19:22
epoch [12/25] batch [20/250] time 0.289 (0.322) data 0.000 (0.033) loss 1.5166 (1.1437) acc 59.3750 (70.7812) lr 1.3090e-03 eta 0:18:42
epoch [12/25] batch [25/250] time 0.289 (0.316) data 0.000 (0.027) loss 0.8950 (1.1721) acc 84.3750 (71.5000) lr 1.3090e-03 eta 0:18:17
epoch [12/25] batch [30/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.6641 (1.2068) acc 65.6250 (70.9375) lr 1.3090e-03 eta 0:18:00
epoch [12/25] batch [35/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.5508 (1.2298) acc 65.6250 (70.3571) lr 1.3090e-03 eta 0:17:47
epoch [12/25] batch [40/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.8037 (1.2385) acc 87.5000 (70.0781) lr 1.3090e-03 eta 0:17:38
epoch [12/25] batch [45/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.6543 (1.2381) acc 56.2500 (69.9306) lr 1.3090e-03 eta 0:17:30
epoch [12/25] batch [50/250] time 0.289 (0.302) data 0.000 (0.014) loss 1.0664 (1.2658) acc 75.0000 (69.7500) lr 1.3090e-03 eta 0:17:23
epoch [12/25] batch [55/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.8794 (1.2567) acc 71.8750 (69.8295) lr 1.3090e-03 eta 0:17:17
epoch [12/25] batch [60/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2383 (1.2403) acc 75.0000 (69.9479) lr 1.3090e-03 eta 0:17:12
epoch [12/25] batch [65/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.2334 (1.2532) acc 65.6250 (69.5673) lr 1.3090e-03 eta 0:17:08
epoch [12/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4854 (1.2597) acc 68.7500 (69.6429) lr 1.3090e-03 eta 0:17:04
epoch [12/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7876 (1.2575) acc 78.1250 (69.3750) lr 1.3090e-03 eta 0:17:00
epoch [12/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6699 (1.2627) acc 65.6250 (69.3750) lr 1.3090e-03 eta 0:16:57
epoch [12/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.7832 (1.2708) acc 56.2500 (69.3015) lr 1.3090e-03 eta 0:16:54
epoch [12/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.6680 (1.2707) acc 62.5000 (69.2361) lr 1.3090e-03 eta 0:16:51
epoch [12/25] batch [95/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.4170 (1.2784) acc 53.1250 (68.8816) lr 1.3090e-03 eta 0:16:48
epoch [12/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.7964 (1.2620) acc 75.0000 (69.2812) lr 1.3090e-03 eta 0:16:45
epoch [12/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1133 (1.2667) acc 71.8750 (69.0774) lr 1.3090e-03 eta 0:16:43
epoch [12/25] batch [110/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8789 (1.2665) acc 81.2500 (69.1193) lr 1.3090e-03 eta 0:16:40
epoch [12/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3916 (1.2650) acc 62.5000 (69.3207) lr 1.3090e-03 eta 0:16:38
epoch [12/25] batch [120/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9653 (1.2536) acc 71.8750 (69.5573) lr 1.3090e-03 eta 0:16:36
epoch [12/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9370 (1.2535) acc 75.0000 (69.5500) lr 1.3090e-03 eta 0:16:34
epoch [12/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2207 (1.2513) acc 65.6250 (69.4952) lr 1.3090e-03 eta 0:16:31
epoch [12/25] batch [135/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.5752 (1.2496) acc 68.7500 (69.4444) lr 1.3090e-03 eta 0:16:29
epoch [12/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1758 (1.2609) acc 65.6250 (69.1518) lr 1.3090e-03 eta 0:16:27
epoch [12/25] batch [145/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.0732 (1.2549) acc 65.6250 (69.2888) lr 1.3090e-03 eta 0:16:25
epoch [12/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8916 (1.2558) acc 81.2500 (69.1875) lr 1.3090e-03 eta 0:16:23
epoch [12/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0156 (1.2571) acc 71.8750 (69.1532) lr 1.3090e-03 eta 0:16:21
epoch [12/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6123 (1.2655) acc 75.0000 (68.9453) lr 1.3090e-03 eta 0:16:19
epoch [12/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7842 (1.2665) acc 81.2500 (68.9015) lr 1.3090e-03 eta 0:16:18
epoch [12/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7900 (1.2688) acc 53.1250 (68.8235) lr 1.3090e-03 eta 0:16:16
epoch [12/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8154 (1.2649) acc 71.8750 (68.7143) lr 1.3090e-03 eta 0:16:14
epoch [12/25] batch [180/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.2402 (1.2701) acc 65.6250 (68.6285) lr 1.3090e-03 eta 0:16:12
epoch [12/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3906 (1.2728) acc 71.8750 (68.5811) lr 1.3090e-03 eta 0:16:10
epoch [12/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0234 (1.2679) acc 78.1250 (68.6842) lr 1.3090e-03 eta 0:16:08
epoch [12/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9087 (1.2728) acc 71.8750 (68.5737) lr 1.3090e-03 eta 0:16:07
epoch [12/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2373 (1.2719) acc 71.8750 (68.5781) lr 1.3090e-03 eta 0:16:05
epoch [12/25] batch [205/250] time 0.290 (0.292) data 0.000 (0.004) loss 1.5361 (1.2732) acc 71.8750 (68.6433) lr 1.3090e-03 eta 0:16:03
epoch [12/25] batch [210/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.8809 (1.2805) acc 56.2500 (68.5714) lr 1.3090e-03 eta 0:16:02
epoch [12/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3555 (1.2821) acc 68.7500 (68.5465) lr 1.3090e-03 eta 0:16:00
epoch [12/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.8174 (1.2847) acc 62.5000 (68.5511) lr 1.3090e-03 eta 0:15:58
epoch [12/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2227 (1.2894) acc 62.5000 (68.4306) lr 1.3090e-03 eta 0:15:56
epoch [12/25] batch [230/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.5391 (1.2918) acc 65.6250 (68.3832) lr 1.3090e-03 eta 0:15:55
epoch [12/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3584 (1.2920) acc 68.7500 (68.3378) lr 1.3090e-03 eta 0:15:53
epoch [12/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4775 (1.2907) acc 59.3750 (68.3073) lr 1.3090e-03 eta 0:15:51
epoch [12/25] batch [245/250] time 0.290 (0.292) data 0.002 (0.003) loss 0.9956 (1.2911) acc 75.0000 (68.2398) lr 1.3090e-03 eta 0:15:50
epoch [12/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5527 (1.2899) acc 68.7500 (68.3375) lr 1.1874e-03 eta 0:15:48
epoch [13/25] batch [5/250] time 0.288 (0.425) data 0.000 (0.137) loss 1.7432 (1.5955) acc 78.1250 (70.0000) lr 1.1874e-03 eta 0:22:59
epoch [13/25] batch [10/250] time 0.289 (0.357) data 0.000 (0.069) loss 0.9727 (1.3500) acc 78.1250 (71.5625) lr 1.1874e-03 eta 0:19:16
epoch [13/25] batch [15/250] time 0.289 (0.334) data 0.001 (0.046) loss 1.2109 (1.3456) acc 68.7500 (69.3750) lr 1.1874e-03 eta 0:18:01
epoch [13/25] batch [20/250] time 0.289 (0.323) data 0.000 (0.034) loss 1.5439 (1.2944) acc 59.3750 (69.3750) lr 1.1874e-03 eta 0:17:23
epoch [13/25] batch [25/250] time 0.288 (0.316) data 0.000 (0.028) loss 1.1338 (1.2627) acc 78.1250 (69.8750) lr 1.1874e-03 eta 0:17:00
epoch [13/25] batch [30/250] time 0.290 (0.312) data 0.001 (0.023) loss 1.9277 (1.2560) acc 53.1250 (69.7917) lr 1.1874e-03 eta 0:16:44
epoch [13/25] batch [35/250] time 0.289 (0.309) data 0.001 (0.020) loss 1.3828 (1.2429) acc 68.7500 (69.2857) lr 1.1874e-03 eta 0:16:32
epoch [13/25] batch [40/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.8716 (1.2189) acc 81.2500 (69.6094) lr 1.1874e-03 eta 0:16:22
epoch [13/25] batch [45/250] time 0.289 (0.304) data 0.000 (0.016) loss 1.2764 (1.2304) acc 65.6250 (69.1667) lr 1.1874e-03 eta 0:16:15
epoch [13/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 0.8770 (1.2157) acc 65.6250 (69.6250) lr 1.1874e-03 eta 0:16:08
epoch [13/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.4424 (1.2318) acc 59.3750 (69.0909) lr 1.1874e-03 eta 0:16:03
epoch [13/25] batch [60/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.1924 (1.2294) acc 71.8750 (68.8542) lr 1.1874e-03 eta 0:15:58
epoch [13/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.1211 (1.2357) acc 71.8750 (68.7500) lr 1.1874e-03 eta 0:15:54
epoch [13/25] batch [70/250] time 0.290 (0.299) data 0.001 (0.010) loss 1.0332 (1.2397) acc 71.8750 (68.7054) lr 1.1874e-03 eta 0:15:50
epoch [13/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.4092 (1.2592) acc 68.7500 (68.2917) lr 1.1874e-03 eta 0:15:46
epoch [13/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7759 (1.2488) acc 78.1250 (68.5156) lr 1.1874e-03 eta 0:15:43
epoch [13/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2119 (1.2563) acc 78.1250 (68.5662) lr 1.1874e-03 eta 0:15:40
epoch [13/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2295 (1.2619) acc 75.0000 (68.6111) lr 1.1874e-03 eta 0:15:37
epoch [13/25] batch [95/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.5049 (1.2604) acc 71.8750 (68.6184) lr 1.1874e-03 eta 0:15:34
epoch [13/25] batch [100/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0469 (1.2512) acc 78.1250 (68.8438) lr 1.1874e-03 eta 0:15:32
epoch [13/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7080 (1.2610) acc 62.5000 (68.6607) lr 1.1874e-03 eta 0:15:29
epoch [13/25] batch [110/250] time 0.291 (0.295) data 0.001 (0.007) loss 1.1006 (1.2567) acc 71.8750 (68.6932) lr 1.1874e-03 eta 0:15:27
epoch [13/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5674 (1.2637) acc 62.5000 (68.6141) lr 1.1874e-03 eta 0:15:25
epoch [13/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0547 (1.2605) acc 68.7500 (68.6979) lr 1.1874e-03 eta 0:15:23
epoch [13/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1182 (1.2652) acc 62.5000 (68.4750) lr 1.1874e-03 eta 0:15:20
epoch [13/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3799 (1.2720) acc 68.7500 (68.4615) lr 1.1874e-03 eta 0:15:18
epoch [13/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2822 (1.2761) acc 68.7500 (68.4954) lr 1.1874e-03 eta 0:15:16
epoch [13/25] batch [140/250] time 0.292 (0.294) data 0.000 (0.005) loss 1.9102 (1.2811) acc 59.3750 (68.4152) lr 1.1874e-03 eta 0:15:14
epoch [13/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4629 (1.2855) acc 59.3750 (68.3621) lr 1.1874e-03 eta 0:15:12
epoch [13/25] batch [150/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.6475 (1.2865) acc 56.2500 (68.3125) lr 1.1874e-03 eta 0:15:10
epoch [13/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9434 (1.2875) acc 62.5000 (68.2863) lr 1.1874e-03 eta 0:15:08
epoch [13/25] batch [160/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.1016 (1.2849) acc 68.7500 (68.3594) lr 1.1874e-03 eta 0:15:06
epoch [13/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5391 (1.2864) acc 75.0000 (68.4280) lr 1.1874e-03 eta 0:15:04
epoch [13/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1572 (1.2807) acc 65.6250 (68.5478) lr 1.1874e-03 eta 0:15:03
epoch [13/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2051 (1.2746) acc 68.7500 (68.6964) lr 1.1874e-03 eta 0:15:01
epoch [13/25] batch [180/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5664 (1.2768) acc 65.6250 (68.6458) lr 1.1874e-03 eta 0:14:59
epoch [13/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9375 (1.2773) acc 78.1250 (68.7331) lr 1.1874e-03 eta 0:14:57
epoch [13/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8203 (1.2823) acc 53.1250 (68.4868) lr 1.1874e-03 eta 0:14:55
epoch [13/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0703 (1.2807) acc 75.0000 (68.6058) lr 1.1874e-03 eta 0:14:54
epoch [13/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0215 (1.2808) acc 68.7500 (68.5625) lr 1.1874e-03 eta 0:14:52
epoch [13/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3535 (1.2787) acc 75.0000 (68.6890) lr 1.1874e-03 eta 0:14:50
epoch [13/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2188 (1.2820) acc 65.6250 (68.5268) lr 1.1874e-03 eta 0:14:49
epoch [13/25] batch [215/250] time 0.290 (0.292) data 0.000 (0.004) loss 1.2021 (1.2849) acc 65.6250 (68.4448) lr 1.1874e-03 eta 0:14:47
epoch [13/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6494 (1.2845) acc 53.1250 (68.4801) lr 1.1874e-03 eta 0:14:45
epoch [13/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.9478 (1.2833) acc 78.1250 (68.4722) lr 1.1874e-03 eta 0:14:43
epoch [13/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2256 (1.2814) acc 75.0000 (68.6005) lr 1.1874e-03 eta 0:14:42
epoch [13/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 0.9492 (1.2813) acc 78.1250 (68.6170) lr 1.1874e-03 eta 0:14:40
epoch [13/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3818 (1.2820) acc 71.8750 (68.5547) lr 1.1874e-03 eta 0:14:39
epoch [13/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.8052 (1.2768) acc 78.1250 (68.6607) lr 1.1874e-03 eta 0:14:37
epoch [13/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3438 (1.2750) acc 68.7500 (68.7750) lr 1.0628e-03 eta 0:14:35
epoch [14/25] batch [5/250] time 0.289 (0.425) data 0.000 (0.136) loss 1.0020 (1.1418) acc 78.1250 (73.1250) lr 1.0628e-03 eta 0:21:12
epoch [14/25] batch [10/250] time 0.289 (0.357) data 0.000 (0.068) loss 0.8760 (1.1757) acc 71.8750 (70.9375) lr 1.0628e-03 eta 0:17:46
epoch [14/25] batch [15/250] time 0.289 (0.334) data 0.000 (0.046) loss 1.0244 (1.1477) acc 71.8750 (71.4583) lr 1.0628e-03 eta 0:16:37
epoch [14/25] batch [20/250] time 0.289 (0.323) data 0.000 (0.034) loss 0.9932 (1.1450) acc 71.8750 (71.7188) lr 1.0628e-03 eta 0:16:02
epoch [14/25] batch [25/250] time 0.289 (0.316) data 0.000 (0.027) loss 2.2930 (1.1947) acc 50.0000 (70.3750) lr 1.0628e-03 eta 0:15:40
epoch [14/25] batch [30/250] time 0.288 (0.312) data 0.000 (0.023) loss 1.3682 (1.2336) acc 56.2500 (69.1667) lr 1.0628e-03 eta 0:15:25
epoch [14/25] batch [35/250] time 0.289 (0.308) data 0.000 (0.020) loss 1.3779 (1.2358) acc 65.6250 (69.1071) lr 1.0628e-03 eta 0:15:14
epoch [14/25] batch [40/250] time 0.290 (0.306) data 0.000 (0.017) loss 1.6631 (1.2475) acc 59.3750 (69.3750) lr 1.0628e-03 eta 0:15:05
epoch [14/25] batch [45/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.4160 (1.2650) acc 68.7500 (68.6111) lr 1.0628e-03 eta 0:14:58
epoch [14/25] batch [50/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.2549 (1.2667) acc 71.8750 (68.9375) lr 1.0628e-03 eta 0:14:52
epoch [14/25] batch [55/250] time 0.289 (0.301) data 0.000 (0.013) loss 1.2188 (1.2818) acc 65.6250 (68.4659) lr 1.0628e-03 eta 0:14:47
epoch [14/25] batch [60/250] time 0.289 (0.300) data 0.000 (0.012) loss 0.9771 (1.2848) acc 71.8750 (68.3333) lr 1.0628e-03 eta 0:14:43
epoch [14/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3760 (1.2691) acc 56.2500 (68.5096) lr 1.0628e-03 eta 0:14:39
epoch [14/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4717 (1.2656) acc 62.5000 (68.4821) lr 1.0628e-03 eta 0:14:35
epoch [14/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9458 (1.2630) acc 75.0000 (68.4167) lr 1.0628e-03 eta 0:14:32
epoch [14/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.5322 (1.2648) acc 71.8750 (68.5156) lr 1.0628e-03 eta 0:14:28
epoch [14/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0820 (1.2698) acc 75.0000 (68.4926) lr 1.0628e-03 eta 0:14:25
epoch [14/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4561 (1.2772) acc 56.2500 (68.2639) lr 1.0628e-03 eta 0:14:23
epoch [14/25] batch [95/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3867 (1.2768) acc 59.3750 (68.0263) lr 1.0628e-03 eta 0:14:20
epoch [14/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1553 (1.2701) acc 75.0000 (68.3125) lr 1.0628e-03 eta 0:14:18
epoch [14/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.6885 (1.2707) acc 65.6250 (68.4226) lr 1.0628e-03 eta 0:14:15
epoch [14/25] batch [110/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2383 (1.2657) acc 71.8750 (68.6648) lr 1.0628e-03 eta 0:14:13
epoch [14/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2461 (1.2612) acc 68.7500 (68.7228) lr 1.0628e-03 eta 0:14:11
epoch [14/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8799 (1.2596) acc 84.3750 (68.8021) lr 1.0628e-03 eta 0:14:08
epoch [14/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1162 (1.2577) acc 62.5000 (68.8500) lr 1.0628e-03 eta 0:14:06
epoch [14/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2754 (1.2559) acc 75.0000 (68.9183) lr 1.0628e-03 eta 0:14:04
epoch [14/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2441 (1.2715) acc 78.1250 (68.6574) lr 1.0628e-03 eta 0:14:02
epoch [14/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4258 (1.2742) acc 68.7500 (68.6384) lr 1.0628e-03 eta 0:14:00
epoch [14/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8535 (1.2750) acc 84.3750 (68.7069) lr 1.0628e-03 eta 0:13:58
epoch [14/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4111 (1.2787) acc 62.5000 (68.5208) lr 1.0628e-03 eta 0:13:56
epoch [14/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5381 (1.2798) acc 62.5000 (68.4476) lr 1.0628e-03 eta 0:13:55
epoch [14/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2783 (1.2738) acc 71.8750 (68.5352) lr 1.0628e-03 eta 0:13:53
epoch [14/25] batch [165/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3955 (1.2718) acc 68.7500 (68.6364) lr 1.0628e-03 eta 0:13:51
epoch [14/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4443 (1.2743) acc 59.3750 (68.5294) lr 1.0628e-03 eta 0:13:49
epoch [14/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0986 (1.2728) acc 71.8750 (68.4821) lr 1.0628e-03 eta 0:13:47
epoch [14/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5830 (1.2771) acc 59.3750 (68.4375) lr 1.0628e-03 eta 0:13:46
epoch [14/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0986 (1.2738) acc 68.7500 (68.4459) lr 1.0628e-03 eta 0:13:44
epoch [14/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3838 (1.2799) acc 68.7500 (68.3388) lr 1.0628e-03 eta 0:13:42
epoch [14/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7695 (1.2795) acc 65.6250 (68.3814) lr 1.0628e-03 eta 0:13:40
epoch [14/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6743 (1.2784) acc 81.2500 (68.3750) lr 1.0628e-03 eta 0:13:39
epoch [14/25] batch [205/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.5791 (1.2794) acc 65.6250 (68.4909) lr 1.0628e-03 eta 0:13:37
epoch [14/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0176 (1.2768) acc 71.8750 (68.5417) lr 1.0628e-03 eta 0:13:35
epoch [14/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2246 (1.2768) acc 78.1250 (68.5610) lr 1.0628e-03 eta 0:13:34
epoch [14/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.5820 (1.2729) acc 87.5000 (68.6648) lr 1.0628e-03 eta 0:13:32
epoch [14/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3223 (1.2749) acc 65.6250 (68.5972) lr 1.0628e-03 eta 0:13:30
epoch [14/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5459 (1.2742) acc 62.5000 (68.5870) lr 1.0628e-03 eta 0:13:29
epoch [14/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.003) loss 1.3447 (1.2706) acc 68.7500 (68.6968) lr 1.0628e-03 eta 0:13:27
epoch [14/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3584 (1.2741) acc 68.7500 (68.5807) lr 1.0628e-03 eta 0:13:25
epoch [14/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4746 (1.2691) acc 59.3750 (68.6480) lr 1.0628e-03 eta 0:13:24
epoch [14/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.1465 (1.2664) acc 75.0000 (68.7125) lr 9.3721e-04 eta 0:13:22
epoch [15/25] batch [5/250] time 0.290 (0.435) data 0.000 (0.145) loss 1.1074 (1.0784) acc 71.8750 (70.6250) lr 9.3721e-04 eta 0:19:55
epoch [15/25] batch [10/250] time 0.290 (0.363) data 0.000 (0.073) loss 1.1250 (1.1197) acc 75.0000 (71.8750) lr 9.3721e-04 eta 0:16:33
epoch [15/25] batch [15/250] time 0.291 (0.338) data 0.000 (0.049) loss 1.4277 (1.1962) acc 53.1250 (70.8333) lr 9.3721e-04 eta 0:15:25
epoch [15/25] batch [20/250] time 0.289 (0.326) data 0.000 (0.037) loss 1.2510 (1.2045) acc 71.8750 (71.5625) lr 9.3721e-04 eta 0:14:50
epoch [15/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.029) loss 0.9180 (1.2222) acc 75.0000 (71.0000) lr 9.3721e-04 eta 0:14:28
epoch [15/25] batch [30/250] time 0.289 (0.314) data 0.000 (0.024) loss 1.3535 (1.2520) acc 71.8750 (70.5208) lr 9.3721e-04 eta 0:14:13
epoch [15/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.0889 (1.2543) acc 68.7500 (70.2679) lr 9.3721e-04 eta 0:14:01
epoch [15/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.1104 (1.2809) acc 65.6250 (69.2188) lr 9.3721e-04 eta 0:13:53
epoch [15/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.0684 (1.2564) acc 75.0000 (69.7917) lr 9.3721e-04 eta 0:13:46
epoch [15/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.9062 (1.2722) acc 71.8750 (69.5000) lr 9.3721e-04 eta 0:13:40
epoch [15/25] batch [55/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.7676 (1.2803) acc 59.3750 (69.2614) lr 9.3721e-04 eta 0:13:34
epoch [15/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.5928 (1.2792) acc 71.8750 (69.8438) lr 9.3721e-04 eta 0:13:30
epoch [15/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.7236 (1.2896) acc 65.6250 (69.6635) lr 9.3721e-04 eta 0:13:26
epoch [15/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.6631 (1.2812) acc 62.5000 (69.6429) lr 9.3721e-04 eta 0:13:22
epoch [15/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1309 (1.2742) acc 75.0000 (69.8333) lr 9.3721e-04 eta 0:13:19
epoch [15/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0801 (1.2757) acc 78.1250 (69.7656) lr 9.3721e-04 eta 0:13:16
epoch [15/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0195 (1.2650) acc 78.1250 (70.0368) lr 9.3721e-04 eta 0:13:13
epoch [15/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4648 (1.2674) acc 59.3750 (69.8958) lr 9.3721e-04 eta 0:13:10
epoch [15/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.6436 (1.2601) acc 84.3750 (70.0658) lr 9.3721e-04 eta 0:13:08
epoch [15/25] batch [100/250] time 0.290 (0.296) data 0.000 (0.008) loss 1.0420 (1.2573) acc 68.7500 (70.1250) lr 9.3721e-04 eta 0:13:05
epoch [15/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4141 (1.2561) acc 65.6250 (70.1488) lr 9.3721e-04 eta 0:13:03
epoch [15/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7285 (1.2586) acc 62.5000 (70.0568) lr 9.3721e-04 eta 0:13:00
epoch [15/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.4307 (1.2578) acc 62.5000 (69.9185) lr 9.3721e-04 eta 0:12:58
epoch [15/25] batch [120/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.0625 (1.2564) acc 71.8750 (69.9740) lr 9.3721e-04 eta 0:12:56
epoch [15/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9902 (1.2642) acc 75.0000 (69.7000) lr 9.3721e-04 eta 0:12:54
epoch [15/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2012 (1.2550) acc 71.8750 (69.8077) lr 9.3721e-04 eta 0:12:52
epoch [15/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0674 (1.2523) acc 75.0000 (69.5833) lr 9.3721e-04 eta 0:12:50
epoch [15/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3916 (1.2527) acc 62.5000 (69.5089) lr 9.3721e-04 eta 0:12:48
epoch [15/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1064 (1.2549) acc 75.0000 (69.5259) lr 9.3721e-04 eta 0:12:46
epoch [15/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2793 (1.2600) acc 68.7500 (69.3958) lr 9.3721e-04 eta 0:12:44
epoch [15/25] batch [155/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3389 (1.2572) acc 71.8750 (69.4556) lr 9.3721e-04 eta 0:12:42
epoch [15/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.6523 (1.2535) acc 65.6250 (69.5703) lr 9.3721e-04 eta 0:12:40
epoch [15/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8770 (1.2617) acc 56.2500 (69.4318) lr 9.3721e-04 eta 0:12:38
epoch [15/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2568 (1.2635) acc 68.7500 (69.3750) lr 9.3721e-04 eta 0:12:37
epoch [15/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3174 (1.2644) acc 71.8750 (69.3571) lr 9.3721e-04 eta 0:12:35
epoch [15/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0156 (1.2678) acc 75.0000 (69.3229) lr 9.3721e-04 eta 0:12:33
epoch [15/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4355 (1.2705) acc 59.3750 (69.2399) lr 9.3721e-04 eta 0:12:31
epoch [15/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1289 (1.2674) acc 68.7500 (69.3092) lr 9.3721e-04 eta 0:12:30
epoch [15/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0303 (1.2628) acc 68.7500 (69.3590) lr 9.3721e-04 eta 0:12:28
epoch [15/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8418 (1.2581) acc 75.0000 (69.4219) lr 9.3721e-04 eta 0:12:26
epoch [15/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2061 (1.2563) acc 68.7500 (69.4817) lr 9.3721e-04 eta 0:12:24
epoch [15/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3516 (1.2595) acc 65.6250 (69.3899) lr 9.3721e-04 eta 0:12:23
epoch [15/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6885 (1.2546) acc 78.1250 (69.5058) lr 9.3721e-04 eta 0:12:21
epoch [15/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0166 (1.2545) acc 68.7500 (69.4176) lr 9.3721e-04 eta 0:12:19
epoch [15/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.9648 (1.2558) acc 78.1250 (69.3750) lr 9.3721e-04 eta 0:12:18
epoch [15/25] batch [230/250] time 0.290 (0.292) data 0.000 (0.003) loss 0.8091 (1.2584) acc 75.0000 (69.2391) lr 9.3721e-04 eta 0:12:16
epoch [15/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.1689 (1.2556) acc 75.0000 (69.2420) lr 9.3721e-04 eta 0:12:15
epoch [15/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.9102 (1.2572) acc 78.1250 (69.1927) lr 9.3721e-04 eta 0:12:13
epoch [15/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2100 (1.2542) acc 71.8750 (69.2985) lr 9.3721e-04 eta 0:12:11
epoch [15/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.0762 (1.2572) acc 75.0000 (69.2500) lr 8.1262e-04 eta 0:12:10
epoch [16/25] batch [5/250] time 0.289 (0.443) data 0.000 (0.153) loss 0.7964 (1.2456) acc 81.2500 (70.0000) lr 8.1262e-04 eta 0:18:24
epoch [16/25] batch [10/250] time 0.289 (0.366) data 0.000 (0.077) loss 0.9912 (1.1515) acc 75.0000 (73.1250) lr 8.1262e-04 eta 0:15:11
epoch [16/25] batch [15/250] time 0.289 (0.340) data 0.000 (0.051) loss 1.4043 (1.1213) acc 71.8750 (73.9583) lr 8.1262e-04 eta 0:14:05
epoch [16/25] batch [20/250] time 0.289 (0.327) data 0.000 (0.039) loss 1.4160 (1.1354) acc 56.2500 (71.7188) lr 8.1262e-04 eta 0:13:32
epoch [16/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 0.9214 (1.1262) acc 84.3750 (72.8750) lr 8.1262e-04 eta 0:13:11
epoch [16/25] batch [30/250] time 0.289 (0.315) data 0.000 (0.026) loss 1.3428 (1.1572) acc 71.8750 (72.2917) lr 8.1262e-04 eta 0:12:57
epoch [16/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.0781 (1.1527) acc 71.8750 (72.2321) lr 8.1262e-04 eta 0:12:46
epoch [16/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.4521 (1.1562) acc 75.0000 (72.0312) lr 8.1262e-04 eta 0:12:38
epoch [16/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.5957 (1.1340) acc 81.2500 (72.0139) lr 8.1262e-04 eta 0:12:31
epoch [16/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.016) loss 1.5439 (1.1518) acc 65.6250 (71.7500) lr 8.1262e-04 eta 0:12:25
epoch [16/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.3340 (1.1846) acc 68.7500 (71.2500) lr 8.1262e-04 eta 0:12:20
epoch [16/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.5518 (1.1853) acc 59.3750 (71.2500) lr 8.1262e-04 eta 0:12:16
epoch [16/25] batch [65/250] time 0.292 (0.301) data 0.001 (0.012) loss 1.1826 (1.1935) acc 71.8750 (71.2019) lr 8.1262e-04 eta 0:12:12
epoch [16/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.4033 (1.1877) acc 62.5000 (71.2054) lr 8.1262e-04 eta 0:12:09
epoch [16/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.011) loss 2.1035 (1.2163) acc 53.1250 (70.4583) lr 8.1262e-04 eta 0:12:05
epoch [16/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.7290 (1.1982) acc 87.5000 (71.0156) lr 8.1262e-04 eta 0:12:02
epoch [16/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3799 (1.1930) acc 62.5000 (71.0294) lr 8.1262e-04 eta 0:11:59
epoch [16/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.2178 (1.1919) acc 62.5000 (70.9722) lr 8.1262e-04 eta 0:11:57
epoch [16/25] batch [95/250] time 0.288 (0.297) data 0.000 (0.008) loss 1.2734 (1.1913) acc 68.7500 (71.0526) lr 8.1262e-04 eta 0:11:54
epoch [16/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4443 (1.2013) acc 75.0000 (70.7500) lr 8.1262e-04 eta 0:11:52
epoch [16/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.0205 (1.2063) acc 65.6250 (70.3571) lr 8.1262e-04 eta 0:11:49
epoch [16/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 2.0977 (1.2116) acc 59.3750 (70.3977) lr 8.1262e-04 eta 0:11:47
epoch [16/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3662 (1.2289) acc 62.5000 (70.0000) lr 8.1262e-04 eta 0:11:45
epoch [16/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.1914 (1.2353) acc 71.8750 (69.8958) lr 8.1262e-04 eta 0:11:43
epoch [16/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2617 (1.2395) acc 71.8750 (69.8750) lr 8.1262e-04 eta 0:11:41
epoch [16/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1660 (1.2343) acc 59.3750 (69.7837) lr 8.1262e-04 eta 0:11:39
epoch [16/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0645 (1.2365) acc 65.6250 (69.6759) lr 8.1262e-04 eta 0:11:37
epoch [16/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5469 (1.2451) acc 62.5000 (69.3527) lr 8.1262e-04 eta 0:11:35
epoch [16/25] batch [145/250] time 0.290 (0.294) data 0.000 (0.006) loss 2.0625 (1.2538) acc 65.6250 (69.2457) lr 8.1262e-04 eta 0:11:33
epoch [16/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1045 (1.2475) acc 62.5000 (69.1875) lr 8.1262e-04 eta 0:11:31
epoch [16/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0303 (1.2413) acc 65.6250 (69.3145) lr 8.1262e-04 eta 0:11:29
epoch [16/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2402 (1.2385) acc 65.6250 (69.5117) lr 8.1262e-04 eta 0:11:27
epoch [16/25] batch [165/250] time 0.291 (0.294) data 0.000 (0.005) loss 0.8164 (1.2410) acc 81.2500 (69.5265) lr 8.1262e-04 eta 0:11:25
epoch [16/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5752 (1.2383) acc 59.3750 (69.5404) lr 8.1262e-04 eta 0:11:24
epoch [16/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3857 (1.2376) acc 56.2500 (69.5536) lr 8.1262e-04 eta 0:11:22
epoch [16/25] batch [180/250] time 0.290 (0.293) data 0.000 (0.005) loss 1.5986 (1.2345) acc 56.2500 (69.5486) lr 8.1262e-04 eta 0:11:20
epoch [16/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4902 (1.2362) acc 65.6250 (69.5270) lr 8.1262e-04 eta 0:11:18
epoch [16/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2715 (1.2312) acc 62.5000 (69.6217) lr 8.1262e-04 eta 0:11:17
epoch [16/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9326 (1.2283) acc 81.2500 (69.8077) lr 8.1262e-04 eta 0:11:15
epoch [16/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5068 (1.2324) acc 62.5000 (69.7344) lr 8.1262e-04 eta 0:11:13
epoch [16/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9688 (1.2294) acc 81.2500 (69.9238) lr 8.1262e-04 eta 0:11:12
epoch [16/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2100 (1.2375) acc 68.7500 (69.7619) lr 8.1262e-04 eta 0:11:10
epoch [16/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0127 (1.2328) acc 81.2500 (69.9128) lr 8.1262e-04 eta 0:11:08
epoch [16/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4082 (1.2385) acc 81.2500 (69.8011) lr 8.1262e-04 eta 0:11:07
epoch [16/25] batch [225/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.3379 (1.2400) acc 62.5000 (69.7639) lr 8.1262e-04 eta 0:11:05
epoch [16/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0977 (1.2363) acc 62.5000 (69.8641) lr 8.1262e-04 eta 0:11:03
epoch [16/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.004) loss 0.6636 (1.2355) acc 84.3750 (69.8404) lr 8.1262e-04 eta 0:11:02
epoch [16/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.1562 (1.2341) acc 75.0000 (69.8568) lr 8.1262e-04 eta 0:11:00
epoch [16/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.0732 (1.2437) acc 68.7500 (69.6811) lr 8.1262e-04 eta 0:10:58
epoch [16/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5977 (1.2463) acc 71.8750 (69.6750) lr 6.9098e-04 eta 0:10:57
epoch [17/25] batch [5/250] time 0.289 (0.440) data 0.000 (0.151) loss 0.9399 (1.2876) acc 71.8750 (67.5000) lr 6.9098e-04 eta 0:16:28
epoch [17/25] batch [10/250] time 0.290 (0.365) data 0.000 (0.075) loss 1.2705 (1.2448) acc 68.7500 (70.0000) lr 6.9098e-04 eta 0:13:36
epoch [17/25] batch [15/250] time 0.289 (0.340) data 0.000 (0.050) loss 0.9893 (1.2952) acc 71.8750 (69.3750) lr 6.9098e-04 eta 0:12:38
epoch [17/25] batch [20/250] time 0.289 (0.327) data 0.000 (0.038) loss 1.1377 (1.2616) acc 68.7500 (69.8438) lr 6.9098e-04 eta 0:12:09
epoch [17/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 0.9546 (1.2482) acc 75.0000 (69.8750) lr 6.9098e-04 eta 0:11:50
epoch [17/25] batch [30/250] time 0.291 (0.314) data 0.000 (0.025) loss 0.9648 (1.2393) acc 78.1250 (70.0000) lr 6.9098e-04 eta 0:11:38
epoch [17/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.6260 (1.2497) acc 53.1250 (69.8214) lr 6.9098e-04 eta 0:11:28
epoch [17/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.0107 (1.2811) acc 71.8750 (69.0625) lr 6.9098e-04 eta 0:11:21
epoch [17/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.2393 (1.2532) acc 65.6250 (69.6528) lr 6.9098e-04 eta 0:11:14
epoch [17/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.9795 (1.2384) acc 71.8750 (70.5625) lr 6.9098e-04 eta 0:11:09
epoch [17/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.8496 (1.2456) acc 68.7500 (70.6250) lr 6.9098e-04 eta 0:11:04
epoch [17/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.2861 (1.2360) acc 71.8750 (71.0417) lr 6.9098e-04 eta 0:11:00
epoch [17/25] batch [65/250] time 0.288 (0.301) data 0.000 (0.012) loss 1.7236 (1.2363) acc 53.1250 (70.9615) lr 6.9098e-04 eta 0:10:57
epoch [17/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.4951 (1.2388) acc 75.0000 (71.1607) lr 6.9098e-04 eta 0:10:53
epoch [17/25] batch [75/250] time 0.290 (0.299) data 0.001 (0.010) loss 1.1895 (1.2333) acc 75.0000 (71.0417) lr 6.9098e-04 eta 0:10:50
epoch [17/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.9683 (1.2421) acc 75.0000 (70.7812) lr 6.9098e-04 eta 0:10:47
epoch [17/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9546 (1.2411) acc 71.8750 (70.5882) lr 6.9098e-04 eta 0:10:45
epoch [17/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.009) loss 1.0674 (1.2269) acc 68.7500 (70.7639) lr 6.9098e-04 eta 0:10:42
epoch [17/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.7700 (1.2343) acc 65.6250 (70.4934) lr 6.9098e-04 eta 0:10:40
epoch [17/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2393 (1.2273) acc 68.7500 (70.5000) lr 6.9098e-04 eta 0:10:37
epoch [17/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9668 (1.2285) acc 78.1250 (70.4762) lr 6.9098e-04 eta 0:10:35
epoch [17/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4678 (1.2355) acc 62.5000 (70.3693) lr 6.9098e-04 eta 0:10:33
epoch [17/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2178 (1.2331) acc 71.8750 (70.4620) lr 6.9098e-04 eta 0:10:31
epoch [17/25] batch [120/250] time 0.290 (0.295) data 0.000 (0.007) loss 1.0342 (1.2453) acc 71.8750 (70.1302) lr 6.9098e-04 eta 0:10:29
epoch [17/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4775 (1.2474) acc 68.7500 (70.1250) lr 6.9098e-04 eta 0:10:27
epoch [17/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8633 (1.2408) acc 78.1250 (70.1202) lr 6.9098e-04 eta 0:10:25
epoch [17/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0996 (1.2423) acc 68.7500 (70.0694) lr 6.9098e-04 eta 0:10:23
epoch [17/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.2773 (1.2446) acc 71.8750 (70.1339) lr 6.9098e-04 eta 0:10:21
epoch [17/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0957 (1.2443) acc 75.0000 (70.2802) lr 6.9098e-04 eta 0:10:19
epoch [17/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.6289 (1.2461) acc 75.0000 (70.0208) lr 6.9098e-04 eta 0:10:17
epoch [17/25] batch [155/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.4316 (1.2547) acc 65.6250 (69.8790) lr 6.9098e-04 eta 0:10:15
epoch [17/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5850 (1.2539) acc 53.1250 (69.8438) lr 6.9098e-04 eta 0:10:14
epoch [17/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4736 (1.2577) acc 65.6250 (69.7727) lr 6.9098e-04 eta 0:10:12
epoch [17/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3701 (1.2594) acc 62.5000 (69.6691) lr 6.9098e-04 eta 0:10:10
epoch [17/25] batch [175/250] time 0.291 (0.293) data 0.000 (0.005) loss 1.0215 (1.2572) acc 75.0000 (69.7679) lr 6.9098e-04 eta 0:10:08
epoch [17/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2295 (1.2607) acc 59.3750 (69.6007) lr 6.9098e-04 eta 0:10:07
epoch [17/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0049 (1.2590) acc 78.1250 (69.7128) lr 6.9098e-04 eta 0:10:05
epoch [17/25] batch [190/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2578 (1.2543) acc 75.0000 (69.8520) lr 6.9098e-04 eta 0:10:03
epoch [17/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1162 (1.2510) acc 68.7500 (69.8237) lr 6.9098e-04 eta 0:10:02
epoch [17/25] batch [200/250] time 0.288 (0.293) data 0.000 (0.004) loss 0.9092 (1.2540) acc 68.7500 (69.7344) lr 6.9098e-04 eta 0:10:00
epoch [17/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3818 (1.2578) acc 71.8750 (69.6037) lr 6.9098e-04 eta 0:09:58
epoch [17/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0918 (1.2554) acc 71.8750 (69.5238) lr 6.9098e-04 eta 0:09:57
epoch [17/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4141 (1.2534) acc 65.6250 (69.6802) lr 6.9098e-04 eta 0:09:55
epoch [17/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1318 (1.2504) acc 71.8750 (69.7159) lr 6.9098e-04 eta 0:09:53
epoch [17/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2334 (1.2491) acc 68.7500 (69.6944) lr 6.9098e-04 eta 0:09:52
epoch [17/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.7490 (1.2503) acc 62.5000 (69.5924) lr 6.9098e-04 eta 0:09:50
epoch [17/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.7998 (1.2537) acc 56.2500 (69.4548) lr 6.9098e-04 eta 0:09:49
epoch [17/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0352 (1.2522) acc 75.0000 (69.4271) lr 6.9098e-04 eta 0:09:47
epoch [17/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.1143 (1.2530) acc 65.6250 (69.3495) lr 6.9098e-04 eta 0:09:45
epoch [17/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1709 (1.2508) acc 68.7500 (69.3375) lr 5.7422e-04 eta 0:09:44
epoch [18/25] batch [5/250] time 0.289 (0.427) data 0.000 (0.137) loss 1.3926 (1.1858) acc 65.6250 (71.2500) lr 5.7422e-04 eta 0:14:11
epoch [18/25] batch [10/250] time 0.290 (0.358) data 0.000 (0.069) loss 1.1650 (1.1301) acc 71.8750 (72.8125) lr 5.7422e-04 eta 0:11:53
epoch [18/25] batch [15/250] time 0.290 (0.336) data 0.000 (0.046) loss 1.0928 (1.1740) acc 71.8750 (71.8750) lr 5.7422e-04 eta 0:11:05
epoch [18/25] batch [20/250] time 0.289 (0.324) data 0.000 (0.035) loss 0.7183 (1.2074) acc 84.3750 (72.0312) lr 5.7422e-04 eta 0:10:41
epoch [18/25] batch [25/250] time 0.289 (0.317) data 0.000 (0.028) loss 1.5107 (1.2187) acc 59.3750 (71.0000) lr 5.7422e-04 eta 0:10:26
epoch [18/25] batch [30/250] time 0.289 (0.312) data 0.000 (0.023) loss 1.3242 (1.2152) acc 71.8750 (70.7292) lr 5.7422e-04 eta 0:10:15
epoch [18/25] batch [35/250] time 0.290 (0.309) data 0.000 (0.020) loss 1.4014 (1.2110) acc 62.5000 (70.9821) lr 5.7422e-04 eta 0:10:07
epoch [18/25] batch [40/250] time 0.290 (0.307) data 0.000 (0.017) loss 0.9814 (1.2052) acc 75.0000 (70.9375) lr 5.7422e-04 eta 0:10:01
epoch [18/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.1689 (1.2153) acc 71.8750 (71.0417) lr 5.7422e-04 eta 0:09:55
epoch [18/25] batch [50/250] time 0.290 (0.303) data 0.000 (0.014) loss 0.9399 (1.2013) acc 78.1250 (71.5000) lr 5.7422e-04 eta 0:09:51
epoch [18/25] batch [55/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.4219 (1.2115) acc 62.5000 (70.9091) lr 5.7422e-04 eta 0:09:47
epoch [18/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.6709 (1.2026) acc 84.3750 (70.9896) lr 5.7422e-04 eta 0:09:43
epoch [18/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.5342 (1.1902) acc 65.6250 (71.0577) lr 5.7422e-04 eta 0:09:40
epoch [18/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.2354 (1.1920) acc 71.8750 (71.3839) lr 5.7422e-04 eta 0:09:37
epoch [18/25] batch [75/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.2480 (1.1794) acc 68.7500 (71.6250) lr 5.7422e-04 eta 0:09:34
epoch [18/25] batch [80/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.7422 (1.1856) acc 53.1250 (71.1719) lr 5.7422e-04 eta 0:09:32
epoch [18/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.1875 (1.1893) acc 65.6250 (71.1765) lr 5.7422e-04 eta 0:09:29
epoch [18/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3086 (1.1817) acc 75.0000 (71.4931) lr 5.7422e-04 eta 0:09:27
epoch [18/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5303 (1.1869) acc 71.8750 (71.5461) lr 5.7422e-04 eta 0:09:25
epoch [18/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8389 (1.1809) acc 81.2500 (71.8125) lr 5.7422e-04 eta 0:09:22
epoch [18/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1562 (1.1765) acc 68.7500 (71.7262) lr 5.7422e-04 eta 0:09:20
epoch [18/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2051 (1.1769) acc 71.8750 (71.7614) lr 5.7422e-04 eta 0:09:18
epoch [18/25] batch [115/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.8511 (1.1740) acc 81.2500 (71.8478) lr 5.7422e-04 eta 0:09:16
epoch [18/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8193 (1.1697) acc 59.3750 (71.9792) lr 5.7422e-04 eta 0:09:14
epoch [18/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2910 (1.1740) acc 68.7500 (71.9500) lr 5.7422e-04 eta 0:09:12
epoch [18/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4707 (1.1761) acc 75.0000 (72.1154) lr 5.7422e-04 eta 0:09:11
epoch [18/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2705 (1.1816) acc 71.8750 (71.8981) lr 5.7422e-04 eta 0:09:09
epoch [18/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4561 (1.1837) acc 71.8750 (71.8750) lr 5.7422e-04 eta 0:09:07
epoch [18/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2686 (1.1850) acc 56.2500 (71.7672) lr 5.7422e-04 eta 0:09:05
epoch [18/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1738 (1.1903) acc 62.5000 (71.5625) lr 5.7422e-04 eta 0:09:03
epoch [18/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4961 (1.1933) acc 56.2500 (71.3508) lr 5.7422e-04 eta 0:09:02
epoch [18/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6104 (1.1981) acc 65.6250 (71.3281) lr 5.7422e-04 eta 0:09:00
epoch [18/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2764 (1.2016) acc 65.6250 (71.1932) lr 5.7422e-04 eta 0:08:58
epoch [18/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2275 (1.2099) acc 56.2500 (70.9926) lr 5.7422e-04 eta 0:08:56
epoch [18/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3662 (1.2038) acc 62.5000 (71.1429) lr 5.7422e-04 eta 0:08:55
epoch [18/25] batch [180/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.5049 (1.2023) acc 68.7500 (71.2326) lr 5.7422e-04 eta 0:08:53
epoch [18/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2100 (1.2013) acc 59.3750 (71.1824) lr 5.7422e-04 eta 0:08:51
epoch [18/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7705 (1.2052) acc 56.2500 (71.0691) lr 5.7422e-04 eta 0:08:50
epoch [18/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4541 (1.2025) acc 65.6250 (71.0096) lr 5.7422e-04 eta 0:08:48
epoch [18/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1348 (1.2075) acc 81.2500 (70.9062) lr 5.7422e-04 eta 0:08:46
epoch [18/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9995 (1.2092) acc 71.8750 (70.7622) lr 5.7422e-04 eta 0:08:45
epoch [18/25] batch [210/250] time 0.289 (0.293) data 0.001 (0.004) loss 0.8823 (1.2125) acc 78.1250 (70.7292) lr 5.7422e-04 eta 0:08:43
epoch [18/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.4814 (1.2175) acc 68.7500 (70.6105) lr 5.7422e-04 eta 0:08:42
epoch [18/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.8384 (1.2151) acc 78.1250 (70.6250) lr 5.7422e-04 eta 0:08:40
epoch [18/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0332 (1.2123) acc 81.2500 (70.7083) lr 5.7422e-04 eta 0:08:38
epoch [18/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4736 (1.2129) acc 53.1250 (70.6250) lr 5.7422e-04 eta 0:08:37
epoch [18/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.6455 (1.2140) acc 56.2500 (70.5585) lr 5.7422e-04 eta 0:08:35
epoch [18/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.7856 (1.2141) acc 78.1250 (70.4818) lr 5.7422e-04 eta 0:08:34
epoch [18/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2842 (1.2154) acc 65.6250 (70.3827) lr 5.7422e-04 eta 0:08:32
epoch [18/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2861 (1.2162) acc 65.6250 (70.3500) lr 4.6417e-04 eta 0:08:30
epoch [19/25] batch [5/250] time 0.288 (0.433) data 0.000 (0.144) loss 0.9297 (1.2078) acc 81.2500 (73.7500) lr 4.6417e-04 eta 0:12:36
epoch [19/25] batch [10/250] time 0.289 (0.361) data 0.000 (0.072) loss 0.9307 (1.0446) acc 81.2500 (76.2500) lr 4.6417e-04 eta 0:10:28
epoch [19/25] batch [15/250] time 0.289 (0.337) data 0.000 (0.048) loss 1.2041 (1.1168) acc 71.8750 (74.5833) lr 4.6417e-04 eta 0:09:44
epoch [19/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.036) loss 0.9111 (1.1189) acc 81.2500 (73.1250) lr 4.6417e-04 eta 0:09:22
epoch [19/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.5645 (1.1222) acc 75.0000 (73.3750) lr 4.6417e-04 eta 0:09:08
epoch [19/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.2363 (1.1137) acc 68.7500 (73.0208) lr 4.6417e-04 eta 0:08:58
epoch [19/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.7759 (1.1112) acc 78.1250 (72.8571) lr 4.6417e-04 eta 0:08:50
epoch [19/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.1982 (1.1215) acc 78.1250 (72.2656) lr 4.6417e-04 eta 0:08:44
epoch [19/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.0352 (1.1203) acc 75.0000 (72.5000) lr 4.6417e-04 eta 0:08:40
epoch [19/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.015) loss 1.2559 (1.1321) acc 65.6250 (72.2500) lr 4.6417e-04 eta 0:08:35
epoch [19/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.3174 (1.1496) acc 65.6250 (71.4773) lr 4.6417e-04 eta 0:08:32
epoch [19/25] batch [60/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.0742 (1.1621) acc 71.8750 (71.1458) lr 4.6417e-04 eta 0:08:28
epoch [19/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.1162 (1.1773) acc 68.7500 (70.8173) lr 4.6417e-04 eta 0:08:25
epoch [19/25] batch [70/250] time 0.290 (0.299) data 0.000 (0.011) loss 1.3105 (1.1907) acc 75.0000 (70.7143) lr 4.6417e-04 eta 0:08:22
epoch [19/25] batch [75/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.5000 (1.2088) acc 56.2500 (69.8750) lr 4.6417e-04 eta 0:08:20
epoch [19/25] batch [80/250] time 0.289 (0.298) data 0.001 (0.009) loss 1.0059 (1.2092) acc 75.0000 (69.7266) lr 4.6417e-04 eta 0:08:17
epoch [19/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7075 (1.2173) acc 78.1250 (69.5588) lr 4.6417e-04 eta 0:08:15
epoch [19/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1055 (1.2091) acc 71.8750 (69.6181) lr 4.6417e-04 eta 0:08:13
epoch [19/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2412 (1.2148) acc 68.7500 (69.6711) lr 4.6417e-04 eta 0:08:10
epoch [19/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3340 (1.2104) acc 71.8750 (69.7500) lr 4.6417e-04 eta 0:08:08
epoch [19/25] batch [105/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.3203 (1.2173) acc 71.8750 (69.8214) lr 4.6417e-04 eta 0:08:06
epoch [19/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.9189 (1.2263) acc 59.3750 (69.6875) lr 4.6417e-04 eta 0:08:04
epoch [19/25] batch [115/250] time 0.290 (0.295) data 0.000 (0.007) loss 1.7363 (1.2322) acc 65.6250 (69.6467) lr 4.6417e-04 eta 0:08:02
epoch [19/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7363 (1.2360) acc 53.1250 (69.5833) lr 4.6417e-04 eta 0:08:01
epoch [19/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5293 (1.2383) acc 56.2500 (69.7000) lr 4.6417e-04 eta 0:07:59
epoch [19/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0361 (1.2370) acc 75.0000 (69.7115) lr 4.6417e-04 eta 0:07:57
epoch [19/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.1504 (1.2350) acc 75.0000 (69.8148) lr 4.6417e-04 eta 0:07:55
epoch [19/25] batch [140/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2656 (1.2318) acc 65.6250 (69.7991) lr 4.6417e-04 eta 0:07:53
epoch [19/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4873 (1.2332) acc 65.6250 (69.7629) lr 4.6417e-04 eta 0:07:52
epoch [19/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6504 (1.2330) acc 59.3750 (69.7917) lr 4.6417e-04 eta 0:07:50
epoch [19/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9331 (1.2287) acc 59.3750 (69.6774) lr 4.6417e-04 eta 0:07:48
epoch [19/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4521 (1.2201) acc 65.6250 (69.8633) lr 4.6417e-04 eta 0:07:46
epoch [19/25] batch [165/250] time 0.289 (0.294) data 0.001 (0.005) loss 1.2852 (1.2205) acc 75.0000 (69.8674) lr 4.6417e-04 eta 0:07:45
epoch [19/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2061 (1.2210) acc 68.7500 (70.0368) lr 4.6417e-04 eta 0:07:43
epoch [19/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2812 (1.2261) acc 56.2500 (69.8929) lr 4.6417e-04 eta 0:07:41
epoch [19/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8691 (1.2187) acc 78.1250 (70.0521) lr 4.6417e-04 eta 0:07:40
epoch [19/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1777 (1.2158) acc 65.6250 (70.1520) lr 4.6417e-04 eta 0:07:38
epoch [19/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8022 (1.2106) acc 75.0000 (70.2632) lr 4.6417e-04 eta 0:07:37
epoch [19/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0283 (1.2125) acc 68.7500 (70.1763) lr 4.6417e-04 eta 0:07:35
epoch [19/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5400 (1.2170) acc 53.1250 (70.0469) lr 4.6417e-04 eta 0:07:33
epoch [19/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1631 (1.2144) acc 75.0000 (70.1067) lr 4.6417e-04 eta 0:07:32
epoch [19/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1484 (1.2121) acc 71.8750 (70.0893) lr 4.6417e-04 eta 0:07:30
epoch [19/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.7979 (1.2131) acc 78.1250 (69.9419) lr 4.6417e-04 eta 0:07:28
epoch [19/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.8013 (1.2124) acc 78.1250 (69.9432) lr 4.6417e-04 eta 0:07:27
epoch [19/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0928 (1.2093) acc 71.8750 (69.9444) lr 4.6417e-04 eta 0:07:25
epoch [19/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2236 (1.2108) acc 71.8750 (69.9728) lr 4.6417e-04 eta 0:07:24
epoch [19/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.2549 (1.2097) acc 65.6250 (69.9601) lr 4.6417e-04 eta 0:07:22
epoch [19/25] batch [240/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.7783 (1.2107) acc 56.2500 (70.0000) lr 4.6417e-04 eta 0:07:21
epoch [19/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.7349 (1.2094) acc 81.2500 (70.1020) lr 4.6417e-04 eta 0:07:19
epoch [19/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4111 (1.2117) acc 81.2500 (70.0250) lr 3.6258e-04 eta 0:07:18
epoch [20/25] batch [5/250] time 0.289 (0.426) data 0.000 (0.136) loss 1.3848 (1.1488) acc 68.7500 (71.2500) lr 3.6258e-04 eta 0:10:37
epoch [20/25] batch [10/250] time 0.289 (0.358) data 0.000 (0.068) loss 1.3926 (1.1374) acc 71.8750 (72.1875) lr 3.6258e-04 eta 0:08:53
epoch [20/25] batch [15/250] time 0.289 (0.335) data 0.000 (0.046) loss 1.1309 (1.1417) acc 78.1250 (73.1250) lr 3.6258e-04 eta 0:08:17
epoch [20/25] batch [20/250] time 0.289 (0.323) data 0.000 (0.034) loss 1.4268 (1.1756) acc 65.6250 (72.0312) lr 3.6258e-04 eta 0:07:58
epoch [20/25] batch [25/250] time 0.289 (0.316) data 0.000 (0.027) loss 1.2061 (1.2122) acc 68.7500 (70.7500) lr 3.6258e-04 eta 0:07:46
epoch [20/25] batch [30/250] time 0.288 (0.312) data 0.000 (0.023) loss 1.7100 (1.2161) acc 59.3750 (70.8333) lr 3.6258e-04 eta 0:07:38
epoch [20/25] batch [35/250] time 0.289 (0.308) data 0.000 (0.020) loss 1.1846 (1.2043) acc 68.7500 (71.0714) lr 3.6258e-04 eta 0:07:31
epoch [20/25] batch [40/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.9526 (1.1974) acc 78.1250 (71.3281) lr 3.6258e-04 eta 0:07:26
epoch [20/25] batch [45/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.0625 (1.1896) acc 75.0000 (71.7361) lr 3.6258e-04 eta 0:07:22
epoch [20/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 0.8516 (1.1814) acc 78.1250 (72.0000) lr 3.6258e-04 eta 0:07:18
epoch [20/25] batch [55/250] time 0.289 (0.301) data 0.000 (0.013) loss 0.8784 (1.1990) acc 81.2500 (72.1023) lr 3.6258e-04 eta 0:07:15
epoch [20/25] batch [60/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.5645 (1.1981) acc 65.6250 (72.2396) lr 3.6258e-04 eta 0:07:12
epoch [20/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.8091 (1.1915) acc 87.5000 (72.4038) lr 3.6258e-04 eta 0:07:09
epoch [20/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.0117 (1.1982) acc 75.0000 (72.2321) lr 3.6258e-04 eta 0:07:07
epoch [20/25] batch [75/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1416 (1.2051) acc 65.6250 (71.8333) lr 3.6258e-04 eta 0:07:04
epoch [20/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.8550 (1.2082) acc 81.2500 (71.7188) lr 3.6258e-04 eta 0:07:02
epoch [20/25] batch [85/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0400 (1.2000) acc 75.0000 (71.9853) lr 3.6258e-04 eta 0:07:00
epoch [20/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0908 (1.2013) acc 71.8750 (71.9444) lr 3.6258e-04 eta 0:06:58
epoch [20/25] batch [95/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.9648 (1.2116) acc 65.6250 (71.9079) lr 3.6258e-04 eta 0:06:56
epoch [20/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.6055 (1.2186) acc 56.2500 (71.7188) lr 3.6258e-04 eta 0:06:54
epoch [20/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7139 (1.2213) acc 53.1250 (71.4583) lr 3.6258e-04 eta 0:06:52
epoch [20/25] batch [110/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7920 (1.2233) acc 56.2500 (71.3068) lr 3.6258e-04 eta 0:06:50
epoch [20/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2725 (1.2211) acc 65.6250 (71.1413) lr 3.6258e-04 eta 0:06:48
epoch [20/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0928 (1.2258) acc 75.0000 (71.0156) lr 3.6258e-04 eta 0:06:46
epoch [20/25] batch [125/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.9092 (1.2322) acc 65.6250 (70.8250) lr 3.6258e-04 eta 0:06:45
epoch [20/25] batch [130/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4229 (1.2346) acc 68.7500 (70.7692) lr 3.6258e-04 eta 0:06:43
epoch [20/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9292 (1.2249) acc 78.1250 (70.8796) lr 3.6258e-04 eta 0:06:41
epoch [20/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0977 (1.2263) acc 65.6250 (70.8705) lr 3.6258e-04 eta 0:06:39
epoch [20/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6240 (1.2270) acc 68.7500 (70.9052) lr 3.6258e-04 eta 0:06:38
epoch [20/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0732 (1.2286) acc 75.0000 (70.8750) lr 3.6258e-04 eta 0:06:36
epoch [20/25] batch [155/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.8447 (1.2205) acc 68.7500 (70.9476) lr 3.6258e-04 eta 0:06:34
epoch [20/25] batch [160/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.8901 (1.2135) acc 81.2500 (71.0352) lr 3.6258e-04 eta 0:06:33
epoch [20/25] batch [165/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3340 (1.2176) acc 68.7500 (71.0985) lr 3.6258e-04 eta 0:06:31
epoch [20/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7300 (1.2093) acc 81.2500 (71.2132) lr 3.6258e-04 eta 0:06:29
epoch [20/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9756 (1.2092) acc 78.1250 (71.2321) lr 3.6258e-04 eta 0:06:28
epoch [20/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3398 (1.2091) acc 62.5000 (71.2153) lr 3.6258e-04 eta 0:06:26
epoch [20/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2803 (1.2117) acc 68.7500 (71.0980) lr 3.6258e-04 eta 0:06:25
epoch [20/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2158 (1.2100) acc 75.0000 (71.2007) lr 3.6258e-04 eta 0:06:23
epoch [20/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2959 (1.2203) acc 65.6250 (71.0096) lr 3.6258e-04 eta 0:06:21
epoch [20/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4824 (1.2189) acc 46.8750 (70.9844) lr 3.6258e-04 eta 0:06:20
epoch [20/25] batch [205/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0205 (1.2173) acc 75.0000 (71.0061) lr 3.6258e-04 eta 0:06:18
epoch [20/25] batch [210/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.7744 (1.2161) acc 50.0000 (70.9970) lr 3.6258e-04 eta 0:06:17
epoch [20/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.8350 (1.2187) acc 65.6250 (71.0174) lr 3.6258e-04 eta 0:06:15
epoch [20/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.8809 (1.2168) acc 78.1250 (70.9375) lr 3.6258e-04 eta 0:06:14
epoch [20/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5459 (1.2198) acc 68.7500 (70.8889) lr 3.6258e-04 eta 0:06:12
epoch [20/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.7041 (1.2186) acc 87.5000 (70.8967) lr 3.6258e-04 eta 0:06:10
epoch [20/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3027 (1.2164) acc 68.7500 (70.9707) lr 3.6258e-04 eta 0:06:09
epoch [20/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5420 (1.2145) acc 65.6250 (70.9766) lr 3.6258e-04 eta 0:06:07
epoch [20/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.9014 (1.2152) acc 68.7500 (70.9439) lr 3.6258e-04 eta 0:06:06
epoch [20/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1484 (1.2123) acc 68.7500 (71.0250) lr 2.7103e-04 eta 0:06:04
epoch [21/25] batch [5/250] time 0.289 (0.451) data 0.001 (0.161) loss 1.6934 (1.2406) acc 62.5000 (70.0000) lr 2.7103e-04 eta 0:09:21
epoch [21/25] batch [10/250] time 0.289 (0.370) data 0.000 (0.081) loss 1.4121 (1.2679) acc 68.7500 (71.2500) lr 2.7103e-04 eta 0:07:39
epoch [21/25] batch [15/250] time 0.289 (0.343) data 0.000 (0.054) loss 1.3330 (1.3251) acc 75.0000 (69.3750) lr 2.7103e-04 eta 0:07:03
epoch [21/25] batch [20/250] time 0.289 (0.330) data 0.000 (0.041) loss 0.9775 (1.2263) acc 84.3750 (71.4062) lr 2.7103e-04 eta 0:06:45
epoch [21/25] batch [25/250] time 0.289 (0.321) data 0.000 (0.032) loss 0.9141 (1.1978) acc 78.1250 (71.5000) lr 2.7103e-04 eta 0:06:33
epoch [21/25] batch [30/250] time 0.289 (0.316) data 0.000 (0.027) loss 1.4033 (1.2218) acc 71.8750 (70.8333) lr 2.7103e-04 eta 0:06:25
epoch [21/25] batch [35/250] time 0.289 (0.312) data 0.000 (0.023) loss 1.3789 (1.2113) acc 65.6250 (71.3393) lr 2.7103e-04 eta 0:06:19
epoch [21/25] batch [40/250] time 0.289 (0.309) data 0.000 (0.020) loss 1.6377 (1.2128) acc 56.2500 (71.1719) lr 2.7103e-04 eta 0:06:14
epoch [21/25] batch [45/250] time 0.290 (0.307) data 0.000 (0.018) loss 1.2451 (1.2170) acc 65.6250 (70.6944) lr 2.7103e-04 eta 0:06:10
epoch [21/25] batch [50/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.5488 (1.2218) acc 56.2500 (70.3750) lr 2.7103e-04 eta 0:06:06
epoch [21/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.5410 (1.2281) acc 59.3750 (70.0568) lr 2.7103e-04 eta 0:06:03
epoch [21/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.1377 (1.2145) acc 65.6250 (70.0521) lr 2.7103e-04 eta 0:06:00
epoch [21/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1426 (1.2060) acc 68.7500 (70.2885) lr 2.7103e-04 eta 0:05:57
epoch [21/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.8013 (1.1881) acc 71.8750 (70.4464) lr 2.7103e-04 eta 0:05:54
epoch [21/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.5020 (1.1992) acc 65.6250 (70.4583) lr 2.7103e-04 eta 0:05:52
epoch [21/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1250 (1.2064) acc 78.1250 (70.2734) lr 2.7103e-04 eta 0:05:50
epoch [21/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.8833 (1.1993) acc 84.3750 (70.5882) lr 2.7103e-04 eta 0:05:47
epoch [21/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1406 (1.2021) acc 75.0000 (70.5556) lr 2.7103e-04 eta 0:05:45
epoch [21/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3525 (1.2145) acc 68.7500 (70.4934) lr 2.7103e-04 eta 0:05:43
epoch [21/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3398 (1.2175) acc 65.6250 (70.3750) lr 2.7103e-04 eta 0:05:41
epoch [21/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.7109 (1.2179) acc 75.0000 (70.3869) lr 2.7103e-04 eta 0:05:39
epoch [21/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3232 (1.2200) acc 71.8750 (70.5398) lr 2.7103e-04 eta 0:05:38
epoch [21/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0420 (1.2211) acc 84.3750 (70.5978) lr 2.7103e-04 eta 0:05:36
epoch [21/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0566 (1.2216) acc 68.7500 (70.5729) lr 2.7103e-04 eta 0:05:34
epoch [21/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7393 (1.2166) acc 62.5000 (70.5750) lr 2.7103e-04 eta 0:05:32
epoch [21/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3682 (1.2153) acc 68.7500 (70.6731) lr 2.7103e-04 eta 0:05:30
epoch [21/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2188 (1.2129) acc 71.8750 (70.7176) lr 2.7103e-04 eta 0:05:29
epoch [21/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3047 (1.2126) acc 68.7500 (70.6696) lr 2.7103e-04 eta 0:05:27
epoch [21/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4404 (1.2133) acc 59.3750 (70.5603) lr 2.7103e-04 eta 0:05:25
epoch [21/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8560 (1.2141) acc 84.3750 (70.7083) lr 2.7103e-04 eta 0:05:24
epoch [21/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4072 (1.2090) acc 71.8750 (70.8669) lr 2.7103e-04 eta 0:05:22
epoch [21/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8267 (1.2093) acc 75.0000 (70.9375) lr 2.7103e-04 eta 0:05:20
epoch [21/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4980 (1.2074) acc 65.6250 (71.0606) lr 2.7103e-04 eta 0:05:19
epoch [21/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7598 (1.2078) acc 84.3750 (71.1029) lr 2.7103e-04 eta 0:05:17
epoch [21/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0293 (1.2120) acc 75.0000 (71.1786) lr 2.7103e-04 eta 0:05:15
epoch [21/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1270 (1.2133) acc 75.0000 (71.1458) lr 2.7103e-04 eta 0:05:14
epoch [21/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4775 (1.2140) acc 65.6250 (71.1993) lr 2.7103e-04 eta 0:05:12
epoch [21/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2178 (1.2157) acc 75.0000 (71.1184) lr 2.7103e-04 eta 0:05:11
epoch [21/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4023 (1.2135) acc 59.3750 (71.1058) lr 2.7103e-04 eta 0:05:09
epoch [21/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7515 (1.2167) acc 81.2500 (71.0000) lr 2.7103e-04 eta 0:05:07
epoch [21/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2646 (1.2171) acc 62.5000 (70.9756) lr 2.7103e-04 eta 0:05:06
epoch [21/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3096 (1.2148) acc 68.7500 (71.0565) lr 2.7103e-04 eta 0:05:04
epoch [21/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1006 (1.2137) acc 75.0000 (71.1337) lr 2.7103e-04 eta 0:05:03
epoch [21/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3027 (1.2160) acc 71.8750 (71.0653) lr 2.7103e-04 eta 0:05:01
epoch [21/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2979 (1.2158) acc 59.3750 (71.0139) lr 2.7103e-04 eta 0:05:00
epoch [21/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4355 (1.2151) acc 68.7500 (71.0734) lr 2.7103e-04 eta 0:04:58
epoch [21/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2217 (1.2160) acc 71.8750 (71.0372) lr 2.7103e-04 eta 0:04:57
epoch [21/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3223 (1.2144) acc 71.8750 (71.1328) lr 2.7103e-04 eta 0:04:55
epoch [21/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3066 (1.2146) acc 68.7500 (71.0459) lr 2.7103e-04 eta 0:04:53
epoch [21/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.9365 (1.2135) acc 78.1250 (71.0250) lr 1.9098e-04 eta 0:04:52
epoch [22/25] batch [5/250] time 0.289 (0.437) data 0.000 (0.148) loss 1.1562 (1.1337) acc 68.7500 (71.8750) lr 1.9098e-04 eta 0:07:14
epoch [22/25] batch [10/250] time 0.289 (0.363) data 0.000 (0.074) loss 1.3848 (1.0879) acc 71.8750 (74.3750) lr 1.9098e-04 eta 0:05:59
epoch [22/25] batch [15/250] time 0.289 (0.338) data 0.000 (0.049) loss 0.8008 (1.0682) acc 78.1250 (75.0000) lr 1.9098e-04 eta 0:05:33
epoch [22/25] batch [20/250] time 0.289 (0.326) data 0.000 (0.037) loss 1.0391 (1.0723) acc 78.1250 (75.0000) lr 1.9098e-04 eta 0:05:19
epoch [22/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 0.9858 (1.0792) acc 81.2500 (75.2500) lr 1.9098e-04 eta 0:05:10
epoch [22/25] batch [30/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.6699 (1.1316) acc 59.3750 (74.0625) lr 1.9098e-04 eta 0:05:04
epoch [22/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.9336 (1.1819) acc 75.0000 (73.1250) lr 1.9098e-04 eta 0:04:59
epoch [22/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.5713 (1.2059) acc 59.3750 (72.2656) lr 1.9098e-04 eta 0:04:55
epoch [22/25] batch [45/250] time 0.290 (0.306) data 0.000 (0.017) loss 0.9673 (1.1713) acc 81.2500 (73.2639) lr 1.9098e-04 eta 0:04:51
epoch [22/25] batch [50/250] time 0.290 (0.304) data 0.000 (0.015) loss 1.4219 (1.1840) acc 68.7500 (73.0000) lr 1.9098e-04 eta 0:04:48
epoch [22/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.1846 (1.1855) acc 62.5000 (72.6705) lr 1.9098e-04 eta 0:04:45
epoch [22/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.013) loss 0.6963 (1.1729) acc 81.2500 (72.7604) lr 1.9098e-04 eta 0:04:43
epoch [22/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.4844 (1.1665) acc 59.3750 (72.8846) lr 1.9098e-04 eta 0:04:40
epoch [22/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9868 (1.1650) acc 84.3750 (72.8571) lr 1.9098e-04 eta 0:04:38
epoch [22/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.2168 (1.1630) acc 71.8750 (73.0417) lr 1.9098e-04 eta 0:04:36
epoch [22/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.8369 (1.1682) acc 50.0000 (72.7344) lr 1.9098e-04 eta 0:04:34
epoch [22/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6211 (1.1741) acc 56.2500 (72.5000) lr 1.9098e-04 eta 0:04:32
epoch [22/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2451 (1.1809) acc 68.7500 (72.1528) lr 1.9098e-04 eta 0:04:30
epoch [22/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2109 (1.1876) acc 68.7500 (72.1382) lr 1.9098e-04 eta 0:04:28
epoch [22/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1279 (1.1910) acc 71.8750 (71.9688) lr 1.9098e-04 eta 0:04:26
epoch [22/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3438 (1.1905) acc 81.2500 (71.9643) lr 1.9098e-04 eta 0:04:25
epoch [22/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9351 (1.1857) acc 75.0000 (72.1307) lr 1.9098e-04 eta 0:04:23
epoch [22/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4180 (1.1806) acc 65.6250 (72.2554) lr 1.9098e-04 eta 0:04:21
epoch [22/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.7949 (1.1829) acc 81.2500 (72.0573) lr 1.9098e-04 eta 0:04:19
epoch [22/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4336 (1.1881) acc 46.8750 (71.8000) lr 1.9098e-04 eta 0:04:18
epoch [22/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4189 (1.1911) acc 65.6250 (71.7788) lr 1.9098e-04 eta 0:04:16
epoch [22/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4893 (1.1906) acc 65.6250 (71.8981) lr 1.9098e-04 eta 0:04:14
epoch [22/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4609 (1.1949) acc 68.7500 (71.9420) lr 1.9098e-04 eta 0:04:13
epoch [22/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6816 (1.1947) acc 62.5000 (71.9612) lr 1.9098e-04 eta 0:04:11
epoch [22/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.6650 (1.1954) acc 87.5000 (71.9167) lr 1.9098e-04 eta 0:04:09
epoch [22/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7617 (1.1908) acc 84.3750 (72.0565) lr 1.9098e-04 eta 0:04:08
epoch [22/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4590 (1.1907) acc 71.8750 (72.0703) lr 1.9098e-04 eta 0:04:06
epoch [22/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1709 (1.1833) acc 62.5000 (72.1402) lr 1.9098e-04 eta 0:04:05
epoch [22/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3252 (1.1838) acc 71.8750 (72.0956) lr 1.9098e-04 eta 0:04:03
epoch [22/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0605 (1.1803) acc 71.8750 (72.1964) lr 1.9098e-04 eta 0:04:02
epoch [22/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5244 (1.1878) acc 65.6250 (72.0486) lr 1.9098e-04 eta 0:04:00
epoch [22/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2402 (1.1869) acc 62.5000 (72.0608) lr 1.9098e-04 eta 0:03:58
epoch [22/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2402 (1.1938) acc 71.8750 (71.9572) lr 1.9098e-04 eta 0:03:57
epoch [22/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0078 (1.1934) acc 78.1250 (71.9551) lr 1.9098e-04 eta 0:03:55
epoch [22/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1865 (1.1921) acc 78.1250 (71.8906) lr 1.9098e-04 eta 0:03:54
epoch [22/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3516 (1.1967) acc 71.8750 (71.8598) lr 1.9098e-04 eta 0:03:52
epoch [22/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9639 (1.1940) acc 78.1250 (71.9792) lr 1.9098e-04 eta 0:03:51
epoch [22/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1826 (1.1966) acc 68.7500 (71.8750) lr 1.9098e-04 eta 0:03:49
epoch [22/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.7515 (1.1980) acc 78.1250 (71.8040) lr 1.9098e-04 eta 0:03:48
epoch [22/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2207 (1.1957) acc 71.8750 (71.7917) lr 1.9098e-04 eta 0:03:46
epoch [22/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4160 (1.1980) acc 75.0000 (71.7935) lr 1.9098e-04 eta 0:03:45
epoch [22/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 1.2002 (1.1988) acc 78.1250 (71.7154) lr 1.9098e-04 eta 0:03:43
epoch [22/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0332 (1.1970) acc 81.2500 (71.7578) lr 1.9098e-04 eta 0:03:42
epoch [22/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3359 (1.1963) acc 71.8750 (71.7602) lr 1.9098e-04 eta 0:03:40
epoch [22/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 0.9204 (1.1971) acc 68.7500 (71.6750) lr 1.2369e-04 eta 0:03:39
epoch [23/25] batch [5/250] time 0.291 (0.438) data 0.000 (0.148) loss 1.1807 (1.2666) acc 71.8750 (73.7500) lr 1.2369e-04 eta 0:05:26
epoch [23/25] batch [10/250] time 0.289 (0.364) data 0.000 (0.074) loss 1.2881 (1.3146) acc 68.7500 (70.3125) lr 1.2369e-04 eta 0:04:29
epoch [23/25] batch [15/250] time 0.290 (0.339) data 0.000 (0.050) loss 1.2188 (1.2785) acc 71.8750 (71.4583) lr 1.2369e-04 eta 0:04:09
epoch [23/25] batch [20/250] time 0.290 (0.327) data 0.000 (0.037) loss 1.6797 (1.2024) acc 59.3750 (71.7188) lr 1.2369e-04 eta 0:03:58
epoch [23/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 0.6567 (1.2009) acc 81.2500 (71.2500) lr 1.2369e-04 eta 0:03:51
epoch [23/25] batch [30/250] time 0.289 (0.314) data 0.000 (0.025) loss 0.8550 (1.1678) acc 78.1250 (72.2917) lr 1.2369e-04 eta 0:03:46
epoch [23/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.021) loss 1.2988 (1.1746) acc 68.7500 (72.2321) lr 1.2369e-04 eta 0:03:42
epoch [23/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.2598 (1.1666) acc 68.7500 (71.9531) lr 1.2369e-04 eta 0:03:38
epoch [23/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.7148 (1.1632) acc 65.6250 (72.5694) lr 1.2369e-04 eta 0:03:35
epoch [23/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.8740 (1.1342) acc 78.1250 (73.1250) lr 1.2369e-04 eta 0:03:32
epoch [23/25] batch [55/250] time 0.290 (0.303) data 0.001 (0.014) loss 0.9746 (1.1400) acc 71.8750 (72.7273) lr 1.2369e-04 eta 0:03:30
epoch [23/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.8755 (1.1320) acc 78.1250 (72.9688) lr 1.2369e-04 eta 0:03:28
epoch [23/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.9331 (1.1187) acc 78.1250 (73.2212) lr 1.2369e-04 eta 0:03:25
epoch [23/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2686 (1.1218) acc 65.6250 (73.0357) lr 1.2369e-04 eta 0:03:23
epoch [23/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1475 (1.1143) acc 78.1250 (73.4167) lr 1.2369e-04 eta 0:03:21
epoch [23/25] batch [80/250] time 0.289 (0.299) data 0.001 (0.010) loss 1.1182 (1.1233) acc 68.7500 (73.0078) lr 1.2369e-04 eta 0:03:20
epoch [23/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1621 (1.1321) acc 75.0000 (72.6838) lr 1.2369e-04 eta 0:03:18
epoch [23/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0332 (1.1258) acc 68.7500 (72.7778) lr 1.2369e-04 eta 0:03:16
epoch [23/25] batch [95/250] time 0.290 (0.297) data 0.001 (0.008) loss 1.4873 (1.1314) acc 71.8750 (72.6645) lr 1.2369e-04 eta 0:03:14
epoch [23/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2686 (1.1353) acc 68.7500 (72.3750) lr 1.2369e-04 eta 0:03:12
epoch [23/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1357 (1.1339) acc 71.8750 (72.1429) lr 1.2369e-04 eta 0:03:11
epoch [23/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0234 (1.1329) acc 75.0000 (72.4148) lr 1.2369e-04 eta 0:03:09
epoch [23/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.3467 (1.1311) acc 75.0000 (72.4728) lr 1.2369e-04 eta 0:03:07
epoch [23/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.1963 (1.1300) acc 78.1250 (72.4479) lr 1.2369e-04 eta 0:03:06
epoch [23/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1816 (1.1354) acc 78.1250 (72.4000) lr 1.2369e-04 eta 0:03:04
epoch [23/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8159 (1.1301) acc 75.0000 (72.3558) lr 1.2369e-04 eta 0:03:02
epoch [23/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5586 (1.1329) acc 65.6250 (72.5000) lr 1.2369e-04 eta 0:03:01
epoch [23/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1787 (1.1273) acc 78.1250 (72.5893) lr 1.2369e-04 eta 0:02:59
epoch [23/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.1396 (1.1335) acc 71.8750 (72.5216) lr 1.2369e-04 eta 0:02:58
epoch [23/25] batch [150/250] time 0.291 (0.294) data 0.001 (0.005) loss 0.8188 (1.1327) acc 81.2500 (72.5833) lr 1.2369e-04 eta 0:02:56
epoch [23/25] batch [155/250] time 0.290 (0.294) data 0.001 (0.005) loss 0.7549 (1.1345) acc 84.3750 (72.6008) lr 1.2369e-04 eta 0:02:54
epoch [23/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2637 (1.1407) acc 71.8750 (72.4609) lr 1.2369e-04 eta 0:02:53
epoch [23/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1367 (1.1414) acc 78.1250 (72.5379) lr 1.2369e-04 eta 0:02:51
epoch [23/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8955 (1.1423) acc 78.1250 (72.4632) lr 1.2369e-04 eta 0:02:50
epoch [23/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.9780 (1.1500) acc 81.2500 (72.2857) lr 1.2369e-04 eta 0:02:48
epoch [23/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1846 (1.1478) acc 75.0000 (72.3438) lr 1.2369e-04 eta 0:02:47
epoch [23/25] batch [185/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4785 (1.1472) acc 68.7500 (72.3818) lr 1.2369e-04 eta 0:02:45
epoch [23/25] batch [190/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0693 (1.1457) acc 65.6250 (72.2697) lr 1.2369e-04 eta 0:02:44
epoch [23/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7939 (1.1504) acc 65.6250 (72.2917) lr 1.2369e-04 eta 0:02:42
epoch [23/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2842 (1.1519) acc 81.2500 (72.2188) lr 1.2369e-04 eta 0:02:41
epoch [23/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4434 (1.1517) acc 59.3750 (72.1646) lr 1.2369e-04 eta 0:02:39
epoch [23/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1367 (1.1464) acc 75.0000 (72.3214) lr 1.2369e-04 eta 0:02:38
epoch [23/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2168 (1.1461) acc 75.0000 (72.3401) lr 1.2369e-04 eta 0:02:36
epoch [23/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2266 (1.1509) acc 71.8750 (72.2585) lr 1.2369e-04 eta 0:02:35
epoch [23/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9761 (1.1524) acc 71.8750 (72.1806) lr 1.2369e-04 eta 0:02:33
epoch [23/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3506 (1.1500) acc 68.7500 (72.2690) lr 1.2369e-04 eta 0:02:32
epoch [23/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.004) loss 1.6133 (1.1496) acc 71.8750 (72.3005) lr 1.2369e-04 eta 0:02:30
epoch [23/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5449 (1.1496) acc 59.3750 (72.3047) lr 1.2369e-04 eta 0:02:29
epoch [23/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.8320 (1.1508) acc 81.2500 (72.3597) lr 1.2369e-04 eta 0:02:27
epoch [23/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1318 (1.1485) acc 78.1250 (72.4375) lr 7.0224e-05 eta 0:02:26
epoch [24/25] batch [5/250] time 0.290 (0.434) data 0.000 (0.143) loss 0.6333 (0.9954) acc 87.5000 (74.3750) lr 7.0224e-05 eta 0:03:34
epoch [24/25] batch [10/250] time 0.289 (0.362) data 0.000 (0.072) loss 1.1426 (1.1110) acc 78.1250 (75.3125) lr 7.0224e-05 eta 0:02:57
epoch [24/25] batch [15/250] time 0.289 (0.338) data 0.000 (0.048) loss 1.4014 (1.1336) acc 65.6250 (74.3750) lr 7.0224e-05 eta 0:02:43
epoch [24/25] batch [20/250] time 0.291 (0.326) data 0.000 (0.036) loss 1.6211 (1.1771) acc 50.0000 (71.7188) lr 7.0224e-05 eta 0:02:36
epoch [24/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.0059 (1.1822) acc 78.1250 (72.6250) lr 7.0224e-05 eta 0:02:31
epoch [24/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.0098 (1.1809) acc 78.1250 (72.5000) lr 7.0224e-05 eta 0:02:27
epoch [24/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.1836 (1.2198) acc 81.2500 (72.7679) lr 7.0224e-05 eta 0:02:24
epoch [24/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.7549 (1.2376) acc 62.5000 (71.8750) lr 7.0224e-05 eta 0:02:21
epoch [24/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.2734 (1.2258) acc 71.8750 (72.2222) lr 7.0224e-05 eta 0:02:18
epoch [24/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.2754 (1.2269) acc 75.0000 (71.9375) lr 7.0224e-05 eta 0:02:16
epoch [24/25] batch [55/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.1006 (1.2253) acc 71.8750 (71.8750) lr 7.0224e-05 eta 0:02:14
epoch [24/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.2422 (1.2322) acc 71.8750 (71.9792) lr 7.0224e-05 eta 0:02:12
epoch [24/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 2.0430 (1.2355) acc 53.1250 (71.8750) lr 7.0224e-05 eta 0:02:10
epoch [24/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.011) loss 0.6841 (1.2273) acc 87.5000 (72.1875) lr 7.0224e-05 eta 0:02:08
epoch [24/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1680 (1.2224) acc 68.7500 (72.1250) lr 7.0224e-05 eta 0:02:06
epoch [24/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.3564 (1.2196) acc 93.7500 (72.3047) lr 7.0224e-05 eta 0:02:05
epoch [24/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2607 (1.2228) acc 84.3750 (72.3897) lr 7.0224e-05 eta 0:02:03
epoch [24/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5439 (1.2201) acc 71.8750 (72.3611) lr 7.0224e-05 eta 0:02:01
epoch [24/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9595 (1.2112) acc 84.3750 (72.5987) lr 7.0224e-05 eta 0:02:00
epoch [24/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1807 (1.2092) acc 78.1250 (72.5312) lr 7.0224e-05 eta 0:01:58
epoch [24/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0000 (1.2156) acc 62.5000 (72.1131) lr 7.0224e-05 eta 0:01:56
epoch [24/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4004 (1.2176) acc 68.7500 (71.8750) lr 7.0224e-05 eta 0:01:55
epoch [24/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.2324 (1.2131) acc 71.8750 (71.6576) lr 7.0224e-05 eta 0:01:53
epoch [24/25] batch [120/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.3662 (1.2099) acc 62.5000 (71.6406) lr 7.0224e-05 eta 0:01:52
epoch [24/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2285 (1.2074) acc 68.7500 (71.5250) lr 7.0224e-05 eta 0:01:50
epoch [24/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8306 (1.1999) acc 84.3750 (71.7548) lr 7.0224e-05 eta 0:01:49
epoch [24/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0107 (1.1970) acc 68.7500 (71.8287) lr 7.0224e-05 eta 0:01:47
epoch [24/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4541 (1.1996) acc 68.7500 (71.7188) lr 7.0224e-05 eta 0:01:45
epoch [24/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9111 (1.2000) acc 75.0000 (71.5948) lr 7.0224e-05 eta 0:01:44
epoch [24/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4736 (1.2045) acc 65.6250 (71.4792) lr 7.0224e-05 eta 0:01:42
epoch [24/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4551 (1.2055) acc 62.5000 (71.3105) lr 7.0224e-05 eta 0:01:41
epoch [24/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.6440 (1.1938) acc 84.3750 (71.5234) lr 7.0224e-05 eta 0:01:39
epoch [24/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8462 (1.1874) acc 84.3750 (71.7992) lr 7.0224e-05 eta 0:01:38
epoch [24/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.6602 (1.1884) acc 71.8750 (71.8015) lr 7.0224e-05 eta 0:01:36
epoch [24/25] batch [175/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5029 (1.1913) acc 68.7500 (71.6964) lr 7.0224e-05 eta 0:01:35
epoch [24/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7344 (1.1877) acc 84.3750 (71.7361) lr 7.0224e-05 eta 0:01:33
epoch [24/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6768 (1.1860) acc 68.7500 (71.8074) lr 7.0224e-05 eta 0:01:32
epoch [24/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9160 (1.1809) acc 75.0000 (72.0066) lr 7.0224e-05 eta 0:01:30
epoch [24/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9795 (1.1744) acc 75.0000 (72.1474) lr 7.0224e-05 eta 0:01:29
epoch [24/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2344 (1.1765) acc 78.1250 (72.2031) lr 7.0224e-05 eta 0:01:27
epoch [24/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3330 (1.1733) acc 65.6250 (72.2256) lr 7.0224e-05 eta 0:01:26
epoch [24/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.5391 (1.1676) acc 87.5000 (72.3661) lr 7.0224e-05 eta 0:01:24
epoch [24/25] batch [215/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.8276 (1.1662) acc 87.5000 (72.4273) lr 7.0224e-05 eta 0:01:23
epoch [24/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3564 (1.1666) acc 68.7500 (72.4148) lr 7.0224e-05 eta 0:01:21
epoch [24/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2412 (1.1670) acc 71.8750 (72.3750) lr 7.0224e-05 eta 0:01:20
epoch [24/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0527 (1.1621) acc 71.8750 (72.4728) lr 7.0224e-05 eta 0:01:18
epoch [24/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.003) loss 0.6260 (1.1553) acc 87.5000 (72.6330) lr 7.0224e-05 eta 0:01:17
epoch [24/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.8735 (1.1552) acc 81.2500 (72.6823) lr 7.0224e-05 eta 0:01:15
epoch [24/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6533 (1.1518) acc 71.8750 (72.7934) lr 7.0224e-05 eta 0:01:14
epoch [24/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.4678 (1.1497) acc 71.8750 (72.8125) lr 3.1417e-05 eta 0:01:13
epoch [25/25] batch [5/250] time 0.289 (0.438) data 0.000 (0.149) loss 1.0889 (1.1535) acc 75.0000 (75.0000) lr 3.1417e-05 eta 0:01:47
epoch [25/25] batch [10/250] time 0.289 (0.364) data 0.000 (0.074) loss 1.1807 (1.1647) acc 75.0000 (74.6875) lr 3.1417e-05 eta 0:01:27
epoch [25/25] batch [15/250] time 0.289 (0.339) data 0.000 (0.050) loss 1.3047 (1.1503) acc 75.0000 (74.1667) lr 3.1417e-05 eta 0:01:19
epoch [25/25] batch [20/250] time 0.289 (0.327) data 0.000 (0.037) loss 1.3682 (1.1280) acc 65.6250 (73.5938) lr 3.1417e-05 eta 0:01:15
epoch [25/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 2.0605 (1.1506) acc 53.1250 (73.3750) lr 3.1417e-05 eta 0:01:11
epoch [25/25] batch [30/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.0488 (1.1555) acc 75.0000 (73.1250) lr 3.1417e-05 eta 0:01:09
epoch [25/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.021) loss 1.0908 (1.1660) acc 68.7500 (72.4107) lr 3.1417e-05 eta 0:01:06
epoch [25/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.1348 (1.1486) acc 68.7500 (72.8125) lr 3.1417e-05 eta 0:01:04
epoch [25/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.5283 (1.1357) acc 93.7500 (73.1944) lr 3.1417e-05 eta 0:01:02
epoch [25/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.2363 (1.1283) acc 65.6250 (73.1875) lr 3.1417e-05 eta 0:01:00
epoch [25/25] batch [55/250] time 0.288 (0.303) data 0.000 (0.014) loss 1.0127 (1.1234) acc 71.8750 (73.2386) lr 3.1417e-05 eta 0:00:59
epoch [25/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.9106 (1.1187) acc 81.2500 (73.4896) lr 3.1417e-05 eta 0:00:57
epoch [25/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.8525 (1.1196) acc 84.3750 (73.5577) lr 3.1417e-05 eta 0:00:55
epoch [25/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.7295 (1.1234) acc 59.3750 (73.4375) lr 3.1417e-05 eta 0:00:53
epoch [25/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.0723 (1.1349) acc 78.1250 (73.1667) lr 3.1417e-05 eta 0:00:52
epoch [25/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.010) loss 1.0625 (1.1293) acc 75.0000 (73.3203) lr 3.1417e-05 eta 0:00:50
epoch [25/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1426 (1.1306) acc 68.7500 (73.1985) lr 3.1417e-05 eta 0:00:49
epoch [25/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.009) loss 1.4805 (1.1503) acc 59.3750 (72.8819) lr 3.1417e-05 eta 0:00:47
epoch [25/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0928 (1.1556) acc 78.1250 (72.7303) lr 3.1417e-05 eta 0:00:46
epoch [25/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.6611 (1.1465) acc 84.3750 (73.0000) lr 3.1417e-05 eta 0:00:44
epoch [25/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8486 (1.1445) acc 68.7500 (73.0952) lr 3.1417e-05 eta 0:00:42
epoch [25/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8408 (1.1483) acc 56.2500 (73.0398) lr 3.1417e-05 eta 0:00:41
epoch [25/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9355 (1.1473) acc 71.8750 (72.9348) lr 3.1417e-05 eta 0:00:39
epoch [25/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.6812 (1.1443) acc 87.5000 (73.0469) lr 3.1417e-05 eta 0:00:38
epoch [25/25] batch [125/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2168 (1.1497) acc 65.6250 (73.0750) lr 3.1417e-05 eta 0:00:36
epoch [25/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0596 (1.1483) acc 68.7500 (73.0288) lr 3.1417e-05 eta 0:00:35
epoch [25/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8467 (1.1425) acc 84.3750 (73.0093) lr 3.1417e-05 eta 0:00:33
epoch [25/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.1367 (1.1415) acc 65.6250 (72.9464) lr 3.1417e-05 eta 0:00:32
epoch [25/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7666 (1.1431) acc 78.1250 (72.8233) lr 3.1417e-05 eta 0:00:30
epoch [25/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8853 (1.1416) acc 75.0000 (72.8125) lr 3.1417e-05 eta 0:00:29
epoch [25/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1436 (1.1379) acc 71.8750 (72.9032) lr 3.1417e-05 eta 0:00:27
epoch [25/25] batch [160/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.0781 (1.1360) acc 78.1250 (73.0273) lr 3.1417e-05 eta 0:00:26
epoch [25/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0996 (1.1418) acc 71.8750 (72.8220) lr 3.1417e-05 eta 0:00:24
epoch [25/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3447 (1.1392) acc 62.5000 (72.8493) lr 3.1417e-05 eta 0:00:23
epoch [25/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.1982 (1.1310) acc 75.0000 (73.0000) lr 3.1417e-05 eta 0:00:22
epoch [25/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3203 (1.1347) acc 68.7500 (72.8993) lr 3.1417e-05 eta 0:00:20
epoch [25/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5781 (1.1352) acc 62.5000 (72.9223) lr 3.1417e-05 eta 0:00:19
epoch [25/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2480 (1.1374) acc 68.7500 (72.8289) lr 3.1417e-05 eta 0:00:17
epoch [25/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5469 (1.1377) acc 68.7500 (72.8205) lr 3.1417e-05 eta 0:00:16
epoch [25/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1064 (1.1385) acc 68.7500 (72.8750) lr 3.1417e-05 eta 0:00:14
epoch [25/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8574 (1.1360) acc 68.7500 (72.8506) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1084 (1.1372) acc 68.7500 (72.7530) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.7549 (1.1379) acc 56.2500 (72.7035) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9497 (1.1380) acc 81.2500 (72.7131) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [225/250] time 0.290 (0.292) data 0.000 (0.004) loss 1.4316 (1.1437) acc 62.5000 (72.5694) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4199 (1.1435) acc 68.7500 (72.5815) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3330 (1.1409) acc 65.6250 (72.5399) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2744 (1.1384) acc 71.8750 (72.6432) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3730 (1.1403) acc 71.8750 (72.6148) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0137 (1.1406) acc 68.7500 (72.5375) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:06<05:12,  6.39s/it]  4%|▍         | 2/50 [00:07<02:25,  3.03s/it]  6%|▌         | 3/50 [00:07<01:31,  1.96s/it]  8%|▊         | 4/50 [00:08<01:07,  1.46s/it] 10%|█         | 5/50 [00:09<00:52,  1.18s/it] 12%|█▏        | 6/50 [00:09<00:44,  1.01s/it] 14%|█▍        | 7/50 [00:10<00:38,  1.11it/s] 16%|█▌        | 8/50 [00:11<00:34,  1.21it/s] 18%|█▊        | 9/50 [00:11<00:32,  1.27it/s] 20%|██        | 10/50 [00:12<00:30,  1.33it/s] 22%|██▏       | 11/50 [00:13<00:28,  1.37it/s] 24%|██▍       | 12/50 [00:13<00:27,  1.40it/s] 26%|██▌       | 13/50 [00:14<00:26,  1.42it/s] 28%|██▊       | 14/50 [00:15<00:25,  1.43it/s] 30%|███       | 15/50 [00:15<00:24,  1.45it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.45it/s] 34%|███▍      | 17/50 [00:17<00:22,  1.46it/s] 36%|███▌      | 18/50 [00:17<00:21,  1.46it/s] 38%|███▊      | 19/50 [00:18<00:21,  1.47it/s] 40%|████      | 20/50 [00:19<00:20,  1.46it/s] 42%|████▏     | 21/50 [00:20<00:19,  1.46it/s] 44%|████▍     | 22/50 [00:20<00:19,  1.47it/s] 46%|████▌     | 23/50 [00:21<00:18,  1.47it/s] 48%|████▊     | 24/50 [00:22<00:17,  1.47it/s] 50%|█████     | 25/50 [00:22<00:16,  1.47it/s] 52%|█████▏    | 26/50 [00:23<00:16,  1.47it/s] 54%|█████▍    | 27/50 [00:24<00:15,  1.47it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.47it/s] 58%|█████▊    | 29/50 [00:25<00:14,  1.47it/s] 60%|██████    | 30/50 [00:26<00:13,  1.47it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.47it/s] 64%|██████▍   | 32/50 [00:27<00:12,  1.47it/s] 66%|██████▌   | 33/50 [00:28<00:11,  1.47it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.47it/s] 70%|███████   | 35/50 [00:29<00:10,  1.47it/s] 72%|███████▏  | 36/50 [00:30<00:09,  1.47it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.47it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.47it/s] 78%|███████▊  | 39/50 [00:32<00:07,  1.47it/s] 80%|████████  | 40/50 [00:32<00:06,  1.47it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.47it/s] 84%|████████▍ | 42/50 [00:34<00:05,  1.47it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.47it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.47it/s] 90%|█████████ | 45/50 [00:36<00:03,  1.47it/s] 92%|█████████▏| 46/50 [00:36<00:02,  1.47it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.47it/s] 96%|█████████▌| 48/50 [00:38<00:01,  1.47it/s] 98%|█████████▊| 49/50 [00:39<00:00,  1.47it/s]100%|██████████| 50/50 [00:39<00:00,  1.47it/s]100%|██████████| 50/50 [00:39<00:00,  1.25it/s]
=> result
* total: 25,000
* correct: 19,329
* accuracy: 77.3%
* error: 22.7%
* macro_f1: 77.0%
Elapsed: 0:31:09
Run this job and save the output to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'rooster', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'American robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'American dipper', 'kite (bird of prey)', 'bald eagle', 'vulture', 'great grey owl', 'fire salamander', 'smooth newt', 'newt', 'spotted salamander', 'axolotl', 'American bullfrog', 'tree frog', 'tailed frog', 'loggerhead sea turtle', 'leatherback sea turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'green iguana', 'Carolina anole', 'desert grassland whiptail lizard', 'agama', 'frilled-necked lizard', 'alligator lizard', 'Gila monster', 'European green lizard', 'chameleon', 'Komodo dragon', 'Nile crocodile', 'American alligator', 'triceratops', 'worm snake', 'ring-necked snake', 'eastern hog-nosed snake', 'smooth green snake', 'kingsnake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'African rock python', 'Indian cobra', 'green mamba', 'sea snake', 'Saharan horned viper', 'eastern diamondback rattlesnake', 'sidewinder rattlesnake', 'trilobite', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie grouse', 'peafowl', 'quail', 'partridge', 'african grey parrot', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'duck', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'red king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'great egret', 'bittern bird', 'crane bird', 'limpkin', 'common gallinule', 'American coot', 'bustard', 'ruddy turnstone', 'dunlin', 'common redshank', 'dowitcher', 'oystercatcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'Papillon', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Otterhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'English Setter', 'Irish Setter', 'Gordon Setter', 'Brittany dog', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniel', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael dog', 'Malinois', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Shetland Sheepdog', 'collie', 'Border Collie', 'Bouvier des Flandres dog', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland dog', 'Great Pyrenees dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Keeshond', 'brussels griffon', 'Pembroke Welsh Corgi', 'Cardigan Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog (xoloitzcuintli)', 'grey wolf', 'Alaskan tundra wolf', 'red wolf or maned wolf', 'coyote', 'dingo', 'dhole', 'African wild dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'polar bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ladybug', 'ground beetle', 'longhorn beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket insect', 'stick insect', 'cockroach', 'praying mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'red admiral butterfly', 'ringlet butterfly', 'monarch butterfly', 'small white butterfly', 'sulphur butterfly', 'gossamer-winged butterfly', 'starfish', 'sea urchin', 'sea cucumber', 'cottontail rabbit', 'hare', 'Angora rabbit', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'common sorrel horse', 'zebra', 'pig', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram (adult male sheep)', 'bighorn sheep', 'Alpine ibex', 'hartebeest', 'impala (antelope)', 'gazelle', 'arabian camel', 'llama', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas monkey', 'baboon', 'macaque', 'langur', 'black-and-white colobus', 'proboscis monkey', 'marmoset', 'white-headed capuchin', 'howler monkey', 'titi monkey', "Geoffroy's spider monkey", 'common squirrel monkey', 'ring-tailed lemur', 'indri', 'Asian elephant', 'African bush elephant', 'red panda', 'giant panda', 'snoek fish', 'eel', 'silver salmon', 'rock beauty fish', 'clownfish', 'sturgeon', 'gar fish', 'lionfish', 'pufferfish', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'altar', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'apron', 'trash can', 'assault rifle', 'backpack', 'bakery', 'balance beam', 'balloon', 'ballpoint pen', 'Band-Aid', 'banjo', 'baluster / handrail', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military hat (bearskin or shako)', 'beer bottle', 'beer glass', 'bell tower', 'baby bib', 'tandem bicycle', 'bikini', 'ring binder', 'binoculars', 'birdhouse', 'boathouse', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bottle cap', 'hunting bow', 'bow tie', 'brass memorial plaque', 'bra', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'canoe', 'can opener', 'cardigan', 'car mirror', 'carousel', 'tool kit', 'cardboard box / carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'mobile phone', 'chain', 'chain-link fence', 'chain mail', 'chainsaw', 'storage chest', 'chiffonier', 'bell or wind chime', 'china cabinet', 'Christmas stocking', 'church', 'movie theater', 'cleaver']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a tench.', 'a photo of a goldfish.', 'a photo of a great white shark.', 'a photo of a tiger shark.', 'a photo of a hammerhead shark.', 'a photo of a electric ray.', 'a photo of a stingray.', 'a photo of a rooster.', 'a photo of a hen.', 'a photo of a ostrich.', 'a photo of a brambling.', 'a photo of a goldfinch.', 'a photo of a house finch.', 'a photo of a junco.', 'a photo of a indigo bunting.', 'a photo of a American robin.', 'a photo of a bulbul.', 'a photo of a jay.', 'a photo of a magpie.', 'a photo of a chickadee.', 'a photo of a American dipper.', 'a photo of a kite (bird of prey).', 'a photo of a bald eagle.', 'a photo of a vulture.', 'a photo of a great grey owl.', 'a photo of a fire salamander.', 'a photo of a smooth newt.', 'a photo of a newt.', 'a photo of a spotted salamander.', 'a photo of a axolotl.', 'a photo of a American bullfrog.', 'a photo of a tree frog.', 'a photo of a tailed frog.', 'a photo of a loggerhead sea turtle.', 'a photo of a leatherback sea turtle.', 'a photo of a mud turtle.', 'a photo of a terrapin.', 'a photo of a box turtle.', 'a photo of a banded gecko.', 'a photo of a green iguana.', 'a photo of a Carolina anole.', 'a photo of a desert grassland whiptail lizard.', 'a photo of a agama.', 'a photo of a frilled-necked lizard.', 'a photo of a alligator lizard.', 'a photo of a Gila monster.', 'a photo of a European green lizard.', 'a photo of a chameleon.', 'a photo of a Komodo dragon.', 'a photo of a Nile crocodile.', 'a photo of a American alligator.', 'a photo of a triceratops.', 'a photo of a worm snake.', 'a photo of a ring-necked snake.', 'a photo of a eastern hog-nosed snake.', 'a photo of a smooth green snake.', 'a photo of a kingsnake.', 'a photo of a garter snake.', 'a photo of a water snake.', 'a photo of a vine snake.', 'a photo of a night snake.', 'a photo of a boa constrictor.', 'a photo of a African rock python.', 'a photo of a Indian cobra.', 'a photo of a green mamba.', 'a photo of a sea snake.', 'a photo of a Saharan horned viper.', 'a photo of a eastern diamondback rattlesnake.', 'a photo of a sidewinder rattlesnake.', 'a photo of a trilobite.', 'a photo of a harvestman.', 'a photo of a scorpion.', 'a photo of a yellow garden spider.', 'a photo of a barn spider.', 'a photo of a European garden spider.', 'a photo of a southern black widow.', 'a photo of a tarantula.', 'a photo of a wolf spider.', 'a photo of a tick.', 'a photo of a centipede.', 'a photo of a black grouse.', 'a photo of a ptarmigan.', 'a photo of a ruffed grouse.', 'a photo of a prairie grouse.', 'a photo of a peafowl.', 'a photo of a quail.', 'a photo of a partridge.', 'a photo of a african grey parrot.', 'a photo of a macaw.', 'a photo of a sulphur-crested cockatoo.', 'a photo of a lorikeet.', 'a photo of a coucal.', 'a photo of a bee eater.', 'a photo of a hornbill.', 'a photo of a hummingbird.', 'a photo of a jacamar.', 'a photo of a toucan.', 'a photo of a duck.', 'a photo of a red-breasted merganser.', 'a photo of a goose.', 'a photo of a black swan.', 'a photo of a tusker.', 'a photo of a echidna.', 'a photo of a platypus.', 'a photo of a wallaby.', 'a photo of a koala.', 'a photo of a wombat.', 'a photo of a jellyfish.', 'a photo of a sea anemone.', 'a photo of a brain coral.', 'a photo of a flatworm.', 'a photo of a nematode.', 'a photo of a conch.', 'a photo of a snail.', 'a photo of a slug.', 'a photo of a sea slug.', 'a photo of a chiton.', 'a photo of a chambered nautilus.', 'a photo of a Dungeness crab.', 'a photo of a rock crab.', 'a photo of a fiddler crab.', 'a photo of a red king crab.', 'a photo of a American lobster.', 'a photo of a spiny lobster.', 'a photo of a crayfish.', 'a photo of a hermit crab.', 'a photo of a isopod.', 'a photo of a white stork.', 'a photo of a black stork.', 'a photo of a spoonbill.', 'a photo of a flamingo.', 'a photo of a little blue heron.', 'a photo of a great egret.', 'a photo of a bittern bird.', 'a photo of a crane bird.', 'a photo of a limpkin.', 'a photo of a common gallinule.', 'a photo of a American coot.', 'a photo of a bustard.', 'a photo of a ruddy turnstone.', 'a photo of a dunlin.', 'a photo of a common redshank.', 'a photo of a dowitcher.', 'a photo of a oystercatcher.', 'a photo of a pelican.', 'a photo of a king penguin.', 'a photo of a albatross.', 'a photo of a grey whale.', 'a photo of a killer whale.', 'a photo of a dugong.', 'a photo of a sea lion.', 'a photo of a Chihuahua.', 'a photo of a Japanese Chin.', 'a photo of a Maltese.', 'a photo of a Pekingese.', 'a photo of a Shih Tzu.', 'a photo of a King Charles Spaniel.', 'a photo of a Papillon.', 'a photo of a toy terrier.', 'a photo of a Rhodesian Ridgeback.', 'a photo of a Afghan Hound.', 'a photo of a Basset Hound.', 'a photo of a Beagle.', 'a photo of a Bloodhound.', 'a photo of a Bluetick Coonhound.', 'a photo of a Black and Tan Coonhound.', 'a photo of a Treeing Walker Coonhound.', 'a photo of a English foxhound.', 'a photo of a Redbone Coonhound.', 'a photo of a borzoi.', 'a photo of a Irish Wolfhound.', 'a photo of a Italian Greyhound.', 'a photo of a Whippet.', 'a photo of a Ibizan Hound.', 'a photo of a Norwegian Elkhound.', 'a photo of a Otterhound.', 'a photo of a Saluki.', 'a photo of a Scottish Deerhound.', 'a photo of a Weimaraner.', 'a photo of a Staffordshire Bull Terrier.', 'a photo of a American Staffordshire Terrier.', 'a photo of a Bedlington Terrier.', 'a photo of a Border Terrier.', 'a photo of a Kerry Blue Terrier.', 'a photo of a Irish Terrier.', 'a photo of a Norfolk Terrier.', 'a photo of a Norwich Terrier.', 'a photo of a Yorkshire Terrier.', 'a photo of a Wire Fox Terrier.', 'a photo of a Lakeland Terrier.', 'a photo of a Sealyham Terrier.', 'a photo of a Airedale Terrier.', 'a photo of a Cairn Terrier.', 'a photo of a Australian Terrier.', 'a photo of a Dandie Dinmont Terrier.', 'a photo of a Boston Terrier.', 'a photo of a Miniature Schnauzer.', 'a photo of a Giant Schnauzer.', 'a photo of a Standard Schnauzer.', 'a photo of a Scottish Terrier.', 'a photo of a Tibetan Terrier.', 'a photo of a Australian Silky Terrier.', 'a photo of a Soft-coated Wheaten Terrier.', 'a photo of a West Highland White Terrier.', 'a photo of a Lhasa Apso.', 'a photo of a Flat-Coated Retriever.', 'a photo of a Curly-coated Retriever.', 'a photo of a Golden Retriever.', 'a photo of a Labrador Retriever.', 'a photo of a Chesapeake Bay Retriever.', 'a photo of a German Shorthaired Pointer.', 'a photo of a Vizsla.', 'a photo of a English Setter.', 'a photo of a Irish Setter.', 'a photo of a Gordon Setter.', 'a photo of a Brittany dog.', 'a photo of a Clumber Spaniel.', 'a photo of a English Springer Spaniel.', 'a photo of a Welsh Springer Spaniel.', 'a photo of a Cocker Spaniel.', 'a photo of a Sussex Spaniel.', 'a photo of a Irish Water Spaniel.', 'a photo of a Kuvasz.', 'a photo of a Schipperke.', 'a photo of a Groenendael dog.', 'a photo of a Malinois.', 'a photo of a Briard.', 'a photo of a Australian Kelpie.', 'a photo of a Komondor.', 'a photo of a Old English Sheepdog.', 'a photo of a Shetland Sheepdog.', 'a photo of a collie.', 'a photo of a Border Collie.', 'a photo of a Bouvier des Flandres dog.', 'a photo of a Rottweiler.', 'a photo of a German Shepherd Dog.', 'a photo of a Dobermann.', 'a photo of a Miniature Pinscher.', 'a photo of a Greater Swiss Mountain Dog.', 'a photo of a Bernese Mountain Dog.', 'a photo of a Appenzeller Sennenhund.', 'a photo of a Entlebucher Sennenhund.', 'a photo of a Boxer.', 'a photo of a Bullmastiff.', 'a photo of a Tibetan Mastiff.', 'a photo of a French Bulldog.', 'a photo of a Great Dane.', 'a photo of a St. Bernard.', 'a photo of a husky.', 'a photo of a Alaskan Malamute.', 'a photo of a Siberian Husky.', 'a photo of a Dalmatian.', 'a photo of a Affenpinscher.', 'a photo of a Basenji.', 'a photo of a pug.', 'a photo of a Leonberger.', 'a photo of a Newfoundland dog.', 'a photo of a Great Pyrenees dog.', 'a photo of a Samoyed.', 'a photo of a Pomeranian.', 'a photo of a Chow Chow.', 'a photo of a Keeshond.', 'a photo of a brussels griffon.', 'a photo of a Pembroke Welsh Corgi.', 'a photo of a Cardigan Welsh Corgi.', 'a photo of a Toy Poodle.', 'a photo of a Miniature Poodle.', 'a photo of a Standard Poodle.', 'a photo of a Mexican hairless dog (xoloitzcuintli).', 'a photo of a grey wolf.', 'a photo of a Alaskan tundra wolf.', 'a photo of a red wolf or maned wolf.', 'a photo of a coyote.', 'a photo of a dingo.', 'a photo of a dhole.', 'a photo of a African wild dog.', 'a photo of a hyena.', 'a photo of a red fox.', 'a photo of a kit fox.', 'a photo of a Arctic fox.', 'a photo of a grey fox.', 'a photo of a tabby cat.', 'a photo of a tiger cat.', 'a photo of a Persian cat.', 'a photo of a Siamese cat.', 'a photo of a Egyptian Mau.', 'a photo of a cougar.', 'a photo of a lynx.', 'a photo of a leopard.', 'a photo of a snow leopard.', 'a photo of a jaguar.', 'a photo of a lion.', 'a photo of a tiger.', 'a photo of a cheetah.', 'a photo of a brown bear.', 'a photo of a American black bear.', 'a photo of a polar bear.', 'a photo of a sloth bear.', 'a photo of a mongoose.', 'a photo of a meerkat.', 'a photo of a tiger beetle.', 'a photo of a ladybug.', 'a photo of a ground beetle.', 'a photo of a longhorn beetle.', 'a photo of a leaf beetle.', 'a photo of a dung beetle.', 'a photo of a rhinoceros beetle.', 'a photo of a weevil.', 'a photo of a fly.', 'a photo of a bee.', 'a photo of a ant.', 'a photo of a grasshopper.', 'a photo of a cricket insect.', 'a photo of a stick insect.', 'a photo of a cockroach.', 'a photo of a praying mantis.', 'a photo of a cicada.', 'a photo of a leafhopper.', 'a photo of a lacewing.', 'a photo of a dragonfly.', 'a photo of a damselfly.', 'a photo of a red admiral butterfly.', 'a photo of a ringlet butterfly.', 'a photo of a monarch butterfly.', 'a photo of a small white butterfly.', 'a photo of a sulphur butterfly.', 'a photo of a gossamer-winged butterfly.', 'a photo of a starfish.', 'a photo of a sea urchin.', 'a photo of a sea cucumber.', 'a photo of a cottontail rabbit.', 'a photo of a hare.', 'a photo of a Angora rabbit.', 'a photo of a hamster.', 'a photo of a porcupine.', 'a photo of a fox squirrel.', 'a photo of a marmot.', 'a photo of a beaver.', 'a photo of a guinea pig.', 'a photo of a common sorrel horse.', 'a photo of a zebra.', 'a photo of a pig.', 'a photo of a wild boar.', 'a photo of a warthog.', 'a photo of a hippopotamus.', 'a photo of a ox.', 'a photo of a water buffalo.', 'a photo of a bison.', 'a photo of a ram (adult male sheep).', 'a photo of a bighorn sheep.', 'a photo of a Alpine ibex.', 'a photo of a hartebeest.', 'a photo of a impala (antelope).', 'a photo of a gazelle.', 'a photo of a arabian camel.', 'a photo of a llama.', 'a photo of a weasel.', 'a photo of a mink.', 'a photo of a European polecat.', 'a photo of a black-footed ferret.', 'a photo of a otter.', 'a photo of a skunk.', 'a photo of a badger.', 'a photo of a armadillo.', 'a photo of a three-toed sloth.', 'a photo of a orangutan.', 'a photo of a gorilla.', 'a photo of a chimpanzee.', 'a photo of a gibbon.', 'a photo of a siamang.', 'a photo of a guenon.', 'a photo of a patas monkey.', 'a photo of a baboon.', 'a photo of a macaque.', 'a photo of a langur.', 'a photo of a black-and-white colobus.', 'a photo of a proboscis monkey.', 'a photo of a marmoset.', 'a photo of a white-headed capuchin.', 'a photo of a howler monkey.', 'a photo of a titi monkey.', "a photo of a Geoffroy's spider monkey.", 'a photo of a common squirrel monkey.', 'a photo of a ring-tailed lemur.', 'a photo of a indri.', 'a photo of a Asian elephant.', 'a photo of a African bush elephant.', 'a photo of a red panda.', 'a photo of a giant panda.', 'a photo of a snoek fish.', 'a photo of a eel.', 'a photo of a silver salmon.', 'a photo of a rock beauty fish.', 'a photo of a clownfish.', 'a photo of a sturgeon.', 'a photo of a gar fish.', 'a photo of a lionfish.', 'a photo of a pufferfish.', 'a photo of a abacus.', 'a photo of a abaya.', 'a photo of a academic gown.', 'a photo of a accordion.', 'a photo of a acoustic guitar.', 'a photo of a aircraft carrier.', 'a photo of a airliner.', 'a photo of a airship.', 'a photo of a altar.', 'a photo of a ambulance.', 'a photo of a amphibious vehicle.', 'a photo of a analog clock.', 'a photo of a apiary.', 'a photo of a apron.', 'a photo of a trash can.', 'a photo of a assault rifle.', 'a photo of a backpack.', 'a photo of a bakery.', 'a photo of a balance beam.', 'a photo of a balloon.', 'a photo of a ballpoint pen.', 'a photo of a Band-Aid.', 'a photo of a banjo.', 'a photo of a baluster / handrail.', 'a photo of a barbell.', 'a photo of a barber chair.', 'a photo of a barbershop.', 'a photo of a barn.', 'a photo of a barometer.', 'a photo of a barrel.', 'a photo of a wheelbarrow.', 'a photo of a baseball.', 'a photo of a basketball.', 'a photo of a bassinet.', 'a photo of a bassoon.', 'a photo of a swimming cap.', 'a photo of a bath towel.', 'a photo of a bathtub.', 'a photo of a station wagon.', 'a photo of a lighthouse.', 'a photo of a beaker.', 'a photo of a military hat (bearskin or shako).', 'a photo of a beer bottle.', 'a photo of a beer glass.', 'a photo of a bell tower.', 'a photo of a baby bib.', 'a photo of a tandem bicycle.', 'a photo of a bikini.', 'a photo of a ring binder.', 'a photo of a binoculars.', 'a photo of a birdhouse.', 'a photo of a boathouse.', 'a photo of a bobsleigh.', 'a photo of a bolo tie.', 'a photo of a poke bonnet.', 'a photo of a bookcase.', 'a photo of a bookstore.', 'a photo of a bottle cap.', 'a photo of a hunting bow.', 'a photo of a bow tie.', 'a photo of a brass memorial plaque.', 'a photo of a bra.', 'a photo of a breakwater.', 'a photo of a breastplate.', 'a photo of a broom.', 'a photo of a bucket.', 'a photo of a buckle.', 'a photo of a bulletproof vest.', 'a photo of a high-speed train.', 'a photo of a butcher shop.', 'a photo of a taxicab.', 'a photo of a cauldron.', 'a photo of a candle.', 'a photo of a cannon.', 'a photo of a canoe.', 'a photo of a can opener.', 'a photo of a cardigan.', 'a photo of a car mirror.', 'a photo of a carousel.', 'a photo of a tool kit.', 'a photo of a cardboard box / carton.', 'a photo of a car wheel.', 'a photo of a automated teller machine.', 'a photo of a cassette.', 'a photo of a cassette player.', 'a photo of a castle.', 'a photo of a catamaran.', 'a photo of a CD player.', 'a photo of a cello.', 'a photo of a mobile phone.', 'a photo of a chain.', 'a photo of a chain-link fence.', 'a photo of a chain mail.', 'a photo of a chainsaw.', 'a photo of a storage chest.', 'a photo of a chiffonier.', 'a photo of a bell or wind chime.', 'a photo of a china cabinet.', 'a photo of a Christmas stocking.', 'a photo of a church.', 'a photo of a movie theater.', 'a photo of a cleaver.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/25] batch [5/250] time 0.288 (0.520) data 0.000 (0.201) loss 2.7266 (3.1613) acc 56.2500 (54.3750) lr 1.0000e-05 eta 0:54:06
epoch [1/25] batch [10/250] time 0.287 (0.404) data 0.000 (0.101) loss 3.1992 (3.0846) acc 59.3750 (55.9375) lr 1.0000e-05 eta 0:41:58
epoch [1/25] batch [15/250] time 0.288 (0.365) data 0.000 (0.067) loss 3.0410 (3.1332) acc 59.3750 (54.7917) lr 1.0000e-05 eta 0:37:53
epoch [1/25] batch [20/250] time 0.287 (0.345) data 0.000 (0.051) loss 2.8770 (3.1113) acc 62.5000 (55.9375) lr 1.0000e-05 eta 0:35:51
epoch [1/25] batch [25/250] time 0.287 (0.334) data 0.000 (0.041) loss 2.7812 (3.0514) acc 59.3750 (56.8750) lr 1.0000e-05 eta 0:34:36
epoch [1/25] batch [30/250] time 0.288 (0.326) data 0.000 (0.034) loss 3.4590 (3.0910) acc 46.8750 (55.5208) lr 1.0000e-05 eta 0:33:47
epoch [1/25] batch [35/250] time 0.287 (0.320) data 0.000 (0.029) loss 3.1230 (3.1030) acc 46.8750 (54.8214) lr 1.0000e-05 eta 0:33:11
epoch [1/25] batch [40/250] time 0.288 (0.316) data 0.000 (0.025) loss 3.0703 (3.0994) acc 59.3750 (55.1562) lr 1.0000e-05 eta 0:32:44
epoch [1/25] batch [45/250] time 0.287 (0.313) data 0.000 (0.023) loss 2.3203 (3.0904) acc 71.8750 (55.5556) lr 1.0000e-05 eta 0:32:22
epoch [1/25] batch [50/250] time 0.288 (0.311) data 0.000 (0.020) loss 2.7676 (3.0650) acc 62.5000 (55.7500) lr 1.0000e-05 eta 0:32:05
epoch [1/25] batch [55/250] time 0.288 (0.308) data 0.001 (0.019) loss 2.7578 (3.0308) acc 62.5000 (56.0227) lr 1.0000e-05 eta 0:31:51
epoch [1/25] batch [60/250] time 0.287 (0.307) data 0.000 (0.017) loss 2.6992 (3.0146) acc 62.5000 (56.6146) lr 1.0000e-05 eta 0:31:38
epoch [1/25] batch [65/250] time 0.289 (0.305) data 0.000 (0.016) loss 3.3613 (3.0032) acc 40.6250 (56.8269) lr 1.0000e-05 eta 0:31:28
epoch [1/25] batch [70/250] time 0.287 (0.304) data 0.000 (0.015) loss 3.0000 (2.9984) acc 50.0000 (56.7411) lr 1.0000e-05 eta 0:31:18
epoch [1/25] batch [75/250] time 0.288 (0.303) data 0.000 (0.014) loss 2.2500 (2.9672) acc 75.0000 (57.2917) lr 1.0000e-05 eta 0:31:10
epoch [1/25] batch [80/250] time 0.288 (0.302) data 0.000 (0.013) loss 3.3535 (2.9580) acc 46.8750 (57.3047) lr 1.0000e-05 eta 0:31:03
epoch [1/25] batch [85/250] time 0.288 (0.301) data 0.000 (0.012) loss 2.7969 (2.9479) acc 56.2500 (57.2426) lr 1.0000e-05 eta 0:30:56
epoch [1/25] batch [90/250] time 0.288 (0.300) data 0.000 (0.012) loss 3.4180 (2.9269) acc 43.7500 (57.3611) lr 1.0000e-05 eta 0:30:50
epoch [1/25] batch [95/250] time 0.288 (0.300) data 0.000 (0.011) loss 2.8223 (2.9064) acc 46.8750 (57.5987) lr 1.0000e-05 eta 0:30:45
epoch [1/25] batch [100/250] time 0.288 (0.299) data 0.000 (0.010) loss 2.5117 (2.8962) acc 65.6250 (57.7188) lr 1.0000e-05 eta 0:30:40
epoch [1/25] batch [105/250] time 0.288 (0.299) data 0.000 (0.010) loss 3.0000 (2.8943) acc 50.0000 (57.5893) lr 1.0000e-05 eta 0:30:35
epoch [1/25] batch [110/250] time 0.288 (0.298) data 0.000 (0.009) loss 2.4473 (2.8830) acc 68.7500 (57.6136) lr 1.0000e-05 eta 0:30:30
epoch [1/25] batch [115/250] time 0.288 (0.298) data 0.000 (0.009) loss 3.1855 (2.8792) acc 65.6250 (57.8261) lr 1.0000e-05 eta 0:30:26
epoch [1/25] batch [120/250] time 0.288 (0.297) data 0.000 (0.009) loss 2.5645 (2.8686) acc 62.5000 (57.9427) lr 1.0000e-05 eta 0:30:22
epoch [1/25] batch [125/250] time 0.288 (0.297) data 0.000 (0.008) loss 2.9961 (2.8620) acc 62.5000 (58.0250) lr 1.0000e-05 eta 0:30:18
epoch [1/25] batch [130/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.9785 (2.8469) acc 68.7500 (58.2212) lr 1.0000e-05 eta 0:30:15
epoch [1/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.008) loss 2.4844 (2.8366) acc 59.3750 (58.2639) lr 1.0000e-05 eta 0:30:12
epoch [1/25] batch [140/250] time 0.290 (0.296) data 0.000 (0.008) loss 2.8594 (2.8266) acc 56.2500 (58.3482) lr 1.0000e-05 eta 0:30:08
epoch [1/25] batch [145/250] time 0.288 (0.296) data 0.000 (0.007) loss 2.5352 (2.8252) acc 62.5000 (58.2974) lr 1.0000e-05 eta 0:30:05
epoch [1/25] batch [150/250] time 0.288 (0.296) data 0.000 (0.007) loss 2.2461 (2.8116) acc 62.5000 (58.4375) lr 1.0000e-05 eta 0:30:02
epoch [1/25] batch [155/250] time 0.288 (0.295) data 0.000 (0.007) loss 2.2695 (2.8038) acc 65.6250 (58.5282) lr 1.0000e-05 eta 0:29:59
epoch [1/25] batch [160/250] time 0.288 (0.295) data 0.000 (0.007) loss 2.8008 (2.7986) acc 46.8750 (58.3398) lr 1.0000e-05 eta 0:29:57
epoch [1/25] batch [165/250] time 0.288 (0.295) data 0.000 (0.006) loss 2.4688 (2.7913) acc 68.7500 (58.4280) lr 1.0000e-05 eta 0:29:54
epoch [1/25] batch [170/250] time 0.288 (0.295) data 0.000 (0.006) loss 2.4043 (2.7796) acc 65.6250 (58.7132) lr 1.0000e-05 eta 0:29:51
epoch [1/25] batch [175/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.6250 (2.7753) acc 65.6250 (58.6071) lr 1.0000e-05 eta 0:29:49
epoch [1/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.006) loss 3.1133 (2.7724) acc 50.0000 (58.5764) lr 1.0000e-05 eta 0:29:46
epoch [1/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.5625 (2.7696) acc 56.2500 (58.4966) lr 1.0000e-05 eta 0:29:44
epoch [1/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.1289 (2.7644) acc 59.3750 (58.4539) lr 1.0000e-05 eta 0:29:42
epoch [1/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.4629 (2.7566) acc 59.3750 (58.5256) lr 1.0000e-05 eta 0:29:39
epoch [1/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.7812 (2.7563) acc 59.3750 (58.4062) lr 1.0000e-05 eta 0:29:37
epoch [1/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.6328 (2.7494) acc 65.6250 (58.4299) lr 1.0000e-05 eta 0:29:35
epoch [1/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.3047 (2.7377) acc 65.6250 (58.5565) lr 1.0000e-05 eta 0:29:33
epoch [1/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.4727 (2.7341) acc 56.2500 (58.4738) lr 1.0000e-05 eta 0:29:31
epoch [1/25] batch [220/250] time 0.293 (0.293) data 0.000 (0.005) loss 2.3086 (2.7323) acc 62.5000 (58.3665) lr 1.0000e-05 eta 0:29:29
epoch [1/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.1875 (2.7269) acc 65.6250 (58.3750) lr 1.0000e-05 eta 0:29:27
epoch [1/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.005) loss 2.0234 (2.7192) acc 71.8750 (58.5190) lr 1.0000e-05 eta 0:29:25
epoch [1/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.005) loss 2.0801 (2.7143) acc 65.6250 (58.5106) lr 1.0000e-05 eta 0:29:23
epoch [1/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.5273 (2.7096) acc 56.2500 (58.6198) lr 1.0000e-05 eta 0:29:21
epoch [1/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.1562 (2.7078) acc 65.6250 (58.6097) lr 1.0000e-05 eta 0:29:19
epoch [1/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.3984 (2.7031) acc 62.5000 (58.6250) lr 2.0000e-03 eta 0:29:17
epoch [2/25] batch [5/250] time 0.291 (0.479) data 0.000 (0.189) loss 1.4199 (1.8857) acc 81.2500 (69.3750) lr 2.0000e-03 eta 0:47:52
epoch [2/25] batch [10/250] time 0.289 (0.384) data 0.000 (0.095) loss 2.0000 (1.9300) acc 50.0000 (64.0625) lr 2.0000e-03 eta 0:38:20
epoch [2/25] batch [15/250] time 0.289 (0.352) data 0.000 (0.063) loss 1.7266 (1.8154) acc 65.6250 (62.7083) lr 2.0000e-03 eta 0:35:09
epoch [2/25] batch [20/250] time 0.290 (0.336) data 0.000 (0.048) loss 1.4795 (1.7203) acc 71.8750 (64.8438) lr 2.0000e-03 eta 0:33:31
epoch [2/25] batch [25/250] time 0.291 (0.327) data 0.001 (0.038) loss 1.6299 (1.7274) acc 59.3750 (64.2500) lr 2.0000e-03 eta 0:32:34
epoch [2/25] batch [30/250] time 0.289 (0.321) data 0.001 (0.032) loss 1.5352 (1.7356) acc 65.6250 (63.6458) lr 2.0000e-03 eta 0:31:55
epoch [2/25] batch [35/250] time 0.288 (0.316) data 0.000 (0.027) loss 1.7539 (1.7178) acc 65.6250 (64.1964) lr 2.0000e-03 eta 0:31:26
epoch [2/25] batch [40/250] time 0.290 (0.313) data 0.000 (0.024) loss 1.4277 (1.6930) acc 65.6250 (64.2969) lr 2.0000e-03 eta 0:31:04
epoch [2/25] batch [45/250] time 0.288 (0.310) data 0.000 (0.021) loss 1.9053 (1.6650) acc 53.1250 (64.4444) lr 2.0000e-03 eta 0:30:46
epoch [2/25] batch [50/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.2422 (1.6626) acc 71.8750 (64.4375) lr 2.0000e-03 eta 0:30:32
epoch [2/25] batch [55/250] time 0.289 (0.306) data 0.000 (0.018) loss 1.7949 (1.6565) acc 50.0000 (63.9205) lr 2.0000e-03 eta 0:30:20
epoch [2/25] batch [60/250] time 0.289 (0.305) data 0.001 (0.016) loss 1.0518 (1.6247) acc 81.2500 (64.5312) lr 2.0000e-03 eta 0:30:10
epoch [2/25] batch [65/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.2412 (1.6158) acc 75.0000 (64.5673) lr 2.0000e-03 eta 0:30:02
epoch [2/25] batch [70/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.8213 (1.6100) acc 59.3750 (64.6429) lr 2.0000e-03 eta 0:29:54
epoch [2/25] batch [75/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.3477 (1.6091) acc 62.5000 (64.6667) lr 2.0000e-03 eta 0:29:47
epoch [2/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.4102 (1.5967) acc 56.2500 (64.4141) lr 2.0000e-03 eta 0:29:41
epoch [2/25] batch [85/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.4131 (1.5865) acc 65.6250 (64.3382) lr 2.0000e-03 eta 0:29:35
epoch [2/25] batch [90/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.5547 (1.5764) acc 71.8750 (64.7222) lr 2.0000e-03 eta 0:29:30
epoch [2/25] batch [95/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.4629 (1.5679) acc 65.6250 (64.8355) lr 2.0000e-03 eta 0:29:26
epoch [2/25] batch [100/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.7725 (1.5676) acc 65.6250 (64.6875) lr 2.0000e-03 eta 0:29:21
epoch [2/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.9102 (1.5635) acc 50.0000 (64.6131) lr 2.0000e-03 eta 0:29:17
epoch [2/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7832 (1.5686) acc 56.2500 (64.5170) lr 2.0000e-03 eta 0:29:13
epoch [2/25] batch [115/250] time 0.288 (0.297) data 0.000 (0.009) loss 1.5537 (1.5632) acc 65.6250 (64.5109) lr 2.0000e-03 eta 0:29:09
epoch [2/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.6729 (1.5532) acc 56.2500 (64.5573) lr 2.0000e-03 eta 0:29:05
epoch [2/25] batch [125/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.9443 (1.5465) acc 65.6250 (64.6250) lr 2.0000e-03 eta 0:29:02
epoch [2/25] batch [130/250] time 0.288 (0.296) data 0.000 (0.008) loss 1.3809 (1.5406) acc 71.8750 (64.5673) lr 2.0000e-03 eta 0:28:59
epoch [2/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8052 (1.5315) acc 78.1250 (64.6296) lr 2.0000e-03 eta 0:28:56
epoch [2/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4307 (1.5302) acc 62.5000 (64.5759) lr 2.0000e-03 eta 0:28:53
epoch [2/25] batch [145/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.9082 (1.5308) acc 53.1250 (64.3750) lr 2.0000e-03 eta 0:28:50
epoch [2/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.007) loss 1.6553 (1.5319) acc 59.3750 (64.3542) lr 2.0000e-03 eta 0:28:47
epoch [2/25] batch [155/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.0645 (1.5218) acc 71.8750 (64.4153) lr 2.0000e-03 eta 0:28:45
epoch [2/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1377 (1.5244) acc 71.8750 (64.3945) lr 2.0000e-03 eta 0:28:42
epoch [2/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3496 (1.5198) acc 68.7500 (64.4508) lr 2.0000e-03 eta 0:28:40
epoch [2/25] batch [170/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.7339 (1.5105) acc 78.1250 (64.6324) lr 2.0000e-03 eta 0:28:37
epoch [2/25] batch [175/250] time 0.288 (0.294) data 0.001 (0.006) loss 1.1299 (1.5031) acc 81.2500 (64.8036) lr 2.0000e-03 eta 0:28:35
epoch [2/25] batch [180/250] time 0.288 (0.294) data 0.000 (0.006) loss 2.1738 (1.5026) acc 43.7500 (64.8090) lr 2.0000e-03 eta 0:28:32
epoch [2/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0957 (1.5071) acc 62.5000 (64.7973) lr 2.0000e-03 eta 0:28:30
epoch [2/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2393 (1.5007) acc 75.0000 (64.9013) lr 2.0000e-03 eta 0:28:28
epoch [2/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5098 (1.4948) acc 65.6250 (64.9199) lr 2.0000e-03 eta 0:28:26
epoch [2/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3818 (1.4923) acc 68.7500 (64.8750) lr 2.0000e-03 eta 0:28:23
epoch [2/25] batch [205/250] time 0.288 (0.294) data 0.000 (0.005) loss 0.8779 (1.4848) acc 75.0000 (65.0305) lr 2.0000e-03 eta 0:28:21
epoch [2/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7520 (1.4793) acc 59.3750 (65.1190) lr 2.0000e-03 eta 0:28:19
epoch [2/25] batch [215/250] time 0.288 (0.293) data 0.000 (0.005) loss 0.9658 (1.4783) acc 71.8750 (65.1744) lr 2.0000e-03 eta 0:28:17
epoch [2/25] batch [220/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.3174 (1.4794) acc 78.1250 (65.1989) lr 2.0000e-03 eta 0:28:15
epoch [2/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.005) loss 1.2305 (1.4765) acc 71.8750 (65.2500) lr 2.0000e-03 eta 0:28:13
epoch [2/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6172 (1.4711) acc 53.1250 (65.3261) lr 2.0000e-03 eta 0:28:11
epoch [2/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.5156 (1.4656) acc 62.5000 (65.3723) lr 2.0000e-03 eta 0:28:09
epoch [2/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5449 (1.4635) acc 56.2500 (65.2734) lr 2.0000e-03 eta 0:28:07
epoch [2/25] batch [245/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.6807 (1.4661) acc 46.8750 (65.1403) lr 2.0000e-03 eta 0:28:05
epoch [2/25] batch [250/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.5898 (1.4668) acc 56.2500 (65.1000) lr 1.9921e-03 eta 0:28:03
epoch [3/25] batch [5/250] time 0.288 (0.472) data 0.000 (0.183) loss 1.8057 (1.3432) acc 56.2500 (65.0000) lr 1.9921e-03 eta 0:45:08
epoch [3/25] batch [10/250] time 0.288 (0.380) data 0.000 (0.092) loss 1.2441 (1.2430) acc 65.6250 (68.1250) lr 1.9921e-03 eta 0:36:20
epoch [3/25] batch [15/250] time 0.288 (0.349) data 0.000 (0.061) loss 1.1260 (1.3114) acc 68.7500 (66.8750) lr 1.9921e-03 eta 0:33:23
epoch [3/25] batch [20/250] time 0.289 (0.334) data 0.000 (0.046) loss 2.3105 (1.3964) acc 34.3750 (64.5312) lr 1.9921e-03 eta 0:31:53
epoch [3/25] batch [25/250] time 0.289 (0.325) data 0.000 (0.037) loss 1.5537 (1.3744) acc 59.3750 (64.8750) lr 1.9921e-03 eta 0:31:00
epoch [3/25] batch [30/250] time 0.288 (0.319) data 0.000 (0.031) loss 1.1816 (1.3531) acc 65.6250 (65.5208) lr 1.9921e-03 eta 0:30:24
epoch [3/25] batch [35/250] time 0.288 (0.314) data 0.000 (0.026) loss 1.0713 (1.3413) acc 75.0000 (65.9821) lr 1.9921e-03 eta 0:29:57
epoch [3/25] batch [40/250] time 0.288 (0.311) data 0.000 (0.023) loss 1.6426 (1.3605) acc 59.3750 (65.8594) lr 1.9921e-03 eta 0:29:36
epoch [3/25] batch [45/250] time 0.288 (0.309) data 0.000 (0.021) loss 1.7148 (1.3827) acc 65.6250 (65.6944) lr 1.9921e-03 eta 0:29:20
epoch [3/25] batch [50/250] time 0.288 (0.307) data 0.000 (0.019) loss 1.4600 (1.3795) acc 65.6250 (65.7500) lr 1.9921e-03 eta 0:29:07
epoch [3/25] batch [55/250] time 0.289 (0.305) data 0.001 (0.017) loss 1.6338 (1.3884) acc 56.2500 (65.1705) lr 1.9921e-03 eta 0:28:56
epoch [3/25] batch [60/250] time 0.288 (0.304) data 0.000 (0.016) loss 1.6416 (1.3912) acc 53.1250 (64.7917) lr 1.9921e-03 eta 0:28:47
epoch [3/25] batch [65/250] time 0.288 (0.302) data 0.000 (0.014) loss 1.4512 (1.4031) acc 65.6250 (64.9038) lr 1.9921e-03 eta 0:28:39
epoch [3/25] batch [70/250] time 0.289 (0.301) data 0.001 (0.013) loss 1.3066 (1.3929) acc 65.6250 (65.4018) lr 1.9921e-03 eta 0:28:32
epoch [3/25] batch [75/250] time 0.289 (0.301) data 0.001 (0.013) loss 1.1914 (1.3924) acc 59.3750 (65.3750) lr 1.9921e-03 eta 0:28:26
epoch [3/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.6465 (1.3935) acc 62.5000 (65.1562) lr 1.9921e-03 eta 0:28:20
epoch [3/25] batch [85/250] time 0.288 (0.299) data 0.000 (0.011) loss 1.2002 (1.3923) acc 81.2500 (65.3309) lr 1.9921e-03 eta 0:28:15
epoch [3/25] batch [90/250] time 0.288 (0.299) data 0.000 (0.010) loss 2.0547 (1.3820) acc 56.2500 (65.5903) lr 1.9921e-03 eta 0:28:10
epoch [3/25] batch [95/250] time 0.288 (0.298) data 0.000 (0.010) loss 1.3223 (1.3849) acc 68.7500 (65.7566) lr 1.9921e-03 eta 0:28:05
epoch [3/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.5293 (1.3746) acc 65.6250 (65.9062) lr 1.9921e-03 eta 0:28:01
epoch [3/25] batch [105/250] time 0.288 (0.297) data 0.000 (0.009) loss 1.5049 (1.3841) acc 62.5000 (65.6548) lr 1.9921e-03 eta 0:27:57
epoch [3/25] batch [110/250] time 0.288 (0.297) data 0.000 (0.009) loss 1.9131 (1.3921) acc 59.3750 (65.3977) lr 1.9921e-03 eta 0:27:53
epoch [3/25] batch [115/250] time 0.288 (0.296) data 0.000 (0.008) loss 2.0430 (1.4011) acc 59.3750 (65.2174) lr 1.9921e-03 eta 0:27:49
epoch [3/25] batch [120/250] time 0.288 (0.296) data 0.000 (0.008) loss 1.8379 (1.4032) acc 53.1250 (65.2083) lr 1.9921e-03 eta 0:27:46
epoch [3/25] batch [125/250] time 0.288 (0.296) data 0.000 (0.008) loss 1.0605 (1.4003) acc 71.8750 (65.2000) lr 1.9921e-03 eta 0:27:43
epoch [3/25] batch [130/250] time 0.288 (0.295) data 0.000 (0.007) loss 1.3135 (1.3997) acc 53.1250 (65.1202) lr 1.9921e-03 eta 0:27:40
epoch [3/25] batch [135/250] time 0.288 (0.295) data 0.000 (0.007) loss 1.0967 (1.4087) acc 62.5000 (65.0000) lr 1.9921e-03 eta 0:27:37
epoch [3/25] batch [140/250] time 0.288 (0.295) data 0.000 (0.007) loss 1.6426 (1.4119) acc 50.0000 (64.7991) lr 1.9921e-03 eta 0:27:34
epoch [3/25] batch [145/250] time 0.288 (0.295) data 0.000 (0.007) loss 1.2666 (1.4143) acc 56.2500 (64.6767) lr 1.9921e-03 eta 0:27:31
epoch [3/25] batch [150/250] time 0.288 (0.294) data 0.000 (0.006) loss 1.2324 (1.4159) acc 71.8750 (64.7292) lr 1.9921e-03 eta 0:27:28
epoch [3/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3740 (1.4117) acc 71.8750 (64.8790) lr 1.9921e-03 eta 0:27:26
epoch [3/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.006) loss 0.9395 (1.4075) acc 78.1250 (64.9219) lr 1.9921e-03 eta 0:27:23
epoch [3/25] batch [165/250] time 0.288 (0.294) data 0.000 (0.006) loss 0.9863 (1.4093) acc 78.1250 (64.8674) lr 1.9921e-03 eta 0:27:21
epoch [3/25] batch [170/250] time 0.287 (0.294) data 0.000 (0.006) loss 1.1885 (1.4009) acc 78.1250 (65.0919) lr 1.9921e-03 eta 0:27:18
epoch [3/25] batch [175/250] time 0.288 (0.294) data 0.000 (0.006) loss 1.0410 (1.4057) acc 81.2500 (65.0000) lr 1.9921e-03 eta 0:27:16
epoch [3/25] batch [180/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.8447 (1.4114) acc 56.2500 (64.7569) lr 1.9921e-03 eta 0:27:14
epoch [3/25] batch [185/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.7637 (1.4081) acc 62.5000 (64.8986) lr 1.9921e-03 eta 0:27:12
epoch [3/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.7100 (1.4089) acc 59.3750 (64.8520) lr 1.9921e-03 eta 0:27:09
epoch [3/25] batch [195/250] time 0.288 (0.293) data 0.000 (0.005) loss 2.1484 (1.4076) acc 56.2500 (64.9038) lr 1.9921e-03 eta 0:27:07
epoch [3/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.7461 (1.4039) acc 53.1250 (65.0625) lr 1.9921e-03 eta 0:27:05
epoch [3/25] batch [205/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.1680 (1.4004) acc 71.8750 (65.1220) lr 1.9921e-03 eta 0:27:03
epoch [3/25] batch [210/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.0312 (1.3963) acc 65.6250 (65.2083) lr 1.9921e-03 eta 0:27:01
epoch [3/25] batch [215/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.0312 (1.3999) acc 68.7500 (65.1017) lr 1.9921e-03 eta 0:26:59
epoch [3/25] batch [220/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.2754 (1.3929) acc 71.8750 (65.2841) lr 1.9921e-03 eta 0:26:57
epoch [3/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.8428 (1.3945) acc 46.8750 (65.1944) lr 1.9921e-03 eta 0:26:55
epoch [3/25] batch [230/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.6182 (1.3956) acc 53.1250 (65.1766) lr 1.9921e-03 eta 0:26:53
epoch [3/25] batch [235/250] time 0.288 (0.292) data 0.001 (0.004) loss 0.6543 (1.3918) acc 84.3750 (65.3324) lr 1.9921e-03 eta 0:26:51
epoch [3/25] batch [240/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.6152 (1.3906) acc 59.3750 (65.2734) lr 1.9921e-03 eta 0:26:49
epoch [3/25] batch [245/250] time 0.288 (0.292) data 0.000 (0.004) loss 2.0000 (1.3891) acc 53.1250 (65.3189) lr 1.9921e-03 eta 0:26:47
epoch [3/25] batch [250/250] time 0.292 (0.292) data 0.000 (0.004) loss 1.5869 (1.3910) acc 62.5000 (65.3125) lr 1.9686e-03 eta 0:26:45
epoch [4/25] batch [5/250] time 0.289 (0.451) data 0.001 (0.162) loss 1.6934 (1.1616) acc 65.6250 (68.1250) lr 1.9686e-03 eta 0:41:17
epoch [4/25] batch [10/250] time 0.289 (0.370) data 0.000 (0.081) loss 1.3457 (1.1800) acc 65.6250 (67.5000) lr 1.9686e-03 eta 0:33:48
epoch [4/25] batch [15/250] time 0.288 (0.343) data 0.000 (0.054) loss 0.9868 (1.2461) acc 75.0000 (66.4583) lr 1.9686e-03 eta 0:31:18
epoch [4/25] batch [20/250] time 0.288 (0.329) data 0.000 (0.041) loss 1.0127 (1.2434) acc 65.6250 (66.7188) lr 1.9686e-03 eta 0:30:02
epoch [4/25] batch [25/250] time 0.288 (0.321) data 0.000 (0.033) loss 1.4658 (1.3055) acc 62.5000 (66.2500) lr 1.9686e-03 eta 0:29:16
epoch [4/25] batch [30/250] time 0.289 (0.315) data 0.000 (0.027) loss 1.7783 (1.3080) acc 56.2500 (66.4583) lr 1.9686e-03 eta 0:28:45
epoch [4/25] batch [35/250] time 0.289 (0.312) data 0.000 (0.023) loss 0.8398 (1.3258) acc 68.7500 (65.6250) lr 1.9686e-03 eta 0:28:22
epoch [4/25] batch [40/250] time 0.288 (0.309) data 0.000 (0.020) loss 1.3467 (1.3437) acc 68.7500 (65.5469) lr 1.9686e-03 eta 0:28:05
epoch [4/25] batch [45/250] time 0.288 (0.306) data 0.000 (0.018) loss 1.0352 (1.3440) acc 78.1250 (65.3472) lr 1.9686e-03 eta 0:27:51
epoch [4/25] batch [50/250] time 0.288 (0.305) data 0.000 (0.016) loss 1.4053 (1.3238) acc 71.8750 (65.8750) lr 1.9686e-03 eta 0:27:39
epoch [4/25] batch [55/250] time 0.288 (0.303) data 0.000 (0.015) loss 1.4102 (1.3226) acc 62.5000 (65.9659) lr 1.9686e-03 eta 0:27:30
epoch [4/25] batch [60/250] time 0.288 (0.302) data 0.000 (0.014) loss 1.4316 (1.3286) acc 65.6250 (65.9896) lr 1.9686e-03 eta 0:27:22
epoch [4/25] batch [65/250] time 0.290 (0.301) data 0.000 (0.013) loss 1.3838 (1.3460) acc 68.7500 (65.4808) lr 1.9686e-03 eta 0:27:15
epoch [4/25] batch [70/250] time 0.288 (0.300) data 0.000 (0.012) loss 1.2490 (1.3327) acc 65.6250 (65.6250) lr 1.9686e-03 eta 0:27:08
epoch [4/25] batch [75/250] time 0.288 (0.299) data 0.000 (0.011) loss 1.3096 (1.3282) acc 65.6250 (65.9583) lr 1.9686e-03 eta 0:27:03
epoch [4/25] batch [80/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.5225 (1.3300) acc 62.5000 (65.9375) lr 1.9686e-03 eta 0:26:57
epoch [4/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.010) loss 1.3965 (1.3457) acc 68.7500 (65.8088) lr 1.9686e-03 eta 0:26:53
epoch [4/25] batch [90/250] time 0.289 (0.297) data 0.001 (0.009) loss 1.0176 (1.3379) acc 71.8750 (66.0417) lr 1.9686e-03 eta 0:26:49
epoch [4/25] batch [95/250] time 0.288 (0.297) data 0.000 (0.009) loss 1.0693 (1.3377) acc 71.8750 (66.1184) lr 1.9686e-03 eta 0:26:45
epoch [4/25] batch [100/250] time 0.287 (0.297) data 0.000 (0.008) loss 0.9194 (1.3181) acc 78.1250 (66.5625) lr 1.9686e-03 eta 0:26:41
epoch [4/25] batch [105/250] time 0.288 (0.296) data 0.000 (0.008) loss 1.5156 (1.3173) acc 62.5000 (66.6369) lr 1.9686e-03 eta 0:26:37
epoch [4/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.1934 (1.3122) acc 65.6250 (66.6761) lr 1.9686e-03 eta 0:26:34
epoch [4/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.2988 (1.3033) acc 65.6250 (66.9293) lr 1.9686e-03 eta 0:26:31
epoch [4/25] batch [120/250] time 0.288 (0.295) data 0.000 (0.007) loss 0.8579 (1.2920) acc 71.8750 (67.3438) lr 1.9686e-03 eta 0:26:28
epoch [4/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.0928 (1.3031) acc 75.0000 (67.0500) lr 1.9686e-03 eta 0:26:25
epoch [4/25] batch [130/250] time 0.288 (0.295) data 0.000 (0.007) loss 1.3555 (1.3127) acc 71.8750 (66.9952) lr 1.9686e-03 eta 0:26:22
epoch [4/25] batch [135/250] time 0.288 (0.294) data 0.000 (0.006) loss 1.5518 (1.3121) acc 59.3750 (67.1759) lr 1.9686e-03 eta 0:26:19
epoch [4/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.6953 (1.3145) acc 59.3750 (67.2098) lr 1.9686e-03 eta 0:26:17
epoch [4/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4932 (1.3090) acc 62.5000 (67.3491) lr 1.9686e-03 eta 0:26:14
epoch [4/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.2500 (1.3155) acc 53.1250 (66.9375) lr 1.9686e-03 eta 0:26:12
epoch [4/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9897 (1.3151) acc 71.8750 (66.8750) lr 1.9686e-03 eta 0:26:09
epoch [4/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3584 (1.3273) acc 68.7500 (66.6992) lr 1.9686e-03 eta 0:26:07
epoch [4/25] batch [165/250] time 0.288 (0.293) data 0.000 (0.005) loss 1.3438 (1.3323) acc 68.7500 (66.6477) lr 1.9686e-03 eta 0:26:05
epoch [4/25] batch [170/250] time 0.291 (0.293) data 0.000 (0.005) loss 1.0059 (1.3285) acc 75.0000 (66.7831) lr 1.9686e-03 eta 0:26:03
epoch [4/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2656 (1.3358) acc 68.7500 (66.6607) lr 1.9686e-03 eta 0:26:00
epoch [4/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.4951 (1.3362) acc 53.1250 (66.6840) lr 1.9686e-03 eta 0:25:58
epoch [4/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.8540 (1.3347) acc 78.1250 (66.7399) lr 1.9686e-03 eta 0:25:56
epoch [4/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.8691 (1.3411) acc 68.7500 (66.7434) lr 1.9686e-03 eta 0:25:54
epoch [4/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0557 (1.3424) acc 71.8750 (66.7949) lr 1.9686e-03 eta 0:25:52
epoch [4/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4766 (1.3447) acc 53.1250 (66.6250) lr 1.9686e-03 eta 0:25:50
epoch [4/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5029 (1.3498) acc 62.5000 (66.5396) lr 1.9686e-03 eta 0:25:48
epoch [4/25] batch [210/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.4023 (1.3449) acc 65.6250 (66.5923) lr 1.9686e-03 eta 0:25:46
epoch [4/25] batch [215/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.3545 (1.3466) acc 62.5000 (66.5843) lr 1.9686e-03 eta 0:25:44
epoch [4/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.1924 (1.3473) acc 65.6250 (66.4915) lr 1.9686e-03 eta 0:25:42
epoch [4/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.4590 (1.3493) acc 56.2500 (66.4444) lr 1.9686e-03 eta 0:25:41
epoch [4/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0410 (1.3506) acc 65.6250 (66.3315) lr 1.9686e-03 eta 0:25:39
epoch [4/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.004) loss 1.2168 (1.3474) acc 65.6250 (66.3165) lr 1.9686e-03 eta 0:25:37
epoch [4/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.6514 (1.3462) acc 59.3750 (66.2370) lr 1.9686e-03 eta 0:25:35
epoch [4/25] batch [245/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.3262 (1.3492) acc 65.6250 (66.1480) lr 1.9686e-03 eta 0:25:33
epoch [4/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.4629 (1.3530) acc 68.7500 (66.1125) lr 1.9298e-03 eta 0:25:32
epoch [5/25] batch [5/250] time 0.288 (0.445) data 0.000 (0.157) loss 0.8184 (1.0872) acc 84.3750 (76.2500) lr 1.9298e-03 eta 0:38:54
epoch [5/25] batch [10/250] time 0.288 (0.367) data 0.000 (0.079) loss 1.4453 (1.2295) acc 59.3750 (71.2500) lr 1.9298e-03 eta 0:32:02
epoch [5/25] batch [15/250] time 0.289 (0.341) data 0.000 (0.053) loss 2.1602 (1.2527) acc 50.0000 (68.9583) lr 1.9298e-03 eta 0:29:44
epoch [5/25] batch [20/250] time 0.290 (0.328) data 0.001 (0.040) loss 1.6641 (1.2957) acc 59.3750 (68.1250) lr 1.9298e-03 eta 0:28:36
epoch [5/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.032) loss 1.2334 (1.3355) acc 68.7500 (66.8750) lr 1.9298e-03 eta 0:27:53
epoch [5/25] batch [30/250] time 0.289 (0.315) data 0.001 (0.027) loss 1.1445 (1.3264) acc 75.0000 (67.6042) lr 1.9298e-03 eta 0:27:25
epoch [5/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.023) loss 1.4658 (1.3246) acc 65.6250 (67.1429) lr 1.9298e-03 eta 0:27:04
epoch [5/25] batch [40/250] time 0.290 (0.309) data 0.001 (0.020) loss 0.6113 (1.2838) acc 87.5000 (68.2031) lr 1.9298e-03 eta 0:26:48
epoch [5/25] batch [45/250] time 0.290 (0.307) data 0.001 (0.018) loss 1.1299 (1.2590) acc 65.6250 (68.4722) lr 1.9298e-03 eta 0:26:35
epoch [5/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.6533 (1.2820) acc 56.2500 (68.0625) lr 1.9298e-03 eta 0:26:24
epoch [5/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.015) loss 1.4502 (1.2956) acc 56.2500 (67.5568) lr 1.9298e-03 eta 0:26:16
epoch [5/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.8843 (1.2903) acc 71.8750 (67.7083) lr 1.9298e-03 eta 0:26:08
epoch [5/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1074 (1.2726) acc 71.8750 (68.3173) lr 1.9298e-03 eta 0:26:01
epoch [5/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.012) loss 1.6025 (1.2704) acc 62.5000 (68.0357) lr 1.9298e-03 eta 0:25:55
epoch [5/25] batch [75/250] time 0.288 (0.300) data 0.000 (0.011) loss 0.9214 (1.2620) acc 75.0000 (68.0417) lr 1.9298e-03 eta 0:25:50
epoch [5/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3350 (1.2645) acc 62.5000 (67.8906) lr 1.9298e-03 eta 0:25:45
epoch [5/25] batch [85/250] time 0.288 (0.298) data 0.000 (0.010) loss 1.2549 (1.2639) acc 68.7500 (67.8309) lr 1.9298e-03 eta 0:25:40
epoch [5/25] batch [90/250] time 0.288 (0.298) data 0.000 (0.009) loss 1.3359 (1.2628) acc 56.2500 (67.7778) lr 1.9298e-03 eta 0:25:36
epoch [5/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.009) loss 1.3975 (1.2639) acc 65.6250 (67.6974) lr 1.9298e-03 eta 0:25:32
epoch [5/25] batch [100/250] time 0.288 (0.297) data 0.000 (0.008) loss 1.6914 (1.2669) acc 46.8750 (67.4688) lr 1.9298e-03 eta 0:25:28
epoch [5/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.2168 (1.2657) acc 68.7500 (67.4107) lr 1.9298e-03 eta 0:25:25
epoch [5/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.0771 (1.2695) acc 65.6250 (67.2727) lr 1.9298e-03 eta 0:25:22
epoch [5/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4746 (1.2853) acc 59.3750 (67.0109) lr 1.9298e-03 eta 0:25:19
epoch [5/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0391 (1.2849) acc 71.8750 (67.0312) lr 1.9298e-03 eta 0:25:16
epoch [5/25] batch [125/250] time 0.291 (0.295) data 0.000 (0.007) loss 0.9072 (1.2756) acc 84.3750 (67.3250) lr 1.9298e-03 eta 0:25:13
epoch [5/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4873 (1.2866) acc 62.5000 (67.0433) lr 1.9298e-03 eta 0:25:10
epoch [5/25] batch [135/250] time 0.288 (0.295) data 0.000 (0.006) loss 1.1348 (1.2899) acc 84.3750 (66.9676) lr 1.9298e-03 eta 0:25:08
epoch [5/25] batch [140/250] time 0.288 (0.295) data 0.000 (0.006) loss 1.1367 (1.2904) acc 75.0000 (66.9643) lr 1.9298e-03 eta 0:25:05
epoch [5/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4951 (1.2908) acc 68.7500 (67.0043) lr 1.9298e-03 eta 0:25:03
epoch [5/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.2256 (1.2943) acc 68.7500 (67.0417) lr 1.9298e-03 eta 0:25:00
epoch [5/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5771 (1.3091) acc 65.6250 (66.9153) lr 1.9298e-03 eta 0:24:58
epoch [5/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4834 (1.3107) acc 62.5000 (66.9336) lr 1.9298e-03 eta 0:24:56
epoch [5/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3213 (1.3105) acc 65.6250 (67.0455) lr 1.9298e-03 eta 0:24:53
epoch [5/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1709 (1.3095) acc 71.8750 (67.0404) lr 1.9298e-03 eta 0:24:51
epoch [5/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9897 (1.3031) acc 78.1250 (67.2143) lr 1.9298e-03 eta 0:24:49
epoch [5/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.1982 (1.3058) acc 75.0000 (67.0833) lr 1.9298e-03 eta 0:24:47
epoch [5/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3613 (1.3075) acc 65.6250 (66.9257) lr 1.9298e-03 eta 0:24:45
epoch [5/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8750 (1.3110) acc 53.1250 (66.7763) lr 1.9298e-03 eta 0:24:43
epoch [5/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2197 (1.3114) acc 68.7500 (66.8109) lr 1.9298e-03 eta 0:24:41
epoch [5/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.0977 (1.3164) acc 37.5000 (66.6250) lr 1.9298e-03 eta 0:24:39
epoch [5/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6523 (1.3175) acc 56.2500 (66.5549) lr 1.9298e-03 eta 0:24:37
epoch [5/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3320 (1.3154) acc 62.5000 (66.5476) lr 1.9298e-03 eta 0:24:35
epoch [5/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0684 (1.3177) acc 75.0000 (66.5407) lr 1.9298e-03 eta 0:24:33
epoch [5/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3340 (1.3196) acc 59.3750 (66.4631) lr 1.9298e-03 eta 0:24:31
epoch [5/25] batch [225/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.6689 (1.3221) acc 53.1250 (66.3194) lr 1.9298e-03 eta 0:24:29
epoch [5/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3887 (1.3222) acc 59.3750 (66.3179) lr 1.9298e-03 eta 0:24:28
epoch [5/25] batch [235/250] time 0.290 (0.292) data 0.001 (0.004) loss 1.0293 (1.3275) acc 71.8750 (66.2101) lr 1.9298e-03 eta 0:24:26
epoch [5/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2578 (1.3257) acc 59.3750 (66.1589) lr 1.9298e-03 eta 0:24:24
epoch [5/25] batch [245/250] time 0.288 (0.292) data 0.000 (0.004) loss 1.7852 (1.3289) acc 56.2500 (66.1224) lr 1.9298e-03 eta 0:24:22
epoch [5/25] batch [250/250] time 0.288 (0.292) data 0.000 (0.003) loss 1.2666 (1.3330) acc 75.0000 (66.0000) lr 1.8763e-03 eta 0:24:20
epoch [6/25] batch [5/250] time 0.289 (0.459) data 0.000 (0.169) loss 1.3730 (1.5266) acc 68.7500 (63.1250) lr 1.8763e-03 eta 0:38:11
epoch [6/25] batch [10/250] time 0.288 (0.374) data 0.000 (0.085) loss 2.0273 (1.3980) acc 46.8750 (64.3750) lr 1.8763e-03 eta 0:31:05
epoch [6/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.056) loss 1.8965 (1.3751) acc 56.2500 (65.2083) lr 1.8763e-03 eta 0:28:42
epoch [6/25] batch [20/250] time 0.289 (0.332) data 0.000 (0.042) loss 1.5742 (1.3302) acc 68.7500 (67.1875) lr 1.8763e-03 eta 0:27:31
epoch [6/25] batch [25/250] time 0.289 (0.323) data 0.000 (0.034) loss 1.5850 (1.3640) acc 59.3750 (66.0000) lr 1.8763e-03 eta 0:26:47
epoch [6/25] batch [30/250] time 0.289 (0.318) data 0.000 (0.028) loss 1.0801 (1.3186) acc 81.2500 (66.9792) lr 1.8763e-03 eta 0:26:18
epoch [6/25] batch [35/250] time 0.288 (0.313) data 0.000 (0.024) loss 1.4668 (1.3273) acc 65.6250 (66.4286) lr 1.8763e-03 eta 0:25:56
epoch [6/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.3311 (1.3323) acc 62.5000 (65.7812) lr 1.8763e-03 eta 0:25:39
epoch [6/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 0.9473 (1.3158) acc 71.8750 (66.0417) lr 1.8763e-03 eta 0:25:26
epoch [6/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.3076 (1.3124) acc 68.7500 (66.5000) lr 1.8763e-03 eta 0:25:15
epoch [6/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.0400 (1.3169) acc 71.8750 (66.7614) lr 1.8763e-03 eta 0:25:06
epoch [6/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 2.0117 (1.3159) acc 56.2500 (66.6146) lr 1.8763e-03 eta 0:24:58
epoch [6/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.6006 (1.3261) acc 59.3750 (66.3462) lr 1.8763e-03 eta 0:24:51
epoch [6/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.6504 (1.3396) acc 62.5000 (66.2946) lr 1.8763e-03 eta 0:24:45
epoch [6/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.7559 (1.3449) acc 62.5000 (66.1667) lr 1.8763e-03 eta 0:24:40
epoch [6/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3955 (1.3499) acc 62.5000 (66.1328) lr 1.8763e-03 eta 0:24:35
epoch [6/25] batch [85/250] time 0.288 (0.299) data 0.000 (0.010) loss 1.1885 (1.3462) acc 71.8750 (66.3235) lr 1.8763e-03 eta 0:24:30
epoch [6/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.8428 (1.3562) acc 62.5000 (66.0069) lr 1.8763e-03 eta 0:24:26
epoch [6/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7666 (1.3483) acc 50.0000 (66.2829) lr 1.8763e-03 eta 0:24:22
epoch [6/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.8545 (1.3444) acc 65.6250 (66.2812) lr 1.8763e-03 eta 0:24:18
epoch [6/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9609 (1.3334) acc 84.3750 (66.6964) lr 1.8763e-03 eta 0:24:15
epoch [6/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4932 (1.3402) acc 62.5000 (66.5909) lr 1.8763e-03 eta 0:24:11
epoch [6/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5439 (1.3373) acc 62.5000 (66.6576) lr 1.8763e-03 eta 0:24:08
epoch [6/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.6729 (1.3470) acc 56.2500 (66.3802) lr 1.8763e-03 eta 0:24:05
epoch [6/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3203 (1.3510) acc 71.8750 (66.3500) lr 1.8763e-03 eta 0:24:02
epoch [6/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.9814 (1.3588) acc 59.3750 (66.2019) lr 1.8763e-03 eta 0:24:00
epoch [6/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.007) loss 0.6577 (1.3528) acc 81.2500 (66.1806) lr 1.8763e-03 eta 0:23:57
epoch [6/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.6592 (1.3547) acc 59.3750 (65.9598) lr 1.8763e-03 eta 0:23:55
epoch [6/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2119 (1.3518) acc 68.7500 (65.9698) lr 1.8763e-03 eta 0:23:52
epoch [6/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2646 (1.3437) acc 71.8750 (66.2500) lr 1.8763e-03 eta 0:23:50
epoch [6/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.0508 (1.3424) acc 43.7500 (66.2097) lr 1.8763e-03 eta 0:23:47
epoch [6/25] batch [160/250] time 0.288 (0.294) data 0.000 (0.006) loss 1.2783 (1.3388) acc 75.0000 (66.3477) lr 1.8763e-03 eta 0:23:45
epoch [6/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6133 (1.3393) acc 71.8750 (66.3258) lr 1.8763e-03 eta 0:23:42
epoch [6/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9146 (1.3404) acc 71.8750 (66.3603) lr 1.8763e-03 eta 0:23:40
epoch [6/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6211 (1.3468) acc 68.7500 (66.1607) lr 1.8763e-03 eta 0:23:38
epoch [6/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1426 (1.3482) acc 71.8750 (66.1806) lr 1.8763e-03 eta 0:23:36
epoch [6/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7363 (1.3474) acc 53.1250 (66.1486) lr 1.8763e-03 eta 0:23:34
epoch [6/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2812 (1.3436) acc 56.2500 (66.1184) lr 1.8763e-03 eta 0:23:32
epoch [6/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1406 (1.3465) acc 71.8750 (66.0577) lr 1.8763e-03 eta 0:23:30
epoch [6/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.9355 (1.3476) acc 62.5000 (66.0625) lr 1.8763e-03 eta 0:23:28
epoch [6/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5732 (1.3477) acc 53.1250 (66.0213) lr 1.8763e-03 eta 0:23:26
epoch [6/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 2.0957 (1.3444) acc 56.2500 (66.1756) lr 1.8763e-03 eta 0:23:24
epoch [6/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0742 (1.3368) acc 68.7500 (66.3663) lr 1.8763e-03 eta 0:23:22
epoch [6/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7451 (1.3363) acc 84.3750 (66.4631) lr 1.8763e-03 eta 0:23:20
epoch [6/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3174 (1.3312) acc 65.6250 (66.4583) lr 1.8763e-03 eta 0:23:19
epoch [6/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8740 (1.3277) acc 71.8750 (66.5082) lr 1.8763e-03 eta 0:23:17
epoch [6/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.1299 (1.3254) acc 65.6250 (66.5160) lr 1.8763e-03 eta 0:23:15
epoch [6/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9556 (1.3238) acc 75.0000 (66.6016) lr 1.8763e-03 eta 0:23:13
epoch [6/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8589 (1.3197) acc 81.2500 (66.5944) lr 1.8763e-03 eta 0:23:11
epoch [6/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3467 (1.3181) acc 62.5000 (66.5250) lr 1.8090e-03 eta 0:23:09
epoch [7/25] batch [5/250] time 0.290 (0.467) data 0.000 (0.178) loss 1.6807 (1.1537) acc 62.5000 (73.7500) lr 1.8090e-03 eta 0:36:56
epoch [7/25] batch [10/250] time 0.289 (0.378) data 0.000 (0.089) loss 1.2148 (1.1510) acc 62.5000 (71.2500) lr 1.8090e-03 eta 0:29:52
epoch [7/25] batch [15/250] time 0.289 (0.349) data 0.001 (0.060) loss 1.3154 (1.2154) acc 56.2500 (70.0000) lr 1.8090e-03 eta 0:27:30
epoch [7/25] batch [20/250] time 0.289 (0.334) data 0.000 (0.045) loss 1.2314 (1.2274) acc 62.5000 (70.1562) lr 1.8090e-03 eta 0:26:18
epoch [7/25] batch [25/250] time 0.289 (0.325) data 0.000 (0.036) loss 1.9111 (1.2455) acc 56.2500 (69.0000) lr 1.8090e-03 eta 0:25:34
epoch [7/25] batch [30/250] time 0.289 (0.319) data 0.000 (0.030) loss 1.1211 (1.2683) acc 84.3750 (69.1667) lr 1.8090e-03 eta 0:25:04
epoch [7/25] batch [35/250] time 0.289 (0.315) data 0.000 (0.026) loss 1.3047 (1.3012) acc 75.0000 (68.4821) lr 1.8090e-03 eta 0:24:43
epoch [7/25] batch [40/250] time 0.290 (0.311) data 0.000 (0.023) loss 1.3330 (1.2899) acc 71.8750 (68.5938) lr 1.8090e-03 eta 0:24:26
epoch [7/25] batch [45/250] time 0.289 (0.309) data 0.000 (0.020) loss 1.1094 (1.2817) acc 78.1250 (68.8194) lr 1.8090e-03 eta 0:24:13
epoch [7/25] batch [50/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.5039 (1.2750) acc 62.5000 (68.5000) lr 1.8090e-03 eta 0:24:02
epoch [7/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.7129 (1.2861) acc 68.7500 (68.5227) lr 1.8090e-03 eta 0:23:53
epoch [7/25] batch [60/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.1367 (1.2996) acc 75.0000 (68.3854) lr 1.8090e-03 eta 0:23:45
epoch [7/25] batch [65/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.5791 (1.3104) acc 62.5000 (68.2212) lr 1.8090e-03 eta 0:23:39
epoch [7/25] batch [70/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.6670 (1.3222) acc 53.1250 (68.0357) lr 1.8090e-03 eta 0:23:33
epoch [7/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3838 (1.3241) acc 62.5000 (67.8333) lr 1.8090e-03 eta 0:23:27
epoch [7/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3018 (1.3167) acc 75.0000 (67.8906) lr 1.8090e-03 eta 0:23:22
epoch [7/25] batch [85/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2539 (1.3014) acc 68.7500 (68.1985) lr 1.8090e-03 eta 0:23:18
epoch [7/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1221 (1.3064) acc 71.8750 (68.1250) lr 1.8090e-03 eta 0:23:14
epoch [7/25] batch [95/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4814 (1.2974) acc 65.6250 (68.2895) lr 1.8090e-03 eta 0:23:10
epoch [7/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3936 (1.2984) acc 68.7500 (68.0938) lr 1.8090e-03 eta 0:23:06
epoch [7/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1455 (1.2940) acc 71.8750 (68.2143) lr 1.8090e-03 eta 0:23:02
epoch [7/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4385 (1.2976) acc 68.7500 (68.1818) lr 1.8090e-03 eta 0:22:59
epoch [7/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4844 (1.2981) acc 56.2500 (67.9891) lr 1.8090e-03 eta 0:22:56
epoch [7/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2168 (1.3034) acc 68.7500 (67.9688) lr 1.8090e-03 eta 0:22:53
epoch [7/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7197 (1.3028) acc 56.2500 (67.9750) lr 1.8090e-03 eta 0:22:50
epoch [7/25] batch [130/250] time 0.289 (0.296) data 0.001 (0.007) loss 1.7139 (1.3104) acc 62.5000 (67.8365) lr 1.8090e-03 eta 0:22:48
epoch [7/25] batch [135/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.9180 (1.3124) acc 65.6250 (67.8704) lr 1.8090e-03 eta 0:22:45
epoch [7/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4619 (1.3108) acc 68.7500 (67.8571) lr 1.8090e-03 eta 0:22:42
epoch [7/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2734 (1.3065) acc 62.5000 (67.8879) lr 1.8090e-03 eta 0:22:40
epoch [7/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.9561 (1.3179) acc 46.8750 (67.6042) lr 1.8090e-03 eta 0:22:38
epoch [7/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5371 (1.3233) acc 68.7500 (67.6008) lr 1.8090e-03 eta 0:22:35
epoch [7/25] batch [160/250] time 0.292 (0.295) data 0.000 (0.006) loss 1.0674 (1.3233) acc 68.7500 (67.5195) lr 1.8090e-03 eta 0:22:33
epoch [7/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3740 (1.3254) acc 71.8750 (67.5189) lr 1.8090e-03 eta 0:22:31
epoch [7/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0234 (1.3236) acc 68.7500 (67.5000) lr 1.8090e-03 eta 0:22:29
epoch [7/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8223 (1.3232) acc 46.8750 (67.3393) lr 1.8090e-03 eta 0:22:27
epoch [7/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5947 (1.3276) acc 68.7500 (67.3264) lr 1.8090e-03 eta 0:22:25
epoch [7/25] batch [185/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.3555 (1.3350) acc 62.5000 (67.1284) lr 1.8090e-03 eta 0:22:23
epoch [7/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7051 (1.3369) acc 81.2500 (67.0888) lr 1.8090e-03 eta 0:22:20
epoch [7/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9097 (1.3323) acc 71.8750 (67.1154) lr 1.8090e-03 eta 0:22:18
epoch [7/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.5244 (1.3300) acc 62.5000 (67.0938) lr 1.8090e-03 eta 0:22:17
epoch [7/25] batch [205/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.9595 (1.3245) acc 71.8750 (67.1341) lr 1.8090e-03 eta 0:22:15
epoch [7/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.6484 (1.3246) acc 59.3750 (67.0387) lr 1.8090e-03 eta 0:22:13
epoch [7/25] batch [215/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.5879 (1.3297) acc 75.0000 (67.0349) lr 1.8090e-03 eta 0:22:11
epoch [7/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.8271 (1.3299) acc 59.3750 (66.9744) lr 1.8090e-03 eta 0:22:09
epoch [7/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3213 (1.3293) acc 68.7500 (67.0000) lr 1.8090e-03 eta 0:22:07
epoch [7/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.9365 (1.3253) acc 53.1250 (67.0788) lr 1.8090e-03 eta 0:22:05
epoch [7/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.3447 (1.3267) acc 59.3750 (67.0612) lr 1.8090e-03 eta 0:22:03
epoch [7/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3076 (1.3272) acc 62.5000 (67.0052) lr 1.8090e-03 eta 0:22:02
epoch [7/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4062 (1.3309) acc 65.6250 (66.8878) lr 1.8090e-03 eta 0:22:00
epoch [7/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5195 (1.3334) acc 59.3750 (66.7500) lr 1.7290e-03 eta 0:21:58
epoch [8/25] batch [5/250] time 0.290 (0.465) data 0.000 (0.174) loss 0.8301 (1.5102) acc 78.1250 (62.5000) lr 1.7290e-03 eta 0:34:49
epoch [8/25] batch [10/250] time 0.289 (0.377) data 0.000 (0.087) loss 1.1592 (1.3865) acc 81.2500 (65.9375) lr 1.7290e-03 eta 0:28:13
epoch [8/25] batch [15/250] time 0.289 (0.348) data 0.000 (0.058) loss 1.1270 (1.3058) acc 65.6250 (67.9167) lr 1.7290e-03 eta 0:26:00
epoch [8/25] batch [20/250] time 0.290 (0.334) data 0.000 (0.044) loss 1.1631 (1.2517) acc 78.1250 (69.5312) lr 1.7290e-03 eta 0:24:54
epoch [8/25] batch [25/250] time 0.290 (0.325) data 0.000 (0.035) loss 0.9785 (1.2360) acc 71.8750 (69.3750) lr 1.7290e-03 eta 0:24:13
epoch [8/25] batch [30/250] time 0.290 (0.319) data 0.000 (0.029) loss 1.1377 (1.2378) acc 65.6250 (69.3750) lr 1.7290e-03 eta 0:23:46
epoch [8/25] batch [35/250] time 0.290 (0.315) data 0.000 (0.025) loss 1.0215 (1.2345) acc 68.7500 (69.0179) lr 1.7290e-03 eta 0:23:26
epoch [8/25] batch [40/250] time 0.290 (0.312) data 0.000 (0.022) loss 1.3027 (1.2242) acc 68.7500 (69.6094) lr 1.7290e-03 eta 0:23:10
epoch [8/25] batch [45/250] time 0.290 (0.309) data 0.000 (0.020) loss 1.3076 (1.2202) acc 68.7500 (69.7917) lr 1.7290e-03 eta 0:22:58
epoch [8/25] batch [50/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.8369 (1.2294) acc 56.2500 (69.3125) lr 1.7290e-03 eta 0:22:48
epoch [8/25] batch [55/250] time 0.289 (0.306) data 0.000 (0.016) loss 1.3789 (1.2467) acc 62.5000 (68.8636) lr 1.7290e-03 eta 0:22:39
epoch [8/25] batch [60/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.1064 (1.2455) acc 65.6250 (69.0625) lr 1.7290e-03 eta 0:22:31
epoch [8/25] batch [65/250] time 0.290 (0.303) data 0.000 (0.014) loss 0.7852 (1.2464) acc 84.3750 (69.1827) lr 1.7290e-03 eta 0:22:25
epoch [8/25] batch [70/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.5957 (1.2625) acc 56.2500 (69.0179) lr 1.7290e-03 eta 0:22:19
epoch [8/25] batch [75/250] time 0.290 (0.302) data 0.000 (0.012) loss 1.6104 (1.2575) acc 53.1250 (69.1667) lr 1.7290e-03 eta 0:22:14
epoch [8/25] batch [80/250] time 0.290 (0.301) data 0.000 (0.011) loss 1.0410 (1.2625) acc 68.7500 (68.7891) lr 1.7290e-03 eta 0:22:09
epoch [8/25] batch [85/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.0850 (1.2792) acc 68.7500 (68.2353) lr 1.7290e-03 eta 0:22:05
epoch [8/25] batch [90/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.1455 (1.2796) acc 71.8750 (68.4028) lr 1.7290e-03 eta 0:22:01
epoch [8/25] batch [95/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1074 (1.2728) acc 62.5000 (68.2566) lr 1.7290e-03 eta 0:21:57
epoch [8/25] batch [100/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.2480 (1.2760) acc 75.0000 (68.2188) lr 1.7290e-03 eta 0:21:53
epoch [8/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.6523 (1.2711) acc 81.2500 (68.3631) lr 1.7290e-03 eta 0:21:50
epoch [8/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.2949 (1.2695) acc 68.7500 (68.4091) lr 1.7290e-03 eta 0:21:46
epoch [8/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.1504 (1.2711) acc 59.3750 (68.1250) lr 1.7290e-03 eta 0:21:43
epoch [8/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.8154 (1.2773) acc 59.3750 (67.9948) lr 1.7290e-03 eta 0:21:40
epoch [8/25] batch [125/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.7314 (1.2799) acc 62.5000 (67.9500) lr 1.7290e-03 eta 0:21:37
epoch [8/25] batch [130/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.6602 (1.2779) acc 56.2500 (67.8606) lr 1.7290e-03 eta 0:21:35
epoch [8/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9575 (1.2799) acc 71.8750 (67.8704) lr 1.7290e-03 eta 0:21:32
epoch [8/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1963 (1.2725) acc 65.6250 (67.9464) lr 1.7290e-03 eta 0:21:30
epoch [8/25] batch [145/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.2832 (1.2738) acc 75.0000 (68.0819) lr 1.7290e-03 eta 0:21:27
epoch [8/25] batch [150/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.7832 (1.2783) acc 53.1250 (67.9167) lr 1.7290e-03 eta 0:21:25
epoch [8/25] batch [155/250] time 0.291 (0.295) data 0.001 (0.006) loss 1.3330 (1.2784) acc 75.0000 (67.9839) lr 1.7290e-03 eta 0:21:23
epoch [8/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4766 (1.2773) acc 75.0000 (68.0859) lr 1.7290e-03 eta 0:21:20
epoch [8/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.3545 (1.2817) acc 68.7500 (67.9924) lr 1.7290e-03 eta 0:21:18
epoch [8/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.4053 (1.2867) acc 71.8750 (67.7941) lr 1.7290e-03 eta 0:21:16
epoch [8/25] batch [175/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.2881 (1.2831) acc 59.3750 (67.7857) lr 1.7290e-03 eta 0:21:14
epoch [8/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5566 (1.2823) acc 68.7500 (67.7951) lr 1.7290e-03 eta 0:21:12
epoch [8/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1201 (1.2825) acc 68.7500 (67.8547) lr 1.7290e-03 eta 0:21:10
epoch [8/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3848 (1.2868) acc 59.3750 (67.6974) lr 1.7290e-03 eta 0:21:08
epoch [8/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1689 (1.2889) acc 62.5000 (67.5962) lr 1.7290e-03 eta 0:21:06
epoch [8/25] batch [200/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.4238 (1.2909) acc 65.6250 (67.4844) lr 1.7290e-03 eta 0:21:04
epoch [8/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9731 (1.2932) acc 75.0000 (67.4390) lr 1.7290e-03 eta 0:21:02
epoch [8/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.7920 (1.2927) acc 71.8750 (67.4405) lr 1.7290e-03 eta 0:21:00
epoch [8/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.2285 (1.2890) acc 65.6250 (67.4564) lr 1.7290e-03 eta 0:20:58
epoch [8/25] batch [220/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.9424 (1.2929) acc 46.8750 (67.3580) lr 1.7290e-03 eta 0:20:56
epoch [8/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9844 (1.2897) acc 75.0000 (67.3472) lr 1.7290e-03 eta 0:20:54
epoch [8/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0029 (1.2898) acc 71.8750 (67.3641) lr 1.7290e-03 eta 0:20:52
epoch [8/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.2412 (1.2867) acc 65.6250 (67.4867) lr 1.7290e-03 eta 0:20:50
epoch [8/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4365 (1.2839) acc 65.6250 (67.5651) lr 1.7290e-03 eta 0:20:48
epoch [8/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6006 (1.2880) acc 59.3750 (67.5128) lr 1.7290e-03 eta 0:20:47
epoch [8/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0928 (1.2896) acc 56.2500 (67.3625) lr 1.6374e-03 eta 0:20:45
epoch [9/25] batch [5/250] time 0.290 (0.456) data 0.000 (0.166) loss 1.5693 (1.3787) acc 62.5000 (67.5000) lr 1.6374e-03 eta 0:32:14
epoch [9/25] batch [10/250] time 0.288 (0.372) data 0.000 (0.083) loss 1.2754 (1.4122) acc 71.8750 (65.3125) lr 1.6374e-03 eta 0:26:17
epoch [9/25] batch [15/250] time 0.289 (0.344) data 0.000 (0.056) loss 0.8374 (1.3378) acc 78.1250 (67.0833) lr 1.6374e-03 eta 0:24:18
epoch [9/25] batch [20/250] time 0.290 (0.331) data 0.000 (0.042) loss 1.9014 (1.3605) acc 59.3750 (66.8750) lr 1.6374e-03 eta 0:23:18
epoch [9/25] batch [25/250] time 0.288 (0.322) data 0.000 (0.034) loss 0.7515 (1.3239) acc 84.3750 (67.6250) lr 1.6374e-03 eta 0:22:41
epoch [9/25] batch [30/250] time 0.289 (0.317) data 0.000 (0.028) loss 1.1738 (1.2803) acc 68.7500 (68.1250) lr 1.6374e-03 eta 0:22:16
epoch [9/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.9888 (1.2621) acc 68.7500 (68.7500) lr 1.6374e-03 eta 0:21:58
epoch [9/25] batch [40/250] time 0.289 (0.310) data 0.001 (0.021) loss 0.7832 (1.2685) acc 78.1250 (69.2188) lr 1.6374e-03 eta 0:21:44
epoch [9/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 0.8594 (1.2591) acc 78.1250 (69.5833) lr 1.6374e-03 eta 0:21:33
epoch [9/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.1641 (1.2723) acc 71.8750 (69.1875) lr 1.6374e-03 eta 0:21:23
epoch [9/25] batch [55/250] time 0.289 (0.304) data 0.001 (0.015) loss 1.8936 (1.2987) acc 56.2500 (68.4091) lr 1.6374e-03 eta 0:21:16
epoch [9/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.4043 (1.3121) acc 65.6250 (68.0729) lr 1.6374e-03 eta 0:21:09
epoch [9/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1611 (1.3062) acc 71.8750 (67.8365) lr 1.6374e-03 eta 0:21:03
epoch [9/25] batch [70/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.5381 (1.3079) acc 62.5000 (68.1696) lr 1.6374e-03 eta 0:20:58
epoch [9/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.0625 (1.2915) acc 78.1250 (68.3333) lr 1.6374e-03 eta 0:20:53
epoch [9/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3076 (1.2898) acc 68.7500 (68.3984) lr 1.6374e-03 eta 0:20:49
epoch [9/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.1289 (1.2980) acc 68.7500 (68.1618) lr 1.6374e-03 eta 0:20:45
epoch [9/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.010) loss 1.1328 (1.2907) acc 71.8750 (68.2639) lr 1.6374e-03 eta 0:20:41
epoch [9/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1611 (1.3023) acc 68.7500 (68.1250) lr 1.6374e-03 eta 0:20:38
epoch [9/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.4629 (1.3043) acc 50.0000 (67.9062) lr 1.6374e-03 eta 0:20:34
epoch [9/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.6582 (1.3019) acc 56.2500 (68.0060) lr 1.6374e-03 eta 0:20:31
epoch [9/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.0654 (1.3047) acc 71.8750 (68.0114) lr 1.6374e-03 eta 0:20:29
epoch [9/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.9766 (1.3094) acc 56.2500 (68.0707) lr 1.6374e-03 eta 0:20:26
epoch [9/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.3047 (1.3004) acc 68.7500 (68.2292) lr 1.6374e-03 eta 0:20:23
epoch [9/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2939 (1.2993) acc 65.6250 (68.1500) lr 1.6374e-03 eta 0:20:20
epoch [9/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5801 (1.2986) acc 59.3750 (68.0769) lr 1.6374e-03 eta 0:20:18
epoch [9/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0293 (1.2995) acc 71.8750 (68.0556) lr 1.6374e-03 eta 0:20:15
epoch [9/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7998 (1.3096) acc 62.5000 (67.9464) lr 1.6374e-03 eta 0:20:13
epoch [9/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9180 (1.3047) acc 78.1250 (68.0388) lr 1.6374e-03 eta 0:20:11
epoch [9/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8965 (1.3088) acc 62.5000 (67.8333) lr 1.6374e-03 eta 0:20:08
epoch [9/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9639 (1.3042) acc 75.0000 (67.8831) lr 1.6374e-03 eta 0:20:06
epoch [9/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.4053 (1.3081) acc 65.6250 (67.8125) lr 1.6374e-03 eta 0:20:04
epoch [9/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0137 (1.3169) acc 56.2500 (67.5758) lr 1.6374e-03 eta 0:20:02
epoch [9/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4502 (1.3156) acc 59.3750 (67.5735) lr 1.6374e-03 eta 0:20:00
epoch [9/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1904 (1.3146) acc 81.2500 (67.6250) lr 1.6374e-03 eta 0:19:58
epoch [9/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3535 (1.3142) acc 62.5000 (67.6736) lr 1.6374e-03 eta 0:19:56
epoch [9/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3604 (1.3089) acc 71.8750 (67.7027) lr 1.6374e-03 eta 0:19:54
epoch [9/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2344 (1.3066) acc 62.5000 (67.7138) lr 1.6374e-03 eta 0:19:52
epoch [9/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5186 (1.3065) acc 62.5000 (67.7083) lr 1.6374e-03 eta 0:19:50
epoch [9/25] batch [200/250] time 0.291 (0.294) data 0.001 (0.005) loss 0.9028 (1.3047) acc 75.0000 (67.7500) lr 1.6374e-03 eta 0:19:48
epoch [9/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 2.0703 (1.3060) acc 50.0000 (67.7439) lr 1.6374e-03 eta 0:19:46
epoch [9/25] batch [210/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.3418 (1.2999) acc 65.6250 (67.8571) lr 1.6374e-03 eta 0:19:45
epoch [9/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0693 (1.2965) acc 65.6250 (67.8198) lr 1.6374e-03 eta 0:19:43
epoch [9/25] batch [220/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.5254 (1.3015) acc 68.7500 (67.8409) lr 1.6374e-03 eta 0:19:41
epoch [9/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8188 (1.3007) acc 75.0000 (67.8611) lr 1.6374e-03 eta 0:19:39
epoch [9/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5098 (1.3038) acc 68.7500 (67.7582) lr 1.6374e-03 eta 0:19:37
epoch [9/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 0.8975 (1.3032) acc 71.8750 (67.7660) lr 1.6374e-03 eta 0:19:35
epoch [9/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2314 (1.3012) acc 62.5000 (67.8385) lr 1.6374e-03 eta 0:19:34
epoch [9/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0332 (1.3017) acc 65.6250 (67.8061) lr 1.6374e-03 eta 0:19:32
epoch [9/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3828 (1.3026) acc 56.2500 (67.7375) lr 1.5358e-03 eta 0:19:30
epoch [10/25] batch [5/250] time 0.290 (0.453) data 0.000 (0.164) loss 1.8662 (1.2783) acc 62.5000 (68.1250) lr 1.5358e-03 eta 0:30:10
epoch [10/25] batch [10/250] time 0.289 (0.371) data 0.000 (0.082) loss 1.0488 (1.1398) acc 62.5000 (69.0625) lr 1.5358e-03 eta 0:24:40
epoch [10/25] batch [15/250] time 0.290 (0.344) data 0.000 (0.055) loss 1.2236 (1.1212) acc 75.0000 (70.4167) lr 1.5358e-03 eta 0:22:50
epoch [10/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.041) loss 1.5498 (1.1380) acc 68.7500 (70.4688) lr 1.5358e-03 eta 0:21:56
epoch [10/25] batch [25/250] time 0.290 (0.322) data 0.000 (0.033) loss 1.3115 (1.1499) acc 75.0000 (71.2500) lr 1.5358e-03 eta 0:21:21
epoch [10/25] batch [30/250] time 0.290 (0.317) data 0.000 (0.028) loss 1.3477 (1.1517) acc 62.5000 (70.8333) lr 1.5358e-03 eta 0:20:58
epoch [10/25] batch [35/250] time 0.291 (0.313) data 0.000 (0.024) loss 1.7441 (1.1912) acc 53.1250 (70.0893) lr 1.5358e-03 eta 0:20:41
epoch [10/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.5195 (1.2485) acc 62.5000 (68.6719) lr 1.5358e-03 eta 0:20:27
epoch [10/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.2188 (1.2699) acc 65.6250 (68.3333) lr 1.5358e-03 eta 0:20:17
epoch [10/25] batch [50/250] time 0.290 (0.306) data 0.001 (0.017) loss 1.2334 (1.2719) acc 71.8750 (68.2500) lr 1.5358e-03 eta 0:20:08
epoch [10/25] batch [55/250] time 0.290 (0.304) data 0.001 (0.015) loss 1.2451 (1.3021) acc 75.0000 (67.2159) lr 1.5358e-03 eta 0:20:00
epoch [10/25] batch [60/250] time 0.289 (0.303) data 0.001 (0.014) loss 1.2695 (1.2856) acc 65.6250 (67.2917) lr 1.5358e-03 eta 0:19:54
epoch [10/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.4707 (1.2847) acc 71.8750 (67.4519) lr 1.5358e-03 eta 0:19:48
epoch [10/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3164 (1.2972) acc 59.3750 (67.1429) lr 1.5358e-03 eta 0:19:43
epoch [10/25] batch [75/250] time 0.289 (0.300) data 0.001 (0.011) loss 1.3779 (1.2936) acc 71.8750 (67.1250) lr 1.5358e-03 eta 0:19:38
epoch [10/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3154 (1.2987) acc 71.8750 (67.2656) lr 1.5358e-03 eta 0:19:34
epoch [10/25] batch [85/250] time 0.289 (0.299) data 0.001 (0.010) loss 0.7808 (1.2914) acc 87.5000 (67.4265) lr 1.5358e-03 eta 0:19:30
epoch [10/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.4219 (1.2980) acc 71.8750 (67.3264) lr 1.5358e-03 eta 0:19:26
epoch [10/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1309 (1.2974) acc 68.7500 (67.3684) lr 1.5358e-03 eta 0:19:23
epoch [10/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7217 (1.3064) acc 56.2500 (67.2500) lr 1.5358e-03 eta 0:19:20
epoch [10/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 0.9292 (1.3002) acc 78.1250 (67.3810) lr 1.5358e-03 eta 0:19:17
epoch [10/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4824 (1.2983) acc 65.6250 (67.3864) lr 1.5358e-03 eta 0:19:14
epoch [10/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.9658 (1.3034) acc 87.5000 (67.5272) lr 1.5358e-03 eta 0:19:11
epoch [10/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9399 (1.3021) acc 75.0000 (67.5781) lr 1.5358e-03 eta 0:19:09
epoch [10/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.6982 (1.2959) acc 81.2500 (67.8250) lr 1.5358e-03 eta 0:19:06
epoch [10/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.007) loss 2.0508 (1.3047) acc 50.0000 (67.5721) lr 1.5358e-03 eta 0:19:04
epoch [10/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3975 (1.3007) acc 53.1250 (67.5926) lr 1.5358e-03 eta 0:19:01
epoch [10/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6924 (1.2983) acc 59.3750 (67.6562) lr 1.5358e-03 eta 0:18:59
epoch [10/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9385 (1.3048) acc 75.0000 (67.5862) lr 1.5358e-03 eta 0:18:57
epoch [10/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2549 (1.3046) acc 71.8750 (67.6667) lr 1.5358e-03 eta 0:18:54
epoch [10/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2773 (1.2998) acc 59.3750 (67.6815) lr 1.5358e-03 eta 0:18:52
epoch [10/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0752 (1.2947) acc 75.0000 (67.7734) lr 1.5358e-03 eta 0:18:50
epoch [10/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6885 (1.2930) acc 71.8750 (67.8409) lr 1.5358e-03 eta 0:18:48
epoch [10/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2012 (1.2900) acc 65.6250 (67.8676) lr 1.5358e-03 eta 0:18:46
epoch [10/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.1660 (1.2932) acc 59.3750 (67.8929) lr 1.5358e-03 eta 0:18:44
epoch [10/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5889 (1.2930) acc 56.2500 (67.9167) lr 1.5358e-03 eta 0:18:42
epoch [10/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6660 (1.2960) acc 65.6250 (67.8547) lr 1.5358e-03 eta 0:18:40
epoch [10/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.2324 (1.2950) acc 59.3750 (67.8947) lr 1.5358e-03 eta 0:18:38
epoch [10/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.0449 (1.2979) acc 65.6250 (67.8686) lr 1.5358e-03 eta 0:18:36
epoch [10/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1787 (1.2981) acc 68.7500 (67.8281) lr 1.5358e-03 eta 0:18:34
epoch [10/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0244 (1.2983) acc 65.6250 (67.7287) lr 1.5358e-03 eta 0:18:32
epoch [10/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3398 (1.2985) acc 59.3750 (67.7083) lr 1.5358e-03 eta 0:18:30
epoch [10/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1084 (1.2999) acc 65.6250 (67.6744) lr 1.5358e-03 eta 0:18:29
epoch [10/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2529 (1.2948) acc 68.7500 (67.7841) lr 1.5358e-03 eta 0:18:27
epoch [10/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8545 (1.2966) acc 53.1250 (67.7222) lr 1.5358e-03 eta 0:18:25
epoch [10/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8447 (1.2952) acc 78.1250 (67.7310) lr 1.5358e-03 eta 0:18:23
epoch [10/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.0527 (1.2918) acc 78.1250 (67.7527) lr 1.5358e-03 eta 0:18:22
epoch [10/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6914 (1.2884) acc 78.1250 (67.8385) lr 1.5358e-03 eta 0:18:20
epoch [10/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2939 (1.2855) acc 65.6250 (67.8699) lr 1.5358e-03 eta 0:18:18
epoch [10/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1523 (1.2830) acc 68.7500 (67.9250) lr 1.4258e-03 eta 0:18:16
epoch [11/25] batch [5/250] time 0.289 (0.448) data 0.001 (0.159) loss 1.0967 (1.2586) acc 68.7500 (68.1250) lr 1.4258e-03 eta 0:27:56
epoch [11/25] batch [10/250] time 0.289 (0.369) data 0.000 (0.080) loss 1.0293 (1.3828) acc 71.8750 (69.0625) lr 1.4258e-03 eta 0:22:58
epoch [11/25] batch [15/250] time 0.289 (0.342) data 0.000 (0.053) loss 1.0215 (1.2837) acc 78.1250 (69.7917) lr 1.4258e-03 eta 0:21:18
epoch [11/25] batch [20/250] time 0.289 (0.329) data 0.000 (0.040) loss 1.6211 (1.2892) acc 68.7500 (68.5938) lr 1.4258e-03 eta 0:20:27
epoch [11/25] batch [25/250] time 0.290 (0.321) data 0.000 (0.032) loss 1.3848 (1.2533) acc 68.7500 (69.5000) lr 1.4258e-03 eta 0:19:56
epoch [11/25] batch [30/250] time 0.289 (0.316) data 0.000 (0.027) loss 1.0859 (1.2591) acc 75.0000 (69.5833) lr 1.4258e-03 eta 0:19:35
epoch [11/25] batch [35/250] time 0.290 (0.312) data 0.000 (0.023) loss 1.5020 (1.2456) acc 75.0000 (70.4464) lr 1.4258e-03 eta 0:19:20
epoch [11/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.020) loss 0.9868 (1.2506) acc 81.2500 (70.2344) lr 1.4258e-03 eta 0:19:08
epoch [11/25] batch [45/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.0410 (1.2349) acc 84.3750 (70.5556) lr 1.4258e-03 eta 0:18:58
epoch [11/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.5381 (1.2588) acc 62.5000 (69.8750) lr 1.4258e-03 eta 0:18:50
epoch [11/25] batch [55/250] time 0.290 (0.304) data 0.001 (0.015) loss 1.0205 (1.2589) acc 78.1250 (69.9432) lr 1.4258e-03 eta 0:18:43
epoch [11/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.3809 (1.2551) acc 65.6250 (69.7917) lr 1.4258e-03 eta 0:18:37
epoch [11/25] batch [65/250] time 0.290 (0.302) data 0.001 (0.013) loss 1.3379 (1.2566) acc 68.7500 (69.6635) lr 1.4258e-03 eta 0:18:32
epoch [11/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1484 (1.2589) acc 75.0000 (69.6429) lr 1.4258e-03 eta 0:18:27
epoch [11/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.4788 (1.2445) acc 93.7500 (69.8333) lr 1.4258e-03 eta 0:18:22
epoch [11/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.7744 (1.2552) acc 56.2500 (69.5312) lr 1.4258e-03 eta 0:18:18
epoch [11/25] batch [85/250] time 0.290 (0.299) data 0.001 (0.010) loss 1.4775 (1.2597) acc 68.7500 (69.5221) lr 1.4258e-03 eta 0:18:15
epoch [11/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 2.2207 (1.2821) acc 40.6250 (69.1667) lr 1.4258e-03 eta 0:18:11
epoch [11/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0986 (1.2798) acc 90.6250 (69.3421) lr 1.4258e-03 eta 0:18:08
epoch [11/25] batch [100/250] time 0.291 (0.298) data 0.000 (0.008) loss 1.4092 (1.2833) acc 59.3750 (69.0000) lr 1.4258e-03 eta 0:18:06
epoch [11/25] batch [105/250] time 0.291 (0.297) data 0.000 (0.008) loss 1.9102 (1.2803) acc 56.2500 (68.9881) lr 1.4258e-03 eta 0:18:03
epoch [11/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4404 (1.2839) acc 68.7500 (68.9205) lr 1.4258e-03 eta 0:18:00
epoch [11/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.007) loss 0.9902 (1.2979) acc 78.1250 (68.7228) lr 1.4258e-03 eta 0:17:57
epoch [11/25] batch [120/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.3672 (1.2992) acc 65.6250 (68.6458) lr 1.4258e-03 eta 0:17:55
epoch [11/25] batch [125/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0508 (1.2940) acc 78.1250 (68.8750) lr 1.4258e-03 eta 0:17:53
epoch [11/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4814 (1.2879) acc 56.2500 (68.9183) lr 1.4258e-03 eta 0:17:50
epoch [11/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.0703 (1.2884) acc 84.3750 (69.0741) lr 1.4258e-03 eta 0:17:48
epoch [11/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.4619 (1.2906) acc 65.6250 (68.9732) lr 1.4258e-03 eta 0:17:46
epoch [11/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.8237 (1.2917) acc 81.2500 (68.9871) lr 1.4258e-03 eta 0:17:44
epoch [11/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9922 (1.2908) acc 78.1250 (69.0000) lr 1.4258e-03 eta 0:17:42
epoch [11/25] batch [155/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2480 (1.2863) acc 78.1250 (69.1734) lr 1.4258e-03 eta 0:17:39
epoch [11/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.9136 (1.2791) acc 78.1250 (69.3164) lr 1.4258e-03 eta 0:17:37
epoch [11/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.0254 (1.2746) acc 75.0000 (69.2803) lr 1.4258e-03 eta 0:17:36
epoch [11/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3340 (1.2720) acc 71.8750 (69.2096) lr 1.4258e-03 eta 0:17:33
epoch [11/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1289 (1.2676) acc 75.0000 (69.2143) lr 1.4258e-03 eta 0:17:31
epoch [11/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4463 (1.2689) acc 65.6250 (69.1146) lr 1.4258e-03 eta 0:17:30
epoch [11/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0039 (1.2716) acc 62.5000 (69.0203) lr 1.4258e-03 eta 0:17:28
epoch [11/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2744 (1.2697) acc 56.2500 (68.9145) lr 1.4258e-03 eta 0:17:26
epoch [11/25] batch [195/250] time 0.290 (0.294) data 0.001 (0.004) loss 1.2627 (1.2655) acc 68.7500 (68.9103) lr 1.4258e-03 eta 0:17:24
epoch [11/25] batch [200/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.2188 (1.2656) acc 81.2500 (68.9375) lr 1.4258e-03 eta 0:17:22
epoch [11/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.2822 (1.2668) acc 65.6250 (68.9177) lr 1.4258e-03 eta 0:17:20
epoch [11/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.8672 (1.2664) acc 56.2500 (68.8690) lr 1.4258e-03 eta 0:17:19
epoch [11/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3291 (1.2694) acc 71.8750 (68.7645) lr 1.4258e-03 eta 0:17:17
epoch [11/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2383 (1.2734) acc 65.6250 (68.6648) lr 1.4258e-03 eta 0:17:15
epoch [11/25] batch [225/250] time 0.293 (0.293) data 0.000 (0.004) loss 1.2832 (1.2765) acc 68.7500 (68.6111) lr 1.4258e-03 eta 0:17:13
epoch [11/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3252 (1.2764) acc 59.3750 (68.6413) lr 1.4258e-03 eta 0:17:12
epoch [11/25] batch [235/250] time 0.291 (0.293) data 0.001 (0.004) loss 1.5928 (1.2823) acc 65.6250 (68.6436) lr 1.4258e-03 eta 0:17:10
epoch [11/25] batch [240/250] time 0.292 (0.293) data 0.000 (0.004) loss 1.4355 (1.2854) acc 62.5000 (68.6198) lr 1.4258e-03 eta 0:17:08
epoch [11/25] batch [245/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.4043 (1.2855) acc 59.3750 (68.5332) lr 1.4258e-03 eta 0:17:07
epoch [11/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0752 (1.2823) acc 68.7500 (68.6375) lr 1.3090e-03 eta 0:17:05
epoch [12/25] batch [5/250] time 0.292 (0.462) data 0.001 (0.171) loss 0.6885 (1.1074) acc 81.2500 (71.8750) lr 1.3090e-03 eta 0:26:54
epoch [12/25] batch [10/250] time 0.291 (0.376) data 0.000 (0.086) loss 1.1279 (1.1622) acc 71.8750 (73.4375) lr 1.3090e-03 eta 0:21:52
epoch [12/25] batch [15/250] time 0.290 (0.347) data 0.000 (0.057) loss 0.8501 (1.2005) acc 71.8750 (72.2917) lr 1.3090e-03 eta 0:20:10
epoch [12/25] batch [20/250] time 0.290 (0.333) data 0.000 (0.043) loss 1.4004 (1.2497) acc 65.6250 (70.4688) lr 1.3090e-03 eta 0:19:19
epoch [12/25] batch [25/250] time 0.293 (0.325) data 0.001 (0.034) loss 1.2891 (1.2559) acc 68.7500 (70.2500) lr 1.3090e-03 eta 0:18:48
epoch [12/25] batch [30/250] time 0.291 (0.319) data 0.000 (0.029) loss 1.3105 (1.2888) acc 78.1250 (70.1042) lr 1.3090e-03 eta 0:18:26
epoch [12/25] batch [35/250] time 0.290 (0.315) data 0.000 (0.025) loss 1.3906 (1.2944) acc 68.7500 (69.7321) lr 1.3090e-03 eta 0:18:11
epoch [12/25] batch [40/250] time 0.290 (0.312) data 0.000 (0.022) loss 1.2979 (1.3037) acc 62.5000 (68.5156) lr 1.3090e-03 eta 0:17:58
epoch [12/25] batch [45/250] time 0.290 (0.309) data 0.000 (0.019) loss 1.4961 (1.3018) acc 65.6250 (68.1944) lr 1.3090e-03 eta 0:17:49
epoch [12/25] batch [50/250] time 0.290 (0.308) data 0.000 (0.017) loss 1.3564 (1.3286) acc 68.7500 (67.5625) lr 1.3090e-03 eta 0:17:41
epoch [12/25] batch [55/250] time 0.290 (0.306) data 0.000 (0.016) loss 1.0996 (1.3142) acc 68.7500 (67.6705) lr 1.3090e-03 eta 0:17:33
epoch [12/25] batch [60/250] time 0.289 (0.305) data 0.000 (0.015) loss 0.7119 (1.3079) acc 81.2500 (67.8646) lr 1.3090e-03 eta 0:17:27
epoch [12/25] batch [65/250] time 0.290 (0.303) data 0.000 (0.013) loss 1.2832 (1.2976) acc 71.8750 (68.3654) lr 1.3090e-03 eta 0:17:22
epoch [12/25] batch [70/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.9248 (1.3026) acc 62.5000 (68.0357) lr 1.3090e-03 eta 0:17:17
epoch [12/25] batch [75/250] time 0.290 (0.302) data 0.000 (0.012) loss 1.9053 (1.3107) acc 59.3750 (67.9167) lr 1.3090e-03 eta 0:17:13
epoch [12/25] batch [80/250] time 0.290 (0.301) data 0.000 (0.011) loss 1.5947 (1.3079) acc 59.3750 (67.6172) lr 1.3090e-03 eta 0:17:09
epoch [12/25] batch [85/250] time 0.290 (0.300) data 0.000 (0.010) loss 1.5068 (1.3097) acc 65.6250 (67.7574) lr 1.3090e-03 eta 0:17:05
epoch [12/25] batch [90/250] time 0.291 (0.300) data 0.000 (0.010) loss 0.8931 (1.3039) acc 81.2500 (67.9514) lr 1.3090e-03 eta 0:17:02
epoch [12/25] batch [95/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.4717 (1.3034) acc 59.3750 (67.8947) lr 1.3090e-03 eta 0:16:59
epoch [12/25] batch [100/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.4033 (1.3032) acc 75.0000 (68.0938) lr 1.3090e-03 eta 0:16:56
epoch [12/25] batch [105/250] time 0.291 (0.298) data 0.000 (0.008) loss 1.0596 (1.2967) acc 71.8750 (68.2738) lr 1.3090e-03 eta 0:16:53
epoch [12/25] batch [110/250] time 0.291 (0.298) data 0.001 (0.008) loss 1.3887 (1.2938) acc 68.7500 (68.3523) lr 1.3090e-03 eta 0:16:50
epoch [12/25] batch [115/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.4346 (1.2973) acc 68.7500 (68.1522) lr 1.3090e-03 eta 0:16:47
epoch [12/25] batch [120/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.6172 (1.2923) acc 65.6250 (68.3854) lr 1.3090e-03 eta 0:16:45
epoch [12/25] batch [125/250] time 0.291 (0.297) data 0.000 (0.007) loss 1.0137 (1.2907) acc 75.0000 (68.4750) lr 1.3090e-03 eta 0:16:42
epoch [12/25] batch [130/250] time 0.291 (0.297) data 0.001 (0.007) loss 1.4111 (1.2830) acc 59.3750 (68.6298) lr 1.3090e-03 eta 0:16:40
epoch [12/25] batch [135/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.3184 (1.2822) acc 75.0000 (68.5648) lr 1.3090e-03 eta 0:16:38
epoch [12/25] batch [140/250] time 0.292 (0.296) data 0.001 (0.006) loss 1.6816 (1.2820) acc 65.6250 (68.5491) lr 1.3090e-03 eta 0:16:36
epoch [12/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.6289 (1.2819) acc 53.1250 (68.5560) lr 1.3090e-03 eta 0:16:34
epoch [12/25] batch [150/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.3291 (1.2735) acc 68.7500 (68.7708) lr 1.3090e-03 eta 0:16:31
epoch [12/25] batch [155/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.0156 (1.2765) acc 81.2500 (68.8306) lr 1.3090e-03 eta 0:16:29
epoch [12/25] batch [160/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.7373 (1.2775) acc 59.3750 (68.8086) lr 1.3090e-03 eta 0:16:27
epoch [12/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2109 (1.2776) acc 78.1250 (68.7689) lr 1.3090e-03 eta 0:16:25
epoch [12/25] batch [170/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.1836 (1.2788) acc 59.3750 (68.5662) lr 1.3090e-03 eta 0:16:23
epoch [12/25] batch [175/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.8047 (1.2794) acc 81.2500 (68.6071) lr 1.3090e-03 eta 0:16:21
epoch [12/25] batch [180/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.8013 (1.2804) acc 84.3750 (68.7153) lr 1.3090e-03 eta 0:16:19
epoch [12/25] batch [185/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.5361 (1.2839) acc 65.6250 (68.5980) lr 1.3090e-03 eta 0:16:17
epoch [12/25] batch [190/250] time 0.290 (0.295) data 0.001 (0.005) loss 1.6465 (1.2878) acc 59.3750 (68.4868) lr 1.3090e-03 eta 0:16:15
epoch [12/25] batch [195/250] time 0.290 (0.295) data 0.001 (0.005) loss 1.5430 (1.2926) acc 59.3750 (68.3654) lr 1.3090e-03 eta 0:16:13
epoch [12/25] batch [200/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.1191 (1.2950) acc 71.8750 (68.3125) lr 1.3090e-03 eta 0:16:11
epoch [12/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1855 (1.2935) acc 68.7500 (68.3994) lr 1.3090e-03 eta 0:16:10
epoch [12/25] batch [210/250] time 0.290 (0.294) data 0.001 (0.004) loss 1.0322 (1.2924) acc 71.8750 (68.4226) lr 1.3090e-03 eta 0:16:08
epoch [12/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.7158 (1.2893) acc 62.5000 (68.4593) lr 1.3090e-03 eta 0:16:06
epoch [12/25] batch [220/250] time 0.294 (0.294) data 0.000 (0.004) loss 1.7881 (1.2897) acc 68.7500 (68.4375) lr 1.3090e-03 eta 0:16:04
epoch [12/25] batch [225/250] time 0.290 (0.294) data 0.000 (0.004) loss 2.0586 (1.2895) acc 62.5000 (68.4722) lr 1.3090e-03 eta 0:16:02
epoch [12/25] batch [230/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.4834 (1.2943) acc 65.6250 (68.3288) lr 1.3090e-03 eta 0:16:01
epoch [12/25] batch [235/250] time 0.290 (0.294) data 0.001 (0.004) loss 1.4746 (1.2916) acc 50.0000 (68.2048) lr 1.3090e-03 eta 0:15:59
epoch [12/25] batch [240/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.2461 (1.2938) acc 75.0000 (68.1120) lr 1.3090e-03 eta 0:15:57
epoch [12/25] batch [245/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.3018 (1.2930) acc 71.8750 (68.1122) lr 1.3090e-03 eta 0:15:55
epoch [12/25] batch [250/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.8477 (1.2954) acc 62.5000 (68.1875) lr 1.1874e-03 eta 0:15:54
epoch [13/25] batch [5/250] time 0.291 (0.452) data 0.000 (0.162) loss 1.0137 (1.2242) acc 81.2500 (71.8750) lr 1.1874e-03 eta 0:24:25
epoch [13/25] batch [10/250] time 0.290 (0.371) data 0.000 (0.081) loss 1.4980 (1.2277) acc 65.6250 (70.9375) lr 1.1874e-03 eta 0:20:03
epoch [13/25] batch [15/250] time 0.289 (0.344) data 0.000 (0.054) loss 0.8940 (1.2049) acc 75.0000 (69.1667) lr 1.1874e-03 eta 0:18:33
epoch [13/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.041) loss 1.2080 (1.2253) acc 78.1250 (69.0625) lr 1.1874e-03 eta 0:17:47
epoch [13/25] batch [25/250] time 0.290 (0.322) data 0.000 (0.033) loss 1.4307 (1.2435) acc 62.5000 (68.3750) lr 1.1874e-03 eta 0:17:19
epoch [13/25] batch [30/250] time 0.289 (0.317) data 0.001 (0.027) loss 1.5244 (1.2909) acc 62.5000 (67.5000) lr 1.1874e-03 eta 0:17:00
epoch [13/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.5977 (1.2489) acc 84.3750 (69.0179) lr 1.1874e-03 eta 0:16:46
epoch [13/25] batch [40/250] time 0.288 (0.310) data 0.000 (0.021) loss 1.1279 (1.2288) acc 68.7500 (69.2188) lr 1.1874e-03 eta 0:16:34
epoch [13/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.018) loss 1.2002 (1.2339) acc 62.5000 (69.5139) lr 1.1874e-03 eta 0:16:25
epoch [13/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.0273 (1.2150) acc 71.8750 (70.0000) lr 1.1874e-03 eta 0:16:18
epoch [13/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.9126 (1.2278) acc 71.8750 (69.5455) lr 1.1874e-03 eta 0:16:11
epoch [13/25] batch [60/250] time 0.289 (0.303) data 0.001 (0.014) loss 1.5488 (1.2409) acc 53.1250 (68.9583) lr 1.1874e-03 eta 0:16:06
epoch [13/25] batch [65/250] time 0.290 (0.302) data 0.001 (0.013) loss 1.4619 (1.2366) acc 65.6250 (68.8942) lr 1.1874e-03 eta 0:16:01
epoch [13/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.5234 (1.2494) acc 68.7500 (68.7054) lr 1.1874e-03 eta 0:15:57
epoch [13/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.7207 (1.2444) acc 65.6250 (69.1250) lr 1.1874e-03 eta 0:15:53
epoch [13/25] batch [80/250] time 0.288 (0.300) data 0.000 (0.011) loss 0.8457 (1.2545) acc 78.1250 (69.0234) lr 1.1874e-03 eta 0:15:49
epoch [13/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3008 (1.2529) acc 71.8750 (69.1176) lr 1.1874e-03 eta 0:15:46
epoch [13/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7949 (1.2537) acc 81.2500 (69.0972) lr 1.1874e-03 eta 0:15:42
epoch [13/25] batch [95/250] time 0.289 (0.298) data 0.001 (0.009) loss 1.0586 (1.2608) acc 71.8750 (68.7171) lr 1.1874e-03 eta 0:15:39
epoch [13/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8179 (1.2502) acc 75.0000 (68.9375) lr 1.1874e-03 eta 0:15:36
epoch [13/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.6279 (1.2529) acc 59.3750 (68.8095) lr 1.1874e-03 eta 0:15:34
epoch [13/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3135 (1.2540) acc 71.8750 (68.8636) lr 1.1874e-03 eta 0:15:31
epoch [13/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4082 (1.2630) acc 65.6250 (68.5870) lr 1.1874e-03 eta 0:15:29
epoch [13/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3047 (1.2560) acc 75.0000 (68.8281) lr 1.1874e-03 eta 0:15:26
epoch [13/25] batch [125/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0586 (1.2507) acc 65.6250 (68.9250) lr 1.1874e-03 eta 0:15:24
epoch [13/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0703 (1.2484) acc 75.0000 (68.9663) lr 1.1874e-03 eta 0:15:22
epoch [13/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0176 (1.2419) acc 75.0000 (69.1667) lr 1.1874e-03 eta 0:15:19
epoch [13/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.5273 (1.2543) acc 65.6250 (68.9286) lr 1.1874e-03 eta 0:15:17
epoch [13/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9932 (1.2534) acc 68.7500 (68.8362) lr 1.1874e-03 eta 0:15:15
epoch [13/25] batch [150/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.0332 (1.2501) acc 78.1250 (68.8542) lr 1.1874e-03 eta 0:15:13
epoch [13/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3965 (1.2480) acc 65.6250 (68.9113) lr 1.1874e-03 eta 0:15:11
epoch [13/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4160 (1.2495) acc 78.1250 (68.8672) lr 1.1874e-03 eta 0:15:09
epoch [13/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5576 (1.2488) acc 62.5000 (68.9394) lr 1.1874e-03 eta 0:15:07
epoch [13/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2197 (1.2474) acc 71.8750 (69.0441) lr 1.1874e-03 eta 0:15:05
epoch [13/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5947 (1.2504) acc 56.2500 (68.7857) lr 1.1874e-03 eta 0:15:03
epoch [13/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2627 (1.2474) acc 62.5000 (68.7847) lr 1.1874e-03 eta 0:15:02
epoch [13/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8691 (1.2461) acc 75.0000 (68.8851) lr 1.1874e-03 eta 0:15:00
epoch [13/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3574 (1.2460) acc 71.8750 (68.8816) lr 1.1874e-03 eta 0:14:58
epoch [13/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.9873 (1.2520) acc 71.8750 (68.6058) lr 1.1874e-03 eta 0:14:56
epoch [13/25] batch [200/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.4600 (1.2489) acc 56.2500 (68.6250) lr 1.1874e-03 eta 0:14:54
epoch [13/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1875 (1.2496) acc 75.0000 (68.6738) lr 1.1874e-03 eta 0:14:53
epoch [13/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5371 (1.2511) acc 71.8750 (68.6756) lr 1.1874e-03 eta 0:14:51
epoch [13/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7524 (1.2458) acc 81.2500 (68.7936) lr 1.1874e-03 eta 0:14:49
epoch [13/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4980 (1.2452) acc 56.2500 (68.7642) lr 1.1874e-03 eta 0:14:47
epoch [13/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5078 (1.2497) acc 56.2500 (68.6389) lr 1.1874e-03 eta 0:14:46
epoch [13/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9062 (1.2537) acc 71.8750 (68.5870) lr 1.1874e-03 eta 0:14:44
epoch [13/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.2451 (1.2533) acc 71.8750 (68.6170) lr 1.1874e-03 eta 0:14:42
epoch [13/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3828 (1.2530) acc 68.7500 (68.6198) lr 1.1874e-03 eta 0:14:40
epoch [13/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3828 (1.2499) acc 56.2500 (68.6224) lr 1.1874e-03 eta 0:14:39
epoch [13/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0156 (1.2480) acc 75.0000 (68.6625) lr 1.0628e-03 eta 0:14:37
epoch [14/25] batch [5/250] time 0.290 (0.457) data 0.000 (0.167) loss 1.1064 (1.0933) acc 68.7500 (72.5000) lr 1.0628e-03 eta 0:22:47
epoch [14/25] batch [10/250] time 0.290 (0.373) data 0.001 (0.084) loss 1.4932 (1.1443) acc 65.6250 (72.1875) lr 1.0628e-03 eta 0:18:36
epoch [14/25] batch [15/250] time 0.290 (0.345) data 0.000 (0.056) loss 1.2383 (1.2242) acc 65.6250 (70.6250) lr 1.0628e-03 eta 0:17:11
epoch [14/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.042) loss 1.0820 (1.2495) acc 68.7500 (68.1250) lr 1.0628e-03 eta 0:16:27
epoch [14/25] batch [25/250] time 0.290 (0.323) data 0.000 (0.034) loss 1.2617 (1.2643) acc 50.0000 (67.2500) lr 1.0628e-03 eta 0:16:01
epoch [14/25] batch [30/250] time 0.292 (0.318) data 0.000 (0.028) loss 0.8306 (1.2345) acc 78.1250 (67.7083) lr 1.0628e-03 eta 0:15:43
epoch [14/25] batch [35/250] time 0.289 (0.314) data 0.001 (0.024) loss 1.4121 (1.2736) acc 68.7500 (67.2321) lr 1.0628e-03 eta 0:15:30
epoch [14/25] batch [40/250] time 0.289 (0.311) data 0.000 (0.021) loss 1.5186 (1.2773) acc 65.6250 (68.0469) lr 1.0628e-03 eta 0:15:19
epoch [14/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 0.7822 (1.2732) acc 78.1250 (67.9167) lr 1.0628e-03 eta 0:15:10
epoch [14/25] batch [50/250] time 0.291 (0.306) data 0.000 (0.017) loss 1.1523 (1.2574) acc 71.8750 (68.4375) lr 1.0628e-03 eta 0:15:03
epoch [14/25] batch [55/250] time 0.290 (0.305) data 0.001 (0.016) loss 1.7129 (1.2516) acc 59.3750 (68.6364) lr 1.0628e-03 eta 0:14:57
epoch [14/25] batch [60/250] time 0.291 (0.304) data 0.001 (0.014) loss 1.0684 (1.2424) acc 68.7500 (69.0104) lr 1.0628e-03 eta 0:14:52
epoch [14/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.9429 (1.2337) acc 71.8750 (69.1346) lr 1.0628e-03 eta 0:14:47
epoch [14/25] batch [70/250] time 0.289 (0.302) data 0.000 (0.012) loss 1.3262 (1.2278) acc 68.7500 (69.1964) lr 1.0628e-03 eta 0:14:43
epoch [14/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.011) loss 1.5508 (1.2358) acc 71.8750 (69.3333) lr 1.0628e-03 eta 0:14:39
epoch [14/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9321 (1.2302) acc 81.2500 (69.5703) lr 1.0628e-03 eta 0:14:35
epoch [14/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.2002 (1.2363) acc 65.6250 (69.2647) lr 1.0628e-03 eta 0:14:32
epoch [14/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1680 (1.2277) acc 65.6250 (69.4097) lr 1.0628e-03 eta 0:14:29
epoch [14/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2490 (1.2236) acc 75.0000 (69.5724) lr 1.0628e-03 eta 0:14:26
epoch [14/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0410 (1.2285) acc 68.7500 (69.5938) lr 1.0628e-03 eta 0:14:23
epoch [14/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5400 (1.2305) acc 59.3750 (69.5833) lr 1.0628e-03 eta 0:14:20
epoch [14/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2666 (1.2286) acc 65.6250 (69.7159) lr 1.0628e-03 eta 0:14:18
epoch [14/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8730 (1.2295) acc 78.1250 (69.8098) lr 1.0628e-03 eta 0:14:15
epoch [14/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2705 (1.2371) acc 68.7500 (69.6615) lr 1.0628e-03 eta 0:14:13
epoch [14/25] batch [125/250] time 0.292 (0.296) data 0.000 (0.007) loss 1.3418 (1.2405) acc 68.7500 (69.4500) lr 1.0628e-03 eta 0:14:11
epoch [14/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9390 (1.2379) acc 81.2500 (69.6394) lr 1.0628e-03 eta 0:14:09
epoch [14/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8877 (1.2529) acc 50.0000 (69.2361) lr 1.0628e-03 eta 0:14:06
epoch [14/25] batch [140/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.2656 (1.2459) acc 81.2500 (69.4420) lr 1.0628e-03 eta 0:14:04
epoch [14/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2051 (1.2506) acc 65.6250 (69.3966) lr 1.0628e-03 eta 0:14:02
epoch [14/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.1689 (1.2500) acc 68.7500 (69.2917) lr 1.0628e-03 eta 0:14:00
epoch [14/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8008 (1.2579) acc 71.8750 (69.0524) lr 1.0628e-03 eta 0:13:58
epoch [14/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2744 (1.2626) acc 68.7500 (68.9844) lr 1.0628e-03 eta 0:13:56
epoch [14/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0645 (1.2645) acc 68.7500 (68.9394) lr 1.0628e-03 eta 0:13:54
epoch [14/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0078 (1.2754) acc 56.2500 (68.7132) lr 1.0628e-03 eta 0:13:52
epoch [14/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4912 (1.2750) acc 59.3750 (68.7321) lr 1.0628e-03 eta 0:13:50
epoch [14/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9917 (1.2768) acc 81.2500 (68.7847) lr 1.0628e-03 eta 0:13:49
epoch [14/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.7705 (1.2794) acc 81.2500 (68.6993) lr 1.0628e-03 eta 0:13:47
epoch [14/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7578 (1.2805) acc 78.1250 (68.6349) lr 1.0628e-03 eta 0:13:45
epoch [14/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8857 (1.2782) acc 71.8750 (68.6218) lr 1.0628e-03 eta 0:13:43
epoch [14/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.7832 (1.2826) acc 56.2500 (68.5156) lr 1.0628e-03 eta 0:13:41
epoch [14/25] batch [205/250] time 0.294 (0.293) data 0.000 (0.004) loss 1.1250 (1.2782) acc 68.7500 (68.5213) lr 1.0628e-03 eta 0:13:40
epoch [14/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6230 (1.2806) acc 62.5000 (68.4375) lr 1.0628e-03 eta 0:13:38
epoch [14/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3496 (1.2805) acc 65.6250 (68.4012) lr 1.0628e-03 eta 0:13:36
epoch [14/25] batch [220/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.1299 (1.2770) acc 68.7500 (68.3665) lr 1.0628e-03 eta 0:13:35
epoch [14/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8135 (1.2718) acc 78.1250 (68.5556) lr 1.0628e-03 eta 0:13:33
epoch [14/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.6851 (1.2709) acc 81.2500 (68.6005) lr 1.0628e-03 eta 0:13:31
epoch [14/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.3857 (1.2723) acc 56.2500 (68.5505) lr 1.0628e-03 eta 0:13:29
epoch [14/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7119 (1.2737) acc 62.5000 (68.5026) lr 1.0628e-03 eta 0:13:28
epoch [14/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0215 (1.2695) acc 78.1250 (68.5969) lr 1.0628e-03 eta 0:13:26
epoch [14/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2930 (1.2677) acc 65.6250 (68.6625) lr 9.3721e-04 eta 0:13:24
epoch [15/25] batch [5/250] time 0.289 (0.464) data 0.000 (0.174) loss 1.1523 (1.2526) acc 75.0000 (68.1250) lr 9.3721e-04 eta 0:21:13
epoch [15/25] batch [10/250] time 0.289 (0.376) data 0.000 (0.087) loss 1.0527 (1.2026) acc 62.5000 (68.1250) lr 9.3721e-04 eta 0:17:11
epoch [15/25] batch [15/250] time 0.291 (0.347) data 0.000 (0.058) loss 2.4160 (1.2921) acc 53.1250 (66.6667) lr 9.3721e-04 eta 0:15:50
epoch [15/25] batch [20/250] time 0.289 (0.333) data 0.000 (0.044) loss 0.7368 (1.2856) acc 81.2500 (67.6562) lr 9.3721e-04 eta 0:15:09
epoch [15/25] batch [25/250] time 0.289 (0.324) data 0.000 (0.035) loss 1.4111 (1.3043) acc 71.8750 (67.3750) lr 9.3721e-04 eta 0:14:43
epoch [15/25] batch [30/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.6045 (1.2985) acc 65.6250 (67.3958) lr 9.3721e-04 eta 0:14:26
epoch [15/25] batch [35/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.8037 (1.3068) acc 62.5000 (67.3214) lr 9.3721e-04 eta 0:14:13
epoch [15/25] batch [40/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.5527 (1.2999) acc 53.1250 (67.3438) lr 9.3721e-04 eta 0:14:02
epoch [15/25] batch [45/250] time 0.289 (0.309) data 0.000 (0.020) loss 1.2930 (1.2854) acc 65.6250 (67.9861) lr 9.3721e-04 eta 0:13:54
epoch [15/25] batch [50/250] time 0.290 (0.307) data 0.000 (0.018) loss 1.7891 (1.2773) acc 59.3750 (68.0000) lr 9.3721e-04 eta 0:13:47
epoch [15/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.1045 (1.2576) acc 78.1250 (68.5795) lr 9.3721e-04 eta 0:13:42
epoch [15/25] batch [60/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.9131 (1.2421) acc 78.1250 (68.9062) lr 9.3721e-04 eta 0:13:37
epoch [15/25] batch [65/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.3525 (1.2360) acc 62.5000 (69.0865) lr 9.3721e-04 eta 0:13:32
epoch [15/25] batch [70/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.5889 (1.2376) acc 53.1250 (68.9732) lr 9.3721e-04 eta 0:13:28
epoch [15/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.2256 (1.2465) acc 68.7500 (68.8333) lr 9.3721e-04 eta 0:13:24
epoch [15/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9976 (1.2422) acc 75.0000 (68.8281) lr 9.3721e-04 eta 0:13:20
epoch [15/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.011) loss 1.0723 (1.2459) acc 78.1250 (68.6765) lr 9.3721e-04 eta 0:13:17
epoch [15/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1650 (1.2437) acc 75.0000 (68.7153) lr 9.3721e-04 eta 0:13:14
epoch [15/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7349 (1.2416) acc 78.1250 (68.8816) lr 9.3721e-04 eta 0:13:11
epoch [15/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6445 (1.2414) acc 46.8750 (68.8438) lr 9.3721e-04 eta 0:13:09
epoch [15/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.009) loss 1.0947 (1.2378) acc 78.1250 (68.9881) lr 9.3721e-04 eta 0:13:06
epoch [15/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.2539 (1.2310) acc 65.6250 (69.2330) lr 9.3721e-04 eta 0:13:04
epoch [15/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0547 (1.2308) acc 71.8750 (69.2935) lr 9.3721e-04 eta 0:13:01
epoch [15/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.008) loss 0.7744 (1.2277) acc 81.2500 (69.3229) lr 9.3721e-04 eta 0:12:59
epoch [15/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9380 (1.2307) acc 75.0000 (69.4000) lr 9.3721e-04 eta 0:12:57
epoch [15/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3975 (1.2312) acc 68.7500 (69.6154) lr 9.3721e-04 eta 0:12:55
epoch [15/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3057 (1.2325) acc 68.7500 (69.6528) lr 9.3721e-04 eta 0:12:53
epoch [15/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.2773 (1.2249) acc 71.8750 (69.8214) lr 9.3721e-04 eta 0:12:51
epoch [15/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2129 (1.2261) acc 75.0000 (69.8491) lr 9.3721e-04 eta 0:12:49
epoch [15/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4473 (1.2338) acc 68.7500 (69.7917) lr 9.3721e-04 eta 0:12:47
epoch [15/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9800 (1.2276) acc 71.8750 (69.9798) lr 9.3721e-04 eta 0:12:45
epoch [15/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.006) loss 2.0078 (1.2296) acc 56.2500 (70.0195) lr 9.3721e-04 eta 0:12:43
epoch [15/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3311 (1.2257) acc 71.8750 (70.2083) lr 9.3721e-04 eta 0:12:41
epoch [15/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3730 (1.2279) acc 62.5000 (70.1654) lr 9.3721e-04 eta 0:12:39
epoch [15/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0732 (1.2305) acc 68.7500 (70.0179) lr 9.3721e-04 eta 0:12:37
epoch [15/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3877 (1.2296) acc 56.2500 (69.8438) lr 9.3721e-04 eta 0:12:35
epoch [15/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1221 (1.2283) acc 71.8750 (69.9662) lr 9.3721e-04 eta 0:12:33
epoch [15/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0625 (1.2294) acc 71.8750 (69.8191) lr 9.3721e-04 eta 0:12:32
epoch [15/25] batch [195/250] time 0.289 (0.294) data 0.001 (0.005) loss 1.6035 (1.2280) acc 59.3750 (69.9359) lr 9.3721e-04 eta 0:12:30
epoch [15/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2783 (1.2298) acc 71.8750 (69.9375) lr 9.3721e-04 eta 0:12:28
epoch [15/25] batch [205/250] time 0.291 (0.293) data 0.000 (0.005) loss 1.4043 (1.2317) acc 62.5000 (69.8476) lr 9.3721e-04 eta 0:12:26
epoch [15/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2188 (1.2370) acc 71.8750 (69.7619) lr 9.3721e-04 eta 0:12:25
epoch [15/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9155 (1.2324) acc 75.0000 (69.8110) lr 9.3721e-04 eta 0:12:23
epoch [15/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3936 (1.2336) acc 71.8750 (69.8438) lr 9.3721e-04 eta 0:12:21
epoch [15/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4521 (1.2383) acc 75.0000 (69.7917) lr 9.3721e-04 eta 0:12:19
epoch [15/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6641 (1.2455) acc 65.6250 (69.6332) lr 9.3721e-04 eta 0:12:18
epoch [15/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.2910 (1.2409) acc 62.5000 (69.7473) lr 9.3721e-04 eta 0:12:16
epoch [15/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9941 (1.2392) acc 75.0000 (69.7786) lr 9.3721e-04 eta 0:12:14
epoch [15/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0771 (1.2377) acc 78.1250 (69.7959) lr 9.3721e-04 eta 0:12:13
epoch [15/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2002 (1.2380) acc 71.8750 (69.8000) lr 8.1262e-04 eta 0:12:11
epoch [16/25] batch [5/250] time 0.289 (0.446) data 0.000 (0.157) loss 0.7549 (0.8061) acc 75.0000 (79.3750) lr 8.1262e-04 eta 0:18:32
epoch [16/25] batch [10/250] time 0.288 (0.367) data 0.000 (0.079) loss 0.9517 (0.9867) acc 75.0000 (74.6875) lr 8.1262e-04 eta 0:15:14
epoch [16/25] batch [15/250] time 0.288 (0.341) data 0.000 (0.053) loss 1.4551 (1.1907) acc 59.3750 (69.7917) lr 8.1262e-04 eta 0:14:07
epoch [16/25] batch [20/250] time 0.288 (0.328) data 0.000 (0.039) loss 1.6035 (1.2688) acc 53.1250 (68.4375) lr 8.1262e-04 eta 0:13:32
epoch [16/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.032) loss 1.3857 (1.2856) acc 71.8750 (68.1250) lr 8.1262e-04 eta 0:13:12
epoch [16/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.026) loss 0.9287 (1.2697) acc 84.3750 (68.8542) lr 8.1262e-04 eta 0:12:57
epoch [16/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.023) loss 1.2637 (1.2622) acc 71.8750 (69.0179) lr 8.1262e-04 eta 0:12:47
epoch [16/25] batch [40/250] time 0.289 (0.309) data 0.000 (0.020) loss 0.7632 (1.2736) acc 75.0000 (68.4375) lr 8.1262e-04 eta 0:12:38
epoch [16/25] batch [45/250] time 0.290 (0.306) data 0.000 (0.018) loss 1.0176 (1.2507) acc 71.8750 (68.6111) lr 8.1262e-04 eta 0:12:32
epoch [16/25] batch [50/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.3613 (1.2637) acc 59.3750 (68.0625) lr 8.1262e-04 eta 0:12:26
epoch [16/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.015) loss 1.3564 (1.2651) acc 71.8750 (68.6364) lr 8.1262e-04 eta 0:12:21
epoch [16/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.7275 (1.2596) acc 65.6250 (68.7500) lr 8.1262e-04 eta 0:12:16
epoch [16/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.2188 (1.2623) acc 81.2500 (68.8942) lr 8.1262e-04 eta 0:12:13
epoch [16/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9653 (1.2608) acc 78.1250 (69.0625) lr 8.1262e-04 eta 0:12:09
epoch [16/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.011) loss 1.0156 (1.2557) acc 78.1250 (69.2083) lr 8.1262e-04 eta 0:12:06
epoch [16/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.9224 (1.2434) acc 71.8750 (69.4922) lr 8.1262e-04 eta 0:12:03
epoch [16/25] batch [85/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.0566 (1.2474) acc 68.7500 (69.2279) lr 8.1262e-04 eta 0:12:00
epoch [16/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 0.9956 (1.2423) acc 75.0000 (69.3056) lr 8.1262e-04 eta 0:11:57
epoch [16/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.009) loss 0.9829 (1.2329) acc 75.0000 (69.4408) lr 8.1262e-04 eta 0:11:55
epoch [16/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9990 (1.2408) acc 75.0000 (69.3438) lr 8.1262e-04 eta 0:11:52
epoch [16/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5879 (1.2396) acc 71.8750 (69.4940) lr 8.1262e-04 eta 0:11:50
epoch [16/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9058 (1.2324) acc 81.2500 (69.6307) lr 8.1262e-04 eta 0:11:47
epoch [16/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8467 (1.2463) acc 50.0000 (69.3478) lr 8.1262e-04 eta 0:11:45
epoch [16/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.6465 (1.2445) acc 78.1250 (69.4531) lr 8.1262e-04 eta 0:11:43
epoch [16/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.5293 (1.2404) acc 68.7500 (69.6250) lr 8.1262e-04 eta 0:11:41
epoch [16/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9668 (1.2465) acc 71.8750 (69.5913) lr 8.1262e-04 eta 0:11:39
epoch [16/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.1328 (1.2546) acc 53.1250 (69.5370) lr 8.1262e-04 eta 0:11:37
epoch [16/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2334 (1.2471) acc 65.6250 (69.5089) lr 8.1262e-04 eta 0:11:35
epoch [16/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0771 (1.2446) acc 68.7500 (69.5474) lr 8.1262e-04 eta 0:11:33
epoch [16/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9229 (1.2361) acc 81.2500 (69.7708) lr 8.1262e-04 eta 0:11:31
epoch [16/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7646 (1.2283) acc 84.3750 (70.0202) lr 8.1262e-04 eta 0:11:29
epoch [16/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0234 (1.2339) acc 65.6250 (69.9023) lr 8.1262e-04 eta 0:11:27
epoch [16/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1611 (1.2261) acc 78.1250 (70.0758) lr 8.1262e-04 eta 0:11:26
epoch [16/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6230 (1.2333) acc 62.5000 (69.8897) lr 8.1262e-04 eta 0:11:24
epoch [16/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0508 (1.2348) acc 75.0000 (69.9107) lr 8.1262e-04 eta 0:11:22
epoch [16/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.0879 (1.2316) acc 68.7500 (69.9479) lr 8.1262e-04 eta 0:11:20
epoch [16/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.1055 (1.2386) acc 71.8750 (69.7804) lr 8.1262e-04 eta 0:11:19
epoch [16/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8818 (1.2368) acc 78.1250 (69.7862) lr 8.1262e-04 eta 0:11:17
epoch [16/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3701 (1.2356) acc 62.5000 (69.7436) lr 8.1262e-04 eta 0:11:15
epoch [16/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8530 (1.2296) acc 84.3750 (69.9062) lr 8.1262e-04 eta 0:11:14
epoch [16/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8613 (1.2292) acc 87.5000 (70.0000) lr 8.1262e-04 eta 0:11:12
epoch [16/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1025 (1.2316) acc 68.7500 (69.9256) lr 8.1262e-04 eta 0:11:10
epoch [16/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4326 (1.2324) acc 59.3750 (69.8837) lr 8.1262e-04 eta 0:11:09
epoch [16/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0049 (1.2315) acc 68.7500 (69.9006) lr 8.1262e-04 eta 0:11:07
epoch [16/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9746 (1.2315) acc 65.6250 (69.8750) lr 8.1262e-04 eta 0:11:05
epoch [16/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1582 (1.2342) acc 78.1250 (69.8505) lr 8.1262e-04 eta 0:11:04
epoch [16/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.004) loss 1.5977 (1.2343) acc 53.1250 (69.8271) lr 8.1262e-04 eta 0:11:02
epoch [16/25] batch [240/250] time 0.290 (0.292) data 0.000 (0.004) loss 1.1338 (1.2334) acc 75.0000 (69.8438) lr 8.1262e-04 eta 0:11:00
epoch [16/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5898 (1.2311) acc 53.1250 (69.9107) lr 8.1262e-04 eta 0:10:59
epoch [16/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0645 (1.2375) acc 81.2500 (69.8750) lr 6.9098e-04 eta 0:10:57
epoch [17/25] batch [5/250] time 0.290 (0.459) data 0.000 (0.169) loss 1.7041 (1.4805) acc 56.2500 (66.8750) lr 6.9098e-04 eta 0:17:10
epoch [17/25] batch [10/250] time 0.291 (0.375) data 0.000 (0.085) loss 1.1699 (1.3587) acc 78.1250 (66.5625) lr 6.9098e-04 eta 0:13:59
epoch [17/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.057) loss 2.0312 (1.3879) acc 53.1250 (65.2083) lr 6.9098e-04 eta 0:12:53
epoch [17/25] batch [20/250] time 0.291 (0.332) data 0.000 (0.043) loss 0.7290 (1.3065) acc 84.3750 (67.3438) lr 6.9098e-04 eta 0:12:20
epoch [17/25] batch [25/250] time 0.291 (0.324) data 0.001 (0.034) loss 1.1904 (1.3023) acc 78.1250 (68.8750) lr 6.9098e-04 eta 0:12:00
epoch [17/25] batch [30/250] time 0.290 (0.318) data 0.000 (0.029) loss 1.4316 (1.2879) acc 62.5000 (68.8542) lr 6.9098e-04 eta 0:11:46
epoch [17/25] batch [35/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.2861 (1.2660) acc 71.8750 (69.2857) lr 6.9098e-04 eta 0:11:36
epoch [17/25] batch [40/250] time 0.289 (0.311) data 0.001 (0.022) loss 0.7256 (1.2378) acc 84.3750 (69.9219) lr 6.9098e-04 eta 0:11:27
epoch [17/25] batch [45/250] time 0.289 (0.309) data 0.000 (0.019) loss 1.1631 (1.2189) acc 78.1250 (70.5556) lr 6.9098e-04 eta 0:11:21
epoch [17/25] batch [50/250] time 0.293 (0.307) data 0.000 (0.017) loss 1.3018 (1.2091) acc 68.7500 (70.6875) lr 6.9098e-04 eta 0:11:15
epoch [17/25] batch [55/250] time 0.290 (0.306) data 0.000 (0.016) loss 1.0508 (1.2032) acc 75.0000 (70.7386) lr 6.9098e-04 eta 0:11:10
epoch [17/25] batch [60/250] time 0.290 (0.304) data 0.001 (0.015) loss 1.6328 (1.2100) acc 56.2500 (70.4167) lr 6.9098e-04 eta 0:11:06
epoch [17/25] batch [65/250] time 0.290 (0.303) data 0.000 (0.013) loss 0.8887 (1.2082) acc 81.2500 (70.2885) lr 6.9098e-04 eta 0:11:02
epoch [17/25] batch [70/250] time 0.289 (0.302) data 0.000 (0.012) loss 1.4414 (1.2283) acc 68.7500 (70.0000) lr 6.9098e-04 eta 0:10:58
epoch [17/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3203 (1.2286) acc 75.0000 (70.1250) lr 6.9098e-04 eta 0:10:55
epoch [17/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.011) loss 1.5479 (1.2259) acc 75.0000 (70.3125) lr 6.9098e-04 eta 0:10:52
epoch [17/25] batch [85/250] time 0.290 (0.300) data 0.000 (0.010) loss 0.9761 (1.2350) acc 78.1250 (70.1103) lr 6.9098e-04 eta 0:10:49
epoch [17/25] batch [90/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.0576 (1.2352) acc 75.0000 (70.2083) lr 6.9098e-04 eta 0:10:46
epoch [17/25] batch [95/250] time 0.290 (0.299) data 0.000 (0.009) loss 0.9863 (1.2452) acc 78.1250 (70.0987) lr 6.9098e-04 eta 0:10:44
epoch [17/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.2139 (1.2459) acc 71.8750 (70.1250) lr 6.9098e-04 eta 0:10:41
epoch [17/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.008) loss 0.9990 (1.2384) acc 71.8750 (70.2679) lr 6.9098e-04 eta 0:10:39
epoch [17/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.4756 (1.2454) acc 43.7500 (70.0284) lr 6.9098e-04 eta 0:10:37
epoch [17/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.008) loss 2.0195 (1.2472) acc 50.0000 (69.7826) lr 6.9098e-04 eta 0:10:34
epoch [17/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.1797 (1.2448) acc 65.6250 (69.8177) lr 6.9098e-04 eta 0:10:32
epoch [17/25] batch [125/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.2070 (1.2426) acc 71.8750 (69.8500) lr 6.9098e-04 eta 0:10:30
epoch [17/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3340 (1.2452) acc 71.8750 (69.8317) lr 6.9098e-04 eta 0:10:28
epoch [17/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5322 (1.2513) acc 71.8750 (69.8148) lr 6.9098e-04 eta 0:10:26
epoch [17/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.1289 (1.2501) acc 71.8750 (69.8438) lr 6.9098e-04 eta 0:10:24
epoch [17/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.1553 (1.2447) acc 56.2500 (69.8922) lr 6.9098e-04 eta 0:10:22
epoch [17/25] batch [150/250] time 0.291 (0.296) data 0.001 (0.006) loss 0.5854 (1.2417) acc 93.7500 (70.1042) lr 6.9098e-04 eta 0:10:20
epoch [17/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0410 (1.2510) acc 71.8750 (69.8790) lr 6.9098e-04 eta 0:10:18
epoch [17/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0840 (1.2469) acc 75.0000 (69.9219) lr 6.9098e-04 eta 0:10:16
epoch [17/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3730 (1.2414) acc 65.6250 (69.9621) lr 6.9098e-04 eta 0:10:15
epoch [17/25] batch [170/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.2119 (1.2396) acc 75.0000 (69.8897) lr 6.9098e-04 eta 0:10:13
epoch [17/25] batch [175/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.1348 (1.2293) acc 65.6250 (70.1250) lr 6.9098e-04 eta 0:10:11
epoch [17/25] batch [180/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.0469 (1.2386) acc 81.2500 (70.0347) lr 6.9098e-04 eta 0:10:09
epoch [17/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.8223 (1.2412) acc 53.1250 (70.0000) lr 6.9098e-04 eta 0:10:08
epoch [17/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0820 (1.2394) acc 71.8750 (70.0164) lr 6.9098e-04 eta 0:10:06
epoch [17/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1592 (1.2378) acc 78.1250 (70.0962) lr 6.9098e-04 eta 0:10:04
epoch [17/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.7632 (1.2316) acc 78.1250 (70.2031) lr 6.9098e-04 eta 0:10:02
epoch [17/25] batch [205/250] time 0.291 (0.294) data 0.001 (0.004) loss 1.3506 (1.2342) acc 65.6250 (70.1677) lr 6.9098e-04 eta 0:10:01
epoch [17/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.0479 (1.2318) acc 71.8750 (70.2381) lr 6.9098e-04 eta 0:09:59
epoch [17/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.8726 (1.2281) acc 78.1250 (70.3488) lr 6.9098e-04 eta 0:09:57
epoch [17/25] batch [220/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.1592 (1.2290) acc 65.6250 (70.3409) lr 6.9098e-04 eta 0:09:56
epoch [17/25] batch [225/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.2979 (1.2320) acc 68.7500 (70.2639) lr 6.9098e-04 eta 0:09:54
epoch [17/25] batch [230/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1445 (1.2340) acc 68.7500 (70.1766) lr 6.9098e-04 eta 0:09:53
epoch [17/25] batch [235/250] time 0.290 (0.294) data 0.001 (0.004) loss 1.1602 (1.2342) acc 53.1250 (70.0399) lr 6.9098e-04 eta 0:09:51
epoch [17/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.6182 (1.2327) acc 90.6250 (70.0391) lr 6.9098e-04 eta 0:09:49
epoch [17/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5039 (1.2371) acc 56.2500 (69.9235) lr 6.9098e-04 eta 0:09:48
epoch [17/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2646 (1.2331) acc 71.8750 (70.0125) lr 5.7422e-04 eta 0:09:46
epoch [18/25] batch [5/250] time 0.289 (0.479) data 0.000 (0.190) loss 1.1230 (1.3248) acc 71.8750 (67.5000) lr 5.7422e-04 eta 0:15:56
epoch [18/25] batch [10/250] time 0.289 (0.385) data 0.000 (0.095) loss 1.0596 (1.1890) acc 65.6250 (70.6250) lr 5.7422e-04 eta 0:12:45
epoch [18/25] batch [15/250] time 0.290 (0.353) data 0.000 (0.064) loss 1.0508 (1.1626) acc 78.1250 (73.3333) lr 5.7422e-04 eta 0:11:41
epoch [18/25] batch [20/250] time 0.291 (0.337) data 0.000 (0.048) loss 0.9380 (1.1445) acc 78.1250 (73.2812) lr 5.7422e-04 eta 0:11:08
epoch [18/25] batch [25/250] time 0.289 (0.328) data 0.000 (0.038) loss 0.7998 (1.1566) acc 75.0000 (72.2500) lr 5.7422e-04 eta 0:10:47
epoch [18/25] batch [30/250] time 0.289 (0.322) data 0.000 (0.032) loss 1.8867 (1.1735) acc 62.5000 (71.7708) lr 5.7422e-04 eta 0:10:33
epoch [18/25] batch [35/250] time 0.289 (0.317) data 0.000 (0.028) loss 1.2051 (1.1710) acc 56.2500 (71.0714) lr 5.7422e-04 eta 0:10:22
epoch [18/25] batch [40/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.7168 (1.2042) acc 62.5000 (70.7031) lr 5.7422e-04 eta 0:10:14
epoch [18/25] batch [45/250] time 0.289 (0.311) data 0.000 (0.022) loss 0.9380 (1.1883) acc 71.8750 (70.6250) lr 5.7422e-04 eta 0:10:07
epoch [18/25] batch [50/250] time 0.289 (0.309) data 0.000 (0.019) loss 1.2402 (1.2187) acc 71.8750 (70.1250) lr 5.7422e-04 eta 0:10:01
epoch [18/25] batch [55/250] time 0.290 (0.307) data 0.000 (0.018) loss 0.9150 (1.2217) acc 84.3750 (70.1705) lr 5.7422e-04 eta 0:09:56
epoch [18/25] batch [60/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.0664 (1.2100) acc 68.7500 (70.3125) lr 5.7422e-04 eta 0:09:52
epoch [18/25] batch [65/250] time 0.291 (0.304) data 0.001 (0.015) loss 0.9956 (1.1917) acc 65.6250 (70.9135) lr 5.7422e-04 eta 0:09:48
epoch [18/25] batch [70/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.5938 (1.1993) acc 62.5000 (70.7589) lr 5.7422e-04 eta 0:09:45
epoch [18/25] batch [75/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.7676 (1.1801) acc 78.1250 (71.2083) lr 5.7422e-04 eta 0:09:41
epoch [18/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.4863 (1.1848) acc 59.3750 (71.2500) lr 5.7422e-04 eta 0:09:38
epoch [18/25] batch [85/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.7334 (1.1967) acc 56.2500 (71.1397) lr 5.7422e-04 eta 0:09:35
epoch [18/25] batch [90/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.8774 (1.1921) acc 78.1250 (71.2847) lr 5.7422e-04 eta 0:09:33
epoch [18/25] batch [95/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.0059 (1.1831) acc 78.1250 (71.3816) lr 5.7422e-04 eta 0:09:30
epoch [18/25] batch [100/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.3750 (1.1897) acc 62.5000 (71.2812) lr 5.7422e-04 eta 0:09:28
epoch [18/25] batch [105/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.2275 (1.1893) acc 75.0000 (71.3988) lr 5.7422e-04 eta 0:09:25
epoch [18/25] batch [110/250] time 0.290 (0.298) data 0.000 (0.009) loss 0.9058 (1.1861) acc 78.1250 (71.3920) lr 5.7422e-04 eta 0:09:23
epoch [18/25] batch [115/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.4746 (1.1764) acc 65.6250 (71.5217) lr 5.7422e-04 eta 0:09:21
epoch [18/25] batch [120/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.2295 (1.1758) acc 71.8750 (71.5885) lr 5.7422e-04 eta 0:09:19
epoch [18/25] batch [125/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5098 (1.1837) acc 65.6250 (71.3000) lr 5.7422e-04 eta 0:09:17
epoch [18/25] batch [130/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.3945 (1.1931) acc 56.2500 (71.0337) lr 5.7422e-04 eta 0:09:14
epoch [18/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0879 (1.2031) acc 78.1250 (70.7870) lr 5.7422e-04 eta 0:09:12
epoch [18/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4160 (1.2024) acc 65.6250 (70.8036) lr 5.7422e-04 eta 0:09:10
epoch [18/25] batch [145/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4805 (1.2062) acc 71.8750 (70.6681) lr 5.7422e-04 eta 0:09:09
epoch [18/25] batch [150/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3457 (1.2086) acc 71.8750 (70.6667) lr 5.7422e-04 eta 0:09:07
epoch [18/25] batch [155/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.4434 (1.2074) acc 68.7500 (70.6653) lr 5.7422e-04 eta 0:09:05
epoch [18/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4863 (1.2115) acc 56.2500 (70.5273) lr 5.7422e-04 eta 0:09:03
epoch [18/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.6309 (1.2037) acc 93.7500 (70.8144) lr 5.7422e-04 eta 0:09:01
epoch [18/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0488 (1.2105) acc 71.8750 (70.7537) lr 5.7422e-04 eta 0:08:59
epoch [18/25] batch [175/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3721 (1.2081) acc 65.6250 (70.7500) lr 5.7422e-04 eta 0:08:58
epoch [18/25] batch [180/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4170 (1.2063) acc 75.0000 (70.8160) lr 5.7422e-04 eta 0:08:56
epoch [18/25] batch [185/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0371 (1.2047) acc 71.8750 (70.8108) lr 5.7422e-04 eta 0:08:54
epoch [18/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7896 (1.2029) acc 71.8750 (70.7401) lr 5.7422e-04 eta 0:08:52
epoch [18/25] batch [195/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.0869 (1.2057) acc 75.0000 (70.7212) lr 5.7422e-04 eta 0:08:51
epoch [18/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4248 (1.2060) acc 71.8750 (70.7344) lr 5.7422e-04 eta 0:08:49
epoch [18/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1680 (1.2046) acc 68.7500 (70.7012) lr 5.7422e-04 eta 0:08:47
epoch [18/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4023 (1.2015) acc 68.7500 (70.7143) lr 5.7422e-04 eta 0:08:46
epoch [18/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4688 (1.2000) acc 68.7500 (70.8140) lr 5.7422e-04 eta 0:08:44
epoch [18/25] batch [220/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.6519 (1.2012) acc 81.2500 (70.7528) lr 5.7422e-04 eta 0:08:42
epoch [18/25] batch [225/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6895 (1.2054) acc 53.1250 (70.5694) lr 5.7422e-04 eta 0:08:41
epoch [18/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2666 (1.2034) acc 68.7500 (70.6114) lr 5.7422e-04 eta 0:08:39
epoch [18/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.3379 (1.2030) acc 62.5000 (70.6117) lr 5.7422e-04 eta 0:08:37
epoch [18/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8296 (1.2001) acc 78.1250 (70.5859) lr 5.7422e-04 eta 0:08:36
epoch [18/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2100 (1.1996) acc 68.7500 (70.5612) lr 5.7422e-04 eta 0:08:34
epoch [18/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3213 (1.1981) acc 71.8750 (70.6250) lr 4.6417e-04 eta 0:08:32
epoch [19/25] batch [5/250] time 0.289 (0.459) data 0.000 (0.169) loss 0.8423 (1.1948) acc 75.0000 (69.3750) lr 4.6417e-04 eta 0:13:20
epoch [19/25] batch [10/250] time 0.288 (0.374) data 0.000 (0.085) loss 0.9736 (1.1984) acc 78.1250 (69.0625) lr 4.6417e-04 eta 0:10:50
epoch [19/25] batch [15/250] time 0.289 (0.345) data 0.000 (0.057) loss 1.2637 (1.2241) acc 53.1250 (67.5000) lr 4.6417e-04 eta 0:09:59
epoch [19/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.043) loss 1.1768 (1.2095) acc 65.6250 (70.0000) lr 4.6417e-04 eta 0:09:33
epoch [19/25] batch [25/250] time 0.289 (0.323) data 0.000 (0.034) loss 1.0820 (1.1723) acc 71.8750 (70.8750) lr 4.6417e-04 eta 0:09:16
epoch [19/25] batch [30/250] time 0.290 (0.317) data 0.000 (0.028) loss 1.2402 (1.1667) acc 71.8750 (70.6250) lr 4.6417e-04 eta 0:09:05
epoch [19/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.8965 (1.1914) acc 53.1250 (70.1786) lr 4.6417e-04 eta 0:08:57
epoch [19/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.2832 (1.1655) acc 68.7500 (70.3906) lr 4.6417e-04 eta 0:08:50
epoch [19/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.2998 (1.1716) acc 62.5000 (69.9306) lr 4.6417e-04 eta 0:08:44
epoch [19/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.1543 (1.1600) acc 65.6250 (70.3125) lr 4.6417e-04 eta 0:08:40
epoch [19/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.016) loss 1.2334 (1.1809) acc 75.0000 (70.0568) lr 4.6417e-04 eta 0:08:35
epoch [19/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 0.9351 (1.1835) acc 75.0000 (70.0521) lr 4.6417e-04 eta 0:08:32
epoch [19/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.5557 (1.1844) acc 56.2500 (70.2404) lr 4.6417e-04 eta 0:08:28
epoch [19/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.9419 (1.1842) acc 75.0000 (70.2679) lr 4.6417e-04 eta 0:08:25
epoch [19/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.012) loss 1.0684 (1.1713) acc 75.0000 (70.7083) lr 4.6417e-04 eta 0:08:23
epoch [19/25] batch [80/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.2021 (1.1627) acc 71.8750 (70.8984) lr 4.6417e-04 eta 0:08:20
epoch [19/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.010) loss 0.8628 (1.1556) acc 75.0000 (70.9191) lr 4.6417e-04 eta 0:08:17
epoch [19/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1787 (1.1486) acc 65.6250 (71.2500) lr 4.6417e-04 eta 0:08:15
epoch [19/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.0684 (1.1501) acc 75.0000 (71.4474) lr 4.6417e-04 eta 0:08:13
epoch [19/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.0469 (1.1550) acc 81.2500 (71.4062) lr 4.6417e-04 eta 0:08:11
epoch [19/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.0742 (1.1622) acc 75.0000 (71.2798) lr 4.6417e-04 eta 0:08:09
epoch [19/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2344 (1.1666) acc 68.7500 (71.2784) lr 4.6417e-04 eta 0:08:06
epoch [19/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.7271 (1.1620) acc 81.2500 (71.4946) lr 4.6417e-04 eta 0:08:04
epoch [19/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.7744 (1.1694) acc 50.0000 (71.3021) lr 4.6417e-04 eta 0:08:02
epoch [19/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9990 (1.1735) acc 68.7500 (71.0750) lr 4.6417e-04 eta 0:08:01
epoch [19/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7979 (1.1811) acc 68.7500 (70.9615) lr 4.6417e-04 eta 0:07:59
epoch [19/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2021 (1.1850) acc 78.1250 (70.8102) lr 4.6417e-04 eta 0:07:57
epoch [19/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4922 (1.1857) acc 62.5000 (70.8929) lr 4.6417e-04 eta 0:07:55
epoch [19/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8354 (1.1901) acc 75.0000 (70.8405) lr 4.6417e-04 eta 0:07:53
epoch [19/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2129 (1.1863) acc 56.2500 (70.8333) lr 4.6417e-04 eta 0:07:51
epoch [19/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0928 (1.1916) acc 71.8750 (70.8266) lr 4.6417e-04 eta 0:07:50
epoch [19/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5557 (1.1942) acc 68.7500 (70.8008) lr 4.6417e-04 eta 0:07:48
epoch [19/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9399 (1.1952) acc 84.3750 (70.7386) lr 4.6417e-04 eta 0:07:46
epoch [19/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9912 (1.1920) acc 87.5000 (70.8456) lr 4.6417e-04 eta 0:07:44
epoch [19/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4336 (1.1988) acc 65.6250 (70.6964) lr 4.6417e-04 eta 0:07:43
epoch [19/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2168 (1.1961) acc 71.8750 (70.7118) lr 4.6417e-04 eta 0:07:41
epoch [19/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.9365 (1.1967) acc 71.8750 (70.7264) lr 4.6417e-04 eta 0:07:39
epoch [19/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8467 (1.2030) acc 71.8750 (70.5921) lr 4.6417e-04 eta 0:07:38
epoch [19/25] batch [195/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.0566 (1.2020) acc 75.0000 (70.6250) lr 4.6417e-04 eta 0:07:36
epoch [19/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.9541 (1.1970) acc 71.8750 (70.7031) lr 4.6417e-04 eta 0:07:34
epoch [19/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4316 (1.1987) acc 62.5000 (70.6555) lr 4.6417e-04 eta 0:07:33
epoch [19/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6865 (1.2012) acc 65.6250 (70.5804) lr 4.6417e-04 eta 0:07:31
epoch [19/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3496 (1.2031) acc 65.6250 (70.5378) lr 4.6417e-04 eta 0:07:30
epoch [19/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3711 (1.2047) acc 65.6250 (70.5256) lr 4.6417e-04 eta 0:07:28
epoch [19/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8643 (1.2111) acc 53.1250 (70.4167) lr 4.6417e-04 eta 0:07:26
epoch [19/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4609 (1.2135) acc 68.7500 (70.3125) lr 4.6417e-04 eta 0:07:25
epoch [19/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.1523 (1.2145) acc 68.7500 (70.3590) lr 4.6417e-04 eta 0:07:23
epoch [19/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7554 (1.2095) acc 84.3750 (70.4818) lr 4.6417e-04 eta 0:07:22
epoch [19/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8652 (1.2045) acc 75.0000 (70.5995) lr 4.6417e-04 eta 0:07:20
epoch [19/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8574 (1.2024) acc 68.7500 (70.6375) lr 3.6258e-04 eta 0:07:18
epoch [20/25] batch [5/250] time 0.290 (0.456) data 0.001 (0.166) loss 0.9888 (1.0346) acc 75.0000 (70.0000) lr 3.6258e-04 eta 0:11:21
epoch [20/25] batch [10/250] time 0.289 (0.373) data 0.000 (0.083) loss 0.7085 (1.0104) acc 81.2500 (74.0625) lr 3.6258e-04 eta 0:09:15
epoch [20/25] batch [15/250] time 0.289 (0.345) data 0.000 (0.055) loss 1.0430 (1.0375) acc 68.7500 (72.2917) lr 3.6258e-04 eta 0:08:31
epoch [20/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.042) loss 1.4521 (1.0908) acc 68.7500 (71.5625) lr 3.6258e-04 eta 0:08:09
epoch [20/25] batch [25/250] time 0.289 (0.322) data 0.000 (0.033) loss 1.5400 (1.1107) acc 65.6250 (71.0000) lr 3.6258e-04 eta 0:07:55
epoch [20/25] batch [30/250] time 0.289 (0.317) data 0.000 (0.028) loss 0.6953 (1.1108) acc 84.3750 (70.9375) lr 3.6258e-04 eta 0:07:45
epoch [20/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.8516 (1.1231) acc 78.1250 (71.0714) lr 3.6258e-04 eta 0:07:38
epoch [20/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.6504 (1.1321) acc 56.2500 (70.5469) lr 3.6258e-04 eta 0:07:32
epoch [20/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.6338 (1.1501) acc 59.3750 (70.6250) lr 3.6258e-04 eta 0:07:27
epoch [20/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.9546 (1.1474) acc 78.1250 (70.8750) lr 3.6258e-04 eta 0:07:23
epoch [20/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.0176 (1.1389) acc 71.8750 (71.5341) lr 3.6258e-04 eta 0:07:19
epoch [20/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 0.9941 (1.1373) acc 75.0000 (71.8229) lr 3.6258e-04 eta 0:07:16
epoch [20/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1396 (1.1329) acc 62.5000 (72.0192) lr 3.6258e-04 eta 0:07:13
epoch [20/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1143 (1.1359) acc 75.0000 (72.0982) lr 3.6258e-04 eta 0:07:10
epoch [20/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.9590 (1.1437) acc 68.7500 (71.9583) lr 3.6258e-04 eta 0:07:07
epoch [20/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2471 (1.1446) acc 71.8750 (71.7969) lr 3.6258e-04 eta 0:07:05
epoch [20/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.5645 (1.1545) acc 62.5000 (71.7279) lr 3.6258e-04 eta 0:07:03
epoch [20/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.4004 (1.1610) acc 56.2500 (71.3889) lr 3.6258e-04 eta 0:07:00
epoch [20/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9868 (1.1688) acc 78.1250 (71.1184) lr 3.6258e-04 eta 0:06:58
epoch [20/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 0.9443 (1.1706) acc 78.1250 (71.2812) lr 3.6258e-04 eta 0:06:56
epoch [20/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0664 (1.1704) acc 75.0000 (71.3095) lr 3.6258e-04 eta 0:06:54
epoch [20/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1357 (1.1711) acc 81.2500 (71.3920) lr 3.6258e-04 eta 0:06:52
epoch [20/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2129 (1.1665) acc 75.0000 (71.6033) lr 3.6258e-04 eta 0:06:50
epoch [20/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1934 (1.1605) acc 71.8750 (71.8229) lr 3.6258e-04 eta 0:06:48
epoch [20/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1348 (1.1565) acc 65.6250 (71.8750) lr 3.6258e-04 eta 0:06:46
epoch [20/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8789 (1.1581) acc 56.2500 (71.7067) lr 3.6258e-04 eta 0:06:45
epoch [20/25] batch [135/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.0137 (1.1564) acc 81.2500 (71.8519) lr 3.6258e-04 eta 0:06:43
epoch [20/25] batch [140/250] time 0.289 (0.295) data 0.001 (0.006) loss 0.5767 (1.1548) acc 87.5000 (71.9866) lr 3.6258e-04 eta 0:06:41
epoch [20/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2656 (1.1608) acc 75.0000 (71.9828) lr 3.6258e-04 eta 0:06:39
epoch [20/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2754 (1.1624) acc 65.6250 (71.8958) lr 3.6258e-04 eta 0:06:37
epoch [20/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0645 (1.1631) acc 71.8750 (71.7540) lr 3.6258e-04 eta 0:06:36
epoch [20/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3086 (1.1665) acc 65.6250 (71.4648) lr 3.6258e-04 eta 0:06:34
epoch [20/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9736 (1.1669) acc 75.0000 (71.4583) lr 3.6258e-04 eta 0:06:32
epoch [20/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0254 (1.1686) acc 71.8750 (71.5074) lr 3.6258e-04 eta 0:06:31
epoch [20/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1553 (1.1677) acc 71.8750 (71.5179) lr 3.6258e-04 eta 0:06:29
epoch [20/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5176 (1.1750) acc 62.5000 (71.3021) lr 3.6258e-04 eta 0:06:27
epoch [20/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9565 (1.1781) acc 78.1250 (71.2838) lr 3.6258e-04 eta 0:06:26
epoch [20/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0762 (1.1773) acc 71.8750 (71.2336) lr 3.6258e-04 eta 0:06:24
epoch [20/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0430 (1.1791) acc 71.8750 (71.1058) lr 3.6258e-04 eta 0:06:23
epoch [20/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0176 (1.1807) acc 68.7500 (71.0156) lr 3.6258e-04 eta 0:06:21
epoch [20/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5439 (1.1815) acc 56.2500 (70.9451) lr 3.6258e-04 eta 0:06:19
epoch [20/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1953 (1.1747) acc 71.8750 (71.0565) lr 3.6258e-04 eta 0:06:18
epoch [20/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7627 (1.1772) acc 62.5000 (71.0320) lr 3.6258e-04 eta 0:06:16
epoch [20/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9653 (1.1741) acc 81.2500 (71.1080) lr 3.6258e-04 eta 0:06:15
epoch [20/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0713 (1.1725) acc 78.1250 (71.1806) lr 3.6258e-04 eta 0:06:13
epoch [20/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5078 (1.1774) acc 68.7500 (71.1549) lr 3.6258e-04 eta 0:06:11
epoch [20/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.1562 (1.1818) acc 78.1250 (71.0372) lr 3.6258e-04 eta 0:06:10
epoch [20/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4727 (1.1828) acc 56.2500 (70.9375) lr 3.6258e-04 eta 0:06:08
epoch [20/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9561 (1.1865) acc 71.8750 (70.8801) lr 3.6258e-04 eta 0:06:07
epoch [20/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1719 (1.1870) acc 68.7500 (70.8500) lr 2.7103e-04 eta 0:06:05
epoch [21/25] batch [5/250] time 0.291 (0.452) data 0.000 (0.163) loss 1.6094 (1.1376) acc 56.2500 (70.6250) lr 2.7103e-04 eta 0:09:23
epoch [21/25] batch [10/250] time 0.290 (0.371) data 0.000 (0.082) loss 1.2148 (1.2228) acc 71.8750 (68.4375) lr 2.7103e-04 eta 0:07:40
epoch [21/25] batch [15/250] time 0.290 (0.344) data 0.000 (0.054) loss 0.8018 (1.1850) acc 87.5000 (70.6250) lr 2.7103e-04 eta 0:07:05
epoch [21/25] batch [20/250] time 0.289 (0.331) data 0.000 (0.041) loss 1.8145 (1.2521) acc 53.1250 (69.3750) lr 2.7103e-04 eta 0:06:46
epoch [21/25] batch [25/250] time 0.289 (0.322) data 0.000 (0.033) loss 1.4248 (1.2587) acc 65.6250 (70.0000) lr 2.7103e-04 eta 0:06:34
epoch [21/25] batch [30/250] time 0.289 (0.317) data 0.000 (0.027) loss 0.8623 (1.2198) acc 71.8750 (70.3125) lr 2.7103e-04 eta 0:06:26
epoch [21/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.3486 (1.1957) acc 62.5000 (70.5357) lr 2.7103e-04 eta 0:06:20
epoch [21/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.9707 (1.1991) acc 75.0000 (70.5469) lr 2.7103e-04 eta 0:06:15
epoch [21/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.018) loss 0.9907 (1.1836) acc 75.0000 (70.8333) lr 2.7103e-04 eta 0:06:10
epoch [21/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.4443 (1.1822) acc 75.0000 (71.3125) lr 2.7103e-04 eta 0:06:06
epoch [21/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.8413 (1.1705) acc 81.2500 (71.3636) lr 2.7103e-04 eta 0:06:03
epoch [21/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.4365 (1.1812) acc 62.5000 (71.0938) lr 2.7103e-04 eta 0:06:00
epoch [21/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.4258 (1.1633) acc 65.6250 (71.5385) lr 2.7103e-04 eta 0:05:57
epoch [21/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.2441 (1.1612) acc 71.8750 (71.3393) lr 2.7103e-04 eta 0:05:55
epoch [21/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.8667 (1.1682) acc 81.2500 (71.3750) lr 2.7103e-04 eta 0:05:52
epoch [21/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.5830 (1.1712) acc 68.7500 (71.5234) lr 2.7103e-04 eta 0:05:50
epoch [21/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.0801 (1.1631) acc 71.8750 (71.6912) lr 2.7103e-04 eta 0:05:48
epoch [21/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.8989 (1.1612) acc 78.1250 (71.6667) lr 2.7103e-04 eta 0:05:46
epoch [21/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0664 (1.1685) acc 68.7500 (71.4145) lr 2.7103e-04 eta 0:05:44
epoch [21/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2451 (1.1764) acc 71.8750 (71.3750) lr 2.7103e-04 eta 0:05:42
epoch [21/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.7510 (1.1782) acc 71.8750 (71.1012) lr 2.7103e-04 eta 0:05:40
epoch [21/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.008) loss 0.9463 (1.1726) acc 68.7500 (71.1364) lr 2.7103e-04 eta 0:05:38
epoch [21/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1299 (1.1662) acc 62.5000 (71.2500) lr 2.7103e-04 eta 0:05:36
epoch [21/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3418 (1.1651) acc 68.7500 (71.2760) lr 2.7103e-04 eta 0:05:34
epoch [21/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1982 (1.1651) acc 65.6250 (71.2500) lr 2.7103e-04 eta 0:05:32
epoch [21/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.7314 (1.1562) acc 71.8750 (71.5144) lr 2.7103e-04 eta 0:05:31
epoch [21/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9097 (1.1597) acc 87.5000 (71.5741) lr 2.7103e-04 eta 0:05:29
epoch [21/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0762 (1.1688) acc 75.0000 (71.5402) lr 2.7103e-04 eta 0:05:27
epoch [21/25] batch [145/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.4873 (1.1717) acc 68.7500 (71.5517) lr 2.7103e-04 eta 0:05:25
epoch [21/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1553 (1.1712) acc 75.0000 (71.5208) lr 2.7103e-04 eta 0:05:24
epoch [21/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4951 (1.1744) acc 65.6250 (71.3911) lr 2.7103e-04 eta 0:05:22
epoch [21/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1367 (1.1669) acc 65.6250 (71.4844) lr 2.7103e-04 eta 0:05:20
epoch [21/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9165 (1.1680) acc 81.2500 (71.4583) lr 2.7103e-04 eta 0:05:19
epoch [21/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3730 (1.1721) acc 75.0000 (71.3787) lr 2.7103e-04 eta 0:05:17
epoch [21/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0332 (1.1726) acc 81.2500 (71.4821) lr 2.7103e-04 eta 0:05:16
epoch [21/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9443 (1.1717) acc 81.2500 (71.6146) lr 2.7103e-04 eta 0:05:14
epoch [21/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2578 (1.1703) acc 71.8750 (71.6047) lr 2.7103e-04 eta 0:05:12
epoch [21/25] batch [190/250] time 0.292 (0.294) data 0.000 (0.005) loss 1.1699 (1.1671) acc 65.6250 (71.7270) lr 2.7103e-04 eta 0:05:11
epoch [21/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.8838 (1.1696) acc 75.0000 (71.6506) lr 2.7103e-04 eta 0:05:09
epoch [21/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9106 (1.1691) acc 81.2500 (71.7344) lr 2.7103e-04 eta 0:05:08
epoch [21/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1016 (1.1718) acc 71.8750 (71.7073) lr 2.7103e-04 eta 0:05:06
epoch [21/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1816 (1.1679) acc 65.6250 (71.8006) lr 2.7103e-04 eta 0:05:04
epoch [21/25] batch [215/250] time 0.294 (0.293) data 0.000 (0.004) loss 1.8633 (1.1703) acc 53.1250 (71.6279) lr 2.7103e-04 eta 0:05:03
epoch [21/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1309 (1.1706) acc 75.0000 (71.6193) lr 2.7103e-04 eta 0:05:01
epoch [21/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0342 (1.1738) acc 78.1250 (71.5417) lr 2.7103e-04 eta 0:05:00
epoch [21/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1543 (1.1690) acc 68.7500 (71.6304) lr 2.7103e-04 eta 0:04:58
epoch [21/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.2979 (1.1707) acc 68.7500 (71.6356) lr 2.7103e-04 eta 0:04:57
epoch [21/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9165 (1.1725) acc 75.0000 (71.5625) lr 2.7103e-04 eta 0:04:55
epoch [21/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6865 (1.1739) acc 56.2500 (71.5306) lr 2.7103e-04 eta 0:04:54
epoch [21/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7793 (1.1760) acc 50.0000 (71.4500) lr 1.9098e-04 eta 0:04:52
epoch [22/25] batch [5/250] time 0.288 (0.454) data 0.000 (0.165) loss 1.4023 (1.1373) acc 62.5000 (73.7500) lr 1.9098e-04 eta 0:07:31
epoch [22/25] batch [10/250] time 0.288 (0.371) data 0.000 (0.083) loss 0.9824 (1.1019) acc 81.2500 (73.7500) lr 1.9098e-04 eta 0:06:07
epoch [22/25] batch [15/250] time 0.289 (0.344) data 0.000 (0.055) loss 0.7915 (1.1136) acc 75.0000 (72.5000) lr 1.9098e-04 eta 0:05:38
epoch [22/25] batch [20/250] time 0.289 (0.330) data 0.000 (0.042) loss 1.3457 (1.1909) acc 75.0000 (71.8750) lr 1.9098e-04 eta 0:05:23
epoch [22/25] batch [25/250] time 0.289 (0.322) data 0.000 (0.033) loss 1.0771 (1.1987) acc 78.1250 (72.1250) lr 1.9098e-04 eta 0:05:13
epoch [22/25] batch [30/250] time 0.288 (0.316) data 0.000 (0.028) loss 0.9067 (1.1796) acc 68.7500 (72.8125) lr 1.9098e-04 eta 0:05:06
epoch [22/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.9639 (1.1718) acc 71.8750 (72.7679) lr 1.9098e-04 eta 0:05:01
epoch [22/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.7905 (1.1795) acc 62.5000 (71.9531) lr 1.9098e-04 eta 0:04:57
epoch [22/25] batch [45/250] time 0.289 (0.307) data 0.000 (0.019) loss 1.2285 (1.1875) acc 68.7500 (72.0139) lr 1.9098e-04 eta 0:04:53
epoch [22/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.0156 (1.1790) acc 81.2500 (72.3125) lr 1.9098e-04 eta 0:04:50
epoch [22/25] batch [55/250] time 0.292 (0.304) data 0.000 (0.015) loss 1.1553 (1.1864) acc 71.8750 (71.7614) lr 1.9098e-04 eta 0:04:47
epoch [22/25] batch [60/250] time 0.293 (0.303) data 0.004 (0.014) loss 1.3271 (1.2032) acc 71.8750 (71.3021) lr 1.9098e-04 eta 0:04:44
epoch [22/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.8887 (1.1936) acc 84.3750 (71.4904) lr 1.9098e-04 eta 0:04:42
epoch [22/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.9795 (1.1827) acc 75.0000 (71.9643) lr 1.9098e-04 eta 0:04:39
epoch [22/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2217 (1.1759) acc 75.0000 (71.9583) lr 1.9098e-04 eta 0:04:37
epoch [22/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.011) loss 1.0244 (1.1584) acc 68.7500 (72.3828) lr 1.9098e-04 eta 0:04:35
epoch [22/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.9683 (1.1582) acc 75.0000 (72.4632) lr 1.9098e-04 eta 0:04:33
epoch [22/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.010) loss 1.2393 (1.1630) acc 65.6250 (72.2569) lr 1.9098e-04 eta 0:04:31
epoch [22/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1875 (1.1701) acc 75.0000 (71.9737) lr 1.9098e-04 eta 0:04:29
epoch [22/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.009) loss 1.2197 (1.1754) acc 68.7500 (71.8125) lr 1.9098e-04 eta 0:04:27
epoch [22/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8906 (1.1575) acc 75.0000 (72.3214) lr 1.9098e-04 eta 0:04:25
epoch [22/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9292 (1.1565) acc 84.3750 (72.3864) lr 1.9098e-04 eta 0:04:23
epoch [22/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.4580 (1.1671) acc 59.3750 (72.0924) lr 1.9098e-04 eta 0:04:22
epoch [22/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2529 (1.1703) acc 68.7500 (71.9271) lr 1.9098e-04 eta 0:04:20
epoch [22/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7002 (1.1700) acc 68.7500 (71.8750) lr 1.9098e-04 eta 0:04:18
epoch [22/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.1162 (1.1722) acc 71.8750 (71.7548) lr 1.9098e-04 eta 0:04:17
epoch [22/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2148 (1.1707) acc 68.7500 (71.6898) lr 1.9098e-04 eta 0:04:15
epoch [22/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4023 (1.1643) acc 71.8750 (71.8973) lr 1.9098e-04 eta 0:04:13
epoch [22/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1572 (1.1612) acc 68.7500 (71.8750) lr 1.9098e-04 eta 0:04:12
epoch [22/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8560 (1.1664) acc 78.1250 (71.9167) lr 1.9098e-04 eta 0:04:10
epoch [22/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.006) loss 0.9351 (1.1661) acc 78.1250 (71.9960) lr 1.9098e-04 eta 0:04:08
epoch [22/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3203 (1.1621) acc 68.7500 (72.1680) lr 1.9098e-04 eta 0:04:07
epoch [22/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4336 (1.1623) acc 71.8750 (72.1402) lr 1.9098e-04 eta 0:04:05
epoch [22/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9556 (1.1590) acc 81.2500 (72.2610) lr 1.9098e-04 eta 0:04:04
epoch [22/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9106 (1.1581) acc 78.1250 (72.3214) lr 1.9098e-04 eta 0:04:02
epoch [22/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0186 (1.1559) acc 75.0000 (72.3785) lr 1.9098e-04 eta 0:04:00
epoch [22/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5664 (1.1590) acc 68.7500 (72.3818) lr 1.9098e-04 eta 0:03:59
epoch [22/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.2539 (1.1560) acc 59.3750 (72.3191) lr 1.9098e-04 eta 0:03:57
epoch [22/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.005) loss 0.8599 (1.1470) acc 84.3750 (72.4679) lr 1.9098e-04 eta 0:03:56
epoch [22/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2744 (1.1482) acc 59.3750 (72.4219) lr 1.9098e-04 eta 0:03:54
epoch [22/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4658 (1.1511) acc 65.6250 (72.2713) lr 1.9098e-04 eta 0:03:53
epoch [22/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5430 (1.1524) acc 65.6250 (72.3214) lr 1.9098e-04 eta 0:03:51
epoch [22/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6230 (1.1519) acc 59.3750 (72.2674) lr 1.9098e-04 eta 0:03:50
epoch [22/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8218 (1.1508) acc 84.3750 (72.3864) lr 1.9098e-04 eta 0:03:48
epoch [22/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0889 (1.1528) acc 75.0000 (72.2917) lr 1.9098e-04 eta 0:03:46
epoch [22/25] batch [230/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.0342 (1.1516) acc 81.2500 (72.3641) lr 1.9098e-04 eta 0:03:45
epoch [22/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 0.8325 (1.1517) acc 75.0000 (72.3138) lr 1.9098e-04 eta 0:03:43
epoch [22/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7920 (1.1560) acc 75.0000 (72.1224) lr 1.9098e-04 eta 0:03:42
epoch [22/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3164 (1.1582) acc 75.0000 (72.0918) lr 1.9098e-04 eta 0:03:40
epoch [22/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.9644 (1.1617) acc 78.1250 (72.0500) lr 1.2369e-04 eta 0:03:39
epoch [23/25] batch [5/250] time 0.288 (0.441) data 0.000 (0.152) loss 0.8628 (1.1140) acc 84.3750 (76.8750) lr 1.2369e-04 eta 0:05:28
epoch [23/25] batch [10/250] time 0.289 (0.365) data 0.000 (0.076) loss 1.8271 (1.1996) acc 62.5000 (71.8750) lr 1.2369e-04 eta 0:04:29
epoch [23/25] batch [15/250] time 0.290 (0.340) data 0.000 (0.051) loss 0.8633 (1.1417) acc 84.3750 (73.1250) lr 1.2369e-04 eta 0:04:09
epoch [23/25] batch [20/250] time 0.290 (0.327) data 0.000 (0.038) loss 0.7661 (1.1482) acc 84.3750 (73.2812) lr 1.2369e-04 eta 0:03:58
epoch [23/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 1.7881 (1.1742) acc 50.0000 (72.2500) lr 1.2369e-04 eta 0:03:51
epoch [23/25] batch [30/250] time 0.289 (0.315) data 0.000 (0.026) loss 1.2041 (1.1393) acc 59.3750 (72.8125) lr 1.2369e-04 eta 0:03:46
epoch [23/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.5537 (1.1402) acc 50.0000 (72.3214) lr 1.2369e-04 eta 0:03:42
epoch [23/25] batch [40/250] time 0.290 (0.308) data 0.000 (0.019) loss 1.5215 (1.1198) acc 71.8750 (73.2812) lr 1.2369e-04 eta 0:03:38
epoch [23/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.9399 (1.1323) acc 71.8750 (72.7778) lr 1.2369e-04 eta 0:03:35
epoch [23/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.016) loss 0.8086 (1.1201) acc 87.5000 (73.0625) lr 1.2369e-04 eta 0:03:33
epoch [23/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.1074 (1.1261) acc 78.1250 (73.0682) lr 1.2369e-04 eta 0:03:30
epoch [23/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.9229 (1.1083) acc 84.3750 (73.5938) lr 1.2369e-04 eta 0:03:28
epoch [23/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1699 (1.0919) acc 68.7500 (73.7019) lr 1.2369e-04 eta 0:03:26
epoch [23/25] batch [70/250] time 0.290 (0.300) data 0.000 (0.011) loss 0.9600 (1.0811) acc 78.1250 (74.1518) lr 1.2369e-04 eta 0:03:24
epoch [23/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.0527 (1.0949) acc 87.5000 (74.2500) lr 1.2369e-04 eta 0:03:22
epoch [23/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.0664 (1.1036) acc 65.6250 (73.8672) lr 1.2369e-04 eta 0:03:20
epoch [23/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7354 (1.1018) acc 84.3750 (73.8971) lr 1.2369e-04 eta 0:03:18
epoch [23/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.3525 (1.1130) acc 71.8750 (73.9236) lr 1.2369e-04 eta 0:03:16
epoch [23/25] batch [95/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.0234 (1.1094) acc 81.2500 (74.1118) lr 1.2369e-04 eta 0:03:14
epoch [23/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.6914 (1.1016) acc 90.6250 (74.3125) lr 1.2369e-04 eta 0:03:12
epoch [23/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.6006 (1.1077) acc 56.2500 (74.1964) lr 1.2369e-04 eta 0:03:11
epoch [23/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0781 (1.1154) acc 75.0000 (73.9489) lr 1.2369e-04 eta 0:03:09
epoch [23/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.6719 (1.1110) acc 84.3750 (74.0489) lr 1.2369e-04 eta 0:03:07
epoch [23/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8740 (1.1102) acc 75.0000 (73.9844) lr 1.2369e-04 eta 0:03:06
epoch [23/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1982 (1.1207) acc 68.7500 (73.6250) lr 1.2369e-04 eta 0:03:04
epoch [23/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8379 (1.1188) acc 78.1250 (73.5577) lr 1.2369e-04 eta 0:03:02
epoch [23/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1348 (1.1199) acc 75.0000 (73.4491) lr 1.2369e-04 eta 0:03:01
epoch [23/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1406 (1.1186) acc 71.8750 (73.5268) lr 1.2369e-04 eta 0:02:59
epoch [23/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.0664 (1.1192) acc 75.0000 (73.4483) lr 1.2369e-04 eta 0:02:58
epoch [23/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2471 (1.1220) acc 62.5000 (73.4167) lr 1.2369e-04 eta 0:02:56
epoch [23/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0176 (1.1146) acc 75.0000 (73.6089) lr 1.2369e-04 eta 0:02:54
epoch [23/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7563 (1.1126) acc 81.2500 (73.6328) lr 1.2369e-04 eta 0:02:53
epoch [23/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.9326 (1.1191) acc 62.5000 (73.5417) lr 1.2369e-04 eta 0:02:51
epoch [23/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1719 (1.1205) acc 62.5000 (73.4375) lr 1.2369e-04 eta 0:02:50
epoch [23/25] batch [175/250] time 0.291 (0.294) data 0.000 (0.005) loss 0.8755 (1.1181) acc 81.2500 (73.5179) lr 1.2369e-04 eta 0:02:48
epoch [23/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.1777 (1.1178) acc 75.0000 (73.5417) lr 1.2369e-04 eta 0:02:47
epoch [23/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7227 (1.1185) acc 87.5000 (73.5473) lr 1.2369e-04 eta 0:02:45
epoch [23/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3848 (1.1181) acc 71.8750 (73.5526) lr 1.2369e-04 eta 0:02:44
epoch [23/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9351 (1.1178) acc 71.8750 (73.4615) lr 1.2369e-04 eta 0:02:42
epoch [23/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0176 (1.1174) acc 71.8750 (73.3750) lr 1.2369e-04 eta 0:02:41
epoch [23/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0664 (1.1188) acc 65.6250 (73.3537) lr 1.2369e-04 eta 0:02:39
epoch [23/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8906 (1.1191) acc 81.2500 (73.3482) lr 1.2369e-04 eta 0:02:38
epoch [23/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6514 (1.1192) acc 62.5000 (73.2994) lr 1.2369e-04 eta 0:02:36
epoch [23/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2148 (1.1209) acc 68.7500 (73.2955) lr 1.2369e-04 eta 0:02:35
epoch [23/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1621 (1.1240) acc 65.6250 (73.1250) lr 1.2369e-04 eta 0:02:33
epoch [23/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.3535 (1.1236) acc 56.2500 (73.1114) lr 1.2369e-04 eta 0:02:32
epoch [23/25] batch [235/250] time 0.289 (0.292) data 0.001 (0.004) loss 0.7983 (1.1232) acc 90.6250 (73.1915) lr 1.2369e-04 eta 0:02:30
epoch [23/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2656 (1.1235) acc 78.1250 (73.3073) lr 1.2369e-04 eta 0:02:29
epoch [23/25] batch [245/250] time 0.288 (0.292) data 0.000 (0.003) loss 1.0215 (1.1213) acc 75.0000 (73.3418) lr 1.2369e-04 eta 0:02:27
epoch [23/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0703 (1.1197) acc 75.0000 (73.3625) lr 7.0224e-05 eta 0:02:26
epoch [24/25] batch [5/250] time 0.288 (0.451) data 0.000 (0.162) loss 1.6943 (1.3301) acc 59.3750 (72.5000) lr 7.0224e-05 eta 0:03:43
epoch [24/25] batch [10/250] time 0.289 (0.370) data 0.000 (0.081) loss 0.9834 (1.1628) acc 81.2500 (74.3750) lr 7.0224e-05 eta 0:03:01
epoch [24/25] batch [15/250] time 0.288 (0.343) data 0.000 (0.054) loss 1.8779 (1.1970) acc 59.3750 (72.7083) lr 7.0224e-05 eta 0:02:46
epoch [24/25] batch [20/250] time 0.289 (0.329) data 0.000 (0.041) loss 1.0869 (1.1778) acc 75.0000 (73.9062) lr 7.0224e-05 eta 0:02:37
epoch [24/25] batch [25/250] time 0.289 (0.321) data 0.000 (0.033) loss 1.1250 (1.1401) acc 71.8750 (74.7500) lr 7.0224e-05 eta 0:02:32
epoch [24/25] batch [30/250] time 0.289 (0.316) data 0.000 (0.027) loss 0.9658 (1.1094) acc 78.1250 (75.3125) lr 7.0224e-05 eta 0:02:28
epoch [24/25] batch [35/250] time 0.290 (0.312) data 0.000 (0.023) loss 1.4932 (1.1107) acc 59.3750 (75.3571) lr 7.0224e-05 eta 0:02:25
epoch [24/25] batch [40/250] time 0.289 (0.309) data 0.000 (0.020) loss 1.0254 (1.1023) acc 71.8750 (75.2344) lr 7.0224e-05 eta 0:02:22
epoch [24/25] batch [45/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.4951 (1.1091) acc 75.0000 (75.1389) lr 7.0224e-05 eta 0:02:19
epoch [24/25] batch [50/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.9131 (1.1075) acc 87.5000 (75.2500) lr 7.0224e-05 eta 0:02:17
epoch [24/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.0215 (1.0953) acc 75.0000 (75.4545) lr 7.0224e-05 eta 0:02:15
epoch [24/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.014) loss 2.4180 (1.1150) acc 53.1250 (74.8958) lr 7.0224e-05 eta 0:02:13
epoch [24/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.013) loss 1.1855 (1.1191) acc 68.7500 (74.7115) lr 7.0224e-05 eta 0:02:11
epoch [24/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3203 (1.1129) acc 75.0000 (74.9107) lr 7.0224e-05 eta 0:02:09
epoch [24/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.4580 (1.1202) acc 59.3750 (74.5417) lr 7.0224e-05 eta 0:02:07
epoch [24/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.7954 (1.1161) acc 84.3750 (74.6484) lr 7.0224e-05 eta 0:02:05
epoch [24/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.1006 (1.1102) acc 75.0000 (74.7794) lr 7.0224e-05 eta 0:02:03
epoch [24/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1338 (1.1113) acc 75.0000 (74.7222) lr 7.0224e-05 eta 0:02:02
epoch [24/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.009) loss 0.9839 (1.1030) acc 68.7500 (74.7039) lr 7.0224e-05 eta 0:02:00
epoch [24/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8198 (1.1005) acc 78.1250 (74.5625) lr 7.0224e-05 eta 0:01:58
epoch [24/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.6045 (1.0945) acc 87.5000 (74.6726) lr 7.0224e-05 eta 0:01:57
epoch [24/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.008) loss 0.9385 (1.0916) acc 75.0000 (74.7159) lr 7.0224e-05 eta 0:01:55
epoch [24/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3984 (1.0958) acc 68.7500 (74.6739) lr 7.0224e-05 eta 0:01:54
epoch [24/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2539 (1.1017) acc 65.6250 (74.6354) lr 7.0224e-05 eta 0:01:52
epoch [24/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0098 (1.0975) acc 65.6250 (74.7250) lr 7.0224e-05 eta 0:01:50
epoch [24/25] batch [130/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.8257 (1.0968) acc 78.1250 (74.6635) lr 7.0224e-05 eta 0:01:49
epoch [24/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3965 (1.0964) acc 62.5000 (74.5370) lr 7.0224e-05 eta 0:01:47
epoch [24/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8760 (1.0984) acc 75.0000 (74.3527) lr 7.0224e-05 eta 0:01:46
epoch [24/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9912 (1.0966) acc 68.7500 (74.3319) lr 7.0224e-05 eta 0:01:44
epoch [24/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.7500 (1.0975) acc 59.3750 (74.2917) lr 7.0224e-05 eta 0:01:43
epoch [24/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3223 (1.1017) acc 68.7500 (74.1331) lr 7.0224e-05 eta 0:01:41
epoch [24/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8052 (1.0988) acc 84.3750 (74.2188) lr 7.0224e-05 eta 0:01:40
epoch [24/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3789 (1.1068) acc 62.5000 (74.0341) lr 7.0224e-05 eta 0:01:38
epoch [24/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8589 (1.1075) acc 78.1250 (73.9890) lr 7.0224e-05 eta 0:01:36
epoch [24/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5518 (1.1086) acc 59.3750 (73.8929) lr 7.0224e-05 eta 0:01:35
epoch [24/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3848 (1.1175) acc 68.7500 (73.6806) lr 7.0224e-05 eta 0:01:33
epoch [24/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.7393 (1.1202) acc 81.2500 (73.6824) lr 7.0224e-05 eta 0:01:32
epoch [24/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.3086 (1.1229) acc 68.7500 (73.6513) lr 7.0224e-05 eta 0:01:30
epoch [24/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3535 (1.1232) acc 62.5000 (73.6058) lr 7.0224e-05 eta 0:01:29
epoch [24/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.5234 (1.1224) acc 84.3750 (73.5938) lr 7.0224e-05 eta 0:01:27
epoch [24/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2021 (1.1271) acc 71.8750 (73.4756) lr 7.0224e-05 eta 0:01:26
epoch [24/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0107 (1.1267) acc 75.0000 (73.4673) lr 7.0224e-05 eta 0:01:24
epoch [24/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1787 (1.1270) acc 75.0000 (73.5320) lr 7.0224e-05 eta 0:01:23
epoch [24/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9893 (1.1241) acc 84.3750 (73.6506) lr 7.0224e-05 eta 0:01:21
epoch [24/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8281 (1.1258) acc 59.3750 (73.6111) lr 7.0224e-05 eta 0:01:20
epoch [24/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2324 (1.1318) acc 68.7500 (73.3560) lr 7.0224e-05 eta 0:01:19
epoch [24/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 0.8677 (1.1268) acc 75.0000 (73.4309) lr 7.0224e-05 eta 0:01:17
epoch [24/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0996 (1.1276) acc 78.1250 (73.3464) lr 7.0224e-05 eta 0:01:16
epoch [24/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.004) loss 0.8535 (1.1268) acc 81.2500 (73.3546) lr 7.0224e-05 eta 0:01:14
epoch [24/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.2412 (1.1278) acc 62.5000 (73.2750) lr 3.1417e-05 eta 0:01:13
epoch [25/25] batch [5/250] time 0.289 (0.457) data 0.000 (0.168) loss 1.4180 (1.2742) acc 68.7500 (70.0000) lr 3.1417e-05 eta 0:01:52
epoch [25/25] batch [10/250] time 0.290 (0.373) data 0.000 (0.084) loss 0.9775 (1.2260) acc 71.8750 (70.6250) lr 3.1417e-05 eta 0:01:29
epoch [25/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.056) loss 1.4717 (1.2163) acc 71.8750 (72.0833) lr 3.1417e-05 eta 0:01:21
epoch [25/25] batch [20/250] time 0.290 (0.332) data 0.000 (0.042) loss 1.3350 (1.2216) acc 59.3750 (72.0312) lr 3.1417e-05 eta 0:01:16
epoch [25/25] batch [25/250] time 0.290 (0.323) data 0.000 (0.034) loss 1.0156 (1.1721) acc 71.8750 (72.2500) lr 3.1417e-05 eta 0:01:12
epoch [25/25] batch [30/250] time 0.289 (0.318) data 0.000 (0.028) loss 0.7466 (1.1489) acc 87.5000 (72.9167) lr 3.1417e-05 eta 0:01:09
epoch [25/25] batch [35/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.1221 (1.1312) acc 68.7500 (72.8571) lr 3.1417e-05 eta 0:01:07
epoch [25/25] batch [40/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.7622 (1.1309) acc 78.1250 (73.2031) lr 3.1417e-05 eta 0:01:05
epoch [25/25] batch [45/250] time 0.290 (0.308) data 0.000 (0.019) loss 1.3506 (1.1526) acc 68.7500 (72.4306) lr 3.1417e-05 eta 0:01:03
epoch [25/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.3408 (1.1502) acc 62.5000 (72.5625) lr 3.1417e-05 eta 0:01:01
epoch [25/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.8726 (1.1375) acc 81.2500 (72.8977) lr 3.1417e-05 eta 0:00:59
epoch [25/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.3535 (1.1482) acc 71.8750 (72.9167) lr 3.1417e-05 eta 0:00:57
epoch [25/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.5293 (1.1511) acc 65.6250 (72.9808) lr 3.1417e-05 eta 0:00:55
epoch [25/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1807 (1.1416) acc 65.6250 (72.9464) lr 3.1417e-05 eta 0:00:54
epoch [25/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9048 (1.1291) acc 81.2500 (73.0417) lr 3.1417e-05 eta 0:00:52
epoch [25/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9062 (1.1285) acc 68.7500 (73.1641) lr 3.1417e-05 eta 0:00:50
epoch [25/25] batch [85/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.6729 (1.1315) acc 84.3750 (72.9779) lr 3.1417e-05 eta 0:00:49
epoch [25/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.5039 (1.1387) acc 65.6250 (72.8125) lr 3.1417e-05 eta 0:00:47
epoch [25/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.7070 (1.1264) acc 87.5000 (73.1250) lr 3.1417e-05 eta 0:00:46
epoch [25/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6924 (1.1292) acc 65.6250 (72.9688) lr 3.1417e-05 eta 0:00:44
epoch [25/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2285 (1.1282) acc 62.5000 (72.9167) lr 3.1417e-05 eta 0:00:43
epoch [25/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4336 (1.1267) acc 59.3750 (72.9545) lr 3.1417e-05 eta 0:00:41
epoch [25/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2539 (1.1218) acc 68.7500 (73.1793) lr 3.1417e-05 eta 0:00:40
epoch [25/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8682 (1.1167) acc 84.3750 (73.2292) lr 3.1417e-05 eta 0:00:38
epoch [25/25] batch [125/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.8569 (1.1237) acc 81.2500 (73.1750) lr 3.1417e-05 eta 0:00:36
epoch [25/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1182 (1.1178) acc 65.6250 (73.2452) lr 3.1417e-05 eta 0:00:35
epoch [25/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4258 (1.1170) acc 62.5000 (73.3333) lr 3.1417e-05 eta 0:00:33
epoch [25/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1514 (1.1254) acc 68.7500 (73.1920) lr 3.1417e-05 eta 0:00:32
epoch [25/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.4590 (1.1208) acc 87.5000 (73.2759) lr 3.1417e-05 eta 0:00:30
epoch [25/25] batch [150/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.0234 (1.1211) acc 71.8750 (73.2292) lr 3.1417e-05 eta 0:00:29
epoch [25/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0664 (1.1134) acc 75.0000 (73.4476) lr 3.1417e-05 eta 0:00:27
epoch [25/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.1143 (1.1157) acc 81.2500 (73.3984) lr 3.1417e-05 eta 0:00:26
epoch [25/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0449 (1.1165) acc 59.3750 (73.3523) lr 3.1417e-05 eta 0:00:25
epoch [25/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8447 (1.1192) acc 81.2500 (73.2353) lr 3.1417e-05 eta 0:00:23
epoch [25/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8896 (1.1177) acc 81.2500 (73.2321) lr 3.1417e-05 eta 0:00:22
epoch [25/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.5293 (1.1118) acc 87.5000 (73.4201) lr 3.1417e-05 eta 0:00:20
epoch [25/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3301 (1.1113) acc 65.6250 (73.3953) lr 3.1417e-05 eta 0:00:19
epoch [25/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5752 (1.1112) acc 71.8750 (73.4704) lr 3.1417e-05 eta 0:00:17
epoch [25/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.005) loss 0.8027 (1.1096) acc 78.1250 (73.4295) lr 3.1417e-05 eta 0:00:16
epoch [25/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3076 (1.1111) acc 65.6250 (73.3750) lr 3.1417e-05 eta 0:00:14
epoch [25/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5586 (1.1155) acc 68.7500 (73.2927) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1475 (1.1187) acc 68.7500 (73.2143) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3594 (1.1214) acc 81.2500 (73.1977) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0684 (1.1204) acc 78.1250 (73.1960) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [225/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.7139 (1.1190) acc 68.7500 (73.2778) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2305 (1.1209) acc 81.2500 (73.2337) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.7432 (1.1263) acc 65.6250 (73.1117) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2539 (1.1250) acc 68.7500 (73.0859) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9746 (1.1250) acc 78.1250 (73.0357) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0430 (1.1229) acc 75.0000 (73.1875) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:06<05:18,  6.49s/it]  4%|▍         | 2/50 [00:07<02:27,  3.07s/it]  6%|▌         | 3/50 [00:07<01:33,  1.98s/it]  8%|▊         | 4/50 [00:08<01:07,  1.47s/it] 10%|█         | 5/50 [00:09<00:53,  1.18s/it] 12%|█▏        | 6/50 [00:09<00:44,  1.01s/it] 14%|█▍        | 7/50 [00:10<00:38,  1.11it/s] 16%|█▌        | 8/50 [00:11<00:34,  1.20it/s] 18%|█▊        | 9/50 [00:11<00:32,  1.28it/s] 20%|██        | 10/50 [00:12<00:30,  1.33it/s] 22%|██▏       | 11/50 [00:13<00:28,  1.37it/s] 24%|██▍       | 12/50 [00:13<00:27,  1.40it/s] 26%|██▌       | 13/50 [00:14<00:25,  1.42it/s] 28%|██▊       | 14/50 [00:15<00:24,  1.44it/s] 30%|███       | 15/50 [00:15<00:24,  1.45it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.46it/s] 34%|███▍      | 17/50 [00:17<00:22,  1.46it/s] 36%|███▌      | 18/50 [00:18<00:21,  1.47it/s] 38%|███▊      | 19/50 [00:18<00:21,  1.47it/s] 40%|████      | 20/50 [00:19<00:20,  1.47it/s] 42%|████▏     | 21/50 [00:20<00:19,  1.47it/s] 44%|████▍     | 22/50 [00:20<00:19,  1.47it/s] 46%|████▌     | 23/50 [00:21<00:18,  1.47it/s] 48%|████▊     | 24/50 [00:22<00:17,  1.47it/s] 50%|█████     | 25/50 [00:22<00:16,  1.47it/s] 52%|█████▏    | 26/50 [00:23<00:16,  1.47it/s] 54%|█████▍    | 27/50 [00:24<00:15,  1.47it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.47it/s] 58%|█████▊    | 29/50 [00:25<00:14,  1.47it/s] 60%|██████    | 30/50 [00:26<00:13,  1.47it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.47it/s] 64%|██████▍   | 32/50 [00:27<00:12,  1.47it/s] 66%|██████▌   | 33/50 [00:28<00:11,  1.47it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.47it/s] 70%|███████   | 35/50 [00:29<00:10,  1.47it/s] 72%|███████▏  | 36/50 [00:30<00:09,  1.47it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.47it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.47it/s] 78%|███████▊  | 39/50 [00:32<00:07,  1.47it/s] 80%|████████  | 40/50 [00:32<00:06,  1.47it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.47it/s] 84%|████████▍ | 42/50 [00:34<00:05,  1.47it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.47it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.47it/s] 90%|█████████ | 45/50 [00:36<00:03,  1.47it/s] 92%|█████████▏| 46/50 [00:37<00:02,  1.48it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.47it/s] 96%|█████████▌| 48/50 [00:38<00:01,  1.47it/s] 98%|█████████▊| 49/50 [00:39<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.25it/s]
=> result
* total: 25,000
* correct: 19,341
* accuracy: 77.4%
* error: 22.6%
* macro_f1: 77.1%
Elapsed: 0:31:13
Run this job and save the output to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'rooster', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'American robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'American dipper', 'kite (bird of prey)', 'bald eagle', 'vulture', 'great grey owl', 'fire salamander', 'smooth newt', 'newt', 'spotted salamander', 'axolotl', 'American bullfrog', 'tree frog', 'tailed frog', 'loggerhead sea turtle', 'leatherback sea turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'green iguana', 'Carolina anole', 'desert grassland whiptail lizard', 'agama', 'frilled-necked lizard', 'alligator lizard', 'Gila monster', 'European green lizard', 'chameleon', 'Komodo dragon', 'Nile crocodile', 'American alligator', 'triceratops', 'worm snake', 'ring-necked snake', 'eastern hog-nosed snake', 'smooth green snake', 'kingsnake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'African rock python', 'Indian cobra', 'green mamba', 'sea snake', 'Saharan horned viper', 'eastern diamondback rattlesnake', 'sidewinder rattlesnake', 'trilobite', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie grouse', 'peafowl', 'quail', 'partridge', 'african grey parrot', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'duck', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'red king crab', 'American lobster', 'spiny lobster', 'crayfish', 'hermit crab', 'isopod', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'great egret', 'bittern bird', 'crane bird', 'limpkin', 'common gallinule', 'American coot', 'bustard', 'ruddy turnstone', 'dunlin', 'common redshank', 'dowitcher', 'oystercatcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'Papillon', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Otterhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'English Setter', 'Irish Setter', 'Gordon Setter', 'Brittany dog', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniel', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael dog', 'Malinois', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Shetland Sheepdog', 'collie', 'Border Collie', 'Bouvier des Flandres dog', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland dog', 'Great Pyrenees dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Keeshond', 'brussels griffon', 'Pembroke Welsh Corgi', 'Cardigan Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog (xoloitzcuintli)', 'grey wolf', 'Alaskan tundra wolf', 'red wolf or maned wolf', 'coyote', 'dingo', 'dhole', 'African wild dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'polar bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ladybug', 'ground beetle', 'longhorn beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket insect', 'stick insect', 'cockroach', 'praying mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'red admiral butterfly', 'ringlet butterfly', 'monarch butterfly', 'small white butterfly', 'sulphur butterfly', 'gossamer-winged butterfly', 'starfish', 'sea urchin', 'sea cucumber', 'cottontail rabbit', 'hare', 'Angora rabbit', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'common sorrel horse', 'zebra', 'pig', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram (adult male sheep)', 'bighorn sheep', 'Alpine ibex', 'hartebeest', 'impala (antelope)', 'gazelle', 'arabian camel', 'llama', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas monkey', 'baboon', 'macaque', 'langur', 'black-and-white colobus', 'proboscis monkey', 'marmoset', 'white-headed capuchin', 'howler monkey', 'titi monkey', "Geoffroy's spider monkey", 'common squirrel monkey', 'ring-tailed lemur', 'indri', 'Asian elephant', 'African bush elephant', 'red panda', 'giant panda', 'snoek fish', 'eel', 'silver salmon', 'rock beauty fish', 'clownfish', 'sturgeon', 'gar fish', 'lionfish', 'pufferfish', 'abacus', 'abaya', 'academic gown', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'altar', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'apron', 'trash can', 'assault rifle', 'backpack', 'bakery', 'balance beam', 'balloon', 'ballpoint pen', 'Band-Aid', 'banjo', 'baluster / handrail', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military hat (bearskin or shako)', 'beer bottle', 'beer glass', 'bell tower', 'baby bib', 'tandem bicycle', 'bikini', 'ring binder', 'binoculars', 'birdhouse', 'boathouse', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bottle cap', 'hunting bow', 'bow tie', 'brass memorial plaque', 'bra', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'canoe', 'can opener', 'cardigan', 'car mirror', 'carousel', 'tool kit', 'cardboard box / carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'mobile phone', 'chain', 'chain-link fence', 'chain mail', 'chainsaw', 'storage chest', 'chiffonier', 'bell or wind chime', 'china cabinet', 'Christmas stocking', 'church', 'movie theater', 'cleaver']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a tench.', 'a photo of a goldfish.', 'a photo of a great white shark.', 'a photo of a tiger shark.', 'a photo of a hammerhead shark.', 'a photo of a electric ray.', 'a photo of a stingray.', 'a photo of a rooster.', 'a photo of a hen.', 'a photo of a ostrich.', 'a photo of a brambling.', 'a photo of a goldfinch.', 'a photo of a house finch.', 'a photo of a junco.', 'a photo of a indigo bunting.', 'a photo of a American robin.', 'a photo of a bulbul.', 'a photo of a jay.', 'a photo of a magpie.', 'a photo of a chickadee.', 'a photo of a American dipper.', 'a photo of a kite (bird of prey).', 'a photo of a bald eagle.', 'a photo of a vulture.', 'a photo of a great grey owl.', 'a photo of a fire salamander.', 'a photo of a smooth newt.', 'a photo of a newt.', 'a photo of a spotted salamander.', 'a photo of a axolotl.', 'a photo of a American bullfrog.', 'a photo of a tree frog.', 'a photo of a tailed frog.', 'a photo of a loggerhead sea turtle.', 'a photo of a leatherback sea turtle.', 'a photo of a mud turtle.', 'a photo of a terrapin.', 'a photo of a box turtle.', 'a photo of a banded gecko.', 'a photo of a green iguana.', 'a photo of a Carolina anole.', 'a photo of a desert grassland whiptail lizard.', 'a photo of a agama.', 'a photo of a frilled-necked lizard.', 'a photo of a alligator lizard.', 'a photo of a Gila monster.', 'a photo of a European green lizard.', 'a photo of a chameleon.', 'a photo of a Komodo dragon.', 'a photo of a Nile crocodile.', 'a photo of a American alligator.', 'a photo of a triceratops.', 'a photo of a worm snake.', 'a photo of a ring-necked snake.', 'a photo of a eastern hog-nosed snake.', 'a photo of a smooth green snake.', 'a photo of a kingsnake.', 'a photo of a garter snake.', 'a photo of a water snake.', 'a photo of a vine snake.', 'a photo of a night snake.', 'a photo of a boa constrictor.', 'a photo of a African rock python.', 'a photo of a Indian cobra.', 'a photo of a green mamba.', 'a photo of a sea snake.', 'a photo of a Saharan horned viper.', 'a photo of a eastern diamondback rattlesnake.', 'a photo of a sidewinder rattlesnake.', 'a photo of a trilobite.', 'a photo of a harvestman.', 'a photo of a scorpion.', 'a photo of a yellow garden spider.', 'a photo of a barn spider.', 'a photo of a European garden spider.', 'a photo of a southern black widow.', 'a photo of a tarantula.', 'a photo of a wolf spider.', 'a photo of a tick.', 'a photo of a centipede.', 'a photo of a black grouse.', 'a photo of a ptarmigan.', 'a photo of a ruffed grouse.', 'a photo of a prairie grouse.', 'a photo of a peafowl.', 'a photo of a quail.', 'a photo of a partridge.', 'a photo of a african grey parrot.', 'a photo of a macaw.', 'a photo of a sulphur-crested cockatoo.', 'a photo of a lorikeet.', 'a photo of a coucal.', 'a photo of a bee eater.', 'a photo of a hornbill.', 'a photo of a hummingbird.', 'a photo of a jacamar.', 'a photo of a toucan.', 'a photo of a duck.', 'a photo of a red-breasted merganser.', 'a photo of a goose.', 'a photo of a black swan.', 'a photo of a tusker.', 'a photo of a echidna.', 'a photo of a platypus.', 'a photo of a wallaby.', 'a photo of a koala.', 'a photo of a wombat.', 'a photo of a jellyfish.', 'a photo of a sea anemone.', 'a photo of a brain coral.', 'a photo of a flatworm.', 'a photo of a nematode.', 'a photo of a conch.', 'a photo of a snail.', 'a photo of a slug.', 'a photo of a sea slug.', 'a photo of a chiton.', 'a photo of a chambered nautilus.', 'a photo of a Dungeness crab.', 'a photo of a rock crab.', 'a photo of a fiddler crab.', 'a photo of a red king crab.', 'a photo of a American lobster.', 'a photo of a spiny lobster.', 'a photo of a crayfish.', 'a photo of a hermit crab.', 'a photo of a isopod.', 'a photo of a white stork.', 'a photo of a black stork.', 'a photo of a spoonbill.', 'a photo of a flamingo.', 'a photo of a little blue heron.', 'a photo of a great egret.', 'a photo of a bittern bird.', 'a photo of a crane bird.', 'a photo of a limpkin.', 'a photo of a common gallinule.', 'a photo of a American coot.', 'a photo of a bustard.', 'a photo of a ruddy turnstone.', 'a photo of a dunlin.', 'a photo of a common redshank.', 'a photo of a dowitcher.', 'a photo of a oystercatcher.', 'a photo of a pelican.', 'a photo of a king penguin.', 'a photo of a albatross.', 'a photo of a grey whale.', 'a photo of a killer whale.', 'a photo of a dugong.', 'a photo of a sea lion.', 'a photo of a Chihuahua.', 'a photo of a Japanese Chin.', 'a photo of a Maltese.', 'a photo of a Pekingese.', 'a photo of a Shih Tzu.', 'a photo of a King Charles Spaniel.', 'a photo of a Papillon.', 'a photo of a toy terrier.', 'a photo of a Rhodesian Ridgeback.', 'a photo of a Afghan Hound.', 'a photo of a Basset Hound.', 'a photo of a Beagle.', 'a photo of a Bloodhound.', 'a photo of a Bluetick Coonhound.', 'a photo of a Black and Tan Coonhound.', 'a photo of a Treeing Walker Coonhound.', 'a photo of a English foxhound.', 'a photo of a Redbone Coonhound.', 'a photo of a borzoi.', 'a photo of a Irish Wolfhound.', 'a photo of a Italian Greyhound.', 'a photo of a Whippet.', 'a photo of a Ibizan Hound.', 'a photo of a Norwegian Elkhound.', 'a photo of a Otterhound.', 'a photo of a Saluki.', 'a photo of a Scottish Deerhound.', 'a photo of a Weimaraner.', 'a photo of a Staffordshire Bull Terrier.', 'a photo of a American Staffordshire Terrier.', 'a photo of a Bedlington Terrier.', 'a photo of a Border Terrier.', 'a photo of a Kerry Blue Terrier.', 'a photo of a Irish Terrier.', 'a photo of a Norfolk Terrier.', 'a photo of a Norwich Terrier.', 'a photo of a Yorkshire Terrier.', 'a photo of a Wire Fox Terrier.', 'a photo of a Lakeland Terrier.', 'a photo of a Sealyham Terrier.', 'a photo of a Airedale Terrier.', 'a photo of a Cairn Terrier.', 'a photo of a Australian Terrier.', 'a photo of a Dandie Dinmont Terrier.', 'a photo of a Boston Terrier.', 'a photo of a Miniature Schnauzer.', 'a photo of a Giant Schnauzer.', 'a photo of a Standard Schnauzer.', 'a photo of a Scottish Terrier.', 'a photo of a Tibetan Terrier.', 'a photo of a Australian Silky Terrier.', 'a photo of a Soft-coated Wheaten Terrier.', 'a photo of a West Highland White Terrier.', 'a photo of a Lhasa Apso.', 'a photo of a Flat-Coated Retriever.', 'a photo of a Curly-coated Retriever.', 'a photo of a Golden Retriever.', 'a photo of a Labrador Retriever.', 'a photo of a Chesapeake Bay Retriever.', 'a photo of a German Shorthaired Pointer.', 'a photo of a Vizsla.', 'a photo of a English Setter.', 'a photo of a Irish Setter.', 'a photo of a Gordon Setter.', 'a photo of a Brittany dog.', 'a photo of a Clumber Spaniel.', 'a photo of a English Springer Spaniel.', 'a photo of a Welsh Springer Spaniel.', 'a photo of a Cocker Spaniel.', 'a photo of a Sussex Spaniel.', 'a photo of a Irish Water Spaniel.', 'a photo of a Kuvasz.', 'a photo of a Schipperke.', 'a photo of a Groenendael dog.', 'a photo of a Malinois.', 'a photo of a Briard.', 'a photo of a Australian Kelpie.', 'a photo of a Komondor.', 'a photo of a Old English Sheepdog.', 'a photo of a Shetland Sheepdog.', 'a photo of a collie.', 'a photo of a Border Collie.', 'a photo of a Bouvier des Flandres dog.', 'a photo of a Rottweiler.', 'a photo of a German Shepherd Dog.', 'a photo of a Dobermann.', 'a photo of a Miniature Pinscher.', 'a photo of a Greater Swiss Mountain Dog.', 'a photo of a Bernese Mountain Dog.', 'a photo of a Appenzeller Sennenhund.', 'a photo of a Entlebucher Sennenhund.', 'a photo of a Boxer.', 'a photo of a Bullmastiff.', 'a photo of a Tibetan Mastiff.', 'a photo of a French Bulldog.', 'a photo of a Great Dane.', 'a photo of a St. Bernard.', 'a photo of a husky.', 'a photo of a Alaskan Malamute.', 'a photo of a Siberian Husky.', 'a photo of a Dalmatian.', 'a photo of a Affenpinscher.', 'a photo of a Basenji.', 'a photo of a pug.', 'a photo of a Leonberger.', 'a photo of a Newfoundland dog.', 'a photo of a Great Pyrenees dog.', 'a photo of a Samoyed.', 'a photo of a Pomeranian.', 'a photo of a Chow Chow.', 'a photo of a Keeshond.', 'a photo of a brussels griffon.', 'a photo of a Pembroke Welsh Corgi.', 'a photo of a Cardigan Welsh Corgi.', 'a photo of a Toy Poodle.', 'a photo of a Miniature Poodle.', 'a photo of a Standard Poodle.', 'a photo of a Mexican hairless dog (xoloitzcuintli).', 'a photo of a grey wolf.', 'a photo of a Alaskan tundra wolf.', 'a photo of a red wolf or maned wolf.', 'a photo of a coyote.', 'a photo of a dingo.', 'a photo of a dhole.', 'a photo of a African wild dog.', 'a photo of a hyena.', 'a photo of a red fox.', 'a photo of a kit fox.', 'a photo of a Arctic fox.', 'a photo of a grey fox.', 'a photo of a tabby cat.', 'a photo of a tiger cat.', 'a photo of a Persian cat.', 'a photo of a Siamese cat.', 'a photo of a Egyptian Mau.', 'a photo of a cougar.', 'a photo of a lynx.', 'a photo of a leopard.', 'a photo of a snow leopard.', 'a photo of a jaguar.', 'a photo of a lion.', 'a photo of a tiger.', 'a photo of a cheetah.', 'a photo of a brown bear.', 'a photo of a American black bear.', 'a photo of a polar bear.', 'a photo of a sloth bear.', 'a photo of a mongoose.', 'a photo of a meerkat.', 'a photo of a tiger beetle.', 'a photo of a ladybug.', 'a photo of a ground beetle.', 'a photo of a longhorn beetle.', 'a photo of a leaf beetle.', 'a photo of a dung beetle.', 'a photo of a rhinoceros beetle.', 'a photo of a weevil.', 'a photo of a fly.', 'a photo of a bee.', 'a photo of a ant.', 'a photo of a grasshopper.', 'a photo of a cricket insect.', 'a photo of a stick insect.', 'a photo of a cockroach.', 'a photo of a praying mantis.', 'a photo of a cicada.', 'a photo of a leafhopper.', 'a photo of a lacewing.', 'a photo of a dragonfly.', 'a photo of a damselfly.', 'a photo of a red admiral butterfly.', 'a photo of a ringlet butterfly.', 'a photo of a monarch butterfly.', 'a photo of a small white butterfly.', 'a photo of a sulphur butterfly.', 'a photo of a gossamer-winged butterfly.', 'a photo of a starfish.', 'a photo of a sea urchin.', 'a photo of a sea cucumber.', 'a photo of a cottontail rabbit.', 'a photo of a hare.', 'a photo of a Angora rabbit.', 'a photo of a hamster.', 'a photo of a porcupine.', 'a photo of a fox squirrel.', 'a photo of a marmot.', 'a photo of a beaver.', 'a photo of a guinea pig.', 'a photo of a common sorrel horse.', 'a photo of a zebra.', 'a photo of a pig.', 'a photo of a wild boar.', 'a photo of a warthog.', 'a photo of a hippopotamus.', 'a photo of a ox.', 'a photo of a water buffalo.', 'a photo of a bison.', 'a photo of a ram (adult male sheep).', 'a photo of a bighorn sheep.', 'a photo of a Alpine ibex.', 'a photo of a hartebeest.', 'a photo of a impala (antelope).', 'a photo of a gazelle.', 'a photo of a arabian camel.', 'a photo of a llama.', 'a photo of a weasel.', 'a photo of a mink.', 'a photo of a European polecat.', 'a photo of a black-footed ferret.', 'a photo of a otter.', 'a photo of a skunk.', 'a photo of a badger.', 'a photo of a armadillo.', 'a photo of a three-toed sloth.', 'a photo of a orangutan.', 'a photo of a gorilla.', 'a photo of a chimpanzee.', 'a photo of a gibbon.', 'a photo of a siamang.', 'a photo of a guenon.', 'a photo of a patas monkey.', 'a photo of a baboon.', 'a photo of a macaque.', 'a photo of a langur.', 'a photo of a black-and-white colobus.', 'a photo of a proboscis monkey.', 'a photo of a marmoset.', 'a photo of a white-headed capuchin.', 'a photo of a howler monkey.', 'a photo of a titi monkey.', "a photo of a Geoffroy's spider monkey.", 'a photo of a common squirrel monkey.', 'a photo of a ring-tailed lemur.', 'a photo of a indri.', 'a photo of a Asian elephant.', 'a photo of a African bush elephant.', 'a photo of a red panda.', 'a photo of a giant panda.', 'a photo of a snoek fish.', 'a photo of a eel.', 'a photo of a silver salmon.', 'a photo of a rock beauty fish.', 'a photo of a clownfish.', 'a photo of a sturgeon.', 'a photo of a gar fish.', 'a photo of a lionfish.', 'a photo of a pufferfish.', 'a photo of a abacus.', 'a photo of a abaya.', 'a photo of a academic gown.', 'a photo of a accordion.', 'a photo of a acoustic guitar.', 'a photo of a aircraft carrier.', 'a photo of a airliner.', 'a photo of a airship.', 'a photo of a altar.', 'a photo of a ambulance.', 'a photo of a amphibious vehicle.', 'a photo of a analog clock.', 'a photo of a apiary.', 'a photo of a apron.', 'a photo of a trash can.', 'a photo of a assault rifle.', 'a photo of a backpack.', 'a photo of a bakery.', 'a photo of a balance beam.', 'a photo of a balloon.', 'a photo of a ballpoint pen.', 'a photo of a Band-Aid.', 'a photo of a banjo.', 'a photo of a baluster / handrail.', 'a photo of a barbell.', 'a photo of a barber chair.', 'a photo of a barbershop.', 'a photo of a barn.', 'a photo of a barometer.', 'a photo of a barrel.', 'a photo of a wheelbarrow.', 'a photo of a baseball.', 'a photo of a basketball.', 'a photo of a bassinet.', 'a photo of a bassoon.', 'a photo of a swimming cap.', 'a photo of a bath towel.', 'a photo of a bathtub.', 'a photo of a station wagon.', 'a photo of a lighthouse.', 'a photo of a beaker.', 'a photo of a military hat (bearskin or shako).', 'a photo of a beer bottle.', 'a photo of a beer glass.', 'a photo of a bell tower.', 'a photo of a baby bib.', 'a photo of a tandem bicycle.', 'a photo of a bikini.', 'a photo of a ring binder.', 'a photo of a binoculars.', 'a photo of a birdhouse.', 'a photo of a boathouse.', 'a photo of a bobsleigh.', 'a photo of a bolo tie.', 'a photo of a poke bonnet.', 'a photo of a bookcase.', 'a photo of a bookstore.', 'a photo of a bottle cap.', 'a photo of a hunting bow.', 'a photo of a bow tie.', 'a photo of a brass memorial plaque.', 'a photo of a bra.', 'a photo of a breakwater.', 'a photo of a breastplate.', 'a photo of a broom.', 'a photo of a bucket.', 'a photo of a buckle.', 'a photo of a bulletproof vest.', 'a photo of a high-speed train.', 'a photo of a butcher shop.', 'a photo of a taxicab.', 'a photo of a cauldron.', 'a photo of a candle.', 'a photo of a cannon.', 'a photo of a canoe.', 'a photo of a can opener.', 'a photo of a cardigan.', 'a photo of a car mirror.', 'a photo of a carousel.', 'a photo of a tool kit.', 'a photo of a cardboard box / carton.', 'a photo of a car wheel.', 'a photo of a automated teller machine.', 'a photo of a cassette.', 'a photo of a cassette player.', 'a photo of a castle.', 'a photo of a catamaran.', 'a photo of a CD player.', 'a photo of a cello.', 'a photo of a mobile phone.', 'a photo of a chain.', 'a photo of a chain-link fence.', 'a photo of a chain mail.', 'a photo of a chainsaw.', 'a photo of a storage chest.', 'a photo of a chiffonier.', 'a photo of a bell or wind chime.', 'a photo of a china cabinet.', 'a photo of a Christmas stocking.', 'a photo of a church.', 'a photo of a movie theater.', 'a photo of a cleaver.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/25] batch [5/250] time 0.288 (0.492) data 0.001 (0.175) loss 4.7031 (4.2891) acc 40.6250 (46.8750) lr 1.0000e-05 eta 0:51:10
epoch [1/25] batch [10/250] time 0.287 (0.390) data 0.000 (0.088) loss 4.5625 (4.2412) acc 37.5000 (45.9375) lr 1.0000e-05 eta 0:40:30
epoch [1/25] batch [15/250] time 0.288 (0.355) data 0.001 (0.059) loss 4.8203 (4.1148) acc 37.5000 (46.6667) lr 1.0000e-05 eta 0:36:55
epoch [1/25] batch [20/250] time 0.287 (0.338) data 0.000 (0.044) loss 3.9180 (4.0339) acc 53.1250 (47.6562) lr 1.0000e-05 eta 0:35:06
epoch [1/25] batch [25/250] time 0.287 (0.328) data 0.000 (0.035) loss 3.5664 (3.9705) acc 62.5000 (48.6250) lr 1.0000e-05 eta 0:34:02
epoch [1/25] batch [30/250] time 0.288 (0.321) data 0.000 (0.030) loss 3.3516 (3.9337) acc 56.2500 (48.8542) lr 1.0000e-05 eta 0:33:18
epoch [1/25] batch [35/250] time 0.287 (0.317) data 0.000 (0.025) loss 3.3711 (3.9294) acc 62.5000 (48.7500) lr 1.0000e-05 eta 0:32:47
epoch [1/25] batch [40/250] time 0.289 (0.313) data 0.000 (0.022) loss 3.8086 (3.8845) acc 50.0000 (49.5312) lr 1.0000e-05 eta 0:32:23
epoch [1/25] batch [45/250] time 0.288 (0.310) data 0.001 (0.020) loss 3.5293 (3.8409) acc 59.3750 (50.2778) lr 1.0000e-05 eta 0:32:04
epoch [1/25] batch [50/250] time 0.288 (0.308) data 0.000 (0.018) loss 3.0469 (3.8058) acc 68.7500 (51.0000) lr 1.0000e-05 eta 0:31:49
epoch [1/25] batch [55/250] time 0.287 (0.306) data 0.000 (0.016) loss 3.4629 (3.7903) acc 65.6250 (51.2500) lr 1.0000e-05 eta 0:31:36
epoch [1/25] batch [60/250] time 0.288 (0.305) data 0.001 (0.015) loss 3.0859 (3.7326) acc 65.6250 (52.1875) lr 1.0000e-05 eta 0:31:25
epoch [1/25] batch [65/250] time 0.288 (0.303) data 0.000 (0.014) loss 3.8750 (3.7066) acc 50.0000 (52.3077) lr 1.0000e-05 eta 0:31:15
epoch [1/25] batch [70/250] time 0.287 (0.302) data 0.000 (0.013) loss 3.5781 (3.6867) acc 53.1250 (52.4107) lr 1.0000e-05 eta 0:31:07
epoch [1/25] batch [75/250] time 0.287 (0.301) data 0.000 (0.012) loss 4.3828 (3.6777) acc 34.3750 (52.1250) lr 1.0000e-05 eta 0:30:59
epoch [1/25] batch [80/250] time 0.288 (0.300) data 0.000 (0.011) loss 3.5625 (3.6618) acc 50.0000 (52.1484) lr 1.0000e-05 eta 0:30:52
epoch [1/25] batch [85/250] time 0.289 (0.300) data 0.001 (0.011) loss 3.4336 (3.6259) acc 59.3750 (52.7206) lr 1.0000e-05 eta 0:30:47
epoch [1/25] batch [90/250] time 0.290 (0.299) data 0.001 (0.010) loss 3.3652 (3.5910) acc 56.2500 (53.2986) lr 1.0000e-05 eta 0:30:41
epoch [1/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.010) loss 3.1914 (3.5728) acc 50.0000 (53.5197) lr 1.0000e-05 eta 0:30:36
epoch [1/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 3.4980 (3.5506) acc 43.7500 (53.6562) lr 1.0000e-05 eta 0:30:32
epoch [1/25] batch [105/250] time 0.288 (0.298) data 0.000 (0.009) loss 3.1953 (3.5343) acc 50.0000 (53.4226) lr 1.0000e-05 eta 0:30:28
epoch [1/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 3.4531 (3.5183) acc 53.1250 (53.5227) lr 1.0000e-05 eta 0:30:24
epoch [1/25] batch [115/250] time 0.289 (0.297) data 0.001 (0.008) loss 3.4590 (3.4985) acc 59.3750 (53.7228) lr 1.0000e-05 eta 0:30:20
epoch [1/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.008) loss 2.1445 (3.4625) acc 81.2500 (54.2969) lr 1.0000e-05 eta 0:30:17
epoch [1/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 2.8594 (3.4424) acc 65.6250 (54.2750) lr 1.0000e-05 eta 0:30:13
epoch [1/25] batch [130/250] time 0.291 (0.296) data 0.000 (0.007) loss 3.2207 (3.4252) acc 53.1250 (54.3990) lr 1.0000e-05 eta 0:30:10
epoch [1/25] batch [135/250] time 0.290 (0.296) data 0.000 (0.007) loss 2.6035 (3.3912) acc 71.8750 (54.9074) lr 1.0000e-05 eta 0:30:07
epoch [1/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.007) loss 3.1328 (3.3709) acc 59.3750 (55.1562) lr 1.0000e-05 eta 0:30:05
epoch [1/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 3.3184 (3.3557) acc 40.6250 (55.1078) lr 1.0000e-05 eta 0:30:02
epoch [1/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.8320 (3.3430) acc 59.3750 (55.1458) lr 1.0000e-05 eta 0:29:59
epoch [1/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.9707 (3.3301) acc 46.8750 (55.1411) lr 1.0000e-05 eta 0:29:57
epoch [1/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.7656 (3.3064) acc 53.1250 (55.1953) lr 1.0000e-05 eta 0:29:54
epoch [1/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.006) loss 2.7539 (3.2914) acc 56.2500 (55.2273) lr 1.0000e-05 eta 0:29:52
epoch [1/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 3.1680 (3.2787) acc 59.3750 (55.3125) lr 1.0000e-05 eta 0:29:49
epoch [1/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 2.6211 (3.2612) acc 65.6250 (55.4464) lr 1.0000e-05 eta 0:29:47
epoch [1/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.5215 (3.2482) acc 68.7500 (55.6771) lr 1.0000e-05 eta 0:29:44
epoch [1/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.8594 (3.2336) acc 59.3750 (55.8277) lr 1.0000e-05 eta 0:29:42
epoch [1/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.0938 (3.2185) acc 75.0000 (56.0362) lr 1.0000e-05 eta 0:29:40
epoch [1/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.005) loss 3.0957 (3.2153) acc 50.0000 (55.8333) lr 1.0000e-05 eta 0:29:38
epoch [1/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.005) loss 2.8457 (3.2005) acc 50.0000 (55.8906) lr 1.0000e-05 eta 0:29:36
epoch [1/25] batch [205/250] time 0.290 (0.294) data 0.000 (0.005) loss 2.4609 (3.1814) acc 53.1250 (56.0976) lr 1.0000e-05 eta 0:29:34
epoch [1/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.005) loss 3.1758 (3.1695) acc 56.2500 (56.2202) lr 1.0000e-05 eta 0:29:32
epoch [1/25] batch [215/250] time 0.292 (0.293) data 0.003 (0.004) loss 2.4414 (3.1614) acc 56.2500 (56.2791) lr 1.0000e-05 eta 0:29:30
epoch [1/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 2.3750 (3.1498) acc 59.3750 (56.2642) lr 1.0000e-05 eta 0:29:28
epoch [1/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.2559 (3.1371) acc 71.8750 (56.3611) lr 1.0000e-05 eta 0:29:26
epoch [1/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 3.3613 (3.1329) acc 56.2500 (56.3315) lr 1.0000e-05 eta 0:29:24
epoch [1/25] batch [235/250] time 0.291 (0.293) data 0.000 (0.004) loss 2.8906 (3.1210) acc 59.3750 (56.4495) lr 1.0000e-05 eta 0:29:22
epoch [1/25] batch [240/250] time 0.291 (0.293) data 0.000 (0.004) loss 3.0703 (3.1141) acc 46.8750 (56.4323) lr 1.0000e-05 eta 0:29:20
epoch [1/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.9551 (3.1079) acc 59.3750 (56.3776) lr 1.0000e-05 eta 0:29:18
epoch [1/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 3.1289 (3.0997) acc 53.1250 (56.4250) lr 2.0000e-03 eta 0:29:17
epoch [2/25] batch [5/250] time 0.289 (0.459) data 0.000 (0.169) loss 1.9062 (2.0729) acc 65.6250 (64.3750) lr 2.0000e-03 eta 0:45:48
epoch [2/25] batch [10/250] time 0.289 (0.374) data 0.000 (0.085) loss 1.9932 (2.0491) acc 68.7500 (63.4375) lr 2.0000e-03 eta 0:37:18
epoch [2/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.057) loss 1.6816 (1.9200) acc 65.6250 (64.7917) lr 2.0000e-03 eta 0:34:30
epoch [2/25] batch [20/250] time 0.290 (0.332) data 0.000 (0.042) loss 1.5361 (1.8438) acc 71.8750 (65.0000) lr 2.0000e-03 eta 0:33:04
epoch [2/25] batch [25/250] time 0.290 (0.324) data 0.000 (0.034) loss 1.2227 (1.7650) acc 68.7500 (65.5000) lr 2.0000e-03 eta 0:32:13
epoch [2/25] batch [30/250] time 0.289 (0.318) data 0.000 (0.028) loss 1.9609 (1.7888) acc 65.6250 (64.3750) lr 2.0000e-03 eta 0:31:38
epoch [2/25] batch [35/250] time 0.289 (0.314) data 0.000 (0.024) loss 1.2822 (1.7630) acc 71.8750 (64.5536) lr 2.0000e-03 eta 0:31:12
epoch [2/25] batch [40/250] time 0.289 (0.311) data 0.000 (0.021) loss 1.7373 (1.7805) acc 59.3750 (63.8281) lr 2.0000e-03 eta 0:30:52
epoch [2/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.5234 (1.7606) acc 59.3750 (63.4722) lr 2.0000e-03 eta 0:30:36
epoch [2/25] batch [50/250] time 0.290 (0.307) data 0.000 (0.017) loss 1.6553 (1.7311) acc 62.5000 (63.5000) lr 2.0000e-03 eta 0:30:23
epoch [2/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.7617 (1.7192) acc 50.0000 (63.3523) lr 2.0000e-03 eta 0:30:13
epoch [2/25] batch [60/250] time 0.289 (0.304) data 0.000 (0.014) loss 1.0840 (1.7112) acc 71.8750 (63.3854) lr 2.0000e-03 eta 0:30:04
epoch [2/25] batch [65/250] time 0.290 (0.303) data 0.000 (0.013) loss 1.3652 (1.6942) acc 62.5000 (63.5577) lr 2.0000e-03 eta 0:29:56
epoch [2/25] batch [70/250] time 0.290 (0.302) data 0.000 (0.012) loss 1.5439 (1.6767) acc 75.0000 (63.9286) lr 2.0000e-03 eta 0:29:49
epoch [2/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.6016 (1.6497) acc 65.6250 (64.0417) lr 2.0000e-03 eta 0:29:43
epoch [2/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.9443 (1.6431) acc 56.2500 (64.1406) lr 2.0000e-03 eta 0:29:37
epoch [2/25] batch [85/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.3662 (1.6242) acc 59.3750 (64.1176) lr 2.0000e-03 eta 0:29:32
epoch [2/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.2490 (1.6190) acc 65.6250 (64.0625) lr 2.0000e-03 eta 0:29:27
epoch [2/25] batch [95/250] time 0.289 (0.299) data 0.000 (0.009) loss 2.5254 (1.6289) acc 37.5000 (63.6184) lr 2.0000e-03 eta 0:29:22
epoch [2/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7139 (1.6248) acc 62.5000 (63.6562) lr 2.0000e-03 eta 0:29:18
epoch [2/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.2588 (1.6171) acc 78.1250 (63.7798) lr 2.0000e-03 eta 0:29:14
epoch [2/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.2188 (1.6050) acc 59.3750 (63.6080) lr 2.0000e-03 eta 0:29:11
epoch [2/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.6709 (1.6018) acc 71.8750 (63.6685) lr 2.0000e-03 eta 0:29:07
epoch [2/25] batch [120/250] time 0.293 (0.297) data 0.000 (0.007) loss 1.3438 (1.5929) acc 53.1250 (63.5677) lr 2.0000e-03 eta 0:29:04
epoch [2/25] batch [125/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.9111 (1.5923) acc 59.3750 (63.5750) lr 2.0000e-03 eta 0:29:01
epoch [2/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0664 (1.5740) acc 71.8750 (63.8221) lr 2.0000e-03 eta 0:28:58
epoch [2/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.7256 (1.5677) acc 59.3750 (63.9120) lr 2.0000e-03 eta 0:28:55
epoch [2/25] batch [140/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.1553 (1.5602) acc 56.2500 (63.8393) lr 2.0000e-03 eta 0:28:53
epoch [2/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.006) loss 0.8296 (1.5414) acc 78.1250 (63.9655) lr 2.0000e-03 eta 0:28:50
epoch [2/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.1953 (1.5302) acc 71.8750 (64.1875) lr 2.0000e-03 eta 0:28:47
epoch [2/25] batch [155/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.1885 (1.5218) acc 65.6250 (64.2540) lr 2.0000e-03 eta 0:28:45
epoch [2/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.0410 (1.5128) acc 81.2500 (64.5117) lr 2.0000e-03 eta 0:28:43
epoch [2/25] batch [165/250] time 0.291 (0.295) data 0.000 (0.005) loss 1.0557 (1.5033) acc 62.5000 (64.7727) lr 2.0000e-03 eta 0:28:40
epoch [2/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.5352 (1.5015) acc 56.2500 (64.6507) lr 2.0000e-03 eta 0:28:38
epoch [2/25] batch [175/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.5010 (1.5013) acc 53.1250 (64.5893) lr 2.0000e-03 eta 0:28:36
epoch [2/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.5762 (1.5017) acc 62.5000 (64.5139) lr 2.0000e-03 eta 0:28:33
epoch [2/25] batch [185/250] time 0.292 (0.294) data 0.000 (0.005) loss 1.5127 (1.5057) acc 71.8750 (64.3919) lr 2.0000e-03 eta 0:28:31
epoch [2/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4004 (1.5030) acc 68.7500 (64.4079) lr 2.0000e-03 eta 0:28:29
epoch [2/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5898 (1.5000) acc 59.3750 (64.4231) lr 2.0000e-03 eta 0:28:27
epoch [2/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.1768 (1.5020) acc 75.0000 (64.4375) lr 2.0000e-03 eta 0:28:25
epoch [2/25] batch [205/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.2344 (1.4917) acc 75.0000 (64.6494) lr 2.0000e-03 eta 0:28:23
epoch [2/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3945 (1.4928) acc 68.7500 (64.5833) lr 2.0000e-03 eta 0:28:21
epoch [2/25] batch [215/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.5850 (1.4929) acc 65.6250 (64.5640) lr 2.0000e-03 eta 0:28:19
epoch [2/25] batch [220/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.2041 (1.4917) acc 84.3750 (64.6875) lr 2.0000e-03 eta 0:28:17
epoch [2/25] batch [225/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.7783 (1.4898) acc 59.3750 (64.7083) lr 2.0000e-03 eta 0:28:15
epoch [2/25] batch [230/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.5039 (1.4887) acc 59.3750 (64.6467) lr 2.0000e-03 eta 0:28:13
epoch [2/25] batch [235/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.0732 (1.4868) acc 65.6250 (64.6543) lr 2.0000e-03 eta 0:28:11
epoch [2/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2129 (1.4848) acc 75.0000 (64.6484) lr 2.0000e-03 eta 0:28:09
epoch [2/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1211 (1.4843) acc 75.0000 (64.6429) lr 2.0000e-03 eta 0:28:08
epoch [2/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0889 (1.4832) acc 68.7500 (64.7250) lr 1.9921e-03 eta 0:28:06
epoch [3/25] batch [5/250] time 0.289 (0.459) data 0.000 (0.169) loss 1.1221 (1.3609) acc 59.3750 (65.6250) lr 1.9921e-03 eta 0:43:54
epoch [3/25] batch [10/250] time 0.290 (0.374) data 0.000 (0.085) loss 1.4404 (1.3487) acc 62.5000 (66.5625) lr 1.9921e-03 eta 0:35:49
epoch [3/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.057) loss 2.1172 (1.3981) acc 43.7500 (65.2083) lr 1.9921e-03 eta 0:33:05
epoch [3/25] batch [20/250] time 0.293 (0.332) data 0.000 (0.043) loss 1.5010 (1.4330) acc 62.5000 (65.1562) lr 1.9921e-03 eta 0:31:44
epoch [3/25] batch [25/250] time 0.289 (0.324) data 0.000 (0.034) loss 1.0195 (1.3752) acc 78.1250 (66.5000) lr 1.9921e-03 eta 0:30:54
epoch [3/25] batch [30/250] time 0.290 (0.318) data 0.000 (0.028) loss 1.3164 (1.3627) acc 59.3750 (66.5625) lr 1.9921e-03 eta 0:30:19
epoch [3/25] batch [35/250] time 0.290 (0.314) data 0.000 (0.024) loss 1.4902 (1.3551) acc 68.7500 (66.1607) lr 1.9921e-03 eta 0:29:54
epoch [3/25] batch [40/250] time 0.290 (0.311) data 0.000 (0.021) loss 1.3340 (1.3734) acc 65.6250 (65.3906) lr 1.9921e-03 eta 0:29:36
epoch [3/25] batch [45/250] time 0.290 (0.309) data 0.000 (0.019) loss 1.2842 (1.3902) acc 59.3750 (64.5833) lr 1.9921e-03 eta 0:29:21
epoch [3/25] batch [50/250] time 0.289 (0.307) data 0.000 (0.017) loss 1.6758 (1.4054) acc 59.3750 (64.4375) lr 1.9921e-03 eta 0:29:09
epoch [3/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.2451 (1.3888) acc 71.8750 (64.7159) lr 1.9921e-03 eta 0:28:58
epoch [3/25] batch [60/250] time 0.289 (0.304) data 0.000 (0.014) loss 2.0059 (1.4039) acc 56.2500 (64.5833) lr 1.9921e-03 eta 0:28:49
epoch [3/25] batch [65/250] time 0.291 (0.303) data 0.000 (0.013) loss 1.5840 (1.4067) acc 50.0000 (64.3269) lr 1.9921e-03 eta 0:28:42
epoch [3/25] batch [70/250] time 0.291 (0.302) data 0.000 (0.012) loss 1.1963 (1.4107) acc 59.3750 (64.0625) lr 1.9921e-03 eta 0:28:35
epoch [3/25] batch [75/250] time 0.292 (0.301) data 0.000 (0.012) loss 1.4014 (1.3906) acc 71.8750 (64.5833) lr 1.9921e-03 eta 0:28:29
epoch [3/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.011) loss 1.5557 (1.3933) acc 65.6250 (64.8047) lr 1.9921e-03 eta 0:28:23
epoch [3/25] batch [85/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.7031 (1.3861) acc 59.3750 (64.9265) lr 1.9921e-03 eta 0:28:18
epoch [3/25] batch [90/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.7109 (1.3973) acc 62.5000 (64.8264) lr 1.9921e-03 eta 0:28:14
epoch [3/25] batch [95/250] time 0.290 (0.299) data 0.000 (0.009) loss 0.8442 (1.3880) acc 75.0000 (65.0987) lr 1.9921e-03 eta 0:28:09
epoch [3/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.009) loss 0.7583 (1.3821) acc 84.3750 (65.3438) lr 1.9921e-03 eta 0:28:05
epoch [3/25] batch [105/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.2549 (1.3806) acc 59.3750 (65.2381) lr 1.9921e-03 eta 0:28:02
epoch [3/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.2363 (1.3714) acc 78.1250 (65.4545) lr 1.9921e-03 eta 0:27:58
epoch [3/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2607 (1.3655) acc 68.7500 (65.6250) lr 1.9921e-03 eta 0:27:55
epoch [3/25] batch [120/250] time 0.291 (0.297) data 0.000 (0.007) loss 1.4590 (1.3687) acc 62.5000 (65.5990) lr 1.9921e-03 eta 0:27:52
epoch [3/25] batch [125/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.3359 (1.3730) acc 65.6250 (65.6500) lr 1.9921e-03 eta 0:27:49
epoch [3/25] batch [130/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.4512 (1.3761) acc 62.5000 (65.4567) lr 1.9921e-03 eta 0:27:46
epoch [3/25] batch [135/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.1514 (1.3676) acc 62.5000 (65.5324) lr 1.9921e-03 eta 0:27:43
epoch [3/25] batch [140/250] time 0.290 (0.296) data 0.000 (0.006) loss 2.0664 (1.3691) acc 62.5000 (65.5804) lr 1.9921e-03 eta 0:27:40
epoch [3/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.006) loss 0.9058 (1.3690) acc 68.7500 (65.5603) lr 1.9921e-03 eta 0:27:38
epoch [3/25] batch [150/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.6680 (1.3719) acc 59.3750 (65.6042) lr 1.9921e-03 eta 0:27:35
epoch [3/25] batch [155/250] time 0.291 (0.295) data 0.000 (0.006) loss 0.8628 (1.3711) acc 78.1250 (65.6653) lr 1.9921e-03 eta 0:27:33
epoch [3/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.6602 (1.3703) acc 56.2500 (65.6445) lr 1.9921e-03 eta 0:27:30
epoch [3/25] batch [165/250] time 0.291 (0.295) data 0.000 (0.005) loss 1.7275 (1.3707) acc 62.5000 (65.6629) lr 1.9921e-03 eta 0:27:28
epoch [3/25] batch [170/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.7578 (1.3716) acc 68.7500 (65.7353) lr 1.9921e-03 eta 0:27:26
epoch [3/25] batch [175/250] time 0.292 (0.295) data 0.001 (0.005) loss 2.1133 (1.3792) acc 53.1250 (65.5714) lr 1.9921e-03 eta 0:27:24
epoch [3/25] batch [180/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.7432 (1.3835) acc 59.3750 (65.5382) lr 1.9921e-03 eta 0:27:21
epoch [3/25] batch [185/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.3311 (1.3859) acc 59.3750 (65.5574) lr 1.9921e-03 eta 0:27:19
epoch [3/25] batch [190/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.9648 (1.3747) acc 81.2500 (65.8388) lr 1.9921e-03 eta 0:27:17
epoch [3/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.7959 (1.3751) acc 59.3750 (65.8333) lr 1.9921e-03 eta 0:27:15
epoch [3/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3535 (1.3814) acc 68.7500 (65.7188) lr 1.9921e-03 eta 0:27:13
epoch [3/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.2939 (1.3858) acc 56.2500 (65.5640) lr 1.9921e-03 eta 0:27:11
epoch [3/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3125 (1.3824) acc 68.7500 (65.5804) lr 1.9921e-03 eta 0:27:09
epoch [3/25] batch [215/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1641 (1.3809) acc 62.5000 (65.5378) lr 1.9921e-03 eta 0:27:07
epoch [3/25] batch [220/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.5791 (1.3804) acc 71.8750 (65.5966) lr 1.9921e-03 eta 0:27:05
epoch [3/25] batch [225/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.5205 (1.3772) acc 59.3750 (65.6806) lr 1.9921e-03 eta 0:27:03
epoch [3/25] batch [230/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.9766 (1.3756) acc 75.0000 (65.6929) lr 1.9921e-03 eta 0:27:01
epoch [3/25] batch [235/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.9810 (1.3748) acc 75.0000 (65.8112) lr 1.9921e-03 eta 0:26:59
epoch [3/25] batch [240/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.7773 (1.3755) acc 53.1250 (65.7682) lr 1.9921e-03 eta 0:26:57
epoch [3/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7432 (1.3777) acc 50.0000 (65.6760) lr 1.9921e-03 eta 0:26:55
epoch [3/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5635 (1.3772) acc 62.5000 (65.7000) lr 1.9686e-03 eta 0:26:53
epoch [4/25] batch [5/250] time 0.291 (0.441) data 0.000 (0.151) loss 1.4648 (1.4074) acc 65.6250 (66.8750) lr 1.9686e-03 eta 0:40:23
epoch [4/25] batch [10/250] time 0.289 (0.366) data 0.000 (0.076) loss 1.0518 (1.3810) acc 75.0000 (68.7500) lr 1.9686e-03 eta 0:33:28
epoch [4/25] batch [15/250] time 0.289 (0.340) data 0.000 (0.050) loss 1.0137 (1.3840) acc 78.1250 (68.9583) lr 1.9686e-03 eta 0:31:06
epoch [4/25] batch [20/250] time 0.291 (0.328) data 0.000 (0.038) loss 1.5752 (1.4279) acc 56.2500 (67.0312) lr 1.9686e-03 eta 0:29:56
epoch [4/25] batch [25/250] time 0.291 (0.320) data 0.001 (0.030) loss 1.5762 (1.4148) acc 68.7500 (67.0000) lr 1.9686e-03 eta 0:29:14
epoch [4/25] batch [30/250] time 0.291 (0.315) data 0.000 (0.025) loss 1.1045 (1.3931) acc 75.0000 (66.9792) lr 1.9686e-03 eta 0:28:45
epoch [4/25] batch [35/250] time 0.291 (0.312) data 0.001 (0.022) loss 1.4053 (1.3916) acc 68.7500 (67.4107) lr 1.9686e-03 eta 0:28:24
epoch [4/25] batch [40/250] time 0.290 (0.309) data 0.000 (0.019) loss 1.5840 (1.4126) acc 59.3750 (66.5625) lr 1.9686e-03 eta 0:28:08
epoch [4/25] batch [45/250] time 0.291 (0.307) data 0.000 (0.017) loss 1.4863 (1.4184) acc 56.2500 (66.2500) lr 1.9686e-03 eta 0:27:55
epoch [4/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.015) loss 1.7578 (1.4264) acc 56.2500 (65.8125) lr 1.9686e-03 eta 0:27:44
epoch [4/25] batch [55/250] time 0.291 (0.304) data 0.001 (0.014) loss 0.9355 (1.4125) acc 78.1250 (66.1932) lr 1.9686e-03 eta 0:27:35
epoch [4/25] batch [60/250] time 0.291 (0.303) data 0.001 (0.013) loss 1.7578 (1.4326) acc 50.0000 (65.3646) lr 1.9686e-03 eta 0:27:28
epoch [4/25] batch [65/250] time 0.290 (0.302) data 0.001 (0.012) loss 1.3408 (1.4170) acc 59.3750 (65.6250) lr 1.9686e-03 eta 0:27:21
epoch [4/25] batch [70/250] time 0.290 (0.301) data 0.000 (0.011) loss 1.7227 (1.4448) acc 62.5000 (65.0893) lr 1.9686e-03 eta 0:27:15
epoch [4/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.010) loss 1.0059 (1.4351) acc 75.0000 (65.4167) lr 1.9686e-03 eta 0:27:10
epoch [4/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.0098 (1.4289) acc 71.8750 (65.6641) lr 1.9686e-03 eta 0:27:04
epoch [4/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.0508 (1.4074) acc 59.3750 (65.8088) lr 1.9686e-03 eta 0:27:00
epoch [4/25] batch [90/250] time 0.291 (0.299) data 0.000 (0.009) loss 0.9961 (1.4106) acc 65.6250 (65.5556) lr 1.9686e-03 eta 0:26:55
epoch [4/25] batch [95/250] time 0.291 (0.298) data 0.001 (0.008) loss 1.9229 (1.4170) acc 56.2500 (65.3947) lr 1.9686e-03 eta 0:26:51
epoch [4/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.5674 (1.4196) acc 56.2500 (65.3125) lr 1.9686e-03 eta 0:26:47
epoch [4/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.2354 (1.4157) acc 65.6250 (65.4464) lr 1.9686e-03 eta 0:26:44
epoch [4/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.3945 (1.4237) acc 68.7500 (65.3409) lr 1.9686e-03 eta 0:26:41
epoch [4/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.7207 (1.4261) acc 50.0000 (64.9728) lr 1.9686e-03 eta 0:26:38
epoch [4/25] batch [120/250] time 0.292 (0.297) data 0.000 (0.007) loss 1.3770 (1.4200) acc 59.3750 (64.9219) lr 1.9686e-03 eta 0:26:35
epoch [4/25] batch [125/250] time 0.293 (0.296) data 0.000 (0.006) loss 1.5625 (1.4262) acc 62.5000 (64.7250) lr 1.9686e-03 eta 0:26:32
epoch [4/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.006) loss 0.7251 (1.4114) acc 81.2500 (64.9519) lr 1.9686e-03 eta 0:26:29
epoch [4/25] batch [135/250] time 0.291 (0.296) data 0.000 (0.006) loss 1.4961 (1.4061) acc 68.7500 (65.0463) lr 1.9686e-03 eta 0:26:27
epoch [4/25] batch [140/250] time 0.291 (0.296) data 0.001 (0.006) loss 1.0957 (1.4081) acc 75.0000 (65.0670) lr 1.9686e-03 eta 0:26:24
epoch [4/25] batch [145/250] time 0.291 (0.295) data 0.000 (0.006) loss 0.6685 (1.4108) acc 81.2500 (64.9353) lr 1.9686e-03 eta 0:26:22
epoch [4/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.5244 (1.4028) acc 53.1250 (64.9792) lr 1.9686e-03 eta 0:26:19
epoch [4/25] batch [155/250] time 0.291 (0.295) data 0.000 (0.005) loss 1.1396 (1.4015) acc 59.3750 (64.9194) lr 1.9686e-03 eta 0:26:17
epoch [4/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.1992 (1.3955) acc 75.0000 (65.0000) lr 1.9686e-03 eta 0:26:15
epoch [4/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.8682 (1.3970) acc 46.8750 (64.8295) lr 1.9686e-03 eta 0:26:12
epoch [4/25] batch [170/250] time 0.291 (0.295) data 0.001 (0.005) loss 0.7471 (1.3896) acc 78.1250 (64.9816) lr 1.9686e-03 eta 0:26:10
epoch [4/25] batch [175/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.9902 (1.3873) acc 71.8750 (64.9821) lr 1.9686e-03 eta 0:26:08
epoch [4/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.005) loss 2.0234 (1.3860) acc 50.0000 (64.9826) lr 1.9686e-03 eta 0:26:06
epoch [4/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3457 (1.3800) acc 68.7500 (65.1351) lr 1.9686e-03 eta 0:26:04
epoch [4/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1201 (1.3765) acc 75.0000 (65.2138) lr 1.9686e-03 eta 0:26:02
epoch [4/25] batch [195/250] time 0.289 (0.294) data 0.001 (0.004) loss 1.2246 (1.3760) acc 71.8750 (65.2083) lr 1.9686e-03 eta 0:26:00
epoch [4/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.2646 (1.3728) acc 56.2500 (65.1875) lr 1.9686e-03 eta 0:25:58
epoch [4/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.3945 (1.3734) acc 65.6250 (65.2439) lr 1.9686e-03 eta 0:25:56
epoch [4/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3242 (1.3668) acc 75.0000 (65.4018) lr 1.9686e-03 eta 0:25:54
epoch [4/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.2676 (1.3702) acc 68.7500 (65.3634) lr 1.9686e-03 eta 0:25:52
epoch [4/25] batch [220/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3457 (1.3710) acc 78.1250 (65.3693) lr 1.9686e-03 eta 0:25:50
epoch [4/25] batch [225/250] time 0.295 (0.294) data 0.000 (0.004) loss 1.4434 (1.3695) acc 65.6250 (65.3611) lr 1.9686e-03 eta 0:25:48
epoch [4/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.5977 (1.3682) acc 78.1250 (65.4212) lr 1.9686e-03 eta 0:25:46
epoch [4/25] batch [235/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3682 (1.3686) acc 65.6250 (65.4122) lr 1.9686e-03 eta 0:25:44
epoch [4/25] batch [240/250] time 0.292 (0.293) data 0.000 (0.004) loss 1.1035 (1.3773) acc 65.6250 (65.2474) lr 1.9686e-03 eta 0:25:43
epoch [4/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.1016 (1.3785) acc 71.8750 (65.2423) lr 1.9686e-03 eta 0:25:41
epoch [4/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.2734 (1.3772) acc 68.7500 (65.2875) lr 1.9298e-03 eta 0:25:39
epoch [5/25] batch [5/250] time 0.290 (0.437) data 0.000 (0.148) loss 1.4590 (1.0899) acc 71.8750 (76.2500) lr 1.9298e-03 eta 0:38:13
epoch [5/25] batch [10/250] time 0.289 (0.363) data 0.000 (0.074) loss 1.5117 (1.1722) acc 65.6250 (71.2500) lr 1.9298e-03 eta 0:31:43
epoch [5/25] batch [15/250] time 0.290 (0.339) data 0.000 (0.049) loss 1.4775 (1.3031) acc 59.3750 (68.5417) lr 1.9298e-03 eta 0:29:33
epoch [5/25] batch [20/250] time 0.291 (0.327) data 0.000 (0.037) loss 1.2109 (1.2502) acc 65.6250 (69.3750) lr 1.9298e-03 eta 0:28:28
epoch [5/25] batch [25/250] time 0.291 (0.319) data 0.000 (0.030) loss 0.9897 (1.2551) acc 68.7500 (68.7500) lr 1.9298e-03 eta 0:27:48
epoch [5/25] batch [30/250] time 0.290 (0.314) data 0.000 (0.025) loss 1.8262 (1.3133) acc 65.6250 (67.7083) lr 1.9298e-03 eta 0:27:21
epoch [5/25] batch [35/250] time 0.292 (0.311) data 0.000 (0.021) loss 1.0938 (1.2885) acc 68.7500 (67.5893) lr 1.9298e-03 eta 0:27:01
epoch [5/25] batch [40/250] time 0.290 (0.308) data 0.000 (0.019) loss 1.4180 (1.2922) acc 71.8750 (67.5781) lr 1.9298e-03 eta 0:26:46
epoch [5/25] batch [45/250] time 0.292 (0.306) data 0.000 (0.017) loss 1.7559 (1.3212) acc 68.7500 (66.9444) lr 1.9298e-03 eta 0:26:35
epoch [5/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.015) loss 0.9165 (1.3325) acc 81.2500 (67.0625) lr 1.9298e-03 eta 0:26:25
epoch [5/25] batch [55/250] time 0.290 (0.304) data 0.000 (0.014) loss 0.9805 (1.3341) acc 71.8750 (66.5341) lr 1.9298e-03 eta 0:26:16
epoch [5/25] batch [60/250] time 0.291 (0.302) data 0.001 (0.013) loss 0.9819 (1.3175) acc 75.0000 (66.6146) lr 1.9298e-03 eta 0:26:09
epoch [5/25] batch [65/250] time 0.290 (0.302) data 0.001 (0.012) loss 1.2510 (1.3303) acc 68.7500 (66.2981) lr 1.9298e-03 eta 0:26:03
epoch [5/25] batch [70/250] time 0.289 (0.301) data 0.000 (0.011) loss 1.5293 (1.3290) acc 62.5000 (66.2054) lr 1.9298e-03 eta 0:25:57
epoch [5/25] batch [75/250] time 0.292 (0.300) data 0.001 (0.010) loss 1.7988 (1.3396) acc 56.2500 (66.1667) lr 1.9298e-03 eta 0:25:52
epoch [5/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.6230 (1.3458) acc 65.6250 (66.1719) lr 1.9298e-03 eta 0:25:47
epoch [5/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.1318 (1.3536) acc 65.6250 (66.2868) lr 1.9298e-03 eta 0:25:43
epoch [5/25] batch [90/250] time 0.290 (0.298) data 0.001 (0.009) loss 1.1914 (1.3431) acc 68.7500 (66.3542) lr 1.9298e-03 eta 0:25:39
epoch [5/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.7285 (1.3565) acc 46.8750 (65.8882) lr 1.9298e-03 eta 0:25:35
epoch [5/25] batch [100/250] time 0.290 (0.297) data 0.000 (0.008) loss 0.9980 (1.3424) acc 71.8750 (66.0625) lr 1.9298e-03 eta 0:25:32
epoch [5/25] batch [105/250] time 0.290 (0.297) data 0.001 (0.007) loss 1.1748 (1.3320) acc 68.7500 (66.3095) lr 1.9298e-03 eta 0:25:28
epoch [5/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.6289 (1.3363) acc 50.0000 (66.2500) lr 1.9298e-03 eta 0:25:25
epoch [5/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.5957 (1.3327) acc 71.8750 (66.3587) lr 1.9298e-03 eta 0:25:22
epoch [5/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5771 (1.3309) acc 56.2500 (66.3542) lr 1.9298e-03 eta 0:25:19
epoch [5/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.0684 (1.3336) acc 65.6250 (66.2750) lr 1.9298e-03 eta 0:25:16
epoch [5/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.006) loss 2.0605 (1.3395) acc 46.8750 (66.2740) lr 1.9298e-03 eta 0:25:14
epoch [5/25] batch [135/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.6064 (1.3354) acc 65.6250 (66.4120) lr 1.9298e-03 eta 0:25:11
epoch [5/25] batch [140/250] time 0.291 (0.295) data 0.000 (0.006) loss 1.4580 (1.3304) acc 59.3750 (66.3839) lr 1.9298e-03 eta 0:25:09
epoch [5/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.3037 (1.3338) acc 71.8750 (66.4009) lr 1.9298e-03 eta 0:25:07
epoch [5/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.2646 (1.3363) acc 68.7500 (66.3333) lr 1.9298e-03 eta 0:25:04
epoch [5/25] batch [155/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.2227 (1.3404) acc 68.7500 (66.2298) lr 1.9298e-03 eta 0:25:02
epoch [5/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.2656 (1.3337) acc 59.3750 (66.2891) lr 1.9298e-03 eta 0:24:59
epoch [5/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.3164 (1.3403) acc 62.5000 (66.1174) lr 1.9298e-03 eta 0:24:57
epoch [5/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.6963 (1.3437) acc 56.2500 (66.0478) lr 1.9298e-03 eta 0:24:55
epoch [5/25] batch [175/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.0566 (1.3504) acc 75.0000 (65.9464) lr 1.9298e-03 eta 0:24:53
epoch [5/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.7197 (1.3511) acc 62.5000 (65.9896) lr 1.9298e-03 eta 0:24:51
epoch [5/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.3545 (1.3571) acc 56.2500 (65.9291) lr 1.9298e-03 eta 0:24:49
epoch [5/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.6172 (1.3578) acc 62.5000 (66.0197) lr 1.9298e-03 eta 0:24:47
epoch [5/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.4424 (1.3527) acc 65.6250 (66.1699) lr 1.9298e-03 eta 0:24:45
epoch [5/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.5332 (1.3502) acc 65.6250 (66.3125) lr 1.9298e-03 eta 0:24:43
epoch [5/25] batch [205/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.4834 (1.3514) acc 65.6250 (66.3567) lr 1.9298e-03 eta 0:24:41
epoch [5/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.6309 (1.3520) acc 56.2500 (66.3988) lr 1.9298e-03 eta 0:24:39
epoch [5/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0986 (1.3503) acc 75.0000 (66.4826) lr 1.9298e-03 eta 0:24:37
epoch [5/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.0430 (1.3559) acc 68.7500 (66.4205) lr 1.9298e-03 eta 0:24:35
epoch [5/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5342 (1.3665) acc 68.7500 (66.2778) lr 1.9298e-03 eta 0:24:33
epoch [5/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3604 (1.3656) acc 62.5000 (66.2636) lr 1.9298e-03 eta 0:24:31
epoch [5/25] batch [235/250] time 0.291 (0.293) data 0.001 (0.004) loss 1.2168 (1.3669) acc 65.6250 (66.2234) lr 1.9298e-03 eta 0:24:30
epoch [5/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.4434 (1.3638) acc 68.7500 (66.2891) lr 1.9298e-03 eta 0:24:28
epoch [5/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.6055 (1.3645) acc 62.5000 (66.2372) lr 1.9298e-03 eta 0:24:26
epoch [5/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.5557 (1.3636) acc 53.1250 (66.1250) lr 1.8763e-03 eta 0:24:24
epoch [6/25] batch [5/250] time 0.289 (0.432) data 0.000 (0.142) loss 0.9902 (1.1028) acc 71.8750 (68.7500) lr 1.8763e-03 eta 0:35:55
epoch [6/25] batch [10/250] time 0.290 (0.361) data 0.000 (0.071) loss 1.3340 (1.2195) acc 68.7500 (68.7500) lr 1.8763e-03 eta 0:29:59
epoch [6/25] batch [15/250] time 0.290 (0.337) data 0.000 (0.048) loss 0.9980 (1.1973) acc 78.1250 (68.3333) lr 1.8763e-03 eta 0:27:59
epoch [6/25] batch [20/250] time 0.290 (0.325) data 0.000 (0.036) loss 1.8418 (1.2709) acc 50.0000 (66.8750) lr 1.8763e-03 eta 0:26:59
epoch [6/25] batch [25/250] time 0.290 (0.318) data 0.000 (0.029) loss 1.1406 (1.2973) acc 65.6250 (65.2500) lr 1.8763e-03 eta 0:26:22
epoch [6/25] batch [30/250] time 0.290 (0.313) data 0.000 (0.024) loss 1.1885 (1.3140) acc 75.0000 (65.4167) lr 1.8763e-03 eta 0:25:57
epoch [6/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.4072 (1.3194) acc 59.3750 (64.7321) lr 1.8763e-03 eta 0:25:39
epoch [6/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.018) loss 1.2627 (1.3260) acc 68.7500 (65.0000) lr 1.8763e-03 eta 0:25:25
epoch [6/25] batch [45/250] time 0.290 (0.306) data 0.000 (0.016) loss 1.4551 (1.3213) acc 62.5000 (65.5556) lr 1.8763e-03 eta 0:25:14
epoch [6/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.014) loss 1.0078 (1.3215) acc 75.0000 (65.8750) lr 1.8763e-03 eta 0:25:04
epoch [6/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.013) loss 1.4854 (1.3261) acc 65.6250 (66.0795) lr 1.8763e-03 eta 0:24:56
epoch [6/25] batch [60/250] time 0.290 (0.302) data 0.000 (0.012) loss 1.6025 (1.3408) acc 68.7500 (65.9896) lr 1.8763e-03 eta 0:24:49
epoch [6/25] batch [65/250] time 0.291 (0.301) data 0.000 (0.011) loss 1.2334 (1.3343) acc 71.8750 (66.2019) lr 1.8763e-03 eta 0:24:44
epoch [6/25] batch [70/250] time 0.290 (0.300) data 0.000 (0.010) loss 0.9414 (1.3281) acc 71.8750 (66.3393) lr 1.8763e-03 eta 0:24:38
epoch [6/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.7852 (1.3462) acc 62.5000 (66.0417) lr 1.8763e-03 eta 0:24:33
epoch [6/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.0566 (1.3448) acc 78.1250 (66.1328) lr 1.8763e-03 eta 0:24:29
epoch [6/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.4600 (1.3456) acc 68.7500 (66.2500) lr 1.8763e-03 eta 0:24:24
epoch [6/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.1426 (1.3437) acc 71.8750 (66.4236) lr 1.8763e-03 eta 0:24:21
epoch [6/25] batch [95/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.4004 (1.3450) acc 62.5000 (66.1184) lr 1.8763e-03 eta 0:24:17
epoch [6/25] batch [100/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.2041 (1.3436) acc 71.8750 (66.2812) lr 1.8763e-03 eta 0:24:14
epoch [6/25] batch [105/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.8105 (1.3357) acc 75.0000 (66.4583) lr 1.8763e-03 eta 0:24:11
epoch [6/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9878 (1.3429) acc 75.0000 (66.3920) lr 1.8763e-03 eta 0:24:08
epoch [6/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.006) loss 2.1855 (1.3451) acc 43.7500 (66.3587) lr 1.8763e-03 eta 0:24:05
epoch [6/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.3301 (1.3540) acc 62.5000 (66.1979) lr 1.8763e-03 eta 0:24:02
epoch [6/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6074 (1.3550) acc 56.2500 (66.2000) lr 1.8763e-03 eta 0:23:59
epoch [6/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6377 (1.3563) acc 56.2500 (66.0096) lr 1.8763e-03 eta 0:23:57
epoch [6/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2959 (1.3667) acc 65.6250 (65.7407) lr 1.8763e-03 eta 0:23:54
epoch [6/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0986 (1.3623) acc 65.6250 (65.6920) lr 1.8763e-03 eta 0:23:52
epoch [6/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.0068 (1.3547) acc 68.7500 (65.8836) lr 1.8763e-03 eta 0:23:50
epoch [6/25] batch [150/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.1289 (1.3507) acc 65.6250 (65.8750) lr 1.8763e-03 eta 0:23:48
epoch [6/25] batch [155/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.1982 (1.3576) acc 65.6250 (65.7460) lr 1.8763e-03 eta 0:23:46
epoch [6/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.6670 (1.3498) acc 81.2500 (65.8008) lr 1.8763e-03 eta 0:23:43
epoch [6/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.8008 (1.3471) acc 56.2500 (65.8712) lr 1.8763e-03 eta 0:23:42
epoch [6/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.004) loss 2.1660 (1.3481) acc 62.5000 (65.9559) lr 1.8763e-03 eta 0:23:39
epoch [6/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.6343 (1.3395) acc 87.5000 (66.1250) lr 1.8763e-03 eta 0:23:37
epoch [6/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.4778 (1.3295) acc 87.5000 (66.3021) lr 1.8763e-03 eta 0:23:35
epoch [6/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.004) loss 2.3066 (1.3335) acc 50.0000 (66.2669) lr 1.8763e-03 eta 0:23:33
epoch [6/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.7861 (1.3335) acc 59.3750 (66.2664) lr 1.8763e-03 eta 0:23:31
epoch [6/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2188 (1.3308) acc 65.6250 (66.3462) lr 1.8763e-03 eta 0:23:29
epoch [6/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2285 (1.3330) acc 68.7500 (66.2500) lr 1.8763e-03 eta 0:23:27
epoch [6/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4385 (1.3266) acc 53.1250 (66.3415) lr 1.8763e-03 eta 0:23:25
epoch [6/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.9023 (1.3266) acc 56.2500 (66.3095) lr 1.8763e-03 eta 0:23:23
epoch [6/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3027 (1.3271) acc 75.0000 (66.2500) lr 1.8763e-03 eta 0:23:22
epoch [6/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6875 (1.3265) acc 59.3750 (66.2642) lr 1.8763e-03 eta 0:23:20
epoch [6/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.5439 (1.3225) acc 65.6250 (66.3333) lr 1.8763e-03 eta 0:23:18
epoch [6/25] batch [230/250] time 0.291 (0.293) data 0.000 (0.003) loss 1.4199 (1.3267) acc 71.8750 (66.3315) lr 1.8763e-03 eta 0:23:16
epoch [6/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.6631 (1.3321) acc 68.7500 (66.3165) lr 1.8763e-03 eta 0:23:15
epoch [6/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.2705 (1.3339) acc 68.7500 (66.3281) lr 1.8763e-03 eta 0:23:13
epoch [6/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.003) loss 2.3652 (1.3376) acc 40.6250 (66.2117) lr 1.8763e-03 eta 0:23:11
epoch [6/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.2510 (1.3398) acc 62.5000 (66.0500) lr 1.8090e-03 eta 0:23:09
epoch [7/25] batch [5/250] time 0.290 (0.444) data 0.000 (0.154) loss 1.8848 (1.4083) acc 50.0000 (64.3750) lr 1.8090e-03 eta 0:35:08
epoch [7/25] batch [10/250] time 0.289 (0.367) data 0.000 (0.077) loss 0.9512 (1.4152) acc 75.0000 (65.9375) lr 1.8090e-03 eta 0:28:59
epoch [7/25] batch [15/250] time 0.289 (0.341) data 0.000 (0.052) loss 0.8345 (1.3862) acc 78.1250 (67.0833) lr 1.8090e-03 eta 0:26:54
epoch [7/25] batch [20/250] time 0.289 (0.328) data 0.000 (0.039) loss 1.1738 (1.4032) acc 65.6250 (65.6250) lr 1.8090e-03 eta 0:25:51
epoch [7/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 1.2256 (1.4158) acc 68.7500 (65.2500) lr 1.8090e-03 eta 0:25:12
epoch [7/25] batch [30/250] time 0.289 (0.315) data 0.000 (0.026) loss 1.3057 (1.3848) acc 59.3750 (65.2083) lr 1.8090e-03 eta 0:24:46
epoch [7/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.2695 (1.3615) acc 59.3750 (65.8929) lr 1.8090e-03 eta 0:24:27
epoch [7/25] batch [40/250] time 0.289 (0.309) data 0.000 (0.020) loss 0.9009 (1.3568) acc 78.1250 (66.2500) lr 1.8090e-03 eta 0:24:13
epoch [7/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.2148 (1.3792) acc 62.5000 (65.8333) lr 1.8090e-03 eta 0:24:01
epoch [7/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.5723 (1.3922) acc 59.3750 (65.5000) lr 1.8090e-03 eta 0:23:52
epoch [7/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.0137 (1.3901) acc 75.0000 (65.5114) lr 1.8090e-03 eta 0:23:44
epoch [7/25] batch [60/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.7266 (1.3829) acc 65.6250 (65.8854) lr 1.8090e-03 eta 0:23:37
epoch [7/25] batch [65/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.3174 (1.3834) acc 68.7500 (66.0096) lr 1.8090e-03 eta 0:23:31
epoch [7/25] batch [70/250] time 0.291 (0.301) data 0.000 (0.011) loss 1.5586 (1.3739) acc 62.5000 (66.1161) lr 1.8090e-03 eta 0:23:26
epoch [7/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.5000 (1.3612) acc 62.5000 (66.5000) lr 1.8090e-03 eta 0:23:21
epoch [7/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.0098 (1.3669) acc 75.0000 (66.3281) lr 1.8090e-03 eta 0:23:17
epoch [7/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.7959 (1.3786) acc 62.5000 (66.1397) lr 1.8090e-03 eta 0:23:12
epoch [7/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6416 (1.3874) acc 65.6250 (65.8333) lr 1.8090e-03 eta 0:23:09
epoch [7/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.008) loss 2.0684 (1.3995) acc 59.3750 (65.7237) lr 1.8090e-03 eta 0:23:05
epoch [7/25] batch [100/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.6172 (1.3898) acc 62.5000 (65.9062) lr 1.8090e-03 eta 0:23:02
epoch [7/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.1338 (1.3850) acc 65.6250 (65.8929) lr 1.8090e-03 eta 0:22:59
epoch [7/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.1025 (1.3734) acc 71.8750 (66.1080) lr 1.8090e-03 eta 0:22:56
epoch [7/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8589 (1.3749) acc 65.6250 (66.1685) lr 1.8090e-03 eta 0:22:53
epoch [7/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9082 (1.3781) acc 75.0000 (66.1458) lr 1.8090e-03 eta 0:22:50
epoch [7/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.8721 (1.3743) acc 75.0000 (66.2250) lr 1.8090e-03 eta 0:22:47
epoch [7/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4326 (1.3726) acc 68.7500 (66.1298) lr 1.8090e-03 eta 0:22:44
epoch [7/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7158 (1.3825) acc 56.2500 (65.7407) lr 1.8090e-03 eta 0:22:42
epoch [7/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8301 (1.3798) acc 59.3750 (65.7143) lr 1.8090e-03 eta 0:22:39
epoch [7/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4355 (1.3719) acc 56.2500 (65.7759) lr 1.8090e-03 eta 0:22:37
epoch [7/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.6641 (1.3691) acc 71.8750 (66.0417) lr 1.8090e-03 eta 0:22:35
epoch [7/25] batch [155/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2354 (1.3671) acc 65.6250 (66.0484) lr 1.8090e-03 eta 0:22:33
epoch [7/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3672 (1.3651) acc 62.5000 (66.1133) lr 1.8090e-03 eta 0:22:30
epoch [7/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.9697 (1.3603) acc 71.8750 (66.1553) lr 1.8090e-03 eta 0:22:28
epoch [7/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4619 (1.3705) acc 65.6250 (65.9007) lr 1.8090e-03 eta 0:22:26
epoch [7/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3926 (1.3710) acc 68.7500 (66.0714) lr 1.8090e-03 eta 0:22:24
epoch [7/25] batch [180/250] time 0.292 (0.294) data 0.000 (0.005) loss 1.1650 (1.3729) acc 81.2500 (66.0417) lr 1.8090e-03 eta 0:22:22
epoch [7/25] batch [185/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.2441 (1.3751) acc 71.8750 (65.9122) lr 1.8090e-03 eta 0:22:20
epoch [7/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.4648 (1.3732) acc 62.5000 (65.9868) lr 1.8090e-03 eta 0:22:19
epoch [7/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.6602 (1.3733) acc 62.5000 (66.0256) lr 1.8090e-03 eta 0:22:17
epoch [7/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3721 (1.3733) acc 71.8750 (66.0469) lr 1.8090e-03 eta 0:22:15
epoch [7/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7314 (1.3731) acc 59.3750 (66.0366) lr 1.8090e-03 eta 0:22:13
epoch [7/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.9062 (1.3725) acc 50.0000 (65.9226) lr 1.8090e-03 eta 0:22:11
epoch [7/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4561 (1.3733) acc 65.6250 (65.9302) lr 1.8090e-03 eta 0:22:09
epoch [7/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1895 (1.3685) acc 78.1250 (66.0795) lr 1.8090e-03 eta 0:22:07
epoch [7/25] batch [225/250] time 0.292 (0.293) data 0.000 (0.004) loss 1.7041 (1.3702) acc 65.6250 (66.0833) lr 1.8090e-03 eta 0:22:06
epoch [7/25] batch [230/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.4375 (1.3697) acc 68.7500 (66.1005) lr 1.8090e-03 eta 0:22:04
epoch [7/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1826 (1.3699) acc 62.5000 (66.0372) lr 1.8090e-03 eta 0:22:02
epoch [7/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.1338 (1.3673) acc 68.7500 (66.0938) lr 1.8090e-03 eta 0:22:00
epoch [7/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 0.6162 (1.3635) acc 87.5000 (66.1735) lr 1.8090e-03 eta 0:21:58
epoch [7/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.2803 (1.3638) acc 68.7500 (66.1500) lr 1.7290e-03 eta 0:21:57
epoch [8/25] batch [5/250] time 0.290 (0.431) data 0.000 (0.140) loss 1.3496 (1.4895) acc 53.1250 (63.1250) lr 1.7290e-03 eta 0:32:17
epoch [8/25] batch [10/250] time 0.290 (0.361) data 0.000 (0.070) loss 1.1777 (1.4235) acc 62.5000 (63.7500) lr 1.7290e-03 eta 0:27:00
epoch [8/25] batch [15/250] time 0.290 (0.337) data 0.000 (0.047) loss 1.2480 (1.3625) acc 71.8750 (65.2083) lr 1.7290e-03 eta 0:25:12
epoch [8/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.035) loss 1.4004 (1.3578) acc 68.7500 (66.8750) lr 1.7290e-03 eta 0:24:17
epoch [8/25] batch [25/250] time 0.290 (0.318) data 0.001 (0.028) loss 2.1602 (1.3385) acc 56.2500 (67.1250) lr 1.7290e-03 eta 0:23:44
epoch [8/25] batch [30/250] time 0.291 (0.313) data 0.000 (0.024) loss 1.1416 (1.3029) acc 59.3750 (67.1875) lr 1.7290e-03 eta 0:23:21
epoch [8/25] batch [35/250] time 0.290 (0.310) data 0.000 (0.020) loss 0.9824 (1.2954) acc 81.2500 (68.1250) lr 1.7290e-03 eta 0:23:04
epoch [8/25] batch [40/250] time 0.291 (0.308) data 0.000 (0.018) loss 2.0332 (1.2939) acc 53.1250 (67.6562) lr 1.7290e-03 eta 0:22:52
epoch [8/25] batch [45/250] time 0.290 (0.306) data 0.000 (0.016) loss 1.1387 (1.2787) acc 81.2500 (68.1944) lr 1.7290e-03 eta 0:22:41
epoch [8/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.014) loss 1.3457 (1.2662) acc 65.6250 (68.7500) lr 1.7290e-03 eta 0:22:33
epoch [8/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.013) loss 1.0342 (1.2774) acc 78.1250 (68.6932) lr 1.7290e-03 eta 0:22:25
epoch [8/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.012) loss 0.8901 (1.2641) acc 75.0000 (68.7500) lr 1.7290e-03 eta 0:22:19
epoch [8/25] batch [65/250] time 0.288 (0.301) data 0.000 (0.011) loss 0.9829 (1.2544) acc 75.0000 (69.0385) lr 1.7290e-03 eta 0:22:13
epoch [8/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.3545 (1.2508) acc 59.3750 (69.0179) lr 1.7290e-03 eta 0:22:08
epoch [8/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.1846 (1.2527) acc 65.6250 (68.9583) lr 1.7290e-03 eta 0:22:03
epoch [8/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.009) loss 0.6592 (1.2393) acc 84.3750 (69.3359) lr 1.7290e-03 eta 0:21:59
epoch [8/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.4639 (1.2439) acc 56.2500 (69.1176) lr 1.7290e-03 eta 0:21:55
epoch [8/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.5176 (1.2589) acc 56.2500 (68.4722) lr 1.7290e-03 eta 0:21:51
epoch [8/25] batch [95/250] time 0.291 (0.297) data 0.000 (0.008) loss 1.3320 (1.2573) acc 59.3750 (68.3553) lr 1.7290e-03 eta 0:21:48
epoch [8/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.0928 (1.2609) acc 81.2500 (68.5000) lr 1.7290e-03 eta 0:21:45
epoch [8/25] batch [105/250] time 0.290 (0.296) data 0.000 (0.007) loss 2.0391 (1.2696) acc 53.1250 (68.3036) lr 1.7290e-03 eta 0:21:42
epoch [8/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4121 (1.2704) acc 68.7500 (68.0682) lr 1.7290e-03 eta 0:21:39
epoch [8/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.7129 (1.2717) acc 56.2500 (67.9348) lr 1.7290e-03 eta 0:21:36
epoch [8/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.7910 (1.2658) acc 81.2500 (68.1771) lr 1.7290e-03 eta 0:21:34
epoch [8/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3955 (1.2706) acc 59.3750 (68.0500) lr 1.7290e-03 eta 0:21:31
epoch [8/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3477 (1.2769) acc 68.7500 (67.9808) lr 1.7290e-03 eta 0:21:29
epoch [8/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1230 (1.2754) acc 75.0000 (68.0556) lr 1.7290e-03 eta 0:21:26
epoch [8/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0635 (1.2664) acc 71.8750 (68.1473) lr 1.7290e-03 eta 0:21:24
epoch [8/25] batch [145/250] time 0.288 (0.294) data 0.000 (0.005) loss 1.2441 (1.2664) acc 68.7500 (68.1466) lr 1.7290e-03 eta 0:21:21
epoch [8/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3555 (1.2704) acc 65.6250 (67.8750) lr 1.7290e-03 eta 0:21:19
epoch [8/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3740 (1.2698) acc 75.0000 (67.9637) lr 1.7290e-03 eta 0:21:17
epoch [8/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2500 (1.2738) acc 68.7500 (67.8711) lr 1.7290e-03 eta 0:21:15
epoch [8/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3096 (1.2787) acc 56.2500 (67.5947) lr 1.7290e-03 eta 0:21:13
epoch [8/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.5898 (1.2835) acc 62.5000 (67.4816) lr 1.7290e-03 eta 0:21:11
epoch [8/25] batch [175/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.5312 (1.2861) acc 71.8750 (67.6071) lr 1.7290e-03 eta 0:21:09
epoch [8/25] batch [180/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3213 (1.2899) acc 62.5000 (67.5347) lr 1.7290e-03 eta 0:21:07
epoch [8/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2227 (1.2866) acc 68.7500 (67.6182) lr 1.7290e-03 eta 0:21:05
epoch [8/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6064 (1.2833) acc 56.2500 (67.6316) lr 1.7290e-03 eta 0:21:03
epoch [8/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3340 (1.2802) acc 71.8750 (67.7564) lr 1.7290e-03 eta 0:21:01
epoch [8/25] batch [200/250] time 0.292 (0.293) data 0.000 (0.004) loss 1.9258 (1.2802) acc 62.5000 (67.6875) lr 1.7290e-03 eta 0:21:00
epoch [8/25] batch [205/250] time 0.291 (0.293) data 0.000 (0.004) loss 0.9731 (1.2732) acc 71.8750 (67.8354) lr 1.7290e-03 eta 0:20:58
epoch [8/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2744 (1.2776) acc 65.6250 (67.6488) lr 1.7290e-03 eta 0:20:56
epoch [8/25] batch [215/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.4365 (1.2775) acc 68.7500 (67.6890) lr 1.7290e-03 eta 0:20:54
epoch [8/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.1562 (1.2789) acc 68.7500 (67.6136) lr 1.7290e-03 eta 0:20:53
epoch [8/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.4141 (1.2841) acc 65.6250 (67.4722) lr 1.7290e-03 eta 0:20:51
epoch [8/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.1006 (1.2832) acc 71.8750 (67.4185) lr 1.7290e-03 eta 0:20:49
epoch [8/25] batch [235/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.4209 (1.2817) acc 65.6250 (67.4734) lr 1.7290e-03 eta 0:20:47
epoch [8/25] batch [240/250] time 0.291 (0.293) data 0.000 (0.003) loss 1.4365 (1.2829) acc 78.1250 (67.4479) lr 1.7290e-03 eta 0:20:46
epoch [8/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.0723 (1.2844) acc 78.1250 (67.3724) lr 1.7290e-03 eta 0:20:44
epoch [8/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.5605 (1.2859) acc 65.6250 (67.3875) lr 1.6374e-03 eta 0:20:42
epoch [9/25] batch [5/250] time 0.290 (0.431) data 0.000 (0.141) loss 1.0469 (1.2523) acc 81.2500 (70.6250) lr 1.6374e-03 eta 0:30:28
epoch [9/25] batch [10/250] time 0.290 (0.360) data 0.000 (0.071) loss 0.7803 (1.1122) acc 78.1250 (70.3125) lr 1.6374e-03 eta 0:25:27
epoch [9/25] batch [15/250] time 0.290 (0.337) data 0.000 (0.047) loss 1.2148 (1.0969) acc 68.7500 (70.8333) lr 1.6374e-03 eta 0:23:45
epoch [9/25] batch [20/250] time 0.290 (0.325) data 0.000 (0.036) loss 0.9312 (1.1176) acc 75.0000 (70.7812) lr 1.6374e-03 eta 0:22:54
epoch [9/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.028) loss 1.3301 (1.1758) acc 62.5000 (69.6250) lr 1.6374e-03 eta 0:22:22
epoch [9/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.8691 (1.2022) acc 84.3750 (69.4792) lr 1.6374e-03 eta 0:22:00
epoch [9/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.020) loss 1.4570 (1.2034) acc 62.5000 (69.1071) lr 1.6374e-03 eta 0:21:44
epoch [9/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.8193 (1.2093) acc 62.5000 (69.5312) lr 1.6374e-03 eta 0:21:32
epoch [9/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.8911 (1.2225) acc 84.3750 (68.9583) lr 1.6374e-03 eta 0:21:22
epoch [9/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.0947 (1.2068) acc 71.8750 (69.1875) lr 1.6374e-03 eta 0:21:14
epoch [9/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.2842 (1.2219) acc 68.7500 (69.2614) lr 1.6374e-03 eta 0:21:07
epoch [9/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.5186 (1.2335) acc 56.2500 (68.8021) lr 1.6374e-03 eta 0:21:01
epoch [9/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2334 (1.2353) acc 65.6250 (68.6538) lr 1.6374e-03 eta 0:20:56
epoch [9/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.8911 (1.2269) acc 84.3750 (69.0179) lr 1.6374e-03 eta 0:20:51
epoch [9/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3477 (1.2340) acc 62.5000 (68.9583) lr 1.6374e-03 eta 0:20:47
epoch [9/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7275 (1.2500) acc 53.1250 (68.3203) lr 1.6374e-03 eta 0:20:43
epoch [9/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.3721 (1.2623) acc 68.7500 (68.1250) lr 1.6374e-03 eta 0:20:39
epoch [9/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5957 (1.2700) acc 56.2500 (67.7778) lr 1.6374e-03 eta 0:20:36
epoch [9/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.8223 (1.2867) acc 62.5000 (67.5987) lr 1.6374e-03 eta 0:20:33
epoch [9/25] batch [100/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.1875 (1.2901) acc 65.6250 (67.6562) lr 1.6374e-03 eta 0:20:30
epoch [9/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8467 (1.2834) acc 81.2500 (67.9762) lr 1.6374e-03 eta 0:20:27
epoch [9/25] batch [110/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.5693 (1.2848) acc 78.1250 (67.9545) lr 1.6374e-03 eta 0:20:24
epoch [9/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.5420 (1.2845) acc 65.6250 (67.7446) lr 1.6374e-03 eta 0:20:22
epoch [9/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2783 (1.2921) acc 75.0000 (67.5521) lr 1.6374e-03 eta 0:20:19
epoch [9/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.9492 (1.3022) acc 62.5000 (67.4500) lr 1.6374e-03 eta 0:20:17
epoch [9/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.7412 (1.2995) acc 53.1250 (67.4279) lr 1.6374e-03 eta 0:20:14
epoch [9/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9771 (1.3008) acc 78.1250 (67.5694) lr 1.6374e-03 eta 0:20:12
epoch [9/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3330 (1.3097) acc 75.0000 (67.5000) lr 1.6374e-03 eta 0:20:10
epoch [9/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2646 (1.3070) acc 68.7500 (67.4353) lr 1.6374e-03 eta 0:20:08
epoch [9/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9590 (1.3010) acc 65.6250 (67.5417) lr 1.6374e-03 eta 0:20:05
epoch [9/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5879 (1.3056) acc 68.7500 (67.3992) lr 1.6374e-03 eta 0:20:03
epoch [9/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2148 (1.2992) acc 53.1250 (67.4609) lr 1.6374e-03 eta 0:20:01
epoch [9/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.3076 (1.2969) acc 62.5000 (67.5379) lr 1.6374e-03 eta 0:19:59
epoch [9/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.6675 (1.2930) acc 78.1250 (67.4449) lr 1.6374e-03 eta 0:19:57
epoch [9/25] batch [175/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9229 (1.2950) acc 78.1250 (67.4464) lr 1.6374e-03 eta 0:19:55
epoch [9/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7285 (1.3029) acc 40.6250 (67.2049) lr 1.6374e-03 eta 0:19:53
epoch [9/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0938 (1.3020) acc 78.1250 (67.2635) lr 1.6374e-03 eta 0:19:51
epoch [9/25] batch [190/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9648 (1.2967) acc 75.0000 (67.4507) lr 1.6374e-03 eta 0:19:50
epoch [9/25] batch [195/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.4873 (1.2926) acc 65.6250 (67.4840) lr 1.6374e-03 eta 0:19:48
epoch [9/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3594 (1.2931) acc 59.3750 (67.5000) lr 1.6374e-03 eta 0:19:46
epoch [9/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2383 (1.2949) acc 62.5000 (67.4390) lr 1.6374e-03 eta 0:19:44
epoch [9/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8726 (1.2949) acc 84.3750 (67.5298) lr 1.6374e-03 eta 0:19:42
epoch [9/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3555 (1.2944) acc 71.8750 (67.4855) lr 1.6374e-03 eta 0:19:41
epoch [9/25] batch [220/250] time 0.288 (0.293) data 0.000 (0.004) loss 1.3281 (1.2944) acc 46.8750 (67.4290) lr 1.6374e-03 eta 0:19:39
epoch [9/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.003) loss 0.7036 (1.2966) acc 78.1250 (67.3472) lr 1.6374e-03 eta 0:19:37
epoch [9/25] batch [230/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.1377 (1.3001) acc 71.8750 (67.2418) lr 1.6374e-03 eta 0:19:35
epoch [9/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4248 (1.2995) acc 56.2500 (67.1410) lr 1.6374e-03 eta 0:19:34
epoch [9/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.8203 (1.3020) acc 53.1250 (67.1094) lr 1.6374e-03 eta 0:19:32
epoch [9/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6875 (1.3132) acc 53.1250 (66.8367) lr 1.6374e-03 eta 0:19:30
epoch [9/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.3916 (1.3138) acc 59.3750 (66.7750) lr 1.5358e-03 eta 0:19:28
epoch [10/25] batch [5/250] time 0.290 (0.463) data 0.001 (0.173) loss 1.8242 (1.4592) acc 53.1250 (63.7500) lr 1.5358e-03 eta 0:30:48
epoch [10/25] batch [10/250] time 0.289 (0.376) data 0.000 (0.087) loss 1.1943 (1.5093) acc 75.0000 (64.0625) lr 1.5358e-03 eta 0:24:59
epoch [10/25] batch [15/250] time 0.289 (0.347) data 0.000 (0.058) loss 1.3594 (1.4878) acc 65.6250 (64.1667) lr 1.5358e-03 eta 0:23:02
epoch [10/25] batch [20/250] time 0.289 (0.332) data 0.000 (0.044) loss 1.3643 (1.4237) acc 84.3750 (67.0312) lr 1.5358e-03 eta 0:22:03
epoch [10/25] batch [25/250] time 0.289 (0.324) data 0.000 (0.035) loss 1.5605 (1.3727) acc 71.8750 (68.1250) lr 1.5358e-03 eta 0:21:26
epoch [10/25] batch [30/250] time 0.290 (0.318) data 0.000 (0.029) loss 1.2012 (1.3587) acc 68.7500 (68.6458) lr 1.5358e-03 eta 0:21:02
epoch [10/25] batch [35/250] time 0.289 (0.314) data 0.000 (0.025) loss 1.5088 (1.3496) acc 75.0000 (68.4821) lr 1.5358e-03 eta 0:20:44
epoch [10/25] batch [40/250] time 0.289 (0.311) data 0.000 (0.022) loss 0.9556 (1.3350) acc 78.1250 (68.5938) lr 1.5358e-03 eta 0:20:30
epoch [10/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.020) loss 0.9531 (1.3246) acc 68.7500 (68.7500) lr 1.5358e-03 eta 0:20:19
epoch [10/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.018) loss 0.8237 (1.3094) acc 71.8750 (68.8750) lr 1.5358e-03 eta 0:20:10
epoch [10/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.9941 (1.3065) acc 71.8750 (68.9773) lr 1.5358e-03 eta 0:20:02
epoch [10/25] batch [60/250] time 0.289 (0.303) data 0.000 (0.015) loss 1.4307 (1.3180) acc 65.6250 (68.3854) lr 1.5358e-03 eta 0:19:55
epoch [10/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.014) loss 1.0684 (1.3037) acc 78.1250 (68.8462) lr 1.5358e-03 eta 0:19:49
epoch [10/25] batch [70/250] time 0.291 (0.302) data 0.000 (0.013) loss 1.8730 (1.3232) acc 62.5000 (68.3929) lr 1.5358e-03 eta 0:19:45
epoch [10/25] batch [75/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.7910 (1.3344) acc 62.5000 (68.0833) lr 1.5358e-03 eta 0:19:40
epoch [10/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.9253 (1.3324) acc 81.2500 (68.2422) lr 1.5358e-03 eta 0:19:36
epoch [10/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.011) loss 1.4795 (1.3418) acc 62.5000 (67.7941) lr 1.5358e-03 eta 0:19:32
epoch [10/25] batch [90/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.0410 (1.3445) acc 62.5000 (67.7083) lr 1.5358e-03 eta 0:19:28
epoch [10/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.0938 (1.3330) acc 62.5000 (67.6316) lr 1.5358e-03 eta 0:19:25
epoch [10/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0000 (1.3278) acc 68.7500 (67.6562) lr 1.5358e-03 eta 0:19:22
epoch [10/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.8228 (1.3272) acc 78.1250 (67.5595) lr 1.5358e-03 eta 0:19:18
epoch [10/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8862 (1.3213) acc 68.7500 (67.6136) lr 1.5358e-03 eta 0:19:15
epoch [10/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0742 (1.3177) acc 75.0000 (67.6359) lr 1.5358e-03 eta 0:19:13
epoch [10/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.008) loss 0.8115 (1.3158) acc 71.8750 (67.5781) lr 1.5358e-03 eta 0:19:10
epoch [10/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5137 (1.3193) acc 65.6250 (67.5250) lr 1.5358e-03 eta 0:19:07
epoch [10/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4512 (1.3188) acc 68.7500 (67.5481) lr 1.5358e-03 eta 0:19:05
epoch [10/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4209 (1.3158) acc 65.6250 (67.5694) lr 1.5358e-03 eta 0:19:02
epoch [10/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.3984 (1.3139) acc 62.5000 (67.5000) lr 1.5358e-03 eta 0:19:00
epoch [10/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6992 (1.3181) acc 62.5000 (67.2629) lr 1.5358e-03 eta 0:18:58
epoch [10/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0527 (1.3165) acc 65.6250 (67.1667) lr 1.5358e-03 eta 0:18:55
epoch [10/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8696 (1.3193) acc 81.2500 (67.1774) lr 1.5358e-03 eta 0:18:53
epoch [10/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5391 (1.3253) acc 71.8750 (67.1484) lr 1.5358e-03 eta 0:18:51
epoch [10/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3438 (1.3200) acc 65.6250 (67.3864) lr 1.5358e-03 eta 0:18:49
epoch [10/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8584 (1.3185) acc 62.5000 (67.3713) lr 1.5358e-03 eta 0:18:47
epoch [10/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.6055 (1.3235) acc 62.5000 (67.2679) lr 1.5358e-03 eta 0:18:45
epoch [10/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4756 (1.3255) acc 68.7500 (67.3264) lr 1.5358e-03 eta 0:18:43
epoch [10/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2637 (1.3257) acc 71.8750 (67.3311) lr 1.5358e-03 eta 0:18:41
epoch [10/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.5957 (1.3301) acc 65.6250 (67.2204) lr 1.5358e-03 eta 0:18:39
epoch [10/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5498 (1.3342) acc 68.7500 (67.0994) lr 1.5358e-03 eta 0:18:37
epoch [10/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3750 (1.3422) acc 65.6250 (67.0000) lr 1.5358e-03 eta 0:18:35
epoch [10/25] batch [205/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.2207 (1.3442) acc 65.6250 (66.9360) lr 1.5358e-03 eta 0:18:33
epoch [10/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2734 (1.3446) acc 68.7500 (66.9196) lr 1.5358e-03 eta 0:18:32
epoch [10/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7969 (1.3486) acc 56.2500 (66.8459) lr 1.5358e-03 eta 0:18:30
epoch [10/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6143 (1.3493) acc 59.3750 (66.8466) lr 1.5358e-03 eta 0:18:28
epoch [10/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2051 (1.3485) acc 65.6250 (66.8750) lr 1.5358e-03 eta 0:18:26
epoch [10/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4756 (1.3455) acc 75.0000 (66.9565) lr 1.5358e-03 eta 0:18:24
epoch [10/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.1816 (1.3427) acc 71.8750 (67.0479) lr 1.5358e-03 eta 0:18:23
epoch [10/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2969 (1.3448) acc 62.5000 (67.1094) lr 1.5358e-03 eta 0:18:21
epoch [10/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3389 (1.3473) acc 75.0000 (67.1173) lr 1.5358e-03 eta 0:18:19
epoch [10/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5820 (1.3514) acc 56.2500 (67.0125) lr 1.4258e-03 eta 0:18:17
epoch [11/25] batch [5/250] time 0.289 (0.432) data 0.000 (0.142) loss 1.0820 (1.1710) acc 68.7500 (72.5000) lr 1.4258e-03 eta 0:26:56
epoch [11/25] batch [10/250] time 0.289 (0.360) data 0.000 (0.071) loss 1.1230 (1.1479) acc 71.8750 (72.1875) lr 1.4258e-03 eta 0:22:28
epoch [11/25] batch [15/250] time 0.291 (0.337) data 0.000 (0.047) loss 1.0283 (1.1625) acc 68.7500 (70.8333) lr 1.4258e-03 eta 0:20:57
epoch [11/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.036) loss 1.0117 (1.1798) acc 75.0000 (70.4688) lr 1.4258e-03 eta 0:20:11
epoch [11/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.8213 (1.2533) acc 59.3750 (68.7500) lr 1.4258e-03 eta 0:19:43
epoch [11/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.4785 (1.2743) acc 65.6250 (68.5417) lr 1.4258e-03 eta 0:19:24
epoch [11/25] batch [35/250] time 0.290 (0.310) data 0.000 (0.020) loss 0.9756 (1.2544) acc 75.0000 (68.4821) lr 1.4258e-03 eta 0:19:10
epoch [11/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.0371 (1.2619) acc 68.7500 (68.3594) lr 1.4258e-03 eta 0:18:59
epoch [11/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.3223 (1.2740) acc 62.5000 (68.1250) lr 1.4258e-03 eta 0:18:50
epoch [11/25] batch [50/250] time 0.289 (0.303) data 0.000 (0.014) loss 1.1689 (1.2658) acc 75.0000 (68.4375) lr 1.4258e-03 eta 0:18:42
epoch [11/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.9941 (1.2626) acc 81.2500 (68.7500) lr 1.4258e-03 eta 0:18:36
epoch [11/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3213 (1.2528) acc 59.3750 (68.5417) lr 1.4258e-03 eta 0:18:31
epoch [11/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.0898 (1.2580) acc 75.0000 (68.7981) lr 1.4258e-03 eta 0:18:26
epoch [11/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3447 (1.2670) acc 68.7500 (68.5268) lr 1.4258e-03 eta 0:18:21
epoch [11/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4307 (1.2851) acc 71.8750 (67.9583) lr 1.4258e-03 eta 0:18:17
epoch [11/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1094 (1.2969) acc 75.0000 (67.7734) lr 1.4258e-03 eta 0:18:14
epoch [11/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.1982 (1.3045) acc 71.8750 (67.5368) lr 1.4258e-03 eta 0:18:10
epoch [11/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9824 (1.3003) acc 75.0000 (67.5694) lr 1.4258e-03 eta 0:18:07
epoch [11/25] batch [95/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.7070 (1.3133) acc 62.5000 (67.5329) lr 1.4258e-03 eta 0:18:04
epoch [11/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.8535 (1.3020) acc 56.2500 (67.7500) lr 1.4258e-03 eta 0:18:01
epoch [11/25] batch [105/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.5908 (1.2988) acc 56.2500 (67.7976) lr 1.4258e-03 eta 0:17:58
epoch [11/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8687 (1.3002) acc 75.0000 (67.8977) lr 1.4258e-03 eta 0:17:56
epoch [11/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3418 (1.2888) acc 65.6250 (68.2065) lr 1.4258e-03 eta 0:17:53
epoch [11/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6152 (1.2903) acc 50.0000 (68.0729) lr 1.4258e-03 eta 0:17:51
epoch [11/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.8887 (1.2955) acc 56.2500 (67.9750) lr 1.4258e-03 eta 0:17:49
epoch [11/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9692 (1.2902) acc 65.6250 (68.0048) lr 1.4258e-03 eta 0:17:46
epoch [11/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.2832 (1.2851) acc 59.3750 (67.9630) lr 1.4258e-03 eta 0:17:44
epoch [11/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2227 (1.2861) acc 65.6250 (67.8125) lr 1.4258e-03 eta 0:17:42
epoch [11/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5498 (1.2871) acc 62.5000 (67.7802) lr 1.4258e-03 eta 0:17:40
epoch [11/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5596 (1.2944) acc 62.5000 (67.7083) lr 1.4258e-03 eta 0:17:38
epoch [11/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7607 (1.2932) acc 62.5000 (67.6815) lr 1.4258e-03 eta 0:17:36
epoch [11/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6084 (1.2892) acc 59.3750 (67.6758) lr 1.4258e-03 eta 0:17:34
epoch [11/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2422 (1.2863) acc 68.7500 (67.7273) lr 1.4258e-03 eta 0:17:32
epoch [11/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9854 (1.2822) acc 71.8750 (67.8493) lr 1.4258e-03 eta 0:17:30
epoch [11/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5605 (1.2811) acc 71.8750 (67.8929) lr 1.4258e-03 eta 0:17:28
epoch [11/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8911 (1.2734) acc 78.1250 (68.0903) lr 1.4258e-03 eta 0:17:26
epoch [11/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1104 (1.2756) acc 81.2500 (68.1419) lr 1.4258e-03 eta 0:17:24
epoch [11/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4570 (1.2768) acc 62.5000 (68.1579) lr 1.4258e-03 eta 0:17:22
epoch [11/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 2.2617 (1.2786) acc 59.3750 (68.1571) lr 1.4258e-03 eta 0:17:21
epoch [11/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0654 (1.2770) acc 59.3750 (68.2344) lr 1.4258e-03 eta 0:17:19
epoch [11/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1875 (1.2769) acc 68.7500 (68.2927) lr 1.4258e-03 eta 0:17:17
epoch [11/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3457 (1.2788) acc 68.7500 (68.2887) lr 1.4258e-03 eta 0:17:15
epoch [11/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8682 (1.2829) acc 46.8750 (68.1831) lr 1.4258e-03 eta 0:17:14
epoch [11/25] batch [220/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.2725 (1.2824) acc 65.6250 (68.2244) lr 1.4258e-03 eta 0:17:12
epoch [11/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3477 (1.2878) acc 71.8750 (68.0556) lr 1.4258e-03 eta 0:17:10
epoch [11/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2227 (1.2843) acc 71.8750 (68.1250) lr 1.4258e-03 eta 0:17:08
epoch [11/25] batch [235/250] time 0.292 (0.292) data 0.001 (0.003) loss 1.1885 (1.2870) acc 59.3750 (68.0851) lr 1.4258e-03 eta 0:17:07
epoch [11/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0449 (1.2848) acc 71.8750 (68.1250) lr 1.4258e-03 eta 0:17:05
epoch [11/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.4707 (1.2835) acc 65.6250 (68.1760) lr 1.4258e-03 eta 0:17:03
epoch [11/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3037 (1.2825) acc 62.5000 (68.1750) lr 1.3090e-03 eta 0:17:02
epoch [12/25] batch [5/250] time 0.291 (0.437) data 0.001 (0.146) loss 1.7480 (1.3070) acc 62.5000 (68.7500) lr 1.3090e-03 eta 0:25:27
epoch [12/25] batch [10/250] time 0.290 (0.363) data 0.000 (0.073) loss 1.9512 (1.4016) acc 50.0000 (67.5000) lr 1.3090e-03 eta 0:21:08
epoch [12/25] batch [15/250] time 0.291 (0.339) data 0.001 (0.049) loss 1.3750 (1.4303) acc 65.6250 (65.4167) lr 1.3090e-03 eta 0:19:41
epoch [12/25] batch [20/250] time 0.289 (0.327) data 0.000 (0.037) loss 0.8452 (1.3659) acc 75.0000 (66.7188) lr 1.3090e-03 eta 0:18:56
epoch [12/25] batch [25/250] time 0.290 (0.319) data 0.000 (0.030) loss 1.2148 (1.3621) acc 75.0000 (67.0000) lr 1.3090e-03 eta 0:18:30
epoch [12/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.025) loss 0.9868 (1.3222) acc 75.0000 (67.8125) lr 1.3090e-03 eta 0:18:11
epoch [12/25] batch [35/250] time 0.290 (0.311) data 0.000 (0.021) loss 0.9976 (1.3387) acc 68.7500 (67.5000) lr 1.3090e-03 eta 0:17:57
epoch [12/25] batch [40/250] time 0.290 (0.308) data 0.001 (0.019) loss 1.5371 (1.3319) acc 68.7500 (67.7344) lr 1.3090e-03 eta 0:17:46
epoch [12/25] batch [45/250] time 0.290 (0.306) data 0.000 (0.017) loss 1.3350 (1.3324) acc 75.0000 (67.8472) lr 1.3090e-03 eta 0:17:38
epoch [12/25] batch [50/250] time 0.291 (0.305) data 0.000 (0.015) loss 1.5332 (1.3389) acc 53.1250 (67.3750) lr 1.3090e-03 eta 0:17:31
epoch [12/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.3730 (1.3296) acc 62.5000 (67.1023) lr 1.3090e-03 eta 0:17:25
epoch [12/25] batch [60/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1729 (1.3227) acc 71.8750 (67.0833) lr 1.3090e-03 eta 0:17:19
epoch [12/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1758 (1.3117) acc 68.7500 (67.1635) lr 1.3090e-03 eta 0:17:14
epoch [12/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3076 (1.3180) acc 65.6250 (67.2768) lr 1.3090e-03 eta 0:17:10
epoch [12/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.010) loss 1.1338 (1.2992) acc 71.8750 (67.6250) lr 1.3090e-03 eta 0:17:06
epoch [12/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.2441 (1.3082) acc 75.0000 (67.8516) lr 1.3090e-03 eta 0:17:02
epoch [12/25] batch [85/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.6572 (1.2959) acc 46.8750 (67.7941) lr 1.3090e-03 eta 0:16:59
epoch [12/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.3154 (1.2917) acc 71.8750 (67.8125) lr 1.3090e-03 eta 0:16:56
epoch [12/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.5576 (1.2973) acc 59.3750 (67.5000) lr 1.3090e-03 eta 0:16:53
epoch [12/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9106 (1.2991) acc 81.2500 (67.4688) lr 1.3090e-03 eta 0:16:50
epoch [12/25] batch [105/250] time 0.291 (0.297) data 0.001 (0.007) loss 1.3867 (1.2987) acc 65.6250 (67.4405) lr 1.3090e-03 eta 0:16:47
epoch [12/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.1943 (1.2971) acc 65.6250 (67.4432) lr 1.3090e-03 eta 0:16:45
epoch [12/25] batch [115/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.2090 (1.3021) acc 62.5000 (67.3370) lr 1.3090e-03 eta 0:16:42
epoch [12/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.5430 (1.3032) acc 65.6250 (67.2656) lr 1.3090e-03 eta 0:16:40
epoch [12/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.0254 (1.2986) acc 78.1250 (67.3000) lr 1.3090e-03 eta 0:16:38
epoch [12/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.1299 (1.2998) acc 71.8750 (67.2115) lr 1.3090e-03 eta 0:16:36
epoch [12/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1143 (1.2984) acc 68.7500 (67.2222) lr 1.3090e-03 eta 0:16:33
epoch [12/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2490 (1.2926) acc 68.7500 (67.4107) lr 1.3090e-03 eta 0:16:31
epoch [12/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.2705 (1.2873) acc 65.6250 (67.5862) lr 1.3090e-03 eta 0:16:29
epoch [12/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0264 (1.2771) acc 65.6250 (67.6875) lr 1.3090e-03 eta 0:16:27
epoch [12/25] batch [155/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.4775 (1.2754) acc 65.6250 (67.7621) lr 1.3090e-03 eta 0:16:25
epoch [12/25] batch [160/250] time 0.290 (0.295) data 0.001 (0.005) loss 1.1318 (1.2777) acc 81.2500 (67.9297) lr 1.3090e-03 eta 0:16:23
epoch [12/25] batch [165/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.4658 (1.2753) acc 65.6250 (67.8977) lr 1.3090e-03 eta 0:16:21
epoch [12/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.7217 (1.2787) acc 50.0000 (67.7022) lr 1.3090e-03 eta 0:16:20
epoch [12/25] batch [175/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3926 (1.2815) acc 65.6250 (67.6250) lr 1.3090e-03 eta 0:16:18
epoch [12/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.3096 (1.2815) acc 62.5000 (67.4826) lr 1.3090e-03 eta 0:16:16
epoch [12/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.3564 (1.2803) acc 62.5000 (67.5507) lr 1.3090e-03 eta 0:16:14
epoch [12/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.5352 (1.2796) acc 65.6250 (67.5987) lr 1.3090e-03 eta 0:16:12
epoch [12/25] batch [195/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.8770 (1.2748) acc 78.1250 (67.6763) lr 1.3090e-03 eta 0:16:10
epoch [12/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.4912 (1.2771) acc 68.7500 (67.5625) lr 1.3090e-03 eta 0:16:08
epoch [12/25] batch [205/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.4980 (1.2793) acc 59.3750 (67.4390) lr 1.3090e-03 eta 0:16:07
epoch [12/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.3350 (1.2851) acc 59.3750 (67.3065) lr 1.3090e-03 eta 0:16:05
epoch [12/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4189 (1.2857) acc 65.6250 (67.3692) lr 1.3090e-03 eta 0:16:03
epoch [12/25] batch [220/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.1680 (1.2879) acc 71.8750 (67.3722) lr 1.3090e-03 eta 0:16:01
epoch [12/25] batch [225/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1484 (1.2886) acc 71.8750 (67.4306) lr 1.3090e-03 eta 0:16:00
epoch [12/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3828 (1.2918) acc 56.2500 (67.2962) lr 1.3090e-03 eta 0:15:58
epoch [12/25] batch [235/250] time 0.290 (0.293) data 0.001 (0.003) loss 1.0859 (1.2880) acc 68.7500 (67.3936) lr 1.3090e-03 eta 0:15:56
epoch [12/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.4092 (1.2923) acc 68.7500 (67.3438) lr 1.3090e-03 eta 0:15:55
epoch [12/25] batch [245/250] time 0.291 (0.293) data 0.000 (0.003) loss 0.7798 (1.2914) acc 75.0000 (67.3852) lr 1.3090e-03 eta 0:15:53
epoch [12/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.2051 (1.2935) acc 62.5000 (67.3625) lr 1.1874e-03 eta 0:15:51
epoch [13/25] batch [5/250] time 0.290 (0.438) data 0.000 (0.147) loss 1.1465 (1.3289) acc 75.0000 (70.6250) lr 1.1874e-03 eta 0:23:40
epoch [13/25] batch [10/250] time 0.290 (0.364) data 0.000 (0.074) loss 1.2324 (1.3147) acc 65.6250 (67.8125) lr 1.1874e-03 eta 0:19:38
epoch [13/25] batch [15/250] time 0.289 (0.339) data 0.000 (0.049) loss 1.0410 (1.2873) acc 78.1250 (69.3750) lr 1.1874e-03 eta 0:18:16
epoch [13/25] batch [20/250] time 0.290 (0.327) data 0.000 (0.037) loss 1.1738 (1.2782) acc 68.7500 (68.9062) lr 1.1874e-03 eta 0:17:34
epoch [13/25] batch [25/250] time 0.289 (0.319) data 0.000 (0.030) loss 1.3750 (1.3047) acc 65.6250 (68.1250) lr 1.1874e-03 eta 0:17:09
epoch [13/25] batch [30/250] time 0.291 (0.314) data 0.000 (0.025) loss 1.4209 (1.2816) acc 59.3750 (68.0208) lr 1.1874e-03 eta 0:16:51
epoch [13/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.021) loss 1.3398 (1.2571) acc 62.5000 (68.6607) lr 1.1874e-03 eta 0:16:38
epoch [13/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.8672 (1.2733) acc 53.1250 (67.9688) lr 1.1874e-03 eta 0:16:28
epoch [13/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.8799 (1.2761) acc 78.1250 (68.0556) lr 1.1874e-03 eta 0:16:20
epoch [13/25] batch [50/250] time 0.290 (0.304) data 0.000 (0.015) loss 1.6348 (1.2929) acc 65.6250 (67.6250) lr 1.1874e-03 eta 0:16:13
epoch [13/25] batch [55/250] time 0.290 (0.303) data 0.001 (0.014) loss 0.9502 (1.2896) acc 71.8750 (67.7841) lr 1.1874e-03 eta 0:16:07
epoch [13/25] batch [60/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.2715 (1.2841) acc 75.0000 (68.0208) lr 1.1874e-03 eta 0:16:02
epoch [13/25] batch [65/250] time 0.289 (0.301) data 0.000 (0.012) loss 0.8872 (1.2845) acc 75.0000 (67.6442) lr 1.1874e-03 eta 0:15:58
epoch [13/25] batch [70/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.0039 (1.2897) acc 75.0000 (67.3214) lr 1.1874e-03 eta 0:15:54
epoch [13/25] batch [75/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.7734 (1.3059) acc 59.3750 (67.1667) lr 1.1874e-03 eta 0:15:50
epoch [13/25] batch [80/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.0254 (1.2948) acc 75.0000 (67.4219) lr 1.1874e-03 eta 0:15:47
epoch [13/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.5391 (1.3006) acc 53.1250 (67.2059) lr 1.1874e-03 eta 0:15:44
epoch [13/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.4033 (1.3018) acc 59.3750 (67.1528) lr 1.1874e-03 eta 0:15:41
epoch [13/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2148 (1.2988) acc 68.7500 (67.4342) lr 1.1874e-03 eta 0:15:38
epoch [13/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.8574 (1.3013) acc 68.7500 (67.2812) lr 1.1874e-03 eta 0:15:35
epoch [13/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.3105 (1.3047) acc 71.8750 (67.4405) lr 1.1874e-03 eta 0:15:32
epoch [13/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9653 (1.2977) acc 78.1250 (67.5852) lr 1.1874e-03 eta 0:15:30
epoch [13/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 2.0117 (1.3065) acc 65.6250 (67.4728) lr 1.1874e-03 eta 0:15:27
epoch [13/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.7129 (1.3010) acc 84.3750 (67.6042) lr 1.1874e-03 eta 0:15:25
epoch [13/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5156 (1.2997) acc 65.6250 (67.7500) lr 1.1874e-03 eta 0:15:23
epoch [13/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4619 (1.2990) acc 56.2500 (67.7163) lr 1.1874e-03 eta 0:15:20
epoch [13/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8926 (1.2955) acc 81.2500 (67.8009) lr 1.1874e-03 eta 0:15:18
epoch [13/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 2.1348 (1.3039) acc 59.3750 (67.7009) lr 1.1874e-03 eta 0:15:16
epoch [13/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.5615 (1.3113) acc 65.6250 (67.6940) lr 1.1874e-03 eta 0:15:14
epoch [13/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6650 (1.3054) acc 59.3750 (67.8750) lr 1.1874e-03 eta 0:15:12
epoch [13/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8496 (1.3126) acc 50.0000 (67.6411) lr 1.1874e-03 eta 0:15:10
epoch [13/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.7783 (1.3084) acc 78.1250 (67.7734) lr 1.1874e-03 eta 0:15:08
epoch [13/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7241 (1.3010) acc 81.2500 (67.9924) lr 1.1874e-03 eta 0:15:06
epoch [13/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2734 (1.2969) acc 62.5000 (68.0882) lr 1.1874e-03 eta 0:15:04
epoch [13/25] batch [175/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.4775 (1.2925) acc 53.1250 (68.1607) lr 1.1874e-03 eta 0:15:03
epoch [13/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.5420 (1.2893) acc 65.6250 (68.2465) lr 1.1874e-03 eta 0:15:01
epoch [13/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5352 (1.2924) acc 68.7500 (68.2939) lr 1.1874e-03 eta 0:14:59
epoch [13/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0889 (1.2883) acc 75.0000 (68.3388) lr 1.1874e-03 eta 0:14:57
epoch [13/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8892 (1.2865) acc 78.1250 (68.3974) lr 1.1874e-03 eta 0:14:55
epoch [13/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0215 (1.2874) acc 78.1250 (68.3438) lr 1.1874e-03 eta 0:14:54
epoch [13/25] batch [205/250] time 0.290 (0.293) data 0.001 (0.004) loss 0.9004 (1.2823) acc 81.2500 (68.5518) lr 1.1874e-03 eta 0:14:52
epoch [13/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4824 (1.2801) acc 53.1250 (68.4970) lr 1.1874e-03 eta 0:14:50
epoch [13/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2773 (1.2788) acc 62.5000 (68.5174) lr 1.1874e-03 eta 0:14:49
epoch [13/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4619 (1.2817) acc 62.5000 (68.4517) lr 1.1874e-03 eta 0:14:47
epoch [13/25] batch [225/250] time 0.290 (0.293) data 0.001 (0.004) loss 1.2998 (1.2792) acc 68.7500 (68.4861) lr 1.1874e-03 eta 0:14:45
epoch [13/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1475 (1.2789) acc 71.8750 (68.5190) lr 1.1874e-03 eta 0:14:44
epoch [13/25] batch [235/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.0498 (1.2812) acc 75.0000 (68.4574) lr 1.1874e-03 eta 0:14:42
epoch [13/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.4639 (1.2799) acc 68.7500 (68.5547) lr 1.1874e-03 eta 0:14:40
epoch [13/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.0918 (1.2796) acc 78.1250 (68.5459) lr 1.1874e-03 eta 0:14:39
epoch [13/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 0.6494 (1.2782) acc 84.3750 (68.5750) lr 1.0628e-03 eta 0:14:37
epoch [14/25] batch [5/250] time 0.290 (0.433) data 0.000 (0.144) loss 1.1260 (1.3195) acc 75.0000 (66.8750) lr 1.0628e-03 eta 0:21:36
epoch [14/25] batch [10/250] time 0.289 (0.361) data 0.000 (0.072) loss 0.7222 (1.1444) acc 75.0000 (71.5625) lr 1.0628e-03 eta 0:18:00
epoch [14/25] batch [15/250] time 0.289 (0.337) data 0.000 (0.048) loss 1.3086 (1.1766) acc 71.8750 (72.0833) lr 1.0628e-03 eta 0:16:46
epoch [14/25] batch [20/250] time 0.291 (0.325) data 0.000 (0.036) loss 1.4639 (1.2199) acc 65.6250 (71.5625) lr 1.0628e-03 eta 0:16:09
epoch [14/25] batch [25/250] time 0.290 (0.318) data 0.000 (0.029) loss 1.5557 (1.2619) acc 62.5000 (70.2500) lr 1.0628e-03 eta 0:15:46
epoch [14/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 1.4951 (1.2820) acc 53.1250 (68.8542) lr 1.0628e-03 eta 0:15:30
epoch [14/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 0.7593 (1.2952) acc 75.0000 (68.9286) lr 1.0628e-03 eta 0:15:18
epoch [14/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.1670 (1.2519) acc 65.6250 (69.5312) lr 1.0628e-03 eta 0:15:09
epoch [14/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 0.9629 (1.2487) acc 75.0000 (69.5139) lr 1.0628e-03 eta 0:15:01
epoch [14/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.5244 (1.2448) acc 65.6250 (69.8750) lr 1.0628e-03 eta 0:14:55
epoch [14/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 1.1836 (1.2604) acc 65.6250 (69.5455) lr 1.0628e-03 eta 0:14:49
epoch [14/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1885 (1.2667) acc 71.8750 (69.0625) lr 1.0628e-03 eta 0:14:45
epoch [14/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3545 (1.2767) acc 65.6250 (68.7500) lr 1.0628e-03 eta 0:14:40
epoch [14/25] batch [70/250] time 0.290 (0.299) data 0.000 (0.011) loss 0.7910 (1.2582) acc 84.3750 (69.1518) lr 1.0628e-03 eta 0:14:37
epoch [14/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3574 (1.2583) acc 56.2500 (68.7500) lr 1.0628e-03 eta 0:14:33
epoch [14/25] batch [80/250] time 0.291 (0.298) data 0.000 (0.009) loss 0.6094 (1.2453) acc 81.2500 (68.8672) lr 1.0628e-03 eta 0:14:30
epoch [14/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.7666 (1.2518) acc 65.6250 (68.6397) lr 1.0628e-03 eta 0:14:27
epoch [14/25] batch [90/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.3086 (1.2543) acc 68.7500 (68.7153) lr 1.0628e-03 eta 0:14:24
epoch [14/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9741 (1.2423) acc 75.0000 (69.0461) lr 1.0628e-03 eta 0:14:22
epoch [14/25] batch [100/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.0645 (1.2391) acc 75.0000 (69.0312) lr 1.0628e-03 eta 0:14:19
epoch [14/25] batch [105/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.1133 (1.2360) acc 81.2500 (69.2857) lr 1.0628e-03 eta 0:14:17
epoch [14/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.6240 (1.2384) acc 75.0000 (69.2045) lr 1.0628e-03 eta 0:14:14
epoch [14/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9351 (1.2408) acc 78.1250 (68.9946) lr 1.0628e-03 eta 0:14:12
epoch [14/25] batch [120/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2324 (1.2461) acc 62.5000 (68.7760) lr 1.0628e-03 eta 0:14:10
epoch [14/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2607 (1.2518) acc 75.0000 (68.6250) lr 1.0628e-03 eta 0:14:08
epoch [14/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2783 (1.2551) acc 65.6250 (68.6298) lr 1.0628e-03 eta 0:14:06
epoch [14/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1064 (1.2633) acc 71.8750 (68.3796) lr 1.0628e-03 eta 0:14:04
epoch [14/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0869 (1.2634) acc 78.1250 (68.4375) lr 1.0628e-03 eta 0:14:02
epoch [14/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3018 (1.2623) acc 71.8750 (68.4267) lr 1.0628e-03 eta 0:14:00
epoch [14/25] batch [150/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.1270 (1.2623) acc 71.8750 (68.5208) lr 1.0628e-03 eta 0:13:58
epoch [14/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1289 (1.2602) acc 71.8750 (68.6492) lr 1.0628e-03 eta 0:13:56
epoch [14/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0830 (1.2650) acc 71.8750 (68.4375) lr 1.0628e-03 eta 0:13:54
epoch [14/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0996 (1.2644) acc 75.0000 (68.4470) lr 1.0628e-03 eta 0:13:52
epoch [14/25] batch [170/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2002 (1.2594) acc 71.8750 (68.4375) lr 1.0628e-03 eta 0:13:50
epoch [14/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1592 (1.2586) acc 65.6250 (68.5536) lr 1.0628e-03 eta 0:13:48
epoch [14/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2285 (1.2667) acc 65.6250 (68.4028) lr 1.0628e-03 eta 0:13:46
epoch [14/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3955 (1.2699) acc 62.5000 (68.2939) lr 1.0628e-03 eta 0:13:45
epoch [14/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5664 (1.2764) acc 59.3750 (68.1414) lr 1.0628e-03 eta 0:13:43
epoch [14/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5693 (1.2786) acc 68.7500 (68.1731) lr 1.0628e-03 eta 0:13:41
epoch [14/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6152 (1.2765) acc 68.7500 (68.2500) lr 1.0628e-03 eta 0:13:39
epoch [14/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4424 (1.2824) acc 65.6250 (68.1098) lr 1.0628e-03 eta 0:13:38
epoch [14/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2646 (1.2840) acc 71.8750 (68.0952) lr 1.0628e-03 eta 0:13:36
epoch [14/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.8350 (1.2834) acc 53.1250 (68.1250) lr 1.0628e-03 eta 0:13:34
epoch [14/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.4541 (1.2846) acc 59.3750 (68.1818) lr 1.0628e-03 eta 0:13:33
epoch [14/25] batch [225/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.5508 (1.2883) acc 59.3750 (68.0694) lr 1.0628e-03 eta 0:13:31
epoch [14/25] batch [230/250] time 0.290 (0.292) data 0.000 (0.003) loss 0.8872 (1.2849) acc 78.1250 (68.2065) lr 1.0628e-03 eta 0:13:30
epoch [14/25] batch [235/250] time 0.291 (0.292) data 0.000 (0.003) loss 0.8247 (1.2815) acc 81.2500 (68.2447) lr 1.0628e-03 eta 0:13:28
epoch [14/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0059 (1.2826) acc 68.7500 (68.2812) lr 1.0628e-03 eta 0:13:26
epoch [14/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5107 (1.2841) acc 62.5000 (68.2270) lr 1.0628e-03 eta 0:13:25
epoch [14/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5869 (1.2881) acc 56.2500 (68.1375) lr 9.3721e-04 eta 0:13:23
epoch [15/25] batch [5/250] time 0.289 (0.431) data 0.000 (0.141) loss 1.2832 (1.3250) acc 65.6250 (65.0000) lr 9.3721e-04 eta 0:19:42
epoch [15/25] batch [10/250] time 0.291 (0.360) data 0.000 (0.071) loss 1.1504 (1.2999) acc 84.3750 (68.1250) lr 9.3721e-04 eta 0:16:27
epoch [15/25] batch [15/250] time 0.290 (0.337) data 0.000 (0.047) loss 1.2227 (1.2665) acc 71.8750 (68.5417) lr 9.3721e-04 eta 0:15:21
epoch [15/25] batch [20/250] time 0.290 (0.325) data 0.000 (0.036) loss 1.7451 (1.2668) acc 56.2500 (68.2812) lr 9.3721e-04 eta 0:14:47
epoch [15/25] batch [25/250] time 0.290 (0.318) data 0.000 (0.028) loss 1.2910 (1.2868) acc 75.0000 (68.2500) lr 9.3721e-04 eta 0:14:26
epoch [15/25] batch [30/250] time 0.290 (0.313) data 0.000 (0.024) loss 0.9512 (1.2456) acc 71.8750 (69.4792) lr 9.3721e-04 eta 0:14:12
epoch [15/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.020) loss 1.2275 (1.2669) acc 65.6250 (69.1071) lr 9.3721e-04 eta 0:14:01
epoch [15/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.2656 (1.2503) acc 59.3750 (69.2188) lr 9.3721e-04 eta 0:13:52
epoch [15/25] batch [45/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.8086 (1.2580) acc 56.2500 (68.8889) lr 9.3721e-04 eta 0:13:45
epoch [15/25] batch [50/250] time 0.293 (0.304) data 0.000 (0.014) loss 1.4365 (1.2486) acc 56.2500 (69.1875) lr 9.3721e-04 eta 0:13:40
epoch [15/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.8394 (1.2433) acc 81.2500 (69.5455) lr 9.3721e-04 eta 0:13:35
epoch [15/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.0742 (1.2249) acc 75.0000 (70.3125) lr 9.3721e-04 eta 0:13:30
epoch [15/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2461 (1.2360) acc 78.1250 (70.1442) lr 9.3721e-04 eta 0:13:26
epoch [15/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.010) loss 0.9536 (1.2410) acc 78.1250 (70.0000) lr 9.3721e-04 eta 0:13:22
epoch [15/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.3320 (1.2522) acc 71.8750 (69.8750) lr 9.3721e-04 eta 0:13:19
epoch [15/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2920 (1.2276) acc 65.6250 (70.1953) lr 9.3721e-04 eta 0:13:16
epoch [15/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.4648 (1.2318) acc 62.5000 (69.7426) lr 9.3721e-04 eta 0:13:13
epoch [15/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5732 (1.2452) acc 62.5000 (69.5833) lr 9.3721e-04 eta 0:13:10
epoch [15/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9956 (1.2532) acc 65.6250 (69.3421) lr 9.3721e-04 eta 0:13:07
epoch [15/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1035 (1.2566) acc 78.1250 (69.2188) lr 9.3721e-04 eta 0:13:05
epoch [15/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9062 (1.2517) acc 81.2500 (69.2262) lr 9.3721e-04 eta 0:13:02
epoch [15/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2266 (1.2543) acc 62.5000 (69.0341) lr 9.3721e-04 eta 0:13:00
epoch [15/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5947 (1.2609) acc 59.3750 (68.8043) lr 9.3721e-04 eta 0:12:58
epoch [15/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0986 (1.2667) acc 71.8750 (68.6979) lr 9.3721e-04 eta 0:12:56
epoch [15/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8711 (1.2598) acc 81.2500 (68.9750) lr 9.3721e-04 eta 0:12:54
epoch [15/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0371 (1.2556) acc 71.8750 (69.1827) lr 9.3721e-04 eta 0:12:52
epoch [15/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3096 (1.2591) acc 68.7500 (69.0741) lr 9.3721e-04 eta 0:12:50
epoch [15/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0381 (1.2579) acc 78.1250 (68.9732) lr 9.3721e-04 eta 0:12:48
epoch [15/25] batch [145/250] time 0.291 (0.294) data 0.000 (0.005) loss 1.1045 (1.2634) acc 75.0000 (68.9655) lr 9.3721e-04 eta 0:12:46
epoch [15/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.7744 (1.2625) acc 53.1250 (69.0000) lr 9.3721e-04 eta 0:12:44
epoch [15/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9468 (1.2590) acc 75.0000 (69.1129) lr 9.3721e-04 eta 0:12:42
epoch [15/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2549 (1.2624) acc 68.7500 (69.1602) lr 9.3721e-04 eta 0:12:40
epoch [15/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.7656 (1.2710) acc 56.2500 (68.9394) lr 9.3721e-04 eta 0:12:38
epoch [15/25] batch [170/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1924 (1.2739) acc 65.6250 (68.9154) lr 9.3721e-04 eta 0:12:37
epoch [15/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1348 (1.2677) acc 71.8750 (68.9464) lr 9.3721e-04 eta 0:12:35
epoch [15/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0156 (1.2653) acc 78.1250 (68.9931) lr 9.3721e-04 eta 0:12:33
epoch [15/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0020 (1.2654) acc 81.2500 (68.9865) lr 9.3721e-04 eta 0:12:32
epoch [15/25] batch [190/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.7476 (1.2614) acc 90.6250 (69.0461) lr 9.3721e-04 eta 0:12:30
epoch [15/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4102 (1.2650) acc 62.5000 (68.8782) lr 9.3721e-04 eta 0:12:28
epoch [15/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2939 (1.2658) acc 62.5000 (68.8594) lr 9.3721e-04 eta 0:12:26
epoch [15/25] batch [205/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2383 (1.2683) acc 71.8750 (68.8262) lr 9.3721e-04 eta 0:12:25
epoch [15/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1787 (1.2677) acc 71.8750 (68.8393) lr 9.3721e-04 eta 0:12:23
epoch [15/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.8960 (1.2604) acc 75.0000 (69.0552) lr 9.3721e-04 eta 0:12:22
epoch [15/25] batch [220/250] time 0.290 (0.293) data 0.000 (0.003) loss 0.7720 (1.2616) acc 78.1250 (69.0057) lr 9.3721e-04 eta 0:12:20
epoch [15/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.1406 (1.2567) acc 71.8750 (69.1389) lr 9.3721e-04 eta 0:12:18
epoch [15/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.3828 (1.2560) acc 65.6250 (69.1984) lr 9.3721e-04 eta 0:12:17
epoch [15/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6670 (1.2543) acc 68.7500 (69.2021) lr 9.3721e-04 eta 0:12:15
epoch [15/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3984 (1.2587) acc 62.5000 (69.0365) lr 9.3721e-04 eta 0:12:13
epoch [15/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.6045 (1.2597) acc 50.0000 (68.9796) lr 9.3721e-04 eta 0:12:12
epoch [15/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 0.8354 (1.2566) acc 75.0000 (69.0000) lr 8.1262e-04 eta 0:12:10
epoch [16/25] batch [5/250] time 0.289 (0.449) data 0.000 (0.159) loss 1.2891 (1.2422) acc 68.7500 (67.5000) lr 8.1262e-04 eta 0:18:39
epoch [16/25] batch [10/250] time 0.290 (0.369) data 0.000 (0.080) loss 1.3145 (1.1892) acc 75.0000 (68.4375) lr 8.1262e-04 eta 0:15:19
epoch [16/25] batch [15/250] time 0.289 (0.343) data 0.000 (0.053) loss 1.2744 (1.2257) acc 65.6250 (68.1250) lr 8.1262e-04 eta 0:14:12
epoch [16/25] batch [20/250] time 0.290 (0.330) data 0.000 (0.040) loss 1.1533 (1.2339) acc 65.6250 (68.7500) lr 8.1262e-04 eta 0:13:38
epoch [16/25] batch [25/250] time 0.290 (0.322) data 0.000 (0.032) loss 1.4795 (1.2317) acc 68.7500 (68.7500) lr 8.1262e-04 eta 0:13:16
epoch [16/25] batch [30/250] time 0.290 (0.317) data 0.000 (0.027) loss 1.1465 (1.2332) acc 78.1250 (69.4792) lr 8.1262e-04 eta 0:13:02
epoch [16/25] batch [35/250] time 0.290 (0.313) data 0.000 (0.023) loss 1.5449 (1.2509) acc 68.7500 (69.2857) lr 8.1262e-04 eta 0:12:51
epoch [16/25] batch [40/250] time 0.290 (0.310) data 0.000 (0.020) loss 1.6172 (1.2645) acc 56.2500 (68.6719) lr 8.1262e-04 eta 0:12:42
epoch [16/25] batch [45/250] time 0.291 (0.308) data 0.000 (0.018) loss 1.0410 (1.2650) acc 71.8750 (69.0972) lr 8.1262e-04 eta 0:12:35
epoch [16/25] batch [50/250] time 0.290 (0.306) data 0.000 (0.016) loss 1.6699 (1.2737) acc 65.6250 (69.1250) lr 8.1262e-04 eta 0:12:29
epoch [16/25] batch [55/250] time 0.290 (0.305) data 0.000 (0.015) loss 1.2783 (1.2850) acc 65.6250 (68.9205) lr 8.1262e-04 eta 0:12:24
epoch [16/25] batch [60/250] time 0.290 (0.303) data 0.001 (0.014) loss 0.9893 (1.2870) acc 75.0000 (68.9062) lr 8.1262e-04 eta 0:12:20
epoch [16/25] batch [65/250] time 0.290 (0.302) data 0.001 (0.013) loss 1.3105 (1.2883) acc 71.8750 (68.7500) lr 8.1262e-04 eta 0:12:16
epoch [16/25] batch [70/250] time 0.290 (0.302) data 0.000 (0.012) loss 0.7559 (1.2693) acc 81.2500 (69.1964) lr 8.1262e-04 eta 0:12:12
epoch [16/25] batch [75/250] time 0.290 (0.301) data 0.001 (0.011) loss 1.1387 (1.2657) acc 75.0000 (69.5000) lr 8.1262e-04 eta 0:12:09
epoch [16/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.2031 (1.2582) acc 71.8750 (69.6875) lr 8.1262e-04 eta 0:12:06
epoch [16/25] batch [85/250] time 0.290 (0.299) data 0.001 (0.010) loss 1.1572 (1.2532) acc 75.0000 (69.6691) lr 8.1262e-04 eta 0:12:03
epoch [16/25] batch [90/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.2705 (1.2476) acc 75.0000 (70.0000) lr 8.1262e-04 eta 0:12:00
epoch [16/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.6147 (1.2331) acc 78.1250 (70.1316) lr 8.1262e-04 eta 0:11:57
epoch [16/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.0371 (1.2306) acc 75.0000 (70.1875) lr 8.1262e-04 eta 0:11:55
epoch [16/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.008) loss 0.9487 (1.2334) acc 81.2500 (70.2083) lr 8.1262e-04 eta 0:11:52
epoch [16/25] batch [110/250] time 0.291 (0.297) data 0.000 (0.008) loss 1.4453 (1.2362) acc 75.0000 (70.3693) lr 8.1262e-04 eta 0:11:50
epoch [16/25] batch [115/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.4785 (1.2361) acc 68.7500 (70.4348) lr 8.1262e-04 eta 0:11:48
epoch [16/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.007) loss 0.7061 (1.2488) acc 81.2500 (70.1823) lr 8.1262e-04 eta 0:11:46
epoch [16/25] batch [125/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.3232 (1.2504) acc 68.7500 (70.0750) lr 8.1262e-04 eta 0:11:44
epoch [16/25] batch [130/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.2324 (1.2480) acc 65.6250 (70.0240) lr 8.1262e-04 eta 0:11:42
epoch [16/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.0703 (1.2476) acc 71.8750 (70.0694) lr 8.1262e-04 eta 0:11:40
epoch [16/25] batch [140/250] time 0.291 (0.296) data 0.000 (0.006) loss 1.2324 (1.2471) acc 71.8750 (70.0000) lr 8.1262e-04 eta 0:11:38
epoch [16/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.0117 (1.2367) acc 71.8750 (70.1509) lr 8.1262e-04 eta 0:11:36
epoch [16/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.1611 (1.2373) acc 59.3750 (69.8750) lr 8.1262e-04 eta 0:11:34
epoch [16/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.4922 (1.2448) acc 68.7500 (69.8185) lr 8.1262e-04 eta 0:11:32
epoch [16/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.005) loss 0.7383 (1.2448) acc 81.2500 (69.7852) lr 8.1262e-04 eta 0:11:30
epoch [16/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0781 (1.2387) acc 71.8750 (70.0000) lr 8.1262e-04 eta 0:11:28
epoch [16/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.7715 (1.2363) acc 56.2500 (69.9632) lr 8.1262e-04 eta 0:11:26
epoch [16/25] batch [175/250] time 0.289 (0.295) data 0.000 (0.005) loss 0.8320 (1.2293) acc 71.8750 (70.0357) lr 8.1262e-04 eta 0:11:24
epoch [16/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.8369 (1.2290) acc 56.2500 (70.0174) lr 8.1262e-04 eta 0:11:22
epoch [16/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2627 (1.2281) acc 68.7500 (70.0000) lr 8.1262e-04 eta 0:11:21
epoch [16/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1875 (1.2294) acc 65.6250 (70.0164) lr 8.1262e-04 eta 0:11:19
epoch [16/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.0195 (1.2304) acc 75.0000 (70.0321) lr 8.1262e-04 eta 0:11:17
epoch [16/25] batch [200/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.9785 (1.2305) acc 65.6250 (69.9219) lr 8.1262e-04 eta 0:11:15
epoch [16/25] batch [205/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.0254 (1.2363) acc 71.8750 (69.7561) lr 8.1262e-04 eta 0:11:14
epoch [16/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.6211 (1.2404) acc 50.0000 (69.6131) lr 8.1262e-04 eta 0:11:12
epoch [16/25] batch [215/250] time 0.291 (0.294) data 0.000 (0.004) loss 0.9619 (1.2429) acc 75.0000 (69.5785) lr 8.1262e-04 eta 0:11:10
epoch [16/25] batch [220/250] time 0.289 (0.294) data 0.000 (0.004) loss 0.8184 (1.2430) acc 81.2500 (69.5739) lr 8.1262e-04 eta 0:11:09
epoch [16/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8882 (1.2405) acc 81.2500 (69.7222) lr 8.1262e-04 eta 0:11:07
epoch [16/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.9014 (1.2434) acc 59.3750 (69.6467) lr 8.1262e-04 eta 0:11:05
epoch [16/25] batch [235/250] time 0.289 (0.293) data 0.001 (0.004) loss 1.0215 (1.2408) acc 78.1250 (69.6809) lr 8.1262e-04 eta 0:11:04
epoch [16/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.7915 (1.2394) acc 81.2500 (69.7135) lr 8.1262e-04 eta 0:11:02
epoch [16/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2051 (1.2401) acc 75.0000 (69.6811) lr 8.1262e-04 eta 0:11:01
epoch [16/25] batch [250/250] time 0.291 (0.293) data 0.000 (0.004) loss 0.9849 (1.2389) acc 75.0000 (69.7250) lr 6.9098e-04 eta 0:10:59
epoch [17/25] batch [5/250] time 0.290 (0.478) data 0.000 (0.188) loss 1.6328 (1.3516) acc 50.0000 (63.7500) lr 6.9098e-04 eta 0:17:52
epoch [17/25] batch [10/250] time 0.289 (0.384) data 0.000 (0.094) loss 1.0254 (1.2180) acc 75.0000 (67.8125) lr 6.9098e-04 eta 0:14:19
epoch [17/25] batch [15/250] time 0.290 (0.352) data 0.000 (0.063) loss 0.8613 (1.2264) acc 78.1250 (68.3333) lr 6.9098e-04 eta 0:13:07
epoch [17/25] batch [20/250] time 0.289 (0.337) data 0.001 (0.047) loss 1.6133 (1.2320) acc 62.5000 (69.5312) lr 6.9098e-04 eta 0:12:30
epoch [17/25] batch [25/250] time 0.289 (0.327) data 0.000 (0.038) loss 1.4785 (1.2463) acc 59.3750 (68.3750) lr 6.9098e-04 eta 0:12:07
epoch [17/25] batch [30/250] time 0.290 (0.321) data 0.000 (0.032) loss 1.0410 (1.2174) acc 78.1250 (69.1667) lr 6.9098e-04 eta 0:11:51
epoch [17/25] batch [35/250] time 0.290 (0.316) data 0.000 (0.027) loss 0.8755 (1.2039) acc 81.2500 (69.5536) lr 6.9098e-04 eta 0:11:40
epoch [17/25] batch [40/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.9336 (1.2089) acc 75.0000 (69.5312) lr 6.9098e-04 eta 0:11:31
epoch [17/25] batch [45/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.2598 (1.2044) acc 65.6250 (69.5139) lr 6.9098e-04 eta 0:11:24
epoch [17/25] batch [50/250] time 0.289 (0.308) data 0.000 (0.019) loss 2.0625 (1.2313) acc 59.3750 (69.1875) lr 6.9098e-04 eta 0:11:18
epoch [17/25] batch [55/250] time 0.288 (0.306) data 0.000 (0.017) loss 0.9741 (1.2341) acc 71.8750 (68.9773) lr 6.9098e-04 eta 0:11:12
epoch [17/25] batch [60/250] time 0.289 (0.305) data 0.000 (0.016) loss 0.6572 (1.2344) acc 84.3750 (68.9583) lr 6.9098e-04 eta 0:11:07
epoch [17/25] batch [65/250] time 0.289 (0.304) data 0.000 (0.015) loss 1.2109 (1.2427) acc 78.1250 (69.2788) lr 6.9098e-04 eta 0:11:03
epoch [17/25] batch [70/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.8320 (1.2463) acc 53.1250 (69.1964) lr 6.9098e-04 eta 0:11:00
epoch [17/25] batch [75/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.9600 (1.2338) acc 71.8750 (69.5000) lr 6.9098e-04 eta 0:10:56
epoch [17/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.3672 (1.2311) acc 68.7500 (69.3359) lr 6.9098e-04 eta 0:10:53
epoch [17/25] batch [85/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.1924 (1.2347) acc 75.0000 (69.4485) lr 6.9098e-04 eta 0:10:50
epoch [17/25] batch [90/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2656 (1.2323) acc 65.6250 (69.4444) lr 6.9098e-04 eta 0:10:47
epoch [17/25] batch [95/250] time 0.291 (0.299) data 0.000 (0.010) loss 1.5811 (1.2396) acc 62.5000 (69.3092) lr 6.9098e-04 eta 0:10:44
epoch [17/25] batch [100/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4102 (1.2533) acc 65.6250 (69.1250) lr 6.9098e-04 eta 0:10:42
epoch [17/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0928 (1.2468) acc 78.1250 (69.3750) lr 6.9098e-04 eta 0:10:39
epoch [17/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0732 (1.2494) acc 62.5000 (69.2330) lr 6.9098e-04 eta 0:10:37
epoch [17/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.7568 (1.2538) acc 53.1250 (68.9946) lr 6.9098e-04 eta 0:10:35
epoch [17/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.9277 (1.2541) acc 59.3750 (69.0885) lr 6.9098e-04 eta 0:10:32
epoch [17/25] batch [125/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.2607 (1.2511) acc 78.1250 (69.3250) lr 6.9098e-04 eta 0:10:30
epoch [17/25] batch [130/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0967 (1.2502) acc 78.1250 (69.3750) lr 6.9098e-04 eta 0:10:28
epoch [17/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.4160 (1.2556) acc 71.8750 (69.4213) lr 6.9098e-04 eta 0:10:26
epoch [17/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0381 (1.2539) acc 75.0000 (69.4866) lr 6.9098e-04 eta 0:10:24
epoch [17/25] batch [145/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.7573 (1.2561) acc 71.8750 (69.2672) lr 6.9098e-04 eta 0:10:22
epoch [17/25] batch [150/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9209 (1.2522) acc 71.8750 (69.3125) lr 6.9098e-04 eta 0:10:20
epoch [17/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6846 (1.2554) acc 59.3750 (69.2944) lr 6.9098e-04 eta 0:10:18
epoch [17/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2012 (1.2543) acc 68.7500 (69.2773) lr 6.9098e-04 eta 0:10:16
epoch [17/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4375 (1.2536) acc 71.8750 (69.2424) lr 6.9098e-04 eta 0:10:14
epoch [17/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5840 (1.2548) acc 56.2500 (69.0993) lr 6.9098e-04 eta 0:10:13
epoch [17/25] batch [175/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.8237 (1.2526) acc 75.0000 (69.2143) lr 6.9098e-04 eta 0:10:11
epoch [17/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3193 (1.2533) acc 68.7500 (69.2361) lr 6.9098e-04 eta 0:10:09
epoch [17/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0430 (1.2539) acc 84.3750 (69.2736) lr 6.9098e-04 eta 0:10:07
epoch [17/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7715 (1.2506) acc 84.3750 (69.3586) lr 6.9098e-04 eta 0:10:05
epoch [17/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1201 (1.2524) acc 75.0000 (69.3750) lr 6.9098e-04 eta 0:10:04
epoch [17/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2031 (1.2543) acc 62.5000 (69.2656) lr 6.9098e-04 eta 0:10:02
epoch [17/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9146 (1.2493) acc 81.2500 (69.4360) lr 6.9098e-04 eta 0:10:00
epoch [17/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5762 (1.2494) acc 62.5000 (69.4048) lr 6.9098e-04 eta 0:09:59
epoch [17/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2148 (1.2459) acc 68.7500 (69.5349) lr 6.9098e-04 eta 0:09:57
epoch [17/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.7256 (1.2489) acc 65.6250 (69.4034) lr 6.9098e-04 eta 0:09:55
epoch [17/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0410 (1.2492) acc 68.7500 (69.3472) lr 6.9098e-04 eta 0:09:54
epoch [17/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1982 (1.2501) acc 62.5000 (69.3342) lr 6.9098e-04 eta 0:09:52
epoch [17/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8359 (1.2534) acc 81.2500 (69.3351) lr 6.9098e-04 eta 0:09:50
epoch [17/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9697 (1.2514) acc 75.0000 (69.3750) lr 6.9098e-04 eta 0:09:49
epoch [17/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5029 (1.2530) acc 62.5000 (69.3878) lr 6.9098e-04 eta 0:09:47
epoch [17/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3789 (1.2515) acc 75.0000 (69.4875) lr 5.7422e-04 eta 0:09:45
epoch [18/25] batch [5/250] time 0.289 (0.461) data 0.000 (0.171) loss 1.5498 (1.1844) acc 62.5000 (74.3750) lr 5.7422e-04 eta 0:15:19
epoch [18/25] batch [10/250] time 0.289 (0.375) data 0.000 (0.086) loss 0.8682 (1.1117) acc 75.0000 (74.3750) lr 5.7422e-04 eta 0:12:26
epoch [18/25] batch [15/250] time 0.290 (0.346) data 0.000 (0.057) loss 1.1406 (1.0674) acc 84.3750 (75.8333) lr 5.7422e-04 eta 0:11:27
epoch [18/25] batch [20/250] time 0.289 (0.332) data 0.000 (0.043) loss 1.6797 (1.1061) acc 56.2500 (74.0625) lr 5.7422e-04 eta 0:10:57
epoch [18/25] batch [25/250] time 0.289 (0.324) data 0.000 (0.034) loss 1.2012 (1.1369) acc 71.8750 (73.1250) lr 5.7422e-04 eta 0:10:39
epoch [18/25] batch [30/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.7988 (1.1733) acc 46.8750 (71.9792) lr 5.7422e-04 eta 0:10:26
epoch [18/25] batch [35/250] time 0.290 (0.314) data 0.000 (0.025) loss 0.9663 (1.1982) acc 75.0000 (71.6071) lr 5.7422e-04 eta 0:10:16
epoch [18/25] batch [40/250] time 0.289 (0.311) data 0.000 (0.022) loss 0.9053 (1.1895) acc 78.1250 (71.7969) lr 5.7422e-04 eta 0:10:08
epoch [18/25] batch [45/250] time 0.289 (0.308) data 0.000 (0.019) loss 0.8403 (1.1539) acc 75.0000 (72.3611) lr 5.7422e-04 eta 0:10:02
epoch [18/25] batch [50/250] time 0.289 (0.306) data 0.000 (0.017) loss 0.8813 (1.1623) acc 71.8750 (71.8125) lr 5.7422e-04 eta 0:09:57
epoch [18/25] batch [55/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.5156 (1.1508) acc 65.6250 (71.9318) lr 5.7422e-04 eta 0:09:53
epoch [18/25] batch [60/250] time 0.290 (0.304) data 0.000 (0.015) loss 1.5801 (1.1744) acc 53.1250 (71.3021) lr 5.7422e-04 eta 0:09:48
epoch [18/25] batch [65/250] time 0.290 (0.303) data 0.000 (0.013) loss 0.7539 (1.1878) acc 78.1250 (71.1538) lr 5.7422e-04 eta 0:09:45
epoch [18/25] batch [70/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.4453 (1.1938) acc 65.6250 (70.8036) lr 5.7422e-04 eta 0:09:42
epoch [18/25] batch [75/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.1982 (1.1996) acc 71.8750 (70.8750) lr 5.7422e-04 eta 0:09:39
epoch [18/25] batch [80/250] time 0.290 (0.300) data 0.001 (0.011) loss 0.9678 (1.2035) acc 71.8750 (70.7031) lr 5.7422e-04 eta 0:09:36
epoch [18/25] batch [85/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.2266 (1.2078) acc 75.0000 (70.6985) lr 5.7422e-04 eta 0:09:33
epoch [18/25] batch [90/250] time 0.290 (0.299) data 0.001 (0.010) loss 1.2949 (1.2138) acc 68.7500 (70.4514) lr 5.7422e-04 eta 0:09:31
epoch [18/25] batch [95/250] time 0.290 (0.299) data 0.000 (0.009) loss 1.1143 (1.2080) acc 75.0000 (70.5921) lr 5.7422e-04 eta 0:09:28
epoch [18/25] batch [100/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0488 (1.2110) acc 68.7500 (70.5625) lr 5.7422e-04 eta 0:09:26
epoch [18/25] batch [105/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.2510 (1.2073) acc 62.5000 (70.4762) lr 5.7422e-04 eta 0:09:24
epoch [18/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.1133 (1.2124) acc 75.0000 (70.2273) lr 5.7422e-04 eta 0:09:22
epoch [18/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4219 (1.2073) acc 71.8750 (70.3533) lr 5.7422e-04 eta 0:09:19
epoch [18/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.6641 (1.2137) acc 62.5000 (70.1562) lr 5.7422e-04 eta 0:09:17
epoch [18/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.3301 (1.2183) acc 62.5000 (70.1250) lr 5.7422e-04 eta 0:09:15
epoch [18/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.6504 (1.2297) acc 71.8750 (70.0000) lr 5.7422e-04 eta 0:09:13
epoch [18/25] batch [135/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.3623 (1.2309) acc 59.3750 (69.8148) lr 5.7422e-04 eta 0:09:11
epoch [18/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.5752 (1.2354) acc 78.1250 (69.9554) lr 5.7422e-04 eta 0:09:10
epoch [18/25] batch [145/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.8667 (1.2321) acc 81.2500 (70.0216) lr 5.7422e-04 eta 0:09:08
epoch [18/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5342 (1.2291) acc 75.0000 (70.2708) lr 5.7422e-04 eta 0:09:06
epoch [18/25] batch [155/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.8848 (1.2315) acc 53.1250 (70.1613) lr 5.7422e-04 eta 0:09:04
epoch [18/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.6641 (1.2299) acc 56.2500 (70.1172) lr 5.7422e-04 eta 0:09:02
epoch [18/25] batch [165/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2334 (1.2234) acc 68.7500 (70.2841) lr 5.7422e-04 eta 0:09:01
epoch [18/25] batch [170/250] time 0.290 (0.295) data 0.000 (0.005) loss 1.2920 (1.2200) acc 71.8750 (70.3125) lr 5.7422e-04 eta 0:08:59
epoch [18/25] batch [175/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.5762 (1.2275) acc 53.1250 (70.1786) lr 5.7422e-04 eta 0:08:57
epoch [18/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2744 (1.2261) acc 62.5000 (70.2431) lr 5.7422e-04 eta 0:08:55
epoch [18/25] batch [185/250] time 0.292 (0.294) data 0.000 (0.005) loss 1.1162 (1.2330) acc 78.1250 (70.2027) lr 5.7422e-04 eta 0:08:54
epoch [18/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9170 (1.2365) acc 71.8750 (70.0987) lr 5.7422e-04 eta 0:08:52
epoch [18/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3779 (1.2377) acc 56.2500 (69.9519) lr 5.7422e-04 eta 0:08:50
epoch [18/25] batch [200/250] time 0.294 (0.294) data 0.001 (0.005) loss 1.1445 (1.2329) acc 75.0000 (70.0469) lr 5.7422e-04 eta 0:08:49
epoch [18/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.6001 (1.2298) acc 87.5000 (70.2287) lr 5.7422e-04 eta 0:08:47
epoch [18/25] batch [210/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.0801 (1.2323) acc 75.0000 (70.1339) lr 5.7422e-04 eta 0:08:45
epoch [18/25] batch [215/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1729 (1.2319) acc 65.6250 (70.1017) lr 5.7422e-04 eta 0:08:44
epoch [18/25] batch [220/250] time 0.291 (0.294) data 0.000 (0.004) loss 1.7988 (1.2373) acc 56.2500 (69.9716) lr 5.7422e-04 eta 0:08:42
epoch [18/25] batch [225/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.1143 (1.2366) acc 78.1250 (70.0000) lr 5.7422e-04 eta 0:08:41
epoch [18/25] batch [230/250] time 0.294 (0.293) data 0.001 (0.004) loss 1.2842 (1.2360) acc 75.0000 (70.0408) lr 5.7422e-04 eta 0:08:39
epoch [18/25] batch [235/250] time 0.291 (0.293) data 0.001 (0.004) loss 1.4756 (1.2350) acc 71.8750 (70.1197) lr 5.7422e-04 eta 0:08:37
epoch [18/25] batch [240/250] time 0.292 (0.293) data 0.000 (0.004) loss 0.9014 (1.2337) acc 84.3750 (70.2214) lr 5.7422e-04 eta 0:08:36
epoch [18/25] batch [245/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.8208 (1.2331) acc 84.3750 (70.2806) lr 5.7422e-04 eta 0:08:34
epoch [18/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0908 (1.2344) acc 71.8750 (70.2375) lr 4.6417e-04 eta 0:08:33
epoch [19/25] batch [5/250] time 0.290 (0.471) data 0.000 (0.180) loss 1.1201 (1.0140) acc 68.7500 (71.2500) lr 4.6417e-04 eta 0:13:41
epoch [19/25] batch [10/250] time 0.289 (0.380) data 0.000 (0.090) loss 1.3174 (1.1078) acc 59.3750 (70.9375) lr 4.6417e-04 eta 0:11:01
epoch [19/25] batch [15/250] time 0.290 (0.350) data 0.000 (0.060) loss 1.0605 (1.1906) acc 75.0000 (69.3750) lr 4.6417e-04 eta 0:10:07
epoch [19/25] batch [20/250] time 0.289 (0.335) data 0.000 (0.045) loss 1.2393 (1.1719) acc 68.7500 (70.1562) lr 4.6417e-04 eta 0:09:39
epoch [19/25] batch [25/250] time 0.289 (0.326) data 0.000 (0.036) loss 1.2393 (1.2234) acc 78.1250 (69.8750) lr 4.6417e-04 eta 0:09:21
epoch [19/25] batch [30/250] time 0.289 (0.320) data 0.000 (0.030) loss 1.4287 (1.2103) acc 78.1250 (71.0417) lr 4.6417e-04 eta 0:09:09
epoch [19/25] batch [35/250] time 0.290 (0.316) data 0.000 (0.026) loss 0.8374 (1.1945) acc 84.3750 (71.7857) lr 4.6417e-04 eta 0:09:01
epoch [19/25] batch [40/250] time 0.290 (0.312) data 0.000 (0.023) loss 1.2617 (1.2027) acc 71.8750 (71.2500) lr 4.6417e-04 eta 0:08:54
epoch [19/25] batch [45/250] time 0.290 (0.310) data 0.001 (0.020) loss 1.0059 (1.2105) acc 78.1250 (70.8333) lr 4.6417e-04 eta 0:08:48
epoch [19/25] batch [50/250] time 0.290 (0.308) data 0.001 (0.018) loss 1.4150 (1.2100) acc 71.8750 (71.0000) lr 4.6417e-04 eta 0:08:43
epoch [19/25] batch [55/250] time 0.293 (0.306) data 0.000 (0.017) loss 1.2861 (1.2076) acc 71.8750 (70.8523) lr 4.6417e-04 eta 0:08:39
epoch [19/25] batch [60/250] time 0.290 (0.305) data 0.000 (0.015) loss 1.1289 (1.2171) acc 78.1250 (70.7812) lr 4.6417e-04 eta 0:08:35
epoch [19/25] batch [65/250] time 0.290 (0.304) data 0.000 (0.014) loss 1.2705 (1.2114) acc 59.3750 (70.4808) lr 4.6417e-04 eta 0:08:31
epoch [19/25] batch [70/250] time 0.289 (0.303) data 0.000 (0.013) loss 1.3506 (1.1982) acc 71.8750 (70.8929) lr 4.6417e-04 eta 0:08:28
epoch [19/25] batch [75/250] time 0.291 (0.302) data 0.000 (0.012) loss 1.4639 (1.1909) acc 65.6250 (70.6667) lr 4.6417e-04 eta 0:08:25
epoch [19/25] batch [80/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.1777 (1.1984) acc 65.6250 (70.4297) lr 4.6417e-04 eta 0:08:22
epoch [19/25] batch [85/250] time 0.291 (0.301) data 0.000 (0.011) loss 1.5029 (1.2058) acc 75.0000 (70.3309) lr 4.6417e-04 eta 0:08:20
epoch [19/25] batch [90/250] time 0.291 (0.300) data 0.001 (0.010) loss 1.2207 (1.1980) acc 62.5000 (70.4861) lr 4.6417e-04 eta 0:08:17
epoch [19/25] batch [95/250] time 0.289 (0.299) data 0.000 (0.010) loss 1.4326 (1.2129) acc 65.6250 (70.2961) lr 4.6417e-04 eta 0:08:15
epoch [19/25] batch [100/250] time 0.289 (0.299) data 0.000 (0.009) loss 1.5674 (1.2138) acc 75.0000 (70.4062) lr 4.6417e-04 eta 0:08:13
epoch [19/25] batch [105/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.2188 (1.2106) acc 71.8750 (70.4762) lr 4.6417e-04 eta 0:08:10
epoch [19/25] batch [110/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0371 (1.2103) acc 75.0000 (70.5966) lr 4.6417e-04 eta 0:08:08
epoch [19/25] batch [115/250] time 0.290 (0.298) data 0.000 (0.008) loss 0.9966 (1.2108) acc 78.1250 (70.5163) lr 4.6417e-04 eta 0:08:06
epoch [19/25] batch [120/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1826 (1.2146) acc 62.5000 (70.3385) lr 4.6417e-04 eta 0:08:04
epoch [19/25] batch [125/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.9097 (1.2207) acc 78.1250 (70.1250) lr 4.6417e-04 eta 0:08:02
epoch [19/25] batch [130/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.5068 (1.2317) acc 56.2500 (69.7837) lr 4.6417e-04 eta 0:08:00
epoch [19/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.5400 (1.2355) acc 65.6250 (69.7454) lr 4.6417e-04 eta 0:07:58
epoch [19/25] batch [140/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.8745 (1.2360) acc 78.1250 (69.7768) lr 4.6417e-04 eta 0:07:56
epoch [19/25] batch [145/250] time 0.291 (0.296) data 0.000 (0.007) loss 1.3340 (1.2416) acc 62.5000 (69.7198) lr 4.6417e-04 eta 0:07:54
epoch [19/25] batch [150/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.4775 (1.2436) acc 56.2500 (69.5625) lr 4.6417e-04 eta 0:07:53
epoch [19/25] batch [155/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.8481 (1.2357) acc 81.2500 (69.6774) lr 4.6417e-04 eta 0:07:51
epoch [19/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1338 (1.2336) acc 62.5000 (69.5508) lr 4.6417e-04 eta 0:07:49
epoch [19/25] batch [165/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.3564 (1.2337) acc 71.8750 (69.6023) lr 4.6417e-04 eta 0:07:47
epoch [19/25] batch [170/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.6816 (1.2317) acc 65.6250 (69.7243) lr 4.6417e-04 eta 0:07:46
epoch [19/25] batch [175/250] time 0.292 (0.295) data 0.000 (0.005) loss 1.3457 (1.2300) acc 62.5000 (69.7500) lr 4.6417e-04 eta 0:07:44
epoch [19/25] batch [180/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.2500 (1.2328) acc 59.3750 (69.7049) lr 4.6417e-04 eta 0:07:42
epoch [19/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.3076 (1.2332) acc 71.8750 (69.7128) lr 4.6417e-04 eta 0:07:40
epoch [19/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0625 (1.2299) acc 78.1250 (69.8191) lr 4.6417e-04 eta 0:07:39
epoch [19/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5801 (1.2342) acc 53.1250 (69.6474) lr 4.6417e-04 eta 0:07:37
epoch [19/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8682 (1.2307) acc 78.1250 (69.7812) lr 4.6417e-04 eta 0:07:35
epoch [19/25] batch [205/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4043 (1.2334) acc 68.7500 (69.6951) lr 4.6417e-04 eta 0:07:34
epoch [19/25] batch [210/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1406 (1.2292) acc 71.8750 (69.8065) lr 4.6417e-04 eta 0:07:32
epoch [19/25] batch [215/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0605 (1.2239) acc 71.8750 (69.8692) lr 4.6417e-04 eta 0:07:30
epoch [19/25] batch [220/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.1484 (1.2221) acc 78.1250 (69.9290) lr 4.6417e-04 eta 0:07:29
epoch [19/25] batch [225/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.4854 (1.2300) acc 65.6250 (69.6944) lr 4.6417e-04 eta 0:07:27
epoch [19/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8735 (1.2248) acc 68.7500 (69.7011) lr 4.6417e-04 eta 0:07:26
epoch [19/25] batch [235/250] time 0.291 (0.293) data 0.001 (0.004) loss 1.4424 (1.2255) acc 56.2500 (69.6144) lr 4.6417e-04 eta 0:07:24
epoch [19/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9517 (1.2238) acc 81.2500 (69.7135) lr 4.6417e-04 eta 0:07:22
epoch [19/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1748 (1.2254) acc 65.6250 (69.6173) lr 4.6417e-04 eta 0:07:21
epoch [19/25] batch [250/250] time 0.291 (0.293) data 0.000 (0.004) loss 0.8774 (1.2226) acc 71.8750 (69.6875) lr 3.6258e-04 eta 0:07:19
epoch [20/25] batch [5/250] time 0.289 (0.444) data 0.000 (0.154) loss 1.1260 (1.3100) acc 75.0000 (65.0000) lr 3.6258e-04 eta 0:11:04
epoch [20/25] batch [10/250] time 0.289 (0.367) data 0.000 (0.077) loss 1.5332 (1.1702) acc 71.8750 (71.2500) lr 3.6258e-04 eta 0:09:06
epoch [20/25] batch [15/250] time 0.289 (0.341) data 0.000 (0.052) loss 0.9888 (1.1028) acc 84.3750 (72.7083) lr 3.6258e-04 eta 0:08:26
epoch [20/25] batch [20/250] time 0.289 (0.328) data 0.000 (0.039) loss 0.8735 (1.0922) acc 81.2500 (72.9688) lr 3.6258e-04 eta 0:08:05
epoch [20/25] batch [25/250] time 0.290 (0.321) data 0.000 (0.031) loss 1.1973 (1.1383) acc 68.7500 (72.0000) lr 3.6258e-04 eta 0:07:52
epoch [20/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.026) loss 1.2061 (1.1708) acc 68.7500 (71.3542) lr 3.6258e-04 eta 0:07:43
epoch [20/25] batch [35/250] time 0.290 (0.312) data 0.000 (0.022) loss 0.7065 (1.1693) acc 84.3750 (71.4286) lr 3.6258e-04 eta 0:07:36
epoch [20/25] batch [40/250] time 0.290 (0.309) data 0.000 (0.020) loss 1.2314 (1.1679) acc 68.7500 (71.4062) lr 3.6258e-04 eta 0:07:31
epoch [20/25] batch [45/250] time 0.290 (0.307) data 0.000 (0.017) loss 1.6680 (1.1803) acc 62.5000 (71.3889) lr 3.6258e-04 eta 0:07:26
epoch [20/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 0.7949 (1.1705) acc 90.6250 (71.9375) lr 3.6258e-04 eta 0:07:22
epoch [20/25] batch [55/250] time 0.289 (0.304) data 0.000 (0.014) loss 0.9731 (1.1655) acc 75.0000 (71.8182) lr 3.6258e-04 eta 0:07:18
epoch [20/25] batch [60/250] time 0.291 (0.303) data 0.000 (0.013) loss 1.5801 (1.1513) acc 71.8750 (72.1875) lr 3.6258e-04 eta 0:07:15
epoch [20/25] batch [65/250] time 0.289 (0.302) data 0.000 (0.012) loss 1.4424 (1.1608) acc 62.5000 (71.8750) lr 3.6258e-04 eta 0:07:12
epoch [20/25] batch [70/250] time 0.290 (0.301) data 0.001 (0.011) loss 0.7905 (1.1380) acc 84.3750 (72.3661) lr 3.6258e-04 eta 0:07:09
epoch [20/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.0527 (1.1457) acc 68.7500 (72.0000) lr 3.6258e-04 eta 0:07:07
epoch [20/25] batch [80/250] time 0.291 (0.299) data 0.000 (0.010) loss 1.2168 (1.1436) acc 75.0000 (72.2266) lr 3.6258e-04 eta 0:07:05
epoch [20/25] batch [85/250] time 0.291 (0.299) data 0.001 (0.009) loss 0.7192 (1.1459) acc 84.3750 (72.2794) lr 3.6258e-04 eta 0:07:02
epoch [20/25] batch [90/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.2002 (1.1556) acc 78.1250 (72.1528) lr 3.6258e-04 eta 0:07:00
epoch [20/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.008) loss 0.7236 (1.1465) acc 81.2500 (72.2697) lr 3.6258e-04 eta 0:06:58
epoch [20/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1377 (1.1560) acc 78.1250 (72.0938) lr 3.6258e-04 eta 0:06:56
epoch [20/25] batch [105/250] time 0.291 (0.297) data 0.001 (0.008) loss 1.0303 (1.1628) acc 71.8750 (71.9048) lr 3.6258e-04 eta 0:06:54
epoch [20/25] batch [110/250] time 0.290 (0.297) data 0.000 (0.007) loss 1.0527 (1.1678) acc 75.0000 (71.8466) lr 3.6258e-04 eta 0:06:52
epoch [20/25] batch [115/250] time 0.292 (0.297) data 0.000 (0.007) loss 1.0283 (1.1727) acc 81.2500 (71.7663) lr 3.6258e-04 eta 0:06:50
epoch [20/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.3047 (1.1714) acc 71.8750 (71.7448) lr 3.6258e-04 eta 0:06:48
epoch [20/25] batch [125/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.6943 (1.1694) acc 87.5000 (71.7750) lr 3.6258e-04 eta 0:06:46
epoch [20/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.3896 (1.1756) acc 59.3750 (71.4183) lr 3.6258e-04 eta 0:06:45
epoch [20/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1650 (1.1736) acc 78.1250 (71.5278) lr 3.6258e-04 eta 0:06:43
epoch [20/25] batch [140/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.1104 (1.1740) acc 71.8750 (71.4732) lr 3.6258e-04 eta 0:06:41
epoch [20/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.8564 (1.1728) acc 75.0000 (71.3793) lr 3.6258e-04 eta 0:06:39
epoch [20/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.005) loss 0.8833 (1.1794) acc 81.2500 (71.2083) lr 3.6258e-04 eta 0:06:38
epoch [20/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.0664 (1.1884) acc 68.7500 (70.9879) lr 3.6258e-04 eta 0:06:36
epoch [20/25] batch [160/250] time 0.290 (0.295) data 0.000 (0.005) loss 2.1016 (1.1925) acc 62.5000 (70.8789) lr 3.6258e-04 eta 0:06:34
epoch [20/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.5762 (1.1913) acc 71.8750 (71.0417) lr 3.6258e-04 eta 0:06:33
epoch [20/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7710 (1.1879) acc 71.8750 (71.0846) lr 3.6258e-04 eta 0:06:31
epoch [20/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.0488 (1.1831) acc 78.1250 (71.1786) lr 3.6258e-04 eta 0:06:29
epoch [20/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.6484 (1.1885) acc 53.1250 (70.9028) lr 3.6258e-04 eta 0:06:28
epoch [20/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.4326 (1.1877) acc 68.7500 (70.9459) lr 3.6258e-04 eta 0:06:26
epoch [20/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.6240 (1.1902) acc 71.8750 (71.0033) lr 3.6258e-04 eta 0:06:24
epoch [20/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.3945 (1.1921) acc 68.7500 (70.9776) lr 3.6258e-04 eta 0:06:23
epoch [20/25] batch [200/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.1377 (1.1943) acc 68.7500 (70.8750) lr 3.6258e-04 eta 0:06:21
epoch [20/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0605 (1.1927) acc 71.8750 (70.9146) lr 3.6258e-04 eta 0:06:20
epoch [20/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2090 (1.1948) acc 65.6250 (70.9226) lr 3.6258e-04 eta 0:06:18
epoch [20/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4326 (1.2005) acc 65.6250 (70.7703) lr 3.6258e-04 eta 0:06:16
epoch [20/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5938 (1.2013) acc 59.3750 (70.7244) lr 3.6258e-04 eta 0:06:15
epoch [20/25] batch [225/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.3535 (1.1978) acc 75.0000 (70.8333) lr 3.6258e-04 eta 0:06:13
epoch [20/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9927 (1.1930) acc 75.0000 (70.9103) lr 3.6258e-04 eta 0:06:12
epoch [20/25] batch [235/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.1641 (1.1927) acc 71.8750 (70.8777) lr 3.6258e-04 eta 0:06:10
epoch [20/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.8530 (1.1887) acc 81.2500 (70.9635) lr 3.6258e-04 eta 0:06:09
epoch [20/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.3027 (1.1897) acc 71.8750 (70.9566) lr 3.6258e-04 eta 0:06:07
epoch [20/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.5576 (1.1900) acc 59.3750 (70.9000) lr 2.7103e-04 eta 0:06:05
epoch [21/25] batch [5/250] time 0.289 (0.443) data 0.000 (0.154) loss 0.9995 (1.2994) acc 71.8750 (70.0000) lr 2.7103e-04 eta 0:09:11
epoch [21/25] batch [10/250] time 0.289 (0.366) data 0.000 (0.077) loss 1.3398 (1.3809) acc 68.7500 (67.8125) lr 2.7103e-04 eta 0:07:33
epoch [21/25] batch [15/250] time 0.289 (0.340) data 0.000 (0.051) loss 1.3281 (1.3792) acc 75.0000 (68.5417) lr 2.7103e-04 eta 0:07:00
epoch [21/25] batch [20/250] time 0.289 (0.328) data 0.000 (0.039) loss 0.8613 (1.3072) acc 78.1250 (70.4688) lr 2.7103e-04 eta 0:06:42
epoch [21/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 1.4658 (1.3370) acc 68.7500 (69.3750) lr 2.7103e-04 eta 0:06:31
epoch [21/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.026) loss 1.2705 (1.3195) acc 75.0000 (70.2083) lr 2.7103e-04 eta 0:06:24
epoch [21/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.0664 (1.3362) acc 71.8750 (69.3750) lr 2.7103e-04 eta 0:06:18
epoch [21/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.7998 (1.3079) acc 65.6250 (70.0781) lr 2.7103e-04 eta 0:06:13
epoch [21/25] batch [45/250] time 0.289 (0.306) data 0.000 (0.017) loss 1.2881 (1.2957) acc 62.5000 (69.7222) lr 2.7103e-04 eta 0:06:09
epoch [21/25] batch [50/250] time 0.289 (0.305) data 0.000 (0.016) loss 1.3965 (1.2891) acc 59.3750 (69.5000) lr 2.7103e-04 eta 0:06:05
epoch [21/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.014) loss 0.9292 (1.2801) acc 78.1250 (69.8864) lr 2.7103e-04 eta 0:06:02
epoch [21/25] batch [60/250] time 0.290 (0.302) data 0.000 (0.013) loss 0.9707 (1.2715) acc 78.1250 (70.0000) lr 2.7103e-04 eta 0:05:59
epoch [21/25] batch [65/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.2734 (1.2634) acc 71.8750 (70.0000) lr 2.7103e-04 eta 0:05:57
epoch [21/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.2969 (1.2495) acc 65.6250 (70.2679) lr 2.7103e-04 eta 0:05:54
epoch [21/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.5439 (1.2554) acc 68.7500 (70.0417) lr 2.7103e-04 eta 0:05:52
epoch [21/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.1660 (1.2560) acc 68.7500 (70.1172) lr 2.7103e-04 eta 0:05:49
epoch [21/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 2.1582 (1.2581) acc 46.8750 (69.8529) lr 2.7103e-04 eta 0:05:47
epoch [21/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.4150 (1.2531) acc 65.6250 (70.0694) lr 2.7103e-04 eta 0:05:45
epoch [21/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4375 (1.2611) acc 68.7500 (69.9671) lr 2.7103e-04 eta 0:05:43
epoch [21/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.1924 (1.2527) acc 62.5000 (70.0312) lr 2.7103e-04 eta 0:05:41
epoch [21/25] batch [105/250] time 0.291 (0.297) data 0.000 (0.008) loss 0.4517 (1.2475) acc 87.5000 (70.1190) lr 2.7103e-04 eta 0:05:39
epoch [21/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1514 (1.2405) acc 71.8750 (70.2273) lr 2.7103e-04 eta 0:05:37
epoch [21/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1484 (1.2414) acc 68.7500 (70.2717) lr 2.7103e-04 eta 0:05:36
epoch [21/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.8843 (1.2333) acc 78.1250 (70.3646) lr 2.7103e-04 eta 0:05:34
epoch [21/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.1123 (1.2413) acc 81.2500 (70.3250) lr 2.7103e-04 eta 0:05:32
epoch [21/25] batch [130/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9629 (1.2355) acc 78.1250 (70.4327) lr 2.7103e-04 eta 0:05:30
epoch [21/25] batch [135/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2432 (1.2361) acc 75.0000 (70.4167) lr 2.7103e-04 eta 0:05:29
epoch [21/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.2939 (1.2351) acc 68.7500 (70.4464) lr 2.7103e-04 eta 0:05:27
epoch [21/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.5557 (1.2374) acc 65.6250 (70.3879) lr 2.7103e-04 eta 0:05:25
epoch [21/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.005) loss 0.8320 (1.2376) acc 75.0000 (70.3958) lr 2.7103e-04 eta 0:05:24
epoch [21/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0137 (1.2335) acc 78.1250 (70.5040) lr 2.7103e-04 eta 0:05:22
epoch [21/25] batch [160/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.8623 (1.2365) acc 56.2500 (70.4102) lr 2.7103e-04 eta 0:05:20
epoch [21/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.4678 (1.2361) acc 78.1250 (70.4924) lr 2.7103e-04 eta 0:05:19
epoch [21/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.9023 (1.2362) acc 62.5000 (70.4779) lr 2.7103e-04 eta 0:05:17
epoch [21/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.4629 (1.2424) acc 71.8750 (70.3393) lr 2.7103e-04 eta 0:05:15
epoch [21/25] batch [180/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2188 (1.2442) acc 65.6250 (70.2951) lr 2.7103e-04 eta 0:05:14
epoch [21/25] batch [185/250] time 0.290 (0.294) data 0.000 (0.004) loss 0.6938 (1.2445) acc 78.1250 (70.3378) lr 2.7103e-04 eta 0:05:12
epoch [21/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.004) loss 1.2725 (1.2410) acc 78.1250 (70.5428) lr 2.7103e-04 eta 0:05:11
epoch [21/25] batch [195/250] time 0.291 (0.293) data 0.001 (0.004) loss 1.2061 (1.2427) acc 71.8750 (70.4968) lr 2.7103e-04 eta 0:05:09
epoch [21/25] batch [200/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.0322 (1.2408) acc 68.7500 (70.5000) lr 2.7103e-04 eta 0:05:08
epoch [21/25] batch [205/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.3408 (1.2369) acc 68.7500 (70.5640) lr 2.7103e-04 eta 0:05:06
epoch [21/25] batch [210/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9551 (1.2373) acc 65.6250 (70.5060) lr 2.7103e-04 eta 0:05:04
epoch [21/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0439 (1.2324) acc 68.7500 (70.5814) lr 2.7103e-04 eta 0:05:03
epoch [21/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.7402 (1.2323) acc 68.7500 (70.6534) lr 2.7103e-04 eta 0:05:01
epoch [21/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3115 (1.2301) acc 68.7500 (70.7222) lr 2.7103e-04 eta 0:05:00
epoch [21/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5977 (1.2289) acc 59.3750 (70.7201) lr 2.7103e-04 eta 0:04:58
epoch [21/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9136 (1.2273) acc 75.0000 (70.6649) lr 2.7103e-04 eta 0:04:57
epoch [21/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.5156 (1.2313) acc 62.5000 (70.5729) lr 2.7103e-04 eta 0:04:55
epoch [21/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.0469 (1.2286) acc 68.7500 (70.5230) lr 2.7103e-04 eta 0:04:54
epoch [21/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.003) loss 0.7871 (1.2286) acc 81.2500 (70.5375) lr 1.9098e-04 eta 0:04:52
epoch [22/25] batch [5/250] time 0.290 (0.446) data 0.000 (0.156) loss 0.9287 (1.0367) acc 78.1250 (75.0000) lr 1.9098e-04 eta 0:07:23
epoch [22/25] batch [10/250] time 0.290 (0.368) data 0.000 (0.078) loss 1.0000 (1.0840) acc 71.8750 (71.5625) lr 1.9098e-04 eta 0:06:04
epoch [22/25] batch [15/250] time 0.291 (0.342) data 0.000 (0.052) loss 1.5410 (1.1523) acc 62.5000 (69.7917) lr 1.9098e-04 eta 0:05:36
epoch [22/25] batch [20/250] time 0.291 (0.329) data 0.000 (0.039) loss 0.9395 (1.1480) acc 78.1250 (69.6875) lr 1.9098e-04 eta 0:05:22
epoch [22/25] batch [25/250] time 0.290 (0.321) data 0.001 (0.031) loss 1.3799 (1.1350) acc 62.5000 (69.8750) lr 1.9098e-04 eta 0:05:13
epoch [22/25] batch [30/250] time 0.290 (0.316) data 0.000 (0.026) loss 0.6401 (1.1414) acc 84.3750 (69.7917) lr 1.9098e-04 eta 0:05:06
epoch [22/25] batch [35/250] time 0.290 (0.312) data 0.000 (0.023) loss 0.8013 (1.1243) acc 84.3750 (70.2679) lr 1.9098e-04 eta 0:05:01
epoch [22/25] batch [40/250] time 0.290 (0.309) data 0.000 (0.020) loss 1.5645 (1.1588) acc 65.6250 (69.8438) lr 1.9098e-04 eta 0:04:56
epoch [22/25] batch [45/250] time 0.290 (0.307) data 0.000 (0.018) loss 0.6802 (1.1443) acc 84.3750 (70.2778) lr 1.9098e-04 eta 0:04:53
epoch [22/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.0225 (1.1461) acc 78.1250 (70.1250) lr 1.9098e-04 eta 0:04:50
epoch [22/25] batch [55/250] time 0.290 (0.304) data 0.000 (0.014) loss 1.3857 (1.1399) acc 65.6250 (70.7386) lr 1.9098e-04 eta 0:04:47
epoch [22/25] batch [60/250] time 0.291 (0.303) data 0.000 (0.013) loss 1.1338 (1.1205) acc 81.2500 (71.4062) lr 1.9098e-04 eta 0:04:44
epoch [22/25] batch [65/250] time 0.290 (0.302) data 0.000 (0.012) loss 1.5088 (1.1342) acc 59.3750 (71.1538) lr 1.9098e-04 eta 0:04:42
epoch [22/25] batch [70/250] time 0.291 (0.301) data 0.001 (0.011) loss 1.1123 (1.1248) acc 71.8750 (71.5179) lr 1.9098e-04 eta 0:04:39
epoch [22/25] batch [75/250] time 0.289 (0.300) data 0.000 (0.011) loss 1.3623 (1.1272) acc 65.6250 (71.4167) lr 1.9098e-04 eta 0:04:37
epoch [22/25] batch [80/250] time 0.289 (0.300) data 0.000 (0.010) loss 0.8706 (1.1272) acc 71.8750 (71.3281) lr 1.9098e-04 eta 0:04:35
epoch [22/25] batch [85/250] time 0.291 (0.299) data 0.001 (0.010) loss 0.8750 (1.1180) acc 78.1250 (71.6176) lr 1.9098e-04 eta 0:04:33
epoch [22/25] batch [90/250] time 0.289 (0.299) data 0.000 (0.009) loss 0.9253 (1.1195) acc 62.5000 (71.3889) lr 1.9098e-04 eta 0:04:31
epoch [22/25] batch [95/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.6133 (1.1200) acc 62.5000 (71.5461) lr 1.9098e-04 eta 0:04:29
epoch [22/25] batch [100/250] time 0.290 (0.298) data 0.000 (0.008) loss 1.2910 (1.1288) acc 68.7500 (71.4375) lr 1.9098e-04 eta 0:04:27
epoch [22/25] batch [105/250] time 0.290 (0.297) data 0.000 (0.008) loss 1.0742 (1.1329) acc 78.1250 (71.3988) lr 1.9098e-04 eta 0:04:26
epoch [22/25] batch [110/250] time 0.289 (0.297) data 0.000 (0.007) loss 0.9634 (1.1345) acc 78.1250 (71.3352) lr 1.9098e-04 eta 0:04:24
epoch [22/25] batch [115/250] time 0.289 (0.297) data 0.000 (0.007) loss 1.2793 (1.1370) acc 68.7500 (71.4674) lr 1.9098e-04 eta 0:04:22
epoch [22/25] batch [120/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.2910 (1.1394) acc 68.7500 (71.4062) lr 1.9098e-04 eta 0:04:20
epoch [22/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.9683 (1.1429) acc 71.8750 (71.3250) lr 1.9098e-04 eta 0:04:19
epoch [22/25] batch [130/250] time 0.290 (0.296) data 0.000 (0.006) loss 1.2520 (1.1415) acc 71.8750 (71.4183) lr 1.9098e-04 eta 0:04:17
epoch [22/25] batch [135/250] time 0.289 (0.296) data 0.000 (0.006) loss 1.1328 (1.1419) acc 62.5000 (71.3426) lr 1.9098e-04 eta 0:04:15
epoch [22/25] batch [140/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.7163 (1.1365) acc 84.3750 (71.6518) lr 1.9098e-04 eta 0:04:13
epoch [22/25] batch [145/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.9204 (1.1362) acc 71.8750 (71.6379) lr 1.9098e-04 eta 0:04:12
epoch [22/25] batch [150/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.2080 (1.1386) acc 71.8750 (71.6667) lr 1.9098e-04 eta 0:04:10
epoch [22/25] batch [155/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.5205 (1.1422) acc 59.3750 (71.5927) lr 1.9098e-04 eta 0:04:09
epoch [22/25] batch [160/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.3438 (1.1441) acc 71.8750 (71.5625) lr 1.9098e-04 eta 0:04:07
epoch [22/25] batch [165/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9600 (1.1416) acc 75.0000 (71.5530) lr 1.9098e-04 eta 0:04:05
epoch [22/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.5205 (1.1440) acc 68.7500 (71.5257) lr 1.9098e-04 eta 0:04:04
epoch [22/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.0664 (1.1452) acc 71.8750 (71.5714) lr 1.9098e-04 eta 0:04:02
epoch [22/25] batch [180/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2598 (1.1510) acc 62.5000 (71.4410) lr 1.9098e-04 eta 0:04:01
epoch [22/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9092 (1.1499) acc 75.0000 (71.4189) lr 1.9098e-04 eta 0:03:59
epoch [22/25] batch [190/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.7812 (1.1575) acc 59.3750 (71.2664) lr 1.9098e-04 eta 0:03:57
epoch [22/25] batch [195/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.4238 (1.1598) acc 68.7500 (71.2500) lr 1.9098e-04 eta 0:03:56
epoch [22/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4551 (1.1637) acc 71.8750 (71.2344) lr 1.9098e-04 eta 0:03:54
epoch [22/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8447 (1.1664) acc 81.2500 (71.1280) lr 1.9098e-04 eta 0:03:53
epoch [22/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.7144 (1.1674) acc 84.3750 (71.1756) lr 1.9098e-04 eta 0:03:51
epoch [22/25] batch [215/250] time 0.290 (0.293) data 0.000 (0.004) loss 0.9639 (1.1668) acc 78.1250 (71.1628) lr 1.9098e-04 eta 0:03:50
epoch [22/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.6328 (1.1717) acc 56.2500 (70.9943) lr 1.9098e-04 eta 0:03:48
epoch [22/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0459 (1.1714) acc 65.6250 (70.9861) lr 1.9098e-04 eta 0:03:47
epoch [22/25] batch [230/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.4873 (1.1736) acc 75.0000 (71.0190) lr 1.9098e-04 eta 0:03:45
epoch [22/25] batch [235/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1592 (1.1738) acc 68.7500 (71.0239) lr 1.9098e-04 eta 0:03:44
epoch [22/25] batch [240/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.3037 (1.1746) acc 75.0000 (71.0286) lr 1.9098e-04 eta 0:03:42
epoch [22/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.1279 (1.1703) acc 68.7500 (71.1224) lr 1.9098e-04 eta 0:03:40
epoch [22/25] batch [250/250] time 0.290 (0.293) data 0.000 (0.003) loss 1.1309 (1.1701) acc 75.0000 (71.1625) lr 1.2369e-04 eta 0:03:39
epoch [23/25] batch [5/250] time 0.290 (0.436) data 0.000 (0.146) loss 1.0410 (1.1882) acc 78.1250 (75.0000) lr 1.2369e-04 eta 0:05:25
epoch [23/25] batch [10/250] time 0.289 (0.363) data 0.000 (0.073) loss 1.5049 (1.2664) acc 68.7500 (71.8750) lr 1.2369e-04 eta 0:04:28
epoch [23/25] batch [15/250] time 0.289 (0.338) data 0.000 (0.049) loss 1.2803 (1.2711) acc 75.0000 (71.6667) lr 1.2369e-04 eta 0:04:08
epoch [23/25] batch [20/250] time 0.288 (0.326) data 0.000 (0.037) loss 1.0352 (1.2766) acc 81.2500 (71.2500) lr 1.2369e-04 eta 0:03:57
epoch [23/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 1.2080 (1.2612) acc 62.5000 (70.2500) lr 1.2369e-04 eta 0:03:50
epoch [23/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.6782 (1.2398) acc 81.2500 (70.3125) lr 1.2369e-04 eta 0:03:45
epoch [23/25] batch [35/250] time 0.289 (0.310) data 0.000 (0.021) loss 1.3555 (1.2261) acc 65.6250 (70.8036) lr 1.2369e-04 eta 0:03:41
epoch [23/25] batch [40/250] time 0.291 (0.307) data 0.000 (0.018) loss 1.7480 (1.2501) acc 59.3750 (70.6250) lr 1.2369e-04 eta 0:03:38
epoch [23/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.2158 (1.2376) acc 75.0000 (71.3889) lr 1.2369e-04 eta 0:03:35
epoch [23/25] batch [50/250] time 0.289 (0.304) data 0.000 (0.015) loss 0.8804 (1.2349) acc 78.1250 (71.3125) lr 1.2369e-04 eta 0:03:32
epoch [23/25] batch [55/250] time 0.289 (0.302) data 0.000 (0.013) loss 0.8452 (1.2190) acc 75.0000 (71.7045) lr 1.2369e-04 eta 0:03:30
epoch [23/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.2607 (1.2121) acc 62.5000 (71.4583) lr 1.2369e-04 eta 0:03:27
epoch [23/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.6494 (1.2042) acc 84.3750 (71.6346) lr 1.2369e-04 eta 0:03:25
epoch [23/25] batch [70/250] time 0.289 (0.299) data 0.000 (0.011) loss 0.6006 (1.1929) acc 81.2500 (71.8304) lr 1.2369e-04 eta 0:03:23
epoch [23/25] batch [75/250] time 0.290 (0.299) data 0.000 (0.010) loss 1.0098 (1.1893) acc 81.2500 (71.7917) lr 1.2369e-04 eta 0:03:21
epoch [23/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.0977 (1.1965) acc 68.7500 (71.9141) lr 1.2369e-04 eta 0:03:19
epoch [23/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9175 (1.1841) acc 71.8750 (72.0588) lr 1.2369e-04 eta 0:03:17
epoch [23/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 0.6011 (1.1646) acc 87.5000 (72.5694) lr 1.2369e-04 eta 0:03:16
epoch [23/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4111 (1.1648) acc 65.6250 (72.6645) lr 1.2369e-04 eta 0:03:14
epoch [23/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.008) loss 1.2861 (1.1599) acc 68.7500 (72.7500) lr 1.2369e-04 eta 0:03:12
epoch [23/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1299 (1.1537) acc 78.1250 (72.9762) lr 1.2369e-04 eta 0:03:10
epoch [23/25] batch [110/250] time 0.290 (0.296) data 0.000 (0.007) loss 1.4463 (1.1549) acc 56.2500 (72.9261) lr 1.2369e-04 eta 0:03:09
epoch [23/25] batch [115/250] time 0.289 (0.295) data 0.000 (0.007) loss 1.6504 (1.1619) acc 68.7500 (72.7717) lr 1.2369e-04 eta 0:03:07
epoch [23/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9985 (1.1618) acc 71.8750 (72.6823) lr 1.2369e-04 eta 0:03:05
epoch [23/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8857 (1.1538) acc 81.2500 (72.9250) lr 1.2369e-04 eta 0:03:04
epoch [23/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.4170 (1.1575) acc 50.0000 (72.7644) lr 1.2369e-04 eta 0:03:02
epoch [23/25] batch [135/250] time 0.289 (0.294) data 0.000 (0.006) loss 1.3203 (1.1572) acc 71.8750 (72.7546) lr 1.2369e-04 eta 0:03:01
epoch [23/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9521 (1.1641) acc 75.0000 (72.6786) lr 1.2369e-04 eta 0:02:59
epoch [23/25] batch [145/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2432 (1.1619) acc 71.8750 (72.7586) lr 1.2369e-04 eta 0:02:57
epoch [23/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.8218 (1.1637) acc 71.8750 (72.7292) lr 1.2369e-04 eta 0:02:56
epoch [23/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.1357 (1.1674) acc 68.7500 (72.5605) lr 1.2369e-04 eta 0:02:54
epoch [23/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0098 (1.1673) acc 71.8750 (72.6172) lr 1.2369e-04 eta 0:02:53
epoch [23/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2734 (1.1704) acc 68.7500 (72.5379) lr 1.2369e-04 eta 0:02:51
epoch [23/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.005) loss 1.0488 (1.1689) acc 81.2500 (72.5551) lr 1.2369e-04 eta 0:02:50
epoch [23/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1572 (1.1676) acc 81.2500 (72.5714) lr 1.2369e-04 eta 0:02:48
epoch [23/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9058 (1.1647) acc 81.2500 (72.6389) lr 1.2369e-04 eta 0:02:47
epoch [23/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8750 (1.1645) acc 81.2500 (72.6858) lr 1.2369e-04 eta 0:02:45
epoch [23/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1396 (1.1623) acc 78.1250 (72.6480) lr 1.2369e-04 eta 0:02:44
epoch [23/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1025 (1.1623) acc 71.8750 (72.6442) lr 1.2369e-04 eta 0:02:42
epoch [23/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9595 (1.1577) acc 65.6250 (72.7031) lr 1.2369e-04 eta 0:02:41
epoch [23/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6836 (1.1573) acc 90.6250 (72.7896) lr 1.2369e-04 eta 0:02:39
epoch [23/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.2119 (1.1557) acc 65.6250 (72.8274) lr 1.2369e-04 eta 0:02:38
epoch [23/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1543 (1.1556) acc 75.0000 (72.7907) lr 1.2369e-04 eta 0:02:36
epoch [23/25] batch [220/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0234 (1.1562) acc 68.7500 (72.8125) lr 1.2369e-04 eta 0:02:34
epoch [23/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.004) loss 1.0664 (1.1587) acc 71.8750 (72.7361) lr 1.2369e-04 eta 0:02:33
epoch [23/25] batch [230/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.9385 (1.1619) acc 78.1250 (72.7038) lr 1.2369e-04 eta 0:02:31
epoch [23/25] batch [235/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.0332 (1.1574) acc 71.8750 (72.7261) lr 1.2369e-04 eta 0:02:30
epoch [23/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.1094 (1.1527) acc 71.8750 (72.7604) lr 1.2369e-04 eta 0:02:29
epoch [23/25] batch [245/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.3906 (1.1551) acc 71.8750 (72.7041) lr 1.2369e-04 eta 0:02:27
epoch [23/25] batch [250/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.2568 (1.1568) acc 71.8750 (72.6625) lr 7.0224e-05 eta 0:02:26
epoch [24/25] batch [5/250] time 0.291 (0.443) data 0.001 (0.153) loss 1.0039 (1.0614) acc 75.0000 (71.2500) lr 7.0224e-05 eta 0:03:39
epoch [24/25] batch [10/250] time 0.289 (0.366) data 0.000 (0.077) loss 1.1758 (1.1191) acc 78.1250 (71.5625) lr 7.0224e-05 eta 0:02:59
epoch [24/25] batch [15/250] time 0.288 (0.340) data 0.000 (0.051) loss 1.0625 (1.1164) acc 71.8750 (72.5000) lr 7.0224e-05 eta 0:02:45
epoch [24/25] batch [20/250] time 0.289 (0.327) data 0.000 (0.039) loss 1.3799 (1.1986) acc 71.8750 (71.8750) lr 7.0224e-05 eta 0:02:37
epoch [24/25] batch [25/250] time 0.289 (0.320) data 0.000 (0.031) loss 0.6455 (1.1323) acc 87.5000 (73.6250) lr 7.0224e-05 eta 0:02:31
epoch [24/25] batch [30/250] time 0.290 (0.315) data 0.000 (0.026) loss 0.8931 (1.0854) acc 78.1250 (74.4792) lr 7.0224e-05 eta 0:02:27
epoch [24/25] batch [35/250] time 0.289 (0.311) data 0.000 (0.022) loss 1.5947 (1.0799) acc 59.3750 (74.7321) lr 7.0224e-05 eta 0:02:24
epoch [24/25] batch [40/250] time 0.289 (0.308) data 0.000 (0.019) loss 1.2529 (1.0735) acc 68.7500 (74.6875) lr 7.0224e-05 eta 0:02:21
epoch [24/25] batch [45/250] time 0.290 (0.306) data 0.001 (0.017) loss 1.2246 (1.0707) acc 68.7500 (74.4444) lr 7.0224e-05 eta 0:02:19
epoch [24/25] batch [50/250] time 0.290 (0.305) data 0.000 (0.016) loss 1.3867 (1.1054) acc 71.8750 (74.0000) lr 7.0224e-05 eta 0:02:17
epoch [24/25] batch [55/250] time 0.290 (0.303) data 0.000 (0.014) loss 1.3564 (1.1347) acc 71.8750 (73.4091) lr 7.0224e-05 eta 0:02:15
epoch [24/25] batch [60/250] time 0.290 (0.302) data 0.000 (0.013) loss 1.3203 (1.1372) acc 65.6250 (73.2292) lr 7.0224e-05 eta 0:02:12
epoch [24/25] batch [65/250] time 0.290 (0.301) data 0.000 (0.012) loss 1.0264 (1.1522) acc 71.8750 (72.7404) lr 7.0224e-05 eta 0:02:11
epoch [24/25] batch [70/250] time 0.290 (0.300) data 0.001 (0.011) loss 0.7847 (1.1493) acc 78.1250 (72.7232) lr 7.0224e-05 eta 0:02:09
epoch [24/25] batch [75/250] time 0.290 (0.300) data 0.000 (0.011) loss 1.1611 (1.1435) acc 71.8750 (72.6250) lr 7.0224e-05 eta 0:02:07
epoch [24/25] batch [80/250] time 0.290 (0.299) data 0.000 (0.010) loss 0.9170 (1.1401) acc 84.3750 (72.5781) lr 7.0224e-05 eta 0:02:05
epoch [24/25] batch [85/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9795 (1.1385) acc 75.0000 (72.5000) lr 7.0224e-05 eta 0:02:03
epoch [24/25] batch [90/250] time 0.289 (0.298) data 0.000 (0.009) loss 0.9399 (1.1349) acc 78.1250 (72.5694) lr 7.0224e-05 eta 0:02:02
epoch [24/25] batch [95/250] time 0.289 (0.298) data 0.000 (0.008) loss 1.4297 (1.1347) acc 65.6250 (72.4671) lr 7.0224e-05 eta 0:02:00
epoch [24/25] batch [100/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4160 (1.1376) acc 65.6250 (72.4688) lr 7.0224e-05 eta 0:01:58
epoch [24/25] batch [105/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.0605 (1.1319) acc 87.5000 (72.5000) lr 7.0224e-05 eta 0:01:57
epoch [24/25] batch [110/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.9023 (1.1214) acc 87.5000 (72.8409) lr 7.0224e-05 eta 0:01:55
epoch [24/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0371 (1.1171) acc 71.8750 (72.8533) lr 7.0224e-05 eta 0:01:54
epoch [24/25] batch [120/250] time 0.290 (0.296) data 0.000 (0.007) loss 0.6455 (1.1203) acc 87.5000 (72.8125) lr 7.0224e-05 eta 0:01:52
epoch [24/25] batch [125/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.8501 (1.1247) acc 81.2500 (72.8000) lr 7.0224e-05 eta 0:01:50
epoch [24/25] batch [130/250] time 0.290 (0.295) data 0.000 (0.006) loss 1.6016 (1.1255) acc 59.3750 (72.7885) lr 7.0224e-05 eta 0:01:49
epoch [24/25] batch [135/250] time 0.290 (0.295) data 0.000 (0.006) loss 0.7012 (1.1224) acc 84.3750 (72.8704) lr 7.0224e-05 eta 0:01:47
epoch [24/25] batch [140/250] time 0.291 (0.295) data 0.001 (0.006) loss 1.1035 (1.1225) acc 78.1250 (72.9241) lr 7.0224e-05 eta 0:01:46
epoch [24/25] batch [145/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.9297 (1.1233) acc 81.2500 (72.7802) lr 7.0224e-05 eta 0:01:44
epoch [24/25] batch [150/250] time 0.289 (0.295) data 0.000 (0.005) loss 1.3330 (1.1235) acc 68.7500 (72.7917) lr 7.0224e-05 eta 0:01:43
epoch [24/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 2.3848 (1.1326) acc 53.1250 (72.7016) lr 7.0224e-05 eta 0:01:41
epoch [24/25] batch [160/250] time 0.290 (0.294) data 0.000 (0.005) loss 0.9399 (1.1335) acc 78.1250 (72.8125) lr 7.0224e-05 eta 0:01:40
epoch [24/25] batch [165/250] time 0.290 (0.294) data 0.001 (0.005) loss 1.1934 (1.1368) acc 71.8750 (72.7652) lr 7.0224e-05 eta 0:01:38
epoch [24/25] batch [170/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.7832 (1.1395) acc 75.0000 (72.5368) lr 7.0224e-05 eta 0:01:37
epoch [24/25] batch [175/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.6514 (1.1453) acc 62.5000 (72.5000) lr 7.0224e-05 eta 0:01:35
epoch [24/25] batch [180/250] time 0.292 (0.294) data 0.000 (0.005) loss 0.9185 (1.1491) acc 75.0000 (72.4306) lr 7.0224e-05 eta 0:01:33
epoch [24/25] batch [185/250] time 0.289 (0.294) data 0.000 (0.004) loss 1.4873 (1.1555) acc 65.6250 (72.2466) lr 7.0224e-05 eta 0:01:32
epoch [24/25] batch [190/250] time 0.290 (0.294) data 0.000 (0.004) loss 2.0273 (1.1597) acc 68.7500 (72.1382) lr 7.0224e-05 eta 0:01:30
epoch [24/25] batch [195/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.6797 (1.1628) acc 59.3750 (72.0353) lr 7.0224e-05 eta 0:01:29
epoch [24/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8335 (1.1608) acc 81.2500 (72.0938) lr 7.0224e-05 eta 0:01:28
epoch [24/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.6514 (1.1550) acc 84.3750 (72.3171) lr 7.0224e-05 eta 0:01:26
epoch [24/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.5439 (1.1554) acc 65.6250 (72.3214) lr 7.0224e-05 eta 0:01:25
epoch [24/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8521 (1.1530) acc 78.1250 (72.3401) lr 7.0224e-05 eta 0:01:23
epoch [24/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1113 (1.1538) acc 62.5000 (72.2443) lr 7.0224e-05 eta 0:01:22
epoch [24/25] batch [225/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1279 (1.1538) acc 75.0000 (72.3472) lr 7.0224e-05 eta 0:01:20
epoch [24/25] batch [230/250] time 0.290 (0.293) data 0.000 (0.004) loss 1.2793 (1.1550) acc 75.0000 (72.3641) lr 7.0224e-05 eta 0:01:19
epoch [24/25] batch [235/250] time 0.291 (0.293) data 0.000 (0.004) loss 1.3730 (1.1598) acc 65.6250 (72.2340) lr 7.0224e-05 eta 0:01:17
epoch [24/25] batch [240/250] time 0.290 (0.293) data 0.000 (0.003) loss 0.8857 (1.1574) acc 87.5000 (72.3568) lr 7.0224e-05 eta 0:01:16
epoch [24/25] batch [245/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.2451 (1.1577) acc 71.8750 (72.2704) lr 7.0224e-05 eta 0:01:14
epoch [24/25] batch [250/250] time 0.289 (0.293) data 0.000 (0.003) loss 1.2773 (1.1564) acc 68.7500 (72.4125) lr 3.1417e-05 eta 0:01:13
epoch [25/25] batch [5/250] time 0.289 (0.433) data 0.000 (0.142) loss 1.3887 (1.2376) acc 68.7500 (75.0000) lr 3.1417e-05 eta 0:01:45
epoch [25/25] batch [10/250] time 0.289 (0.361) data 0.000 (0.071) loss 1.3457 (1.2793) acc 62.5000 (72.8125) lr 3.1417e-05 eta 0:01:26
epoch [25/25] batch [15/250] time 0.289 (0.337) data 0.000 (0.048) loss 0.8740 (1.2289) acc 81.2500 (72.7083) lr 3.1417e-05 eta 0:01:19
epoch [25/25] batch [20/250] time 0.289 (0.325) data 0.000 (0.036) loss 1.3086 (1.2045) acc 68.7500 (72.0312) lr 3.1417e-05 eta 0:01:14
epoch [25/25] batch [25/250] time 0.289 (0.318) data 0.000 (0.029) loss 0.9883 (1.2107) acc 75.0000 (71.2500) lr 3.1417e-05 eta 0:01:11
epoch [25/25] batch [30/250] time 0.289 (0.313) data 0.000 (0.024) loss 0.9204 (1.2042) acc 78.1250 (71.1458) lr 3.1417e-05 eta 0:01:08
epoch [25/25] batch [35/250] time 0.290 (0.310) data 0.000 (0.021) loss 1.7119 (1.2040) acc 65.6250 (71.1607) lr 3.1417e-05 eta 0:01:06
epoch [25/25] batch [40/250] time 0.289 (0.307) data 0.000 (0.018) loss 1.0762 (1.1829) acc 65.6250 (71.4844) lr 3.1417e-05 eta 0:01:04
epoch [25/25] batch [45/250] time 0.290 (0.305) data 0.000 (0.016) loss 0.8413 (1.1716) acc 71.8750 (71.6667) lr 3.1417e-05 eta 0:01:02
epoch [25/25] batch [50/250] time 0.290 (0.304) data 0.000 (0.015) loss 1.5537 (1.1702) acc 59.3750 (71.8750) lr 3.1417e-05 eta 0:01:00
epoch [25/25] batch [55/250] time 0.289 (0.303) data 0.000 (0.013) loss 1.0176 (1.1911) acc 68.7500 (71.3068) lr 3.1417e-05 eta 0:00:58
epoch [25/25] batch [60/250] time 0.289 (0.301) data 0.000 (0.012) loss 1.6738 (1.1915) acc 59.3750 (71.3542) lr 3.1417e-05 eta 0:00:57
epoch [25/25] batch [65/250] time 0.289 (0.300) data 0.000 (0.011) loss 0.8643 (1.1781) acc 78.1250 (71.7308) lr 3.1417e-05 eta 0:00:55
epoch [25/25] batch [70/250] time 0.289 (0.300) data 0.000 (0.010) loss 1.3330 (1.1903) acc 78.1250 (71.6071) lr 3.1417e-05 eta 0:00:53
epoch [25/25] batch [75/250] time 0.289 (0.299) data 0.000 (0.010) loss 0.6587 (1.1814) acc 84.3750 (71.8333) lr 3.1417e-05 eta 0:00:52
epoch [25/25] batch [80/250] time 0.289 (0.298) data 0.000 (0.009) loss 1.6484 (1.1991) acc 65.6250 (71.5234) lr 3.1417e-05 eta 0:00:50
epoch [25/25] batch [85/250] time 0.290 (0.298) data 0.000 (0.009) loss 1.4473 (1.1953) acc 65.6250 (71.6176) lr 3.1417e-05 eta 0:00:49
epoch [25/25] batch [90/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.4141 (1.1929) acc 71.8750 (71.8403) lr 3.1417e-05 eta 0:00:47
epoch [25/25] batch [95/250] time 0.289 (0.297) data 0.000 (0.008) loss 1.5869 (1.1988) acc 71.8750 (71.7434) lr 3.1417e-05 eta 0:00:46
epoch [25/25] batch [100/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.0391 (1.1993) acc 71.8750 (71.6875) lr 3.1417e-05 eta 0:00:44
epoch [25/25] batch [105/250] time 0.289 (0.296) data 0.000 (0.007) loss 1.1602 (1.2011) acc 78.1250 (71.7560) lr 3.1417e-05 eta 0:00:42
epoch [25/25] batch [110/250] time 0.289 (0.296) data 0.000 (0.007) loss 0.5811 (1.1958) acc 87.5000 (72.0739) lr 3.1417e-05 eta 0:00:41
epoch [25/25] batch [115/250] time 0.289 (0.296) data 0.000 (0.006) loss 0.8394 (1.1856) acc 87.5000 (72.2826) lr 3.1417e-05 eta 0:00:39
epoch [25/25] batch [120/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8921 (1.1732) acc 75.0000 (72.5260) lr 3.1417e-05 eta 0:00:38
epoch [25/25] batch [125/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.0596 (1.1693) acc 81.2500 (72.3750) lr 3.1417e-05 eta 0:00:36
epoch [25/25] batch [130/250] time 0.289 (0.295) data 0.000 (0.006) loss 1.1836 (1.1640) acc 71.8750 (72.4279) lr 3.1417e-05 eta 0:00:35
epoch [25/25] batch [135/250] time 0.289 (0.295) data 0.000 (0.006) loss 0.8481 (1.1644) acc 81.2500 (72.5000) lr 3.1417e-05 eta 0:00:33
epoch [25/25] batch [140/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.0410 (1.1676) acc 71.8750 (72.5000) lr 3.1417e-05 eta 0:00:32
epoch [25/25] batch [145/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9263 (1.1679) acc 81.2500 (72.5000) lr 3.1417e-05 eta 0:00:30
epoch [25/25] batch [150/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2197 (1.1642) acc 65.6250 (72.5000) lr 3.1417e-05 eta 0:00:29
epoch [25/25] batch [155/250] time 0.289 (0.294) data 0.000 (0.005) loss 0.9678 (1.1612) acc 71.8750 (72.5403) lr 3.1417e-05 eta 0:00:27
epoch [25/25] batch [160/250] time 0.289 (0.294) data 0.000 (0.005) loss 1.2236 (1.1617) acc 65.6250 (72.5195) lr 3.1417e-05 eta 0:00:26
epoch [25/25] batch [165/250] time 0.290 (0.294) data 0.000 (0.005) loss 1.2764 (1.1663) acc 75.0000 (72.5000) lr 3.1417e-05 eta 0:00:24
epoch [25/25] batch [170/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8447 (1.1650) acc 87.5000 (72.5368) lr 3.1417e-05 eta 0:00:23
epoch [25/25] batch [175/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0625 (1.1685) acc 78.1250 (72.5000) lr 3.1417e-05 eta 0:00:22
epoch [25/25] batch [180/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9741 (1.1728) acc 78.1250 (72.5174) lr 3.1417e-05 eta 0:00:20
epoch [25/25] batch [185/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8496 (1.1710) acc 81.2500 (72.4493) lr 3.1417e-05 eta 0:00:19
epoch [25/25] batch [190/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9287 (1.1710) acc 71.8750 (72.4671) lr 3.1417e-05 eta 0:00:17
epoch [25/25] batch [195/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9834 (1.1715) acc 68.7500 (72.3718) lr 3.1417e-05 eta 0:00:16
epoch [25/25] batch [200/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.9585 (1.1705) acc 71.8750 (72.4219) lr 3.1417e-05 eta 0:00:14
epoch [25/25] batch [205/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.1494 (1.1695) acc 75.0000 (72.3780) lr 3.1417e-05 eta 0:00:13
epoch [25/25] batch [210/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8154 (1.1671) acc 81.2500 (72.4405) lr 3.1417e-05 eta 0:00:11
epoch [25/25] batch [215/250] time 0.289 (0.293) data 0.000 (0.004) loss 0.8779 (1.1663) acc 84.3750 (72.5145) lr 3.1417e-05 eta 0:00:10
epoch [25/25] batch [220/250] time 0.289 (0.293) data 0.000 (0.004) loss 1.0625 (1.1667) acc 75.0000 (72.5284) lr 3.1417e-05 eta 0:00:08
epoch [25/25] batch [225/250] time 0.289 (0.292) data 0.000 (0.003) loss 0.7822 (1.1652) acc 81.2500 (72.6111) lr 3.1417e-05 eta 0:00:07
epoch [25/25] batch [230/250] time 0.290 (0.292) data 0.001 (0.003) loss 1.1455 (1.1601) acc 78.1250 (72.7582) lr 3.1417e-05 eta 0:00:05
epoch [25/25] batch [235/250] time 0.291 (0.292) data 0.000 (0.003) loss 1.4277 (1.1576) acc 59.3750 (72.7261) lr 3.1417e-05 eta 0:00:04
epoch [25/25] batch [240/250] time 0.289 (0.292) data 0.000 (0.003) loss 1.5449 (1.1619) acc 65.6250 (72.6302) lr 3.1417e-05 eta 0:00:02
epoch [25/25] batch [245/250] time 0.290 (0.292) data 0.000 (0.003) loss 0.6533 (1.1577) acc 81.2500 (72.5893) lr 3.1417e-05 eta 0:00:01
epoch [25/25] batch [250/250] time 0.290 (0.292) data 0.000 (0.003) loss 1.2617 (1.1531) acc 71.8750 (72.6250) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-25
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:06<05:12,  6.38s/it]  4%|▍         | 2/50 [00:07<02:25,  3.03s/it]  6%|▌         | 3/50 [00:07<01:32,  1.97s/it]  8%|▊         | 4/50 [00:08<01:07,  1.48s/it] 10%|█         | 5/50 [00:09<00:53,  1.19s/it] 12%|█▏        | 6/50 [00:09<00:44,  1.02s/it] 14%|█▍        | 7/50 [00:10<00:38,  1.11it/s] 16%|█▌        | 8/50 [00:11<00:35,  1.19it/s] 18%|█▊        | 9/50 [00:11<00:32,  1.27it/s] 20%|██        | 10/50 [00:12<00:30,  1.32it/s] 22%|██▏       | 11/50 [00:13<00:28,  1.37it/s] 24%|██▍       | 12/50 [00:13<00:27,  1.39it/s] 26%|██▌       | 13/50 [00:14<00:26,  1.42it/s] 28%|██▊       | 14/50 [00:15<00:25,  1.43it/s] 30%|███       | 15/50 [00:15<00:24,  1.45it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.45it/s] 34%|███▍      | 17/50 [00:17<00:22,  1.46it/s] 36%|███▌      | 18/50 [00:18<00:21,  1.47it/s] 38%|███▊      | 19/50 [00:18<00:21,  1.47it/s] 40%|████      | 20/50 [00:19<00:20,  1.47it/s] 42%|████▏     | 21/50 [00:20<00:19,  1.47it/s] 44%|████▍     | 22/50 [00:20<00:19,  1.47it/s] 46%|████▌     | 23/50 [00:21<00:18,  1.47it/s] 48%|████▊     | 24/50 [00:22<00:17,  1.47it/s] 50%|█████     | 25/50 [00:22<00:16,  1.47it/s] 52%|█████▏    | 26/50 [00:23<00:16,  1.47it/s] 54%|█████▍    | 27/50 [00:24<00:15,  1.47it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.47it/s] 58%|█████▊    | 29/50 [00:25<00:14,  1.47it/s] 60%|██████    | 30/50 [00:26<00:13,  1.47it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.47it/s] 64%|██████▍   | 32/50 [00:27<00:12,  1.47it/s] 66%|██████▌   | 33/50 [00:28<00:11,  1.47it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.47it/s] 70%|███████   | 35/50 [00:29<00:10,  1.47it/s] 72%|███████▏  | 36/50 [00:30<00:09,  1.47it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.47it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.47it/s] 78%|███████▊  | 39/50 [00:32<00:07,  1.47it/s] 80%|████████  | 40/50 [00:32<00:06,  1.47it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.47it/s] 84%|████████▍ | 42/50 [00:34<00:05,  1.48it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.48it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.47it/s] 90%|█████████ | 45/50 [00:36<00:03,  1.47it/s] 92%|█████████▏| 46/50 [00:37<00:02,  1.47it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.47it/s] 96%|█████████▌| 48/50 [00:38<00:01,  1.47it/s] 98%|█████████▊| 49/50 [00:39<00:00,  1.47it/s]100%|██████████| 50/50 [00:39<00:00,  1.47it/s]100%|██████████| 50/50 [00:39<00:00,  1.25it/s]
=> result
* total: 25,000
* correct: 19,267
* accuracy: 77.1%
* error: 22.9%
* macro_f1: 76.7%
Elapsed: 0:31:14
Run this job and save the output to output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'spiral or coil', 'combination lock', 'computer keyboard', 'candy store', 'container ship', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'construction crane', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fireboat', 'fire truck', 'fire screen', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gas mask or respirator', 'gas pump', 'goblet', 'go-kart', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'radiator grille', 'grocery store', 'guillotine', 'hair clip', 'hair spray', 'half-track', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'harp', 'combine harvester', 'hatchet', 'holster', 'home theater', 'honeycomb', 'hook', 'hoop skirt', 'gymnastic horizontal bar', 'horse-drawn vehicle', 'hourglass', 'iPod', 'clothes iron', 'carved pumpkin', 'jeans', 'jeep', 'T-shirt', 'jigsaw puzzle', 'rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lawn mower', 'lens cap', 'letter opener', 'library', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'lipstick', 'slip-on shoe', 'lotion', 'music speaker', 'loupe magnifying glass', 'sawmill', 'magnetic compass', 'messenger bag', 'mailbox', 'tights', 'one-piece bathing suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'matchstick', 'maypole', 'maze', 'measuring cup', 'medicine cabinet', 'megalith', 'microphone', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'ford model t', 'modem', 'monastery', 'monitor', 'moped', 'mortar and pestle', 'graduation cap', 'mosque', 'mosquito net', 'vespa', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'muzzle', 'metal nail', 'neck brace', 'necklace', 'baby pacifier', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'odometer', 'oil filter', 'pipe organ', 'oscilloscope', 'overskirt', 'bullock cart', 'oxygen mask', 'product packet / packaging', 'paddle', 'paddle wheel', 'padlock', 'paintbrush', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'parking meter', 'railroad car', 'patio', 'payphone', 'pedestal', 'pencil case', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'picket fence', 'pickup truck', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pinwheel', 'pirate ship', 'drink pitcher', 'block plane', 'planetarium', 'plastic bag', 'plate rack', 'farm plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'poncho', 'pool table', 'soda bottle', 'plant pot', "potter's wheel", 'power drill', 'prayer rug', 'printer', 'prison', 'missile', 'projector', 'hockey puck', 'punching bag', 'purse', 'quill', 'quilt', 'race car', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'fishing casting reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'eraser', 'rugby ball', 'ruler measuring stick', 'sneaker', 'safe', 'safety pin', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'schooner', 'scoreboard', 'CRT monitor', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe store', 'shoji screen / room divider', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski', 'balaclava ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'soup bowl', 'keyboard space bar', 'space heater', 'space shuttle', 'spatula', 'motorboat', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'steam locomotive', 'through arch bridge', 'steel drum', 'stethoscope', 'scarf', 'stone wall', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglasses', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swim trunks / shorts', 'swing', 'electrical switch', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'thatched roof', 'front curtain', 'thimble', 'threshing machine', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'trombone', 'hot tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vase', 'vaulted or arched ceiling', 'velvet fabric', 'vending machine', 'vestment', 'viaduct', 'violin', 'volleyball', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'hair wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'airplane wing', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'shipwreck', 'sailboat', 'yurt', 'website', 'comic book', 'crossword', 'traffic or street sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'trifle', 'ice cream', 'popsicle', 'baguette', 'bagel', 'pretzel', 'cheeseburger', 'hot dog', 'mashed potatoes', 'cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith apple', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'cherimoya (custard apple)', 'pomegranate', 'hay', 'carbonara', 'chocolate syrup', 'dough', 'meatloaf', 'pizza', 'pot pie', 'burrito', 'red wine', 'espresso', 'tea cup', 'eggnog', 'mountain', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'sandbar', 'beach', 'valley', 'volcano', 'baseball player', 'bridegroom', 'scuba diver', 'rapeseed', 'daisy', "yellow lady's slipper", 'corn', 'acorn', 'rose hip', 'horse chestnut seed', 'coral fungus', 'agaric', 'gyromitra', 'stinkhorn mushroom', 'earth star fungus', 'hen of the woods mushroom', 'bolete', 'corn cob', 'toilet paper']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a cliff dwelling.', 'a photo of a cloak.', 'a photo of a clogs.', 'a photo of a cocktail shaker.', 'a photo of a coffee mug.', 'a photo of a coffeemaker.', 'a photo of a spiral or coil.', 'a photo of a combination lock.', 'a photo of a computer keyboard.', 'a photo of a candy store.', 'a photo of a container ship.', 'a photo of a convertible.', 'a photo of a corkscrew.', 'a photo of a cornet.', 'a photo of a cowboy boot.', 'a photo of a cowboy hat.', 'a photo of a cradle.', 'a photo of a construction crane.', 'a photo of a crash helmet.', 'a photo of a crate.', 'a photo of a infant bed.', 'a photo of a Crock Pot.', 'a photo of a croquet ball.', 'a photo of a crutch.', 'a photo of a cuirass.', 'a photo of a dam.', 'a photo of a desk.', 'a photo of a desktop computer.', 'a photo of a rotary dial telephone.', 'a photo of a diaper.', 'a photo of a digital clock.', 'a photo of a digital watch.', 'a photo of a dining table.', 'a photo of a dishcloth.', 'a photo of a dishwasher.', 'a photo of a disc brake.', 'a photo of a dock.', 'a photo of a dog sled.', 'a photo of a dome.', 'a photo of a doormat.', 'a photo of a drilling rig.', 'a photo of a drum.', 'a photo of a drumstick.', 'a photo of a dumbbell.', 'a photo of a Dutch oven.', 'a photo of a electric fan.', 'a photo of a electric guitar.', 'a photo of a electric locomotive.', 'a photo of a entertainment center.', 'a photo of a envelope.', 'a photo of a espresso machine.', 'a photo of a face powder.', 'a photo of a feather boa.', 'a photo of a filing cabinet.', 'a photo of a fireboat.', 'a photo of a fire truck.', 'a photo of a fire screen.', 'a photo of a flagpole.', 'a photo of a flute.', 'a photo of a folding chair.', 'a photo of a football helmet.', 'a photo of a forklift.', 'a photo of a fountain.', 'a photo of a fountain pen.', 'a photo of a four-poster bed.', 'a photo of a freight car.', 'a photo of a French horn.', 'a photo of a frying pan.', 'a photo of a fur coat.', 'a photo of a garbage truck.', 'a photo of a gas mask or respirator.', 'a photo of a gas pump.', 'a photo of a goblet.', 'a photo of a go-kart.', 'a photo of a golf ball.', 'a photo of a golf cart.', 'a photo of a gondola.', 'a photo of a gong.', 'a photo of a gown.', 'a photo of a grand piano.', 'a photo of a greenhouse.', 'a photo of a radiator grille.', 'a photo of a grocery store.', 'a photo of a guillotine.', 'a photo of a hair clip.', 'a photo of a hair spray.', 'a photo of a half-track.', 'a photo of a hammer.', 'a photo of a hamper.', 'a photo of a hair dryer.', 'a photo of a hand-held computer.', 'a photo of a handkerchief.', 'a photo of a hard disk drive.', 'a photo of a harmonica.', 'a photo of a harp.', 'a photo of a combine harvester.', 'a photo of a hatchet.', 'a photo of a holster.', 'a photo of a home theater.', 'a photo of a honeycomb.', 'a photo of a hook.', 'a photo of a hoop skirt.', 'a photo of a gymnastic horizontal bar.', 'a photo of a horse-drawn vehicle.', 'a photo of a hourglass.', 'a photo of a iPod.', 'a photo of a clothes iron.', 'a photo of a carved pumpkin.', 'a photo of a jeans.', 'a photo of a jeep.', 'a photo of a T-shirt.', 'a photo of a jigsaw puzzle.', 'a photo of a rickshaw.', 'a photo of a joystick.', 'a photo of a kimono.', 'a photo of a knee pad.', 'a photo of a knot.', 'a photo of a lab coat.', 'a photo of a ladle.', 'a photo of a lampshade.', 'a photo of a laptop computer.', 'a photo of a lawn mower.', 'a photo of a lens cap.', 'a photo of a letter opener.', 'a photo of a library.', 'a photo of a lifeboat.', 'a photo of a lighter.', 'a photo of a limousine.', 'a photo of a ocean liner.', 'a photo of a lipstick.', 'a photo of a slip-on shoe.', 'a photo of a lotion.', 'a photo of a music speaker.', 'a photo of a loupe magnifying glass.', 'a photo of a sawmill.', 'a photo of a magnetic compass.', 'a photo of a messenger bag.', 'a photo of a mailbox.', 'a photo of a tights.', 'a photo of a one-piece bathing suit.', 'a photo of a manhole cover.', 'a photo of a maraca.', 'a photo of a marimba.', 'a photo of a mask.', 'a photo of a matchstick.', 'a photo of a maypole.', 'a photo of a maze.', 'a photo of a measuring cup.', 'a photo of a medicine cabinet.', 'a photo of a megalith.', 'a photo of a microphone.', 'a photo of a microwave oven.', 'a photo of a military uniform.', 'a photo of a milk can.', 'a photo of a minibus.', 'a photo of a miniskirt.', 'a photo of a minivan.', 'a photo of a missile.', 'a photo of a mitten.', 'a photo of a mixing bowl.', 'a photo of a mobile home.', 'a photo of a ford model t.', 'a photo of a modem.', 'a photo of a monastery.', 'a photo of a monitor.', 'a photo of a moped.', 'a photo of a mortar and pestle.', 'a photo of a graduation cap.', 'a photo of a mosque.', 'a photo of a mosquito net.', 'a photo of a vespa.', 'a photo of a mountain bike.', 'a photo of a tent.', 'a photo of a computer mouse.', 'a photo of a mousetrap.', 'a photo of a moving van.', 'a photo of a muzzle.', 'a photo of a metal nail.', 'a photo of a neck brace.', 'a photo of a necklace.', 'a photo of a baby pacifier.', 'a photo of a notebook computer.', 'a photo of a obelisk.', 'a photo of a oboe.', 'a photo of a ocarina.', 'a photo of a odometer.', 'a photo of a oil filter.', 'a photo of a pipe organ.', 'a photo of a oscilloscope.', 'a photo of a overskirt.', 'a photo of a bullock cart.', 'a photo of a oxygen mask.', 'a photo of a product packet / packaging.', 'a photo of a paddle.', 'a photo of a paddle wheel.', 'a photo of a padlock.', 'a photo of a paintbrush.', 'a photo of a pajamas.', 'a photo of a palace.', 'a photo of a pan flute.', 'a photo of a paper towel.', 'a photo of a parachute.', 'a photo of a parallel bars.', 'a photo of a park bench.', 'a photo of a parking meter.', 'a photo of a railroad car.', 'a photo of a patio.', 'a photo of a payphone.', 'a photo of a pedestal.', 'a photo of a pencil case.', 'a photo of a pencil sharpener.', 'a photo of a perfume.', 'a photo of a Petri dish.', 'a photo of a photocopier.', 'a photo of a plectrum.', 'a photo of a Pickelhaube.', 'a photo of a picket fence.', 'a photo of a pickup truck.', 'a photo of a pier.', 'a photo of a piggy bank.', 'a photo of a pill bottle.', 'a photo of a pillow.', 'a photo of a ping-pong ball.', 'a photo of a pinwheel.', 'a photo of a pirate ship.', 'a photo of a drink pitcher.', 'a photo of a block plane.', 'a photo of a planetarium.', 'a photo of a plastic bag.', 'a photo of a plate rack.', 'a photo of a farm plow.', 'a photo of a plunger.', 'a photo of a Polaroid camera.', 'a photo of a pole.', 'a photo of a police van.', 'a photo of a poncho.', 'a photo of a pool table.', 'a photo of a soda bottle.', 'a photo of a plant pot.', "a photo of a potter's wheel.", 'a photo of a power drill.', 'a photo of a prayer rug.', 'a photo of a printer.', 'a photo of a prison.', 'a photo of a missile.', 'a photo of a projector.', 'a photo of a hockey puck.', 'a photo of a punching bag.', 'a photo of a purse.', 'a photo of a quill.', 'a photo of a quilt.', 'a photo of a race car.', 'a photo of a racket.', 'a photo of a radiator.', 'a photo of a radio.', 'a photo of a radio telescope.', 'a photo of a rain barrel.', 'a photo of a recreational vehicle.', 'a photo of a fishing casting reel.', 'a photo of a reflex camera.', 'a photo of a refrigerator.', 'a photo of a remote control.', 'a photo of a restaurant.', 'a photo of a revolver.', 'a photo of a rifle.', 'a photo of a rocking chair.', 'a photo of a rotisserie.', 'a photo of a eraser.', 'a photo of a rugby ball.', 'a photo of a ruler measuring stick.', 'a photo of a sneaker.', 'a photo of a safe.', 'a photo of a safety pin.', 'a photo of a salt shaker.', 'a photo of a sandal.', 'a photo of a sarong.', 'a photo of a saxophone.', 'a photo of a scabbard.', 'a photo of a weighing scale.', 'a photo of a school bus.', 'a photo of a schooner.', 'a photo of a scoreboard.', 'a photo of a CRT monitor.', 'a photo of a screw.', 'a photo of a screwdriver.', 'a photo of a seat belt.', 'a photo of a sewing machine.', 'a photo of a shield.', 'a photo of a shoe store.', 'a photo of a shoji screen / room divider.', 'a photo of a shopping basket.', 'a photo of a shopping cart.', 'a photo of a shovel.', 'a photo of a shower cap.', 'a photo of a shower curtain.', 'a photo of a ski.', 'a photo of a balaclava ski mask.', 'a photo of a sleeping bag.', 'a photo of a slide rule.', 'a photo of a sliding door.', 'a photo of a slot machine.', 'a photo of a snorkel.', 'a photo of a snowmobile.', 'a photo of a snowplow.', 'a photo of a soap dispenser.', 'a photo of a soccer ball.', 'a photo of a sock.', 'a photo of a solar thermal collector.', 'a photo of a sombrero.', 'a photo of a soup bowl.', 'a photo of a keyboard space bar.', 'a photo of a space heater.', 'a photo of a space shuttle.', 'a photo of a spatula.', 'a photo of a motorboat.', 'a photo of a spider web.', 'a photo of a spindle.', 'a photo of a sports car.', 'a photo of a spotlight.', 'a photo of a stage.', 'a photo of a steam locomotive.', 'a photo of a through arch bridge.', 'a photo of a steel drum.', 'a photo of a stethoscope.', 'a photo of a scarf.', 'a photo of a stone wall.', 'a photo of a stopwatch.', 'a photo of a stove.', 'a photo of a strainer.', 'a photo of a tram.', 'a photo of a stretcher.', 'a photo of a couch.', 'a photo of a stupa.', 'a photo of a submarine.', 'a photo of a suit.', 'a photo of a sundial.', 'a photo of a sunglasses.', 'a photo of a sunglasses.', 'a photo of a sunscreen.', 'a photo of a suspension bridge.', 'a photo of a mop.', 'a photo of a sweatshirt.', 'a photo of a swim trunks / shorts.', 'a photo of a swing.', 'a photo of a electrical switch.', 'a photo of a syringe.', 'a photo of a table lamp.', 'a photo of a tank.', 'a photo of a tape player.', 'a photo of a teapot.', 'a photo of a teddy bear.', 'a photo of a television.', 'a photo of a tennis ball.', 'a photo of a thatched roof.', 'a photo of a front curtain.', 'a photo of a thimble.', 'a photo of a threshing machine.', 'a photo of a throne.', 'a photo of a tile roof.', 'a photo of a toaster.', 'a photo of a tobacco shop.', 'a photo of a toilet seat.', 'a photo of a torch.', 'a photo of a totem pole.', 'a photo of a tow truck.', 'a photo of a toy store.', 'a photo of a tractor.', 'a photo of a semi-trailer truck.', 'a photo of a tray.', 'a photo of a trench coat.', 'a photo of a tricycle.', 'a photo of a trimaran.', 'a photo of a tripod.', 'a photo of a triumphal arch.', 'a photo of a trolleybus.', 'a photo of a trombone.', 'a photo of a hot tub.', 'a photo of a turnstile.', 'a photo of a typewriter keyboard.', 'a photo of a umbrella.', 'a photo of a unicycle.', 'a photo of a upright piano.', 'a photo of a vacuum cleaner.', 'a photo of a vase.', 'a photo of a vaulted or arched ceiling.', 'a photo of a velvet fabric.', 'a photo of a vending machine.', 'a photo of a vestment.', 'a photo of a viaduct.', 'a photo of a violin.', 'a photo of a volleyball.', 'a photo of a waffle iron.', 'a photo of a wall clock.', 'a photo of a wallet.', 'a photo of a wardrobe.', 'a photo of a military aircraft.', 'a photo of a sink.', 'a photo of a washing machine.', 'a photo of a water bottle.', 'a photo of a water jug.', 'a photo of a water tower.', 'a photo of a whiskey jug.', 'a photo of a whistle.', 'a photo of a hair wig.', 'a photo of a window screen.', 'a photo of a window shade.', 'a photo of a Windsor tie.', 'a photo of a wine bottle.', 'a photo of a airplane wing.', 'a photo of a wok.', 'a photo of a wooden spoon.', 'a photo of a wool.', 'a photo of a split-rail fence.', 'a photo of a shipwreck.', 'a photo of a sailboat.', 'a photo of a yurt.', 'a photo of a website.', 'a photo of a comic book.', 'a photo of a crossword.', 'a photo of a traffic or street sign.', 'a photo of a traffic light.', 'a photo of a dust jacket.', 'a photo of a menu.', 'a photo of a plate.', 'a photo of a guacamole.', 'a photo of a consomme.', 'a photo of a hot pot.', 'a photo of a trifle.', 'a photo of a ice cream.', 'a photo of a popsicle.', 'a photo of a baguette.', 'a photo of a bagel.', 'a photo of a pretzel.', 'a photo of a cheeseburger.', 'a photo of a hot dog.', 'a photo of a mashed potatoes.', 'a photo of a cabbage.', 'a photo of a broccoli.', 'a photo of a cauliflower.', 'a photo of a zucchini.', 'a photo of a spaghetti squash.', 'a photo of a acorn squash.', 'a photo of a butternut squash.', 'a photo of a cucumber.', 'a photo of a artichoke.', 'a photo of a bell pepper.', 'a photo of a cardoon.', 'a photo of a mushroom.', 'a photo of a Granny Smith apple.', 'a photo of a strawberry.', 'a photo of a orange.', 'a photo of a lemon.', 'a photo of a fig.', 'a photo of a pineapple.', 'a photo of a banana.', 'a photo of a jackfruit.', 'a photo of a cherimoya (custard apple).', 'a photo of a pomegranate.', 'a photo of a hay.', 'a photo of a carbonara.', 'a photo of a chocolate syrup.', 'a photo of a dough.', 'a photo of a meatloaf.', 'a photo of a pizza.', 'a photo of a pot pie.', 'a photo of a burrito.', 'a photo of a red wine.', 'a photo of a espresso.', 'a photo of a tea cup.', 'a photo of a eggnog.', 'a photo of a mountain.', 'a photo of a bubble.', 'a photo of a cliff.', 'a photo of a coral reef.', 'a photo of a geyser.', 'a photo of a lakeshore.', 'a photo of a promontory.', 'a photo of a sandbar.', 'a photo of a beach.', 'a photo of a valley.', 'a photo of a volcano.', 'a photo of a baseball player.', 'a photo of a bridegroom.', 'a photo of a scuba diver.', 'a photo of a rapeseed.', 'a photo of a daisy.', "a photo of a yellow lady's slipper.", 'a photo of a corn.', 'a photo of a acorn.', 'a photo of a rose hip.', 'a photo of a horse chestnut seed.', 'a photo of a coral fungus.', 'a photo of a agaric.', 'a photo of a gyromitra.', 'a photo of a stinkhorn mushroom.', 'a photo of a earth star fungus.', 'a photo of a hen of the woods mushroom.', 'a photo of a bolete.', 'a photo of a corn cob.', 'a photo of a toilet paper.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:05<04:52,  5.97s/it]  4%|▍         | 2/50 [00:06<02:16,  2.85s/it]  6%|▌         | 3/50 [00:07<01:27,  1.85s/it]  8%|▊         | 4/50 [00:07<01:03,  1.39s/it] 10%|█         | 5/50 [00:08<00:52,  1.17s/it] 12%|█▏        | 6/50 [00:09<00:43,  1.00it/s] 14%|█▍        | 7/50 [00:10<00:38,  1.12it/s] 16%|█▌        | 8/50 [00:10<00:34,  1.22it/s] 18%|█▊        | 9/50 [00:11<00:31,  1.29it/s] 20%|██        | 10/50 [00:12<00:29,  1.33it/s] 22%|██▏       | 11/50 [00:12<00:28,  1.38it/s] 24%|██▍       | 12/50 [00:13<00:26,  1.41it/s] 26%|██▌       | 13/50 [00:14<00:25,  1.43it/s] 28%|██▊       | 14/50 [00:14<00:24,  1.45it/s] 30%|███       | 15/50 [00:15<00:23,  1.46it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.47it/s] 34%|███▍      | 17/50 [00:16<00:22,  1.48it/s] 36%|███▌      | 18/50 [00:17<00:21,  1.48it/s] 38%|███▊      | 19/50 [00:18<00:20,  1.48it/s] 40%|████      | 20/50 [00:18<00:20,  1.48it/s] 42%|████▏     | 21/50 [00:19<00:19,  1.48it/s] 44%|████▍     | 22/50 [00:20<00:18,  1.49it/s] 46%|████▌     | 23/50 [00:20<00:18,  1.49it/s] 48%|████▊     | 24/50 [00:21<00:17,  1.49it/s] 50%|█████     | 25/50 [00:22<00:16,  1.49it/s] 52%|█████▏    | 26/50 [00:22<00:16,  1.49it/s] 54%|█████▍    | 27/50 [00:23<00:15,  1.49it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.49it/s] 58%|█████▊    | 29/50 [00:24<00:14,  1.49it/s] 60%|██████    | 30/50 [00:25<00:13,  1.49it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.49it/s] 64%|██████▍   | 32/50 [00:26<00:12,  1.48it/s] 66%|██████▌   | 33/50 [00:27<00:11,  1.49it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.49it/s] 70%|███████   | 35/50 [00:29<00:10,  1.45it/s] 72%|███████▏  | 36/50 [00:29<00:09,  1.46it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.47it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.47it/s] 78%|███████▊  | 39/50 [00:31<00:07,  1.48it/s] 80%|████████  | 40/50 [00:32<00:06,  1.48it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.48it/s] 84%|████████▍ | 42/50 [00:33<00:05,  1.48it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.48it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.48it/s] 90%|█████████ | 45/50 [00:35<00:03,  1.48it/s] 92%|█████████▏| 46/50 [00:36<00:02,  1.48it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.48it/s] 96%|█████████▌| 48/50 [00:37<00:01,  1.48it/s] 98%|█████████▊| 49/50 [00:38<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.27it/s]
=> result
* total: 25,000
* correct: 17,496
* accuracy: 70.0%
* error: 30.0%
* macro_f1: 69.1%
Run this job and save the output to output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'spiral or coil', 'combination lock', 'computer keyboard', 'candy store', 'container ship', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'construction crane', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fireboat', 'fire truck', 'fire screen', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gas mask or respirator', 'gas pump', 'goblet', 'go-kart', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'radiator grille', 'grocery store', 'guillotine', 'hair clip', 'hair spray', 'half-track', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'harp', 'combine harvester', 'hatchet', 'holster', 'home theater', 'honeycomb', 'hook', 'hoop skirt', 'gymnastic horizontal bar', 'horse-drawn vehicle', 'hourglass', 'iPod', 'clothes iron', 'carved pumpkin', 'jeans', 'jeep', 'T-shirt', 'jigsaw puzzle', 'rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lawn mower', 'lens cap', 'letter opener', 'library', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'lipstick', 'slip-on shoe', 'lotion', 'music speaker', 'loupe magnifying glass', 'sawmill', 'magnetic compass', 'messenger bag', 'mailbox', 'tights', 'one-piece bathing suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'matchstick', 'maypole', 'maze', 'measuring cup', 'medicine cabinet', 'megalith', 'microphone', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'ford model t', 'modem', 'monastery', 'monitor', 'moped', 'mortar and pestle', 'graduation cap', 'mosque', 'mosquito net', 'vespa', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'muzzle', 'metal nail', 'neck brace', 'necklace', 'baby pacifier', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'odometer', 'oil filter', 'pipe organ', 'oscilloscope', 'overskirt', 'bullock cart', 'oxygen mask', 'product packet / packaging', 'paddle', 'paddle wheel', 'padlock', 'paintbrush', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'parking meter', 'railroad car', 'patio', 'payphone', 'pedestal', 'pencil case', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'picket fence', 'pickup truck', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pinwheel', 'pirate ship', 'drink pitcher', 'block plane', 'planetarium', 'plastic bag', 'plate rack', 'farm plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'poncho', 'pool table', 'soda bottle', 'plant pot', "potter's wheel", 'power drill', 'prayer rug', 'printer', 'prison', 'missile', 'projector', 'hockey puck', 'punching bag', 'purse', 'quill', 'quilt', 'race car', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'fishing casting reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'eraser', 'rugby ball', 'ruler measuring stick', 'sneaker', 'safe', 'safety pin', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'schooner', 'scoreboard', 'CRT monitor', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe store', 'shoji screen / room divider', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski', 'balaclava ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'soup bowl', 'keyboard space bar', 'space heater', 'space shuttle', 'spatula', 'motorboat', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'steam locomotive', 'through arch bridge', 'steel drum', 'stethoscope', 'scarf', 'stone wall', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglasses', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swim trunks / shorts', 'swing', 'electrical switch', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'thatched roof', 'front curtain', 'thimble', 'threshing machine', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'trombone', 'hot tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vase', 'vaulted or arched ceiling', 'velvet fabric', 'vending machine', 'vestment', 'viaduct', 'violin', 'volleyball', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'hair wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'airplane wing', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'shipwreck', 'sailboat', 'yurt', 'website', 'comic book', 'crossword', 'traffic or street sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'trifle', 'ice cream', 'popsicle', 'baguette', 'bagel', 'pretzel', 'cheeseburger', 'hot dog', 'mashed potatoes', 'cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith apple', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'cherimoya (custard apple)', 'pomegranate', 'hay', 'carbonara', 'chocolate syrup', 'dough', 'meatloaf', 'pizza', 'pot pie', 'burrito', 'red wine', 'espresso', 'tea cup', 'eggnog', 'mountain', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'sandbar', 'beach', 'valley', 'volcano', 'baseball player', 'bridegroom', 'scuba diver', 'rapeseed', 'daisy', "yellow lady's slipper", 'corn', 'acorn', 'rose hip', 'horse chestnut seed', 'coral fungus', 'agaric', 'gyromitra', 'stinkhorn mushroom', 'earth star fungus', 'hen of the woods mushroom', 'bolete', 'corn cob', 'toilet paper']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a cliff dwelling.', 'a photo of a cloak.', 'a photo of a clogs.', 'a photo of a cocktail shaker.', 'a photo of a coffee mug.', 'a photo of a coffeemaker.', 'a photo of a spiral or coil.', 'a photo of a combination lock.', 'a photo of a computer keyboard.', 'a photo of a candy store.', 'a photo of a container ship.', 'a photo of a convertible.', 'a photo of a corkscrew.', 'a photo of a cornet.', 'a photo of a cowboy boot.', 'a photo of a cowboy hat.', 'a photo of a cradle.', 'a photo of a construction crane.', 'a photo of a crash helmet.', 'a photo of a crate.', 'a photo of a infant bed.', 'a photo of a Crock Pot.', 'a photo of a croquet ball.', 'a photo of a crutch.', 'a photo of a cuirass.', 'a photo of a dam.', 'a photo of a desk.', 'a photo of a desktop computer.', 'a photo of a rotary dial telephone.', 'a photo of a diaper.', 'a photo of a digital clock.', 'a photo of a digital watch.', 'a photo of a dining table.', 'a photo of a dishcloth.', 'a photo of a dishwasher.', 'a photo of a disc brake.', 'a photo of a dock.', 'a photo of a dog sled.', 'a photo of a dome.', 'a photo of a doormat.', 'a photo of a drilling rig.', 'a photo of a drum.', 'a photo of a drumstick.', 'a photo of a dumbbell.', 'a photo of a Dutch oven.', 'a photo of a electric fan.', 'a photo of a electric guitar.', 'a photo of a electric locomotive.', 'a photo of a entertainment center.', 'a photo of a envelope.', 'a photo of a espresso machine.', 'a photo of a face powder.', 'a photo of a feather boa.', 'a photo of a filing cabinet.', 'a photo of a fireboat.', 'a photo of a fire truck.', 'a photo of a fire screen.', 'a photo of a flagpole.', 'a photo of a flute.', 'a photo of a folding chair.', 'a photo of a football helmet.', 'a photo of a forklift.', 'a photo of a fountain.', 'a photo of a fountain pen.', 'a photo of a four-poster bed.', 'a photo of a freight car.', 'a photo of a French horn.', 'a photo of a frying pan.', 'a photo of a fur coat.', 'a photo of a garbage truck.', 'a photo of a gas mask or respirator.', 'a photo of a gas pump.', 'a photo of a goblet.', 'a photo of a go-kart.', 'a photo of a golf ball.', 'a photo of a golf cart.', 'a photo of a gondola.', 'a photo of a gong.', 'a photo of a gown.', 'a photo of a grand piano.', 'a photo of a greenhouse.', 'a photo of a radiator grille.', 'a photo of a grocery store.', 'a photo of a guillotine.', 'a photo of a hair clip.', 'a photo of a hair spray.', 'a photo of a half-track.', 'a photo of a hammer.', 'a photo of a hamper.', 'a photo of a hair dryer.', 'a photo of a hand-held computer.', 'a photo of a handkerchief.', 'a photo of a hard disk drive.', 'a photo of a harmonica.', 'a photo of a harp.', 'a photo of a combine harvester.', 'a photo of a hatchet.', 'a photo of a holster.', 'a photo of a home theater.', 'a photo of a honeycomb.', 'a photo of a hook.', 'a photo of a hoop skirt.', 'a photo of a gymnastic horizontal bar.', 'a photo of a horse-drawn vehicle.', 'a photo of a hourglass.', 'a photo of a iPod.', 'a photo of a clothes iron.', 'a photo of a carved pumpkin.', 'a photo of a jeans.', 'a photo of a jeep.', 'a photo of a T-shirt.', 'a photo of a jigsaw puzzle.', 'a photo of a rickshaw.', 'a photo of a joystick.', 'a photo of a kimono.', 'a photo of a knee pad.', 'a photo of a knot.', 'a photo of a lab coat.', 'a photo of a ladle.', 'a photo of a lampshade.', 'a photo of a laptop computer.', 'a photo of a lawn mower.', 'a photo of a lens cap.', 'a photo of a letter opener.', 'a photo of a library.', 'a photo of a lifeboat.', 'a photo of a lighter.', 'a photo of a limousine.', 'a photo of a ocean liner.', 'a photo of a lipstick.', 'a photo of a slip-on shoe.', 'a photo of a lotion.', 'a photo of a music speaker.', 'a photo of a loupe magnifying glass.', 'a photo of a sawmill.', 'a photo of a magnetic compass.', 'a photo of a messenger bag.', 'a photo of a mailbox.', 'a photo of a tights.', 'a photo of a one-piece bathing suit.', 'a photo of a manhole cover.', 'a photo of a maraca.', 'a photo of a marimba.', 'a photo of a mask.', 'a photo of a matchstick.', 'a photo of a maypole.', 'a photo of a maze.', 'a photo of a measuring cup.', 'a photo of a medicine cabinet.', 'a photo of a megalith.', 'a photo of a microphone.', 'a photo of a microwave oven.', 'a photo of a military uniform.', 'a photo of a milk can.', 'a photo of a minibus.', 'a photo of a miniskirt.', 'a photo of a minivan.', 'a photo of a missile.', 'a photo of a mitten.', 'a photo of a mixing bowl.', 'a photo of a mobile home.', 'a photo of a ford model t.', 'a photo of a modem.', 'a photo of a monastery.', 'a photo of a monitor.', 'a photo of a moped.', 'a photo of a mortar and pestle.', 'a photo of a graduation cap.', 'a photo of a mosque.', 'a photo of a mosquito net.', 'a photo of a vespa.', 'a photo of a mountain bike.', 'a photo of a tent.', 'a photo of a computer mouse.', 'a photo of a mousetrap.', 'a photo of a moving van.', 'a photo of a muzzle.', 'a photo of a metal nail.', 'a photo of a neck brace.', 'a photo of a necklace.', 'a photo of a baby pacifier.', 'a photo of a notebook computer.', 'a photo of a obelisk.', 'a photo of a oboe.', 'a photo of a ocarina.', 'a photo of a odometer.', 'a photo of a oil filter.', 'a photo of a pipe organ.', 'a photo of a oscilloscope.', 'a photo of a overskirt.', 'a photo of a bullock cart.', 'a photo of a oxygen mask.', 'a photo of a product packet / packaging.', 'a photo of a paddle.', 'a photo of a paddle wheel.', 'a photo of a padlock.', 'a photo of a paintbrush.', 'a photo of a pajamas.', 'a photo of a palace.', 'a photo of a pan flute.', 'a photo of a paper towel.', 'a photo of a parachute.', 'a photo of a parallel bars.', 'a photo of a park bench.', 'a photo of a parking meter.', 'a photo of a railroad car.', 'a photo of a patio.', 'a photo of a payphone.', 'a photo of a pedestal.', 'a photo of a pencil case.', 'a photo of a pencil sharpener.', 'a photo of a perfume.', 'a photo of a Petri dish.', 'a photo of a photocopier.', 'a photo of a plectrum.', 'a photo of a Pickelhaube.', 'a photo of a picket fence.', 'a photo of a pickup truck.', 'a photo of a pier.', 'a photo of a piggy bank.', 'a photo of a pill bottle.', 'a photo of a pillow.', 'a photo of a ping-pong ball.', 'a photo of a pinwheel.', 'a photo of a pirate ship.', 'a photo of a drink pitcher.', 'a photo of a block plane.', 'a photo of a planetarium.', 'a photo of a plastic bag.', 'a photo of a plate rack.', 'a photo of a farm plow.', 'a photo of a plunger.', 'a photo of a Polaroid camera.', 'a photo of a pole.', 'a photo of a police van.', 'a photo of a poncho.', 'a photo of a pool table.', 'a photo of a soda bottle.', 'a photo of a plant pot.', "a photo of a potter's wheel.", 'a photo of a power drill.', 'a photo of a prayer rug.', 'a photo of a printer.', 'a photo of a prison.', 'a photo of a missile.', 'a photo of a projector.', 'a photo of a hockey puck.', 'a photo of a punching bag.', 'a photo of a purse.', 'a photo of a quill.', 'a photo of a quilt.', 'a photo of a race car.', 'a photo of a racket.', 'a photo of a radiator.', 'a photo of a radio.', 'a photo of a radio telescope.', 'a photo of a rain barrel.', 'a photo of a recreational vehicle.', 'a photo of a fishing casting reel.', 'a photo of a reflex camera.', 'a photo of a refrigerator.', 'a photo of a remote control.', 'a photo of a restaurant.', 'a photo of a revolver.', 'a photo of a rifle.', 'a photo of a rocking chair.', 'a photo of a rotisserie.', 'a photo of a eraser.', 'a photo of a rugby ball.', 'a photo of a ruler measuring stick.', 'a photo of a sneaker.', 'a photo of a safe.', 'a photo of a safety pin.', 'a photo of a salt shaker.', 'a photo of a sandal.', 'a photo of a sarong.', 'a photo of a saxophone.', 'a photo of a scabbard.', 'a photo of a weighing scale.', 'a photo of a school bus.', 'a photo of a schooner.', 'a photo of a scoreboard.', 'a photo of a CRT monitor.', 'a photo of a screw.', 'a photo of a screwdriver.', 'a photo of a seat belt.', 'a photo of a sewing machine.', 'a photo of a shield.', 'a photo of a shoe store.', 'a photo of a shoji screen / room divider.', 'a photo of a shopping basket.', 'a photo of a shopping cart.', 'a photo of a shovel.', 'a photo of a shower cap.', 'a photo of a shower curtain.', 'a photo of a ski.', 'a photo of a balaclava ski mask.', 'a photo of a sleeping bag.', 'a photo of a slide rule.', 'a photo of a sliding door.', 'a photo of a slot machine.', 'a photo of a snorkel.', 'a photo of a snowmobile.', 'a photo of a snowplow.', 'a photo of a soap dispenser.', 'a photo of a soccer ball.', 'a photo of a sock.', 'a photo of a solar thermal collector.', 'a photo of a sombrero.', 'a photo of a soup bowl.', 'a photo of a keyboard space bar.', 'a photo of a space heater.', 'a photo of a space shuttle.', 'a photo of a spatula.', 'a photo of a motorboat.', 'a photo of a spider web.', 'a photo of a spindle.', 'a photo of a sports car.', 'a photo of a spotlight.', 'a photo of a stage.', 'a photo of a steam locomotive.', 'a photo of a through arch bridge.', 'a photo of a steel drum.', 'a photo of a stethoscope.', 'a photo of a scarf.', 'a photo of a stone wall.', 'a photo of a stopwatch.', 'a photo of a stove.', 'a photo of a strainer.', 'a photo of a tram.', 'a photo of a stretcher.', 'a photo of a couch.', 'a photo of a stupa.', 'a photo of a submarine.', 'a photo of a suit.', 'a photo of a sundial.', 'a photo of a sunglasses.', 'a photo of a sunglasses.', 'a photo of a sunscreen.', 'a photo of a suspension bridge.', 'a photo of a mop.', 'a photo of a sweatshirt.', 'a photo of a swim trunks / shorts.', 'a photo of a swing.', 'a photo of a electrical switch.', 'a photo of a syringe.', 'a photo of a table lamp.', 'a photo of a tank.', 'a photo of a tape player.', 'a photo of a teapot.', 'a photo of a teddy bear.', 'a photo of a television.', 'a photo of a tennis ball.', 'a photo of a thatched roof.', 'a photo of a front curtain.', 'a photo of a thimble.', 'a photo of a threshing machine.', 'a photo of a throne.', 'a photo of a tile roof.', 'a photo of a toaster.', 'a photo of a tobacco shop.', 'a photo of a toilet seat.', 'a photo of a torch.', 'a photo of a totem pole.', 'a photo of a tow truck.', 'a photo of a toy store.', 'a photo of a tractor.', 'a photo of a semi-trailer truck.', 'a photo of a tray.', 'a photo of a trench coat.', 'a photo of a tricycle.', 'a photo of a trimaran.', 'a photo of a tripod.', 'a photo of a triumphal arch.', 'a photo of a trolleybus.', 'a photo of a trombone.', 'a photo of a hot tub.', 'a photo of a turnstile.', 'a photo of a typewriter keyboard.', 'a photo of a umbrella.', 'a photo of a unicycle.', 'a photo of a upright piano.', 'a photo of a vacuum cleaner.', 'a photo of a vase.', 'a photo of a vaulted or arched ceiling.', 'a photo of a velvet fabric.', 'a photo of a vending machine.', 'a photo of a vestment.', 'a photo of a viaduct.', 'a photo of a violin.', 'a photo of a volleyball.', 'a photo of a waffle iron.', 'a photo of a wall clock.', 'a photo of a wallet.', 'a photo of a wardrobe.', 'a photo of a military aircraft.', 'a photo of a sink.', 'a photo of a washing machine.', 'a photo of a water bottle.', 'a photo of a water jug.', 'a photo of a water tower.', 'a photo of a whiskey jug.', 'a photo of a whistle.', 'a photo of a hair wig.', 'a photo of a window screen.', 'a photo of a window shade.', 'a photo of a Windsor tie.', 'a photo of a wine bottle.', 'a photo of a airplane wing.', 'a photo of a wok.', 'a photo of a wooden spoon.', 'a photo of a wool.', 'a photo of a split-rail fence.', 'a photo of a shipwreck.', 'a photo of a sailboat.', 'a photo of a yurt.', 'a photo of a website.', 'a photo of a comic book.', 'a photo of a crossword.', 'a photo of a traffic or street sign.', 'a photo of a traffic light.', 'a photo of a dust jacket.', 'a photo of a menu.', 'a photo of a plate.', 'a photo of a guacamole.', 'a photo of a consomme.', 'a photo of a hot pot.', 'a photo of a trifle.', 'a photo of a ice cream.', 'a photo of a popsicle.', 'a photo of a baguette.', 'a photo of a bagel.', 'a photo of a pretzel.', 'a photo of a cheeseburger.', 'a photo of a hot dog.', 'a photo of a mashed potatoes.', 'a photo of a cabbage.', 'a photo of a broccoli.', 'a photo of a cauliflower.', 'a photo of a zucchini.', 'a photo of a spaghetti squash.', 'a photo of a acorn squash.', 'a photo of a butternut squash.', 'a photo of a cucumber.', 'a photo of a artichoke.', 'a photo of a bell pepper.', 'a photo of a cardoon.', 'a photo of a mushroom.', 'a photo of a Granny Smith apple.', 'a photo of a strawberry.', 'a photo of a orange.', 'a photo of a lemon.', 'a photo of a fig.', 'a photo of a pineapple.', 'a photo of a banana.', 'a photo of a jackfruit.', 'a photo of a cherimoya (custard apple).', 'a photo of a pomegranate.', 'a photo of a hay.', 'a photo of a carbonara.', 'a photo of a chocolate syrup.', 'a photo of a dough.', 'a photo of a meatloaf.', 'a photo of a pizza.', 'a photo of a pot pie.', 'a photo of a burrito.', 'a photo of a red wine.', 'a photo of a espresso.', 'a photo of a tea cup.', 'a photo of a eggnog.', 'a photo of a mountain.', 'a photo of a bubble.', 'a photo of a cliff.', 'a photo of a coral reef.', 'a photo of a geyser.', 'a photo of a lakeshore.', 'a photo of a promontory.', 'a photo of a sandbar.', 'a photo of a beach.', 'a photo of a valley.', 'a photo of a volcano.', 'a photo of a baseball player.', 'a photo of a bridegroom.', 'a photo of a scuba diver.', 'a photo of a rapeseed.', 'a photo of a daisy.', "a photo of a yellow lady's slipper.", 'a photo of a corn.', 'a photo of a acorn.', 'a photo of a rose hip.', 'a photo of a horse chestnut seed.', 'a photo of a coral fungus.', 'a photo of a agaric.', 'a photo of a gyromitra.', 'a photo of a stinkhorn mushroom.', 'a photo of a earth star fungus.', 'a photo of a hen of the woods mushroom.', 'a photo of a bolete.', 'a photo of a corn cob.', 'a photo of a toilet paper.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:05<04:50,  5.94s/it]  4%|▍         | 2/50 [00:06<02:16,  2.84s/it]  6%|▌         | 3/50 [00:07<01:26,  1.85s/it]  8%|▊         | 4/50 [00:07<01:03,  1.39s/it] 10%|█         | 5/50 [00:08<00:50,  1.13s/it] 12%|█▏        | 6/50 [00:09<00:42,  1.03it/s] 14%|█▍        | 7/50 [00:09<00:37,  1.14it/s] 16%|█▌        | 8/50 [00:10<00:34,  1.23it/s] 18%|█▊        | 9/50 [00:11<00:31,  1.30it/s] 20%|██        | 10/50 [00:11<00:29,  1.36it/s] 22%|██▏       | 11/50 [00:12<00:27,  1.40it/s] 24%|██▍       | 12/50 [00:13<00:26,  1.42it/s] 26%|██▌       | 13/50 [00:13<00:25,  1.44it/s] 28%|██▊       | 14/50 [00:14<00:24,  1.45it/s] 30%|███       | 15/50 [00:15<00:23,  1.46it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.47it/s] 34%|███▍      | 17/50 [00:16<00:22,  1.47it/s] 36%|███▌      | 18/50 [00:17<00:21,  1.48it/s] 38%|███▊      | 19/50 [00:18<00:20,  1.48it/s] 40%|████      | 20/50 [00:18<00:20,  1.48it/s] 42%|████▏     | 21/50 [00:19<00:19,  1.48it/s] 44%|████▍     | 22/50 [00:20<00:18,  1.48it/s] 46%|████▌     | 23/50 [00:20<00:18,  1.49it/s] 48%|████▊     | 24/50 [00:21<00:17,  1.49it/s] 50%|█████     | 25/50 [00:22<00:16,  1.49it/s] 52%|█████▏    | 26/50 [00:22<00:16,  1.48it/s] 54%|█████▍    | 27/50 [00:23<00:15,  1.48it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.48it/s] 58%|█████▊    | 29/50 [00:24<00:14,  1.48it/s] 60%|██████    | 30/50 [00:25<00:13,  1.48it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.48it/s] 64%|██████▍   | 32/50 [00:26<00:12,  1.48it/s] 66%|██████▌   | 33/50 [00:27<00:11,  1.48it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.48it/s] 70%|███████   | 35/50 [00:28<00:10,  1.48it/s] 72%|███████▏  | 36/50 [00:29<00:09,  1.48it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.48it/s] 76%|███████▌  | 38/50 [00:30<00:08,  1.48it/s] 78%|███████▊  | 39/50 [00:31<00:07,  1.48it/s] 80%|████████  | 40/50 [00:32<00:06,  1.48it/s] 82%|████████▏ | 41/50 [00:32<00:06,  1.48it/s] 84%|████████▍ | 42/50 [00:33<00:05,  1.48it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.48it/s] 88%|████████▊ | 44/50 [00:34<00:04,  1.48it/s] 90%|█████████ | 45/50 [00:35<00:03,  1.48it/s] 92%|█████████▏| 46/50 [00:36<00:02,  1.48it/s] 94%|█████████▍| 47/50 [00:36<00:02,  1.48it/s] 96%|█████████▌| 48/50 [00:37<00:01,  1.48it/s] 98%|█████████▊| 49/50 [00:38<00:00,  1.48it/s]100%|██████████| 50/50 [00:38<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.28it/s]
=> result
* total: 25,000
* correct: 17,435
* accuracy: 69.7%
* error: 30.3%
* macro_f1: 68.8%
Run this job and save the output to output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 25
model_dir: output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 25
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_1108_4/base2new/test_new/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.6 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.4
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] numpy                     1.24.4                    <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: ImageNet
Loading preprocessed few-shot data from /data/yht/data/cl/data/imagenet/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
['cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'spiral or coil', 'combination lock', 'computer keyboard', 'candy store', 'container ship', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'construction crane', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'cuirass', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fireboat', 'fire truck', 'fire screen', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gas mask or respirator', 'gas pump', 'goblet', 'go-kart', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'radiator grille', 'grocery store', 'guillotine', 'hair clip', 'hair spray', 'half-track', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'harp', 'combine harvester', 'hatchet', 'holster', 'home theater', 'honeycomb', 'hook', 'hoop skirt', 'gymnastic horizontal bar', 'horse-drawn vehicle', 'hourglass', 'iPod', 'clothes iron', 'carved pumpkin', 'jeans', 'jeep', 'T-shirt', 'jigsaw puzzle', 'rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lawn mower', 'lens cap', 'letter opener', 'library', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'lipstick', 'slip-on shoe', 'lotion', 'music speaker', 'loupe magnifying glass', 'sawmill', 'magnetic compass', 'messenger bag', 'mailbox', 'tights', 'one-piece bathing suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'matchstick', 'maypole', 'maze', 'measuring cup', 'medicine cabinet', 'megalith', 'microphone', 'microwave oven', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'ford model t', 'modem', 'monastery', 'monitor', 'moped', 'mortar and pestle', 'graduation cap', 'mosque', 'mosquito net', 'vespa', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'muzzle', 'metal nail', 'neck brace', 'necklace', 'baby pacifier', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'odometer', 'oil filter', 'pipe organ', 'oscilloscope', 'overskirt', 'bullock cart', 'oxygen mask', 'product packet / packaging', 'paddle', 'paddle wheel', 'padlock', 'paintbrush', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'parking meter', 'railroad car', 'patio', 'payphone', 'pedestal', 'pencil case', 'pencil sharpener', 'perfume', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'picket fence', 'pickup truck', 'pier', 'piggy bank', 'pill bottle', 'pillow', 'ping-pong ball', 'pinwheel', 'pirate ship', 'drink pitcher', 'block plane', 'planetarium', 'plastic bag', 'plate rack', 'farm plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'poncho', 'pool table', 'soda bottle', 'plant pot', "potter's wheel", 'power drill', 'prayer rug', 'printer', 'prison', 'missile', 'projector', 'hockey puck', 'punching bag', 'purse', 'quill', 'quilt', 'race car', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'fishing casting reel', 'reflex camera', 'refrigerator', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'eraser', 'rugby ball', 'ruler measuring stick', 'sneaker', 'safe', 'safety pin', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'schooner', 'scoreboard', 'CRT monitor', 'screw', 'screwdriver', 'seat belt', 'sewing machine', 'shield', 'shoe store', 'shoji screen / room divider', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski', 'balaclava ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'soup bowl', 'keyboard space bar', 'space heater', 'space shuttle', 'spatula', 'motorboat', 'spider web', 'spindle', 'sports car', 'spotlight', 'stage', 'steam locomotive', 'through arch bridge', 'steel drum', 'stethoscope', 'scarf', 'stone wall', 'stopwatch', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglasses', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swim trunks / shorts', 'swing', 'electrical switch', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'thatched roof', 'front curtain', 'thimble', 'threshing machine', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'tray', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'trombone', 'hot tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vase', 'vaulted or arched ceiling', 'velvet fabric', 'vending machine', 'vestment', 'viaduct', 'violin', 'volleyball', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'hair wig', 'window screen', 'window shade', 'Windsor tie', 'wine bottle', 'airplane wing', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'shipwreck', 'sailboat', 'yurt', 'website', 'comic book', 'crossword', 'traffic or street sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'trifle', 'ice cream', 'popsicle', 'baguette', 'bagel', 'pretzel', 'cheeseburger', 'hot dog', 'mashed potatoes', 'cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith apple', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'cherimoya (custard apple)', 'pomegranate', 'hay', 'carbonara', 'chocolate syrup', 'dough', 'meatloaf', 'pizza', 'pot pie', 'burrito', 'red wine', 'espresso', 'tea cup', 'eggnog', 'mountain', 'bubble', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'sandbar', 'beach', 'valley', 'volcano', 'baseball player', 'bridegroom', 'scuba diver', 'rapeseed', 'daisy', "yellow lady's slipper", 'corn', 'acorn', 'rose hip', 'horse chestnut seed', 'coral fungus', 'agaric', 'gyromitra', 'stinkhorn mushroom', 'earth star fungus', 'hen of the woods mushroom', 'bolete', 'corn cob', 'toilet paper']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a cliff dwelling.', 'a photo of a cloak.', 'a photo of a clogs.', 'a photo of a cocktail shaker.', 'a photo of a coffee mug.', 'a photo of a coffeemaker.', 'a photo of a spiral or coil.', 'a photo of a combination lock.', 'a photo of a computer keyboard.', 'a photo of a candy store.', 'a photo of a container ship.', 'a photo of a convertible.', 'a photo of a corkscrew.', 'a photo of a cornet.', 'a photo of a cowboy boot.', 'a photo of a cowboy hat.', 'a photo of a cradle.', 'a photo of a construction crane.', 'a photo of a crash helmet.', 'a photo of a crate.', 'a photo of a infant bed.', 'a photo of a Crock Pot.', 'a photo of a croquet ball.', 'a photo of a crutch.', 'a photo of a cuirass.', 'a photo of a dam.', 'a photo of a desk.', 'a photo of a desktop computer.', 'a photo of a rotary dial telephone.', 'a photo of a diaper.', 'a photo of a digital clock.', 'a photo of a digital watch.', 'a photo of a dining table.', 'a photo of a dishcloth.', 'a photo of a dishwasher.', 'a photo of a disc brake.', 'a photo of a dock.', 'a photo of a dog sled.', 'a photo of a dome.', 'a photo of a doormat.', 'a photo of a drilling rig.', 'a photo of a drum.', 'a photo of a drumstick.', 'a photo of a dumbbell.', 'a photo of a Dutch oven.', 'a photo of a electric fan.', 'a photo of a electric guitar.', 'a photo of a electric locomotive.', 'a photo of a entertainment center.', 'a photo of a envelope.', 'a photo of a espresso machine.', 'a photo of a face powder.', 'a photo of a feather boa.', 'a photo of a filing cabinet.', 'a photo of a fireboat.', 'a photo of a fire truck.', 'a photo of a fire screen.', 'a photo of a flagpole.', 'a photo of a flute.', 'a photo of a folding chair.', 'a photo of a football helmet.', 'a photo of a forklift.', 'a photo of a fountain.', 'a photo of a fountain pen.', 'a photo of a four-poster bed.', 'a photo of a freight car.', 'a photo of a French horn.', 'a photo of a frying pan.', 'a photo of a fur coat.', 'a photo of a garbage truck.', 'a photo of a gas mask or respirator.', 'a photo of a gas pump.', 'a photo of a goblet.', 'a photo of a go-kart.', 'a photo of a golf ball.', 'a photo of a golf cart.', 'a photo of a gondola.', 'a photo of a gong.', 'a photo of a gown.', 'a photo of a grand piano.', 'a photo of a greenhouse.', 'a photo of a radiator grille.', 'a photo of a grocery store.', 'a photo of a guillotine.', 'a photo of a hair clip.', 'a photo of a hair spray.', 'a photo of a half-track.', 'a photo of a hammer.', 'a photo of a hamper.', 'a photo of a hair dryer.', 'a photo of a hand-held computer.', 'a photo of a handkerchief.', 'a photo of a hard disk drive.', 'a photo of a harmonica.', 'a photo of a harp.', 'a photo of a combine harvester.', 'a photo of a hatchet.', 'a photo of a holster.', 'a photo of a home theater.', 'a photo of a honeycomb.', 'a photo of a hook.', 'a photo of a hoop skirt.', 'a photo of a gymnastic horizontal bar.', 'a photo of a horse-drawn vehicle.', 'a photo of a hourglass.', 'a photo of a iPod.', 'a photo of a clothes iron.', 'a photo of a carved pumpkin.', 'a photo of a jeans.', 'a photo of a jeep.', 'a photo of a T-shirt.', 'a photo of a jigsaw puzzle.', 'a photo of a rickshaw.', 'a photo of a joystick.', 'a photo of a kimono.', 'a photo of a knee pad.', 'a photo of a knot.', 'a photo of a lab coat.', 'a photo of a ladle.', 'a photo of a lampshade.', 'a photo of a laptop computer.', 'a photo of a lawn mower.', 'a photo of a lens cap.', 'a photo of a letter opener.', 'a photo of a library.', 'a photo of a lifeboat.', 'a photo of a lighter.', 'a photo of a limousine.', 'a photo of a ocean liner.', 'a photo of a lipstick.', 'a photo of a slip-on shoe.', 'a photo of a lotion.', 'a photo of a music speaker.', 'a photo of a loupe magnifying glass.', 'a photo of a sawmill.', 'a photo of a magnetic compass.', 'a photo of a messenger bag.', 'a photo of a mailbox.', 'a photo of a tights.', 'a photo of a one-piece bathing suit.', 'a photo of a manhole cover.', 'a photo of a maraca.', 'a photo of a marimba.', 'a photo of a mask.', 'a photo of a matchstick.', 'a photo of a maypole.', 'a photo of a maze.', 'a photo of a measuring cup.', 'a photo of a medicine cabinet.', 'a photo of a megalith.', 'a photo of a microphone.', 'a photo of a microwave oven.', 'a photo of a military uniform.', 'a photo of a milk can.', 'a photo of a minibus.', 'a photo of a miniskirt.', 'a photo of a minivan.', 'a photo of a missile.', 'a photo of a mitten.', 'a photo of a mixing bowl.', 'a photo of a mobile home.', 'a photo of a ford model t.', 'a photo of a modem.', 'a photo of a monastery.', 'a photo of a monitor.', 'a photo of a moped.', 'a photo of a mortar and pestle.', 'a photo of a graduation cap.', 'a photo of a mosque.', 'a photo of a mosquito net.', 'a photo of a vespa.', 'a photo of a mountain bike.', 'a photo of a tent.', 'a photo of a computer mouse.', 'a photo of a mousetrap.', 'a photo of a moving van.', 'a photo of a muzzle.', 'a photo of a metal nail.', 'a photo of a neck brace.', 'a photo of a necklace.', 'a photo of a baby pacifier.', 'a photo of a notebook computer.', 'a photo of a obelisk.', 'a photo of a oboe.', 'a photo of a ocarina.', 'a photo of a odometer.', 'a photo of a oil filter.', 'a photo of a pipe organ.', 'a photo of a oscilloscope.', 'a photo of a overskirt.', 'a photo of a bullock cart.', 'a photo of a oxygen mask.', 'a photo of a product packet / packaging.', 'a photo of a paddle.', 'a photo of a paddle wheel.', 'a photo of a padlock.', 'a photo of a paintbrush.', 'a photo of a pajamas.', 'a photo of a palace.', 'a photo of a pan flute.', 'a photo of a paper towel.', 'a photo of a parachute.', 'a photo of a parallel bars.', 'a photo of a park bench.', 'a photo of a parking meter.', 'a photo of a railroad car.', 'a photo of a patio.', 'a photo of a payphone.', 'a photo of a pedestal.', 'a photo of a pencil case.', 'a photo of a pencil sharpener.', 'a photo of a perfume.', 'a photo of a Petri dish.', 'a photo of a photocopier.', 'a photo of a plectrum.', 'a photo of a Pickelhaube.', 'a photo of a picket fence.', 'a photo of a pickup truck.', 'a photo of a pier.', 'a photo of a piggy bank.', 'a photo of a pill bottle.', 'a photo of a pillow.', 'a photo of a ping-pong ball.', 'a photo of a pinwheel.', 'a photo of a pirate ship.', 'a photo of a drink pitcher.', 'a photo of a block plane.', 'a photo of a planetarium.', 'a photo of a plastic bag.', 'a photo of a plate rack.', 'a photo of a farm plow.', 'a photo of a plunger.', 'a photo of a Polaroid camera.', 'a photo of a pole.', 'a photo of a police van.', 'a photo of a poncho.', 'a photo of a pool table.', 'a photo of a soda bottle.', 'a photo of a plant pot.', "a photo of a potter's wheel.", 'a photo of a power drill.', 'a photo of a prayer rug.', 'a photo of a printer.', 'a photo of a prison.', 'a photo of a missile.', 'a photo of a projector.', 'a photo of a hockey puck.', 'a photo of a punching bag.', 'a photo of a purse.', 'a photo of a quill.', 'a photo of a quilt.', 'a photo of a race car.', 'a photo of a racket.', 'a photo of a radiator.', 'a photo of a radio.', 'a photo of a radio telescope.', 'a photo of a rain barrel.', 'a photo of a recreational vehicle.', 'a photo of a fishing casting reel.', 'a photo of a reflex camera.', 'a photo of a refrigerator.', 'a photo of a remote control.', 'a photo of a restaurant.', 'a photo of a revolver.', 'a photo of a rifle.', 'a photo of a rocking chair.', 'a photo of a rotisserie.', 'a photo of a eraser.', 'a photo of a rugby ball.', 'a photo of a ruler measuring stick.', 'a photo of a sneaker.', 'a photo of a safe.', 'a photo of a safety pin.', 'a photo of a salt shaker.', 'a photo of a sandal.', 'a photo of a sarong.', 'a photo of a saxophone.', 'a photo of a scabbard.', 'a photo of a weighing scale.', 'a photo of a school bus.', 'a photo of a schooner.', 'a photo of a scoreboard.', 'a photo of a CRT monitor.', 'a photo of a screw.', 'a photo of a screwdriver.', 'a photo of a seat belt.', 'a photo of a sewing machine.', 'a photo of a shield.', 'a photo of a shoe store.', 'a photo of a shoji screen / room divider.', 'a photo of a shopping basket.', 'a photo of a shopping cart.', 'a photo of a shovel.', 'a photo of a shower cap.', 'a photo of a shower curtain.', 'a photo of a ski.', 'a photo of a balaclava ski mask.', 'a photo of a sleeping bag.', 'a photo of a slide rule.', 'a photo of a sliding door.', 'a photo of a slot machine.', 'a photo of a snorkel.', 'a photo of a snowmobile.', 'a photo of a snowplow.', 'a photo of a soap dispenser.', 'a photo of a soccer ball.', 'a photo of a sock.', 'a photo of a solar thermal collector.', 'a photo of a sombrero.', 'a photo of a soup bowl.', 'a photo of a keyboard space bar.', 'a photo of a space heater.', 'a photo of a space shuttle.', 'a photo of a spatula.', 'a photo of a motorboat.', 'a photo of a spider web.', 'a photo of a spindle.', 'a photo of a sports car.', 'a photo of a spotlight.', 'a photo of a stage.', 'a photo of a steam locomotive.', 'a photo of a through arch bridge.', 'a photo of a steel drum.', 'a photo of a stethoscope.', 'a photo of a scarf.', 'a photo of a stone wall.', 'a photo of a stopwatch.', 'a photo of a stove.', 'a photo of a strainer.', 'a photo of a tram.', 'a photo of a stretcher.', 'a photo of a couch.', 'a photo of a stupa.', 'a photo of a submarine.', 'a photo of a suit.', 'a photo of a sundial.', 'a photo of a sunglasses.', 'a photo of a sunglasses.', 'a photo of a sunscreen.', 'a photo of a suspension bridge.', 'a photo of a mop.', 'a photo of a sweatshirt.', 'a photo of a swim trunks / shorts.', 'a photo of a swing.', 'a photo of a electrical switch.', 'a photo of a syringe.', 'a photo of a table lamp.', 'a photo of a tank.', 'a photo of a tape player.', 'a photo of a teapot.', 'a photo of a teddy bear.', 'a photo of a television.', 'a photo of a tennis ball.', 'a photo of a thatched roof.', 'a photo of a front curtain.', 'a photo of a thimble.', 'a photo of a threshing machine.', 'a photo of a throne.', 'a photo of a tile roof.', 'a photo of a toaster.', 'a photo of a tobacco shop.', 'a photo of a toilet seat.', 'a photo of a torch.', 'a photo of a totem pole.', 'a photo of a tow truck.', 'a photo of a toy store.', 'a photo of a tractor.', 'a photo of a semi-trailer truck.', 'a photo of a tray.', 'a photo of a trench coat.', 'a photo of a tricycle.', 'a photo of a trimaran.', 'a photo of a tripod.', 'a photo of a triumphal arch.', 'a photo of a trolleybus.', 'a photo of a trombone.', 'a photo of a hot tub.', 'a photo of a turnstile.', 'a photo of a typewriter keyboard.', 'a photo of a umbrella.', 'a photo of a unicycle.', 'a photo of a upright piano.', 'a photo of a vacuum cleaner.', 'a photo of a vase.', 'a photo of a vaulted or arched ceiling.', 'a photo of a velvet fabric.', 'a photo of a vending machine.', 'a photo of a vestment.', 'a photo of a viaduct.', 'a photo of a violin.', 'a photo of a volleyball.', 'a photo of a waffle iron.', 'a photo of a wall clock.', 'a photo of a wallet.', 'a photo of a wardrobe.', 'a photo of a military aircraft.', 'a photo of a sink.', 'a photo of a washing machine.', 'a photo of a water bottle.', 'a photo of a water jug.', 'a photo of a water tower.', 'a photo of a whiskey jug.', 'a photo of a whistle.', 'a photo of a hair wig.', 'a photo of a window screen.', 'a photo of a window shade.', 'a photo of a Windsor tie.', 'a photo of a wine bottle.', 'a photo of a airplane wing.', 'a photo of a wok.', 'a photo of a wooden spoon.', 'a photo of a wool.', 'a photo of a split-rail fence.', 'a photo of a shipwreck.', 'a photo of a sailboat.', 'a photo of a yurt.', 'a photo of a website.', 'a photo of a comic book.', 'a photo of a crossword.', 'a photo of a traffic or street sign.', 'a photo of a traffic light.', 'a photo of a dust jacket.', 'a photo of a menu.', 'a photo of a plate.', 'a photo of a guacamole.', 'a photo of a consomme.', 'a photo of a hot pot.', 'a photo of a trifle.', 'a photo of a ice cream.', 'a photo of a popsicle.', 'a photo of a baguette.', 'a photo of a bagel.', 'a photo of a pretzel.', 'a photo of a cheeseburger.', 'a photo of a hot dog.', 'a photo of a mashed potatoes.', 'a photo of a cabbage.', 'a photo of a broccoli.', 'a photo of a cauliflower.', 'a photo of a zucchini.', 'a photo of a spaghetti squash.', 'a photo of a acorn squash.', 'a photo of a butternut squash.', 'a photo of a cucumber.', 'a photo of a artichoke.', 'a photo of a bell pepper.', 'a photo of a cardoon.', 'a photo of a mushroom.', 'a photo of a Granny Smith apple.', 'a photo of a strawberry.', 'a photo of a orange.', 'a photo of a lemon.', 'a photo of a fig.', 'a photo of a pineapple.', 'a photo of a banana.', 'a photo of a jackfruit.', 'a photo of a cherimoya (custard apple).', 'a photo of a pomegranate.', 'a photo of a hay.', 'a photo of a carbonara.', 'a photo of a chocolate syrup.', 'a photo of a dough.', 'a photo of a meatloaf.', 'a photo of a pizza.', 'a photo of a pot pie.', 'a photo of a burrito.', 'a photo of a red wine.', 'a photo of a espresso.', 'a photo of a tea cup.', 'a photo of a eggnog.', 'a photo of a mountain.', 'a photo of a bubble.', 'a photo of a cliff.', 'a photo of a coral reef.', 'a photo of a geyser.', 'a photo of a lakeshore.', 'a photo of a promontory.', 'a photo of a sandbar.', 'a photo of a beach.', 'a photo of a valley.', 'a photo of a volcano.', 'a photo of a baseball player.', 'a photo of a bridegroom.', 'a photo of a scuba diver.', 'a photo of a rapeseed.', 'a photo of a daisy.', "a photo of a yellow lady's slipper.", 'a photo of a corn.', 'a photo of a acorn.', 'a photo of a rose hip.', 'a photo of a horse chestnut seed.', 'a photo of a coral fungus.', 'a photo of a agaric.', 'a photo of a gyromitra.', 'a photo of a stinkhorn mushroom.', 'a photo of a earth star fungus.', 'a photo of a hen of the woods mushroom.', 'a photo of a bolete.', 'a photo of a corn cob.', 'a photo of a toilet paper.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_1108_4/base2new/train_base/imagenet/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-25" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:06<04:56,  6.06s/it]  4%|▍         | 2/50 [00:06<02:18,  2.89s/it]  6%|▌         | 3/50 [00:07<01:28,  1.88s/it]  8%|▊         | 4/50 [00:08<01:06,  1.44s/it] 10%|█         | 5/50 [00:08<00:52,  1.17s/it] 12%|█▏        | 6/50 [00:09<00:44,  1.00s/it] 14%|█▍        | 7/50 [00:10<00:38,  1.12it/s] 16%|█▌        | 8/50 [00:10<00:34,  1.22it/s] 18%|█▊        | 9/50 [00:11<00:31,  1.29it/s] 20%|██        | 10/50 [00:12<00:29,  1.33it/s] 22%|██▏       | 11/50 [00:12<00:28,  1.38it/s] 24%|██▍       | 12/50 [00:13<00:26,  1.41it/s] 26%|██▌       | 13/50 [00:14<00:25,  1.43it/s] 28%|██▊       | 14/50 [00:14<00:24,  1.45it/s] 30%|███       | 15/50 [00:15<00:24,  1.44it/s] 32%|███▏      | 16/50 [00:16<00:23,  1.45it/s] 34%|███▍      | 17/50 [00:16<00:22,  1.46it/s] 36%|███▌      | 18/50 [00:17<00:21,  1.47it/s] 38%|███▊      | 19/50 [00:18<00:21,  1.48it/s] 40%|████      | 20/50 [00:18<00:20,  1.48it/s] 42%|████▏     | 21/50 [00:19<00:19,  1.48it/s] 44%|████▍     | 22/50 [00:20<00:18,  1.48it/s] 46%|████▌     | 23/50 [00:21<00:18,  1.48it/s] 48%|████▊     | 24/50 [00:21<00:17,  1.49it/s] 50%|█████     | 25/50 [00:22<00:16,  1.48it/s] 52%|█████▏    | 26/50 [00:23<00:16,  1.48it/s] 54%|█████▍    | 27/50 [00:23<00:15,  1.48it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.48it/s] 58%|█████▊    | 29/50 [00:25<00:14,  1.49it/s] 60%|██████    | 30/50 [00:25<00:13,  1.49it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.49it/s] 64%|██████▍   | 32/50 [00:27<00:12,  1.49it/s] 66%|██████▌   | 33/50 [00:27<00:11,  1.49it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.48it/s] 70%|███████   | 35/50 [00:29<00:10,  1.48it/s] 72%|███████▏  | 36/50 [00:29<00:09,  1.48it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.48it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.48it/s] 78%|███████▊  | 39/50 [00:31<00:07,  1.48it/s] 80%|████████  | 40/50 [00:32<00:06,  1.48it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.48it/s] 84%|████████▍ | 42/50 [00:33<00:05,  1.48it/s] 86%|████████▌ | 43/50 [00:34<00:04,  1.48it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.48it/s] 90%|█████████ | 45/50 [00:35<00:03,  1.48it/s] 92%|█████████▏| 46/50 [00:36<00:02,  1.48it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.48it/s] 96%|█████████▌| 48/50 [00:37<00:01,  1.48it/s] 98%|█████████▊| 49/50 [00:38<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.48it/s]100%|██████████| 50/50 [00:39<00:00,  1.27it/s]
=> result
* total: 25,000
* correct: 17,472
* accuracy: 69.9%
* error: 30.1%
* macro_f1: 68.9%
